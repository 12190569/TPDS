"Memory-Efficient and Skew-Tolerant MapReduce Over MPI for Supercomputing Systems,""T. Gao"; Y. Guo; B. Zhang; P. Cicotti; Y. Lu; P. Balaji;" M. Taufer"",""Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA"; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA; NVIDIA, San Diego, USA; National Supercomputing Center in Guangzhou, China; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, USA;" Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxville, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jun 2020"",""2020"",""31"",""12"",""2734"",""2748"",""Data analytics has become an integral part of large-scale scientific computing. Among various data analytics frameworks, MapReduce has gained the most traction. Although some efforts have been made to enable efficient MapReduce for supercomputing systems, they are often limited to fairly homogeneous workloads where equal partitioning of input data across tasks results in essentially equal output or temporary data generated on each task. For workloads that are more skewed, however, current implementations can result in imbalance in memory usage and, consequently, can cause a slowdown in execution time and a loss in data scalability. To tackle this problem, we enhance a previously published memory-conscious MapReduce over MPI framework called Mimir. Our enhancements to Mimir include combiner and dynamic repartition optimizations to minimize and balance memory usage and to achieve close to optimal balance of the memory usage across processes and to reduce the execution time by up to 12 times. Experimental results show that Mimir can scale to at least 3072 processes on the Tianhe-2 supercomputer on skewed datasets."",""1558-2183"","""",""10.1109/TPDS.2019.2932066"",""U.S. Department of Energy(grant numbers:DE-AC02-06CH11357)"; National Science Foundation(grant numbers:#1318445,#1318417,#1841758); National Key R&D Project in China(grant numbers:2016YFB1000302); National Natural Science Foundation of China(grant numbers:U1611261,NSFC61402503); Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); China Scholarship Council; National Science Foundation(grant numbers:ACI-1053575);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103270"",""Skew mitigation";load balancing;high-performance computing;data analytics;MapReduce;memory efficiency;"performance and scalability"",""Supercomputers";Data models;Optimization;Programming;Operating systems;Aggregates;"Data analysis"","""",""4"","""",""38"",""CCBY"",""28 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"A Black-Box Fork-Join Latency Prediction Model for Data-Intensive Applications,""M. Nguyen"; S. Alesawi; N. Li; H. Che;" H. Jiang"",""Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, USA"; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, USA; Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, USA;" Department of Computer Science and Engineering, The University of Texas at Arlington, Arlington, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Apr 2020"",""2020"",""31"",""9"",""1983"",""2000"",""The workflows of the predominant datacenter services are underlaid by various Fork-Join structures. Due to the lack of good understanding of the performance of Fork-Join structures in general, today's datacenters often operate under low resource utilization to meet stringent service level objectives (SLOs), e.g., in terms of tail and/or mean latency, for such services. Hence, to achieve high resource utilization, while meeting stringent SLOs, it is of paramount importance to be able to accurately predict the tail and/or mean latency for a broad range of Fork-Join structures of practical interests. In this article, we propose a black-box Fork-Join model that covers a wide range of Fork-Join structures for the prediction of tail and mean latency, called ForkTail and ForkMean, respectively. We derive highly computational effective, empirical expressions for tail and mean latency as functions of means and variances of task response times. Our extensive testing results based on model-based and trace-driven simulations, as well as a real-world case study in a cloud environment demonstrate that the models can consistently predict the tail and mean latency within 20 and 15 percent prediction errors at 80 and 90 percent load levels, respectively, for heavy-tailed workloads, and at any load levels for light-tailed workloads. Moreover, our sensitivity analysis demonstrates that such errors can be well compensated for with no more than 7 percent resource overprovisioning. Consequently, the proposed prediction model can be used as a powerful tool to aid the design of tail-and-mean-latency guaranteed job scheduling and resource provisioning, especially at high load, for datacenter applications."",""1558-2183"","""",""10.1109/TPDS.2020.2982137"",""NSF(grant numbers:CCF XPS 1629625,CCF 1704504)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9043685"",""Tail latency";mean response time;Fork Join queuing networks;"datacenter resource provisioning"",""Task analysis";Load modeling;Time factors;Servers;Predictive models;Computational modeling;"Resource management"","""",""9"","""",""52"",""IEEE"",""20 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"A Comment on Privacy-Preserving Scalar Product Protocols as Proposed in “SPOC”,""T. Schneider";" A. Treiber"",""Cryptography and Privacy Engineering Group (ENCRYPTO), TU Darmstadt, Darmstadt, Germany";" Cryptography and Privacy Engineering Group (ENCRYPTO), TU Darmstadt, Darmstadt, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""543"",""546"",""Privacy-preserving scalar product (PPSP) protocols are an important building block for secure computation tasks in various applications. Lu et al. (TPDS'13) introduced a PPSP protocol that does not rely on cryptographic assumptions and that is used in a wide range of publications to date. In this comment paper, we show that Lu et al.'s protocol is insecure and should not be used. We describe specific attacks against it and, using impossibility results of Impagliazzo and Rudich (STOC'89), show that it is inherently insecure and cannot be fixed without relying on at least some cryptographic assumptions."",""1558-2183"","""",""10.1109/TPDS.2019.2939313"",""DFG"; BMBF; HMWK;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823041"",""Privacy-preserving scalar product protocols";secure computation;"oblivious transfer"",""Protocols";Privacy;Public key cryptography;Task analysis;"Encryption"","""",""7"","""",""27"",""IEEE"",""3 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Dynamic Multi–Objective Approach for Dynamic Load Balancing in Heterogeneous Systems,""A. Cabrera"; A. Acosta; F. Almeida;" V. Blanco"",""HPC Group of Universidad de La Laguna, Escuela Superior de Ingeniería y Tecnología, San Cristóbal de La Laguna, Spain"; HPC Group of Universidad de La Laguna, Escuela Superior de Ingeniería y Tecnología, San Cristóbal de La Laguna, Spain; HPC Group of Universidad de La Laguna, Escuela Superior de Ingeniería y Tecnología, San Cristóbal de La Laguna, Spain;" HPC Group of Universidad de La Laguna, Escuela Superior de Ingeniería y Tecnología, San Cristóbal de La Laguna, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""15 May 2020"",""2020"",""31"",""10"",""2421"",""2434"",""Modern standards in High Performance Computing (HPC) have started to consider energy consumption and power draw as a limiting factor. New and more complex architectures have been introduced in HPC systems to afford these new restrictions, and include coprocessors such as GPGPUs for intensive computational tasks. As systems increase in heterogeneity, workload distribution becomes a more core problem to achieve the maximum efficiency in every computational component. We present a Multi-Objective Dynamic Load Balancing (DLB) approach where several objectives can be applied to tune an application. These objectives can be dynamically exchanged during the execution of an algorithm to better adapt to the resources available in a system. We have implemented the Multi-Objective DLB together with a generic heuristic engine, designed to perform multiple strategies for DLB in iterative problems. We also present Ull Multiobjective Framework (UllMF), an open-source tool that implements the Multi-Objective generic approach. UllMF separates metric gathering, objective functions to be optimized and load balancing algorithms, and improves code portability using a simple interface to reduce the costs of new implementations. We illustrate how performance and energy consumption are improved for the implemented techniques, and analyze their quality using different DLB techniques from the literature."",""1558-2183"","""",""10.1109/TPDS.2020.2989869"",""Spanish Ministry of Science, Innovation and Universities(grant numbers:TIN2016-78919-R)"; Government of the Canary Islands(grant numbers:ProID2017010130,TESIS2017010134);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076818"",""Dynamic load balancing";energy efficiency;iterative algorithms;"heterogeneous computing"",""Load management";Heuristic algorithms;Measurement;Linear programming;Energy consumption;Task analysis;"Optimization"","""",""6"","""",""35"",""IEEE"",""23 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"A Game-Theoretical Approach for User Allocation in Edge Computing Environment,""Q. He"; G. Cui; X. Zhang; F. Chen; S. Deng; H. Jin; Y. Li;" Y. Yang"",""School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia"; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; University of Auckland, Auckland, New Zealand; School of Information Technology, Deakin University, Geelong, Australia; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technolgoy, HuaZhong University of Science and Technology, Wuhan Shi, China; State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing Shi, China;" School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""515"",""529"",""Edge Computing provides mobile and Internet-of-Things (IoT) app vendors with a new distributed computing paradigm which allows an app vendor to deploy its app at hired edge servers distributed near app users at the edge of the cloud. This way, app users can be allocated to hired edge servers nearby to minimize network latency and energy consumption. A cost-effective edge user allocation (EUA) requires maximum app users to be served with minimum overall system cost. Finding a centralized optimal solution to this EUA problem is NP-hard. Thus, we propose EUAGame, a game-theoretic approach that formulates the EUA problem as a potential game. We analyze the game and show that it admits a Nash equilibrium. Then, we design a novel decentralized algorithm for finding a Nash equilibrium in the game as a solution to the EUA problem. The performance of this algorithm is theoretically analyzed and experimentally evaluated. The results show that the EUA problem can be solved effectively and efficiently."",""1558-2183"","""",""10.1109/TPDS.2019.2938944"",""Australian Research Council(grant numbers:DP170101932,DP180100212)"; National Natural Science Foundation of China(grant numbers:61772461); Natural Science Foundation of Zhejiang Province(grant numbers:LR18F020003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823046"",""Edge user allocation";edge server;cost-effectiveness;pay-as-you-go;game theory;Nash equilibrium;multi-tenancy;"edge computing"",""Servers";Games;Edge computing;Resource management;Nash equilibrium;Cloud computing;"Bandwidth"","""",""217"","""",""43"",""IEEE"",""3 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A General Design for a Scalable MPI-GPU Multi-Resolution 2D Numerical Solver,""M. Turchetto"; A. D. Palù;" R. Vacondio"",""Engineering and Architecture Department, University of Parma, Parma, Italy"; Mathematics Physics Computer Science Department, University of Parma, Parma, Italy;" Engineering and Architecture Department, University of Parma, Parma, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1036"",""1047"",""This article presents a multi-GPU implementation of a Finite-Volume solver on a multi-resolution grid. The implementation completely offloads the computation to the GPUs and communications between different GPUs are implemented by means of the Message Passing Interface (MPI) API. Different domain decomposition techniques have been considered and the one based on the Hilbert Space Filling Curves (HSFC) showed optimal scalability. Several optimizations are introduced: One-to-one MPI communications among MPI ranks are completely masked by GPU computations on internal cells and a novel dynamic load balancing algorithm is introduced to minimize the waiting times at global MPI synchronization barriers. Such algorithm adapts the computational load of ranks in response to dynamical changes in the execution time of blocks and in network performances";" Its capability to converge to a balanced computation has been empirically shown by numerical experiments. Tests exploit up to 64 GPUs and 83M cells and achieve an efficiency of 90 percent in weak scalability and 85 percent for strong scalability. The framework is general and the results of the article can be ported to a wide range of explicit 2D Partial Differential Equations solvers."",""1558-2183"","""",""10.1109/TPDS.2019.2961909"",""the Italian MIUR(grant numbers:RBSI14R1GP,D92I15000190001)"; Italian INdAM – GNCS(grant numbers:2019);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941298"",""CUDA";multi-GPU;MPI;dynamic load balancing;hilbert space filling curves;multi-resolution grid;shallow water equations (SWE);"AMR"",""Graphics processing units";Mathematical model;Computational modeling;Load management;Numerical models;Load modeling;"Two dimensional displays"","""",""8"","""",""25"",""IEEE"",""24 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"A High Throughput B+tree for SIMD Architectures,""W. Zhang"; Z. Yan; Y. Lin; C. Zhao;" L. Peng"",""Shanghai Key Laboratory of Data Science, Institute for Big Data, and Shanghai Institute of Intelligent Electrontics & System, Software School, Fudan University, Shanghai, China"; Shanghai Key Laboratory of Data Science, Institute for Big Data, and Shanghai Institute of Intelligent Electrontics & System, Software School, Fudan University, Shanghai, China; Shanghai Key Laboratory of Data Science, Institute for Big Data, and Shanghai Institute of Intelligent Electrontics & System, Software School, Fudan University, Shanghai, China; Shanghai Key Laboratory of Data Science, Institute for Big Data, and Shanghai Institute of Intelligent Electrontics & System, Software School, Fudan University, Shanghai, China;" Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""707"",""720"",""B+tree is one of the most important data structures and has been widely used in different fields. With the increase of concurrent queries and data-scale in storage, designing an efficient B+tree structure has become critical. Due to abundant computation resources, SIMD architectures provide potential opportunities to achieve high query throughput for B+tree. However, prior methods cannot achieve satisfactory performance results due to low resource utilization and poor memory performance. In this paper, we first identify the gaps between B+tree and SIMD architectures. Concurrent B+tree queries involve many global memory accesses and different divergences, which mismatch with SIMD architecture features. Based on this observation, we propose Harmonia, a novel B+tree structure to bridge the gaps. In Harmonia, a B+tree structure is divided into a key region and a child region. The key region stores the nodeswith its keys in a breadth-first order. The child region is organized as a prefix-sum array, which only stores each node's first child index in the key region. Since the prefix-sum child region is small and the children's index can be retrieved through index computations, most of it can be stored in on-chip caches, which can achieve good cache locality. To make it more efficient, Harmonia also includes two optimizations: partially-sorted aggregation and narrowed thread-group traversal, which can mitigate memory and execution divergence and improve resource utilization. Evaluations on a 28-core INTEL CPU show that Harmonia can achieve up to 207 million queries per second, which is about 1.7X faster than that of CPU-based HB+Tree, a recent state-of-the-art solution. And on a Volta TITAN V GPU, it can achieve up to 3.6 billion queries per second, which is about 3.4X faster than that of GPU-based HB+Tree."",""1558-2183"","""",""10.1109/TPDS.2019.2942918"",""National Natural Science Foundation of China(grant numbers:61672160)"; Shanghai Municipal Science and Technology Major Project(grant numbers:2018SHZDZX01); Shanghai Technology Development and Entrepreneurship Platform for Neuromorphic and AI SoC;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846100"",""SIMD";B+tree;"high-throughput"",""Indexes";Throughput;Resource management;Vegetation;Memory management;"Data structures"","""",""6"","""",""52"",""IEEE"",""23 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"A Highly Reliable Metadata Service for Large-Scale Distributed File Systems,""J. Zhou"; Y. Chen; W. Wang; S. He;" D. Meng"",""Chinese Academy of Sciences, Beijing, China"; Department of Computer Science, Texas Tech University, Lubbock, USA; Chinese Academy of Sciences, Beijing, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China;" Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""374"",""392"",""Many massive data processing applications nowadays often need long, continuous, and uninterrupted data accesses. Distributed file systems are used as the back-end storage to provide the global namespace management and reliability guarantee. Due to increasing hardware failures and software issues with the growing system scale, metadata service reliability has become a critical issue as it has a direct impact on file and directory operations. Existing metadata management mechanisms can provide fault tolerance capability to some level but are inadequate. They often have limitations in system availability, state consistence, and performance overhead and lack an effective mechanism to offer metadata reliability. This paper introduces a novel highly reliable metadata service to address these issues in large-scale file systems. Different from traditional strategies, this proposed reliable metadata service adopts a new active-standby architecture for fault tolerance and uses a holistic approach to improve file system availability. A new shared storage pool (SSP) is designed for transparent metadata synchronization and replication between active and standby servers. Based on the SSP, a new policy called multiple actives multiple standbys (MAMS) is presented to perform metadata service recovery in case of failures. A new global state recovery strategy and a smart client fault tolerance mechanism are achieved to maintain the continuity of metadata service. We have implemented such highly reliable metadata service in a prototype file system CFS (Clover file system) and conducted extensive tests to evaluate it. Experimental results confirm that it can significantly improve file system reliability with fast failover under different failure scenarios while having negligible influence on performance. Compared with typical reliability designs in Hadoop Avatar, Hadoop HA, and Boom-FS file systems, the mean-time-to-recovery (MTTR) with the highly reliable metadata service was reduced by 80.23, 65.46 and 28.13 percent, respectively."",""1558-2183"","""",""10.1109/TPDS.2019.2937492"",""Beijing Municipal Science and Technology Commission(grant numbers:Z191100007119002)"; National Science Foundation(grant numbers:CNS-1338078,CNS-1362134,CCF-1409946,CCF-1718336,OAC-1835892,CNS-1817094); National Natural Science Foundation of China(grant numbers:61572377); Natural Science Foundation of Hubei Province(grant numbers:2017CFC889); Fundamental Research Funds for the Central Universities(grant numbers:2018QNA5015);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812918"",""Distributed file systems";metadata service;metadata reliability;fault tolerance;"shared metadata storage"",""Metadata";Servers;Protocols;Synchronization;Fault tolerance;"Fault tolerant systems"","""",""9"","""",""61"",""IEEE"",""26 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"A Holistic Heterogeneity-Aware Data Placement Scheme for Hybrid Parallel I/O Systems,""S. He"; Z. Li; J. Zhou; Y. Yin; X. Xu; Y. Chen;" X. -H. Sun"",""College of Computer Science and Technology, Zhejiang University, Hangzhou, China"; Computer Science Program, School of Business, Stockton University, Galloway, USA; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Zhejiang Lab, Intelligent Computing System Research Center, Institute of Artificial Intelligence, Hangzhou, China; Department of Computer Science, Kennesaw State University, Kennesaw, USA; Department of Computer Science, Texas Tech University, Lubbock, USA;" Department of Computer Science, Illinois Institute of Technology, Chicago, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""830"",""842"",""We present H2DP, a holistic heterogeneity-aware data placement scheme for hybrid parallel I/O systems, which consist of HDD servers and SSD servers. Most of the existing approaches focus on server performance or application I/O pattern heterogeneity in data placement. H2DP considers three axes of heterogeneity: server performance, server space, and application I/O pattern. More specifically, H2DP determines the optimized stripe sizes on servers based on server performance, keeps only critical data on all hybrid servers and the rest data on HDD servers, and dynamically migrates data among different types of servers at run-time. This holistic heterogeneity-awareness enables H2DP to achieve high performance by alleviating server load imbalance, efficiently utilizing SSD space, and accommodating application pattern variation. We have implemented a prototype of H2DP under MPICH2 atop OrangeFS. Extensive experimental results demonstrate that H2DP significantly improve I/O system performance compared to existing data placement schemes."",""1558-2183"","""",""10.1109/TPDS.2019.2948901"",""National Natural Science Foundation of China(grant numbers:61572377)"; Natural Science Foundation of Hubei Province(grant numbers:2017CFC889); Fundamental Research Funds for the Central Universities(grant numbers:2018QNA5015); Zhejiang Lab Research Project(grant numbers:2019KC0AC01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880508"",""Parallel I/O system";parallel file system;hybrid parallel file system;data placement;"solid state drive"",""Servers";System performance;Bandwidth;Computer science;Distributed databases;Sun;"File systems"","""",""7"","""",""50"",""IEEE"",""24 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"A Hybrid Update Strategy for I/O-Efficient Out-of-Core Graph Processing,""X. Xu"; F. Wang; H. Jiang; Y. Cheng; D. Feng;" Y. Zhang"",""Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, USA; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Mar 2020"",""2020"",""31"",""8"",""1767"",""1782"",""In recent years, a number of out-of-core graph processing systems have been proposed to process graphs with billions of edges on just one commodity computer, due to their high cost efficiency. To obtain a better performance, these systems adopt a full I/O model that scans all edges during the computation to avoid the inefficiency of random I/Os. Although this model ensures good I/O access locality, it leads to a large number of useless edges to be loaded when running graph algorithms that only access a small portion of edges in each iteration. An intuitive method to solve this I/O inefficiency problem is the on-demand I/O model that only accesses the active edges. However, this method only works well for the graph algorithms with very few active edges, since the I/O cost will grow rapidly as the number of active edges increases due to the increasing amount of random I/Os. In this article, we present HUS-Graph, an efficient out-of-core graph processing system to address the above I/O issues and achieve a good balance between I/O traffic and I/O access locality. HUS-Graph adopts a hybrid update strategy including two update models, Row-oriented Push (ROP) and Column-oriented Pull (COP). It supports switching between ROP and COP adaptively, for the graph algorithms that have different computation and I/O features. For traversal-based algorithms, HUS-Graph also provides an immediate propagation-based vertex update scheme to accelerate the vertex state propagation and convergence speed. Furthermore, HUS-Graph adopts a locality-optimized dual-block representation to organize graph data and an I/O-based performance prediction method to enable the system to dynamically select the optimal update model between ROP and COP. To save the disk space and further reduce I/O traffic, HUS-Graph implements a space-efficient storage format by combining several graph compression methods. Extensive experimental results show that HUS-Graph outperforms two existing out-of-core systems GraphChi and GridGraph by 1.2x-52.8x."",""1558-2183"","""",""10.1109/TPDS.2020.2973143"",""National Defense Preliminary Research Project(grant numbers:31511010202)"; National Natural Science Foundation of China(grant numbers:61832020,61772216,61821003,U1705261); Wuhan application basic research(grant numbers:2017010201010103); Hubei province technical innovation special Project(grant numbers:2017AAA129); Fundamental Research Funds for the Central Universities; Wuhan National Laboratory for Optoelectronics(grant numbers:2018WNLOKF006);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8994089"",""Graph processing";out-of-core;I/O;"hybrid update strategy"",""Adaptation models";Load modeling;Data models;Computational modeling;Switches;Computer science;"Loading"","""",""6"","""",""42"",""IEEE"",""11 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"A Novel Low Cost Interconnection Architecture Based on the Generalized Hypercube,""G. Wang"; C. -K. Lin; J. Fan; B. Cheng;" X. Jia"",""School of Computer Science and Technology, Soochow University, Suzhou, China"; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China; School of Computer Science and Technology, Soochow University, Suzhou, China;" Department of Computer Science, City University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""647"",""662"",""The generalized hypercube (GH) is one key interconnection network with excellent topological properties. It contains many other interconnection topologies, such as the hypercube network, the complete graph, the mesh network, and the k-ary n-cube network. It can also be used to construct some data center networks, such as HyperX, BCube, FBFLY, and SWCube. However, the construction cost of GH is high since it contains too many links. In this paper, we propose a novel low cost interconnection architecture called the exchanged generalized hypercube (EGH). We study the properties of EGH, such as the number of edges, the degree of vertices, connectivity, diameter, and diagnosability. Then, we give a routing algorithm to find the shortest path between any two distinct vertices of EGH. Furthermore, we design an algorithm to give disjoint paths between any two distinct vertices of EGH. In addition, we propose two local diagnosis algorithms: LDTEGH and LDWBEGH in EGH under PMC model and MM model, respectively. Simulation results demonstrate that even if the proportion of faulty vertices in EGH is up to 25 percent, the probability that these two diagnosis algorithms can successfully determine the status of vertices is more than 90 percent. As far as the number of edges is concerned, the analysis shows that the construction cost of EGH is much less than that of GH. We could regard this work as the basis for proposing future new high performance topologies."",""1558-2183"","""",""10.1109/TPDS.2019.2941207"",""National Natural Science Foundation of China(grant numbers:61572337,61872257)"; Natural Science Research of Jiangsu Higher Education Institutions of China(grant numbers:18KJA520009); Application Foundation Research of Suzhou of China(grant numbers:SYG201653); Priority Academic Program Development of Jiangsu Higher Education Institutions;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8836112"",""Exchanged generalized hypercube";low cost;local diagnosis;disjoint path;"routing"",""Hypercubes";Routing;Network topology;Topology;Program processors;"Fault tolerance"","""",""19"","""",""43"",""IEEE"",""13 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"A Novel Multi-Stage Forest-Based Key-Value Store for Holistic Performance Improvement,""Z. Lu"; Q. Cao; F. Mei; H. Jiang;" J. Li"",""Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education, Wuhan National Laboratory for Optoelectronics, and the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education, Wuhan National Laboratory for Optoelectronics, Wuhan, China; Huawei Technologies, Hangzhou; University of Texas at Arlington, Arlington, USA;" Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""856"",""870"",""Key-value (KV) stores based on multi-stage structures are widely deployed to organize massive amounts of easily searchable user data. However, current KV storage systems inevitably sacrifice at least one of the performance objectives, such as write, read, space efficiency etc., for the optimization of others. To understand the root cause of and ultimately remove such performance disparities among the representative existing KV stores, we analyze their enabling mechanisms and classify them into two fundamental models of data structures facilitating KV operations, namely, the multi-stage tree (MS-tree), and the multi-stage forest (MS-forest). We build SifrDB, a KV store on a novel split forest structure, that achieves the lowest write amplification across all workload patterns and minimizes space reservation for the compaction. To mitigate the read amplification inherent in MS-forest, we introduce a bloom filer mechanism based on Sorted String Tables (SSTs). Furthermore, we also present a highly efficient parallel search approach that fully exploits the access parallelism of modern flash-based storage devices to substantially boost the read performance. Evaluation results show that under both micro and YCSB benchmarks, SifrDB outperforms its closest competitors, i.e., the popular MS-forest implementations, making it a highly desirable choice for the modern KV stores."",""1558-2183"","""",""10.1109/TPDS.2019.2950248"",""National Natural Science Foundation of China(grant numbers:61872156)"; National Natural Science Foundation of China(grant numbers:61821003); National Basic Research Program of China (973 Program)(grant numbers:2018YFA0701804); Fundamental Research Funds for the Central Universities(grant numbers:2018KFYXKJC037); National Science Foundation(grant numbers:CCF-1704504,CCF-1629625); Alibaba Group through Alibaba Innovative Research (AIR) Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8886435"",""Key-value";multi-stage;LSM-tree;"parallel search"",""Vegetation";Compaction;Forestry;Indexes;Optimization;Data models;"Silicon"","""","""","""",""47"",""IEEE"",""30 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"A Survey of Phase Classification Techniques for Characterizing Variable Application Behavior,""K. Criswell";" T. Adegbija"",""Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA";" Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""224"",""236"",""Adaptable computing is an increasingly important paradigm that specializes system resources to variable application requirements, environmental conditions, or user requirements. Adapting computing resources to variable application requirements (or application phases) is otherwise known as phase-based optimization. Phase-based optimization takes advantage of application phases, or execution intervals of an application that behave similarly, to enable effective and beneficial adaptability. In order for phase-based optimization to be effective, the phases must first be classified to determine when application phases begin and end, and ensure that system resources are accurately specialized. In this paper, we present a survey of phase classification techniques that have been proposed to exploit the advantages of adaptable computing through phase-based optimization. We focus on recent techniques and classify these techniques with respect to several factors in orderto highlight their similarities and differences. We divide the techniques by their major defining characteristics-online/offline and serial/parallel. In addition, we discuss other characteristics such as prediction and detection techniques, the characteristics used for prediction, interval type, etc. We also identify gaps in the state-of-the-art and discuss future research directions to enable and fully exploit the benefits of adaptable computing."",""1558-2183"","""",""10.1109/TPDS.2019.2929781"",""NSF(grant numbers:1844952)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8769866"",""Phase classification";adaptable computing;workload characterization;variable program behavior;dynamic optimization;edge computing;multithreaded applications;big data;"emerging applications"",""Optimization";Hardware;Computational modeling;Multicore processing;Big Data;Clocks;"Runtime"","""",""4"","""",""108"",""IEEE"",""23 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"A Truthful and Efficient Incentive Mechanism for Demand Response in Green Datacenters,""Z. Zhou"; F. Liu; S. Chen;" Z. Li"",""Guangdong Key Laboratory of Big Data Analysis and Processing, School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China"; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" School of Computer Science, Wuhan University, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Dec 2019"",""2020"",""31"",""1"",""1"",""15"",""Datacenter demand response is envisioned as a promising tool for mitigating operational stability issues faced by smart grids. It enables significant potentials in peak load reduction and facilitates the incorporation of distributed generation. Monetary refund from the smart grid can also alleviate the cloud's burden in escalating electricity cost. However, the current demand response paradigm is inefficient towards incentivizing a cloud service provider (CSP) that operates geo-distributed datacenters. To incentivize CSP participation, this work presents an auction mechanism that enables smart grids to voluntarily submit bids to the CSP to procure diverse amounts of demand response with different payments. To maximize the social welfare of the auction, the CSP that acts as the auctioneer needs to solve the winner determination problem at large-scale. By applying the proximal Jacobian alternating direction method of multipliers, we propose a distributed algorithm for each datacenter to solve a small-scale problem in a parallel fashion. Desirable properties of the proposed auction, such as social welfare maximization and truthfulness are achieved through Vickrey-Clarke-Groves (VCG) payment. Through extensive evaluations based on real datacenter workload traces and IEEE 14-bus test systems, we demonstrate that our incentive mechanism constitutes a win-win mechanism for both the geo-distributed cloud and the smart grid."",""1558-2183"","""",""10.1109/TPDS.2018.2882174"",""National Key Research & Development (R&D)(grant numbers:2017YFB1001703)"; National Natural Science Foundation of China(grant numbers:61722206,61802449,61761136014 (NSFC-DFG),61520106005); Fundamental Research Funds for the Central Universities(grant numbers:2017KFKJXX009,2017LGJC40);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540414"",""Geo-distributed datacenters";smart grid;demand response;incentive mechanism;"distributed algorithm"",""Load management";Smart grids;Power demand;Cloud computing;Economics;Electric potential;"Distributed power generation"","""",""30"","""",""42"",""IEEE"",""18 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"A Ubiquitous Machine Learning Accelerator With Automatic Parallelization on FPGA,""C. Wang"; L. Gong; X. Li;" X. Zhou"",""School of Computer Science, University of Science and Technology of China, Hefei, China"; School of Computer Science, University of Science and Technology of China, Hefei, China; School of Computer Science, University of Science and Technology of China, Hefei, China;" School of Computer Science, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 May 2020"",""2020"",""31"",""10"",""2346"",""2359"",""Machine learning has been widely applied in various emerging data-intensive applications, and has to be optimized and accelerated by powerful engines to process very large scale data. Recently, the instruction set based accelerators on Field Progarmmable Gate Arrays (FPGAs) have been a promising topic for machine learning applications. The customized instructions can be further scheduled to achieve higher instruction-level parallelism. In this article, we design a ubiquitous accelerator with out-of-order automatic parallelization for large-scale data-intensive applications. The accelerator accommodates four representative applications, including clustering algorithms, deep neural networks, genome sequencing, and collaborative filtering. In order to improve the coarse-grained instruction-level parallelism, the accelerator employs an out-of-order scheduling method to enable parallel dataflow computation. We use Colored Petri Net (CPN) tools to analyze the dependences in the applications, and build a hardware prototype on the real FPGA platform. For cluster applications, the accelerator can support four different algorithms, including K-Means, SLINK, PAM, and DBSCAN. For collaborative filtering applications, it accommodates Tanimoto, euclidean, Cosine, and Pearson Correlation as Similarity metrics. For deep learning applications, we implement hardware accelerators for both training process and inference process. Finally, for genome sequencing, we design a hardware accelerator for the BWA-SW algorithm. Experimental results show that the accelerator architecture can reach up to 25X speedup against Intel processors with affordable hardware cost, insignificant power consumption, and high flexibility."",""1558-2183"","""",""10.1109/TPDS.2020.2990924"",""National Basic Research Program of China (973 Program)(grant numbers:2017YFA0700900)"; National Natural Science Foundation of China(grant numbers:61976200); Natural Science Foundation of Jiangsu Province(grant numbers:BK20181193); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2017497); Fundamental Research Funds for the Central Universities(grant numbers:WK2150110003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079640"",""Machine learning";accelerator;FPGA;out-of-order execution;"automatic parallelization"",""Clustering algorithms";Hardware;Machine learning algorithms;Machine learning;Out of order;Partitioning algorithms;"Parallel processing"","""",""24"","""",""53"",""IEEE"",""27 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"A Value-Oriented Job Scheduling Approach for Power-Constrained and Oversubscribed HPC Systems,""N. Kumbhare"; A. Marathe; A. Akoglu; H. J. Siegel; G. Abdulla;" S. Hariri"",""Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA"; Lawrence Livermore National Laboratory, Livermore, USA; Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA; Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, USA; Lawrence Livermore National Laboratory, Livermore, USA;" Department of Electrical and Computer Engineering, University of Arizona, Tucson, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Feb 2020"",""2020"",""31"",""6"",""1419"",""1433"",""In this article, we investigate limitations in the traditional value-based algorithms for a power-constrained HPC system and evaluate their impact on HPC productivity. We expose the trade-off between allocating system-wide power budget uniformly and greedily under different system-wide power constraints in an oversubscribed system. We experimentally demonstrate that, under the tightest power constraint, the mean productivity of the greedy allocation is 38 percent higher than the uniform allocation whereas, under the intermediate power constraint, the uniform allocation has a mean productivity of 6 percent higher than the greedy allocation. We then propose a new algorithm that adapts its behavior to deliver the combined benefits of the two allocation strategies. We design a methodology with online retraining capability to create application-specific power-execution time models for a class of HPC applications. These models are used in predicting the execution time of an application on the available resources at the time of making scheduling decisions in the power-aware algorithms. We evaluate the proposed algorithm using emulation and simulation environments, and show that our adaptive strategy results in improving HPC resource utilization while delivering a mean productivity that is almost the same as the best performing algorithm across various system-wide power constraints."",""1558-2183"","""",""10.1109/TPDS.2020.2967373"",""National Science Foundation(grant numbers:NSF CNS-1624668,CCF-1302693)"; Lawrence Livermore National Laboratory(grant numbers:DE-AC52-07NA27344 (LLNL-JRNL-775437));" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8961147"",""High performance computing";power-constrained computing;power-aware scheduling;value heuristics;"HPC productivity"",""Productivity";Resource management;Mathematical model;Measurement;Power demand;Adaptation models;"Time factors"","""",""8"","""",""47"",""IEEE"",""16 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Abstraction Layer For Standardizing APIs of Task-Based Engines,""R. Alomairy"; H. Ltaief; M. Abduljabbar;" D. Keyes"",""Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia;" Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering (CEMSE) Division, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""22 May 2020"",""2020"",""31"",""11"",""2482"",""2495"",""We introduce AL4SAN, a lightweight library for abstracting the APIs of task-based runtime engines. AL4SAN unifies the expression of tasks and their data dependencies. It supports various dynamic runtime systems relying on compiler technology and user-defined APIs. It enables a single application to employ different runtimes and their respective scheduling components, while providing user-obliviousness to the underlying hardware configurations. AL4SAN exposes common front-end APIs and connects to different back-end runtimes. Experiments on performance and overhead assessments are reported on various shared- and distributed-memory systems, possibly equipped with hardware accelerators. A range of workloads, from compute-bound to memory-bound regimes, are employed as proxies for current scientific applications. The low overhead (less than 10 percent) achieved using a variety of workloads enables AL4SAN to be deployed for fast development of task-based numerical algorithms. More interestingly, AL4SAN enables runtime interoperability by switching runtimes at runtime. Blending runtime systems permits to achieve a twofold speedup on a task-based generalized symmetric eigenvalue solver, relative to state-of-the-art implementations. The ultimate goal of AL4SAN is not to create a new runtime, but to strengthen co-design of existing runtimes/applications, while facilitating user productivity and code portability. The code of AL4SAN is freely available at https://github.com/ecrc/al4san, with extensions in progress."",""1558-2183"","""",""10.1109/TPDS.2020.2992923"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9089317"",""Task-based programming model";dynamic runtime systems;abstraction layer;API standardization;user productivity;LLVM compiler infrastructure;"runtime interoperability"",""Runtime";Task analysis;Hardware;Engines;Libraries;Programming;"Productivity"","""",""2"","""",""53"",""IEEE"",""7 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Accelerating Federated Learning via Momentum Gradient Descent,""W. Liu"; L. Chen; Y. Chen;" W. Zhang"",""Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China"; Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China; School of Engineering, University of Warwick, Coventry, United Kingdom;" Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Mar 2020"",""2020"",""31"",""8"",""1754"",""1766"",""Federated learning (FL) provides a communication-efficient approach to solve machine learning problems concerning distributed data, without sending raw data to a central server. However, existing works on FL only utilize first-order gradient descent (GD) and do not consider the preceding iterations to gradient update which can potentially accelerate convergence. In this article, we consider momentum term which relates to the last iteration. The proposed momentum federated learning (MFL) uses momentum gradient descent (MGD) in the local update step of FL system. We establish global convergence properties of MFL and derive an upper bound on MFL convergence rate. Comparing the upper bounds on MFL and FL convergence rates, we provide conditions in which MFL accelerates the convergence. For different machine learning models, the convergence performance of MFL is evaluated based on experiments with MNIST and CIFAR-10 datasets. Simulation results confirm that MFL is globally convergent and further reveal significant convergence improvement over FL."",""1558-2183"","""",""10.1109/TPDS.2020.2975189"",""National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFA0701603)"; National Natural Science Foundation of China(grant numbers:61722114); USTC Research Funds(grant numbers:YD3500002001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003425"",""Accelerating convergence";distributed machine learning;federated learning;"momentum gradient descent"",""Convergence";Machine learning;Servers;Distributed databases;Data models;Acceleration;"Computational modeling"","""",""170"","""",""34"",""IEEE"",""19 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Accelerating Sparse Cholesky Factorization on Sunway Manycore Architecture,""M. Li"; Y. Liu; H. Yang; Z. Luan; L. Gan; G. Yang;" D. Qian"",""Beihang University, Beijing, China"; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Beihang University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Mar 2020"",""2020"",""31"",""7"",""1636"",""1650"",""To improve the performance of sparse Cholesky factorization, existing research divides the adjacent columns of the sparse matrix with the same nonzero patterns into supernodes for parallelization. However, due to the various structures of sparse matrices, the computation of the generated supernodes varies significantly, and thus hard to optimize when computed by dense matrix kernels. Therefore, how to efficiently map sparse Choleksy factorization to the emerging architectures, such as Sunway many-core processor, remains an active research direction. In this article, we propose swCholesky, which is a highly optimized implementation of sparse Cholesky factorization on Sunway processor. Specifically, we design three kernel task queues and a dense matrix library to dynamically adapt to the kernel characteristics and architecture features. In addition, we propose an auto-tuning mechanism to search for the optimal settings of the important parameters in swCholesky. Our experiments show that swCholesky achieves better performance than state-of-the-art implementations."",""1558-2183"","""",""10.1109/TPDS.2019.2953852"",""National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2016YFB1000503,2016YFB0200100)"; National Natural Science Foundation of China(grant numbers:61502019,61732002); Center for High Performance Computing and System Simulation; Pilot National Laboratory for Marine Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903486"",""Sunway architecture";Sparse Cholesky factorization;"performance optimization"",""Sparse matrices";Kernel;Computer architecture;Acceleration;Libraries;Task analysis;"Linear algebra"","""",""8"","""",""56"",""IEEE"",""18 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Accelerating Stochastic Gradient Descent Based Matrix Factorization on FPGA,""S. Zhou"; R. Kannan;" V. K. Prasanna"",""Microsoft Corporation, Redmond, USA"; US Army Research Lab, Los Angeles, USA;" Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Apr 2020"",""2020"",""31"",""8"",""1897"",""1911"",""Matrix Factorization (MF) based on Stochastic Gradient Descent (SGD) is a powerful machine learning technique to derive hidden features of objects from observations. In this article, we design a highly parallel architecture based on Field-Programmable Gate Array (FPGA) to accelerate the training process of the SGD-based MF algorithm. We identify the challenges for the acceleration and propose novel algorithmic optimizations to overcome them. By transforming the SGD-based MF algorithm into a bipartite graph processing problem, we propose a 3-level hierarchical partitioning scheme that enables conflict-minimizing scheduling and processing of edges to achieve significant speedup. First, we develop a fast heuristic graph partitioning approach to partition the bipartite graph into induced subgraphs";" this enables to efficiently use the on-chip memory resources of FPGA for data reuse and completely hide the data communication between FPGA and external memory. Second, we partition all the edges of each subgraph into non-overlapping matchings to extract the maximum parallelism. Third, we propose a batching algorithm to schedule the execution of the edges inside each matching to reduce the memory access conflicts to the on-chip RAMs of FPGA. Compared with non-optimized FPGA-based baseline designs, the proposed optimizations result in up to 60× data dependency reduction, 4.2× bank conflict reduction, and 15.4× speedup. We evaluate the performance of our design using a state-of-the-art FPGA device. Experimental results show that our FPGA accelerator sustains a high computing throughput of up to 217 billion floating-point operations per second (GFLOPS) for training very large real-life sparse matrices. Compared with highly-optimized GPU-based accelerators, our FPGA accelerator achieves up to 12.7× speedup. Based on our optimization methodology, we also implement a software-based design on a multi-core platform, which demonstrates 1.3× speedup compared with the state-of-the-art multi-core implementation."",""1558-2183"","""",""10.1109/TPDS.2020.2974744"",""Intel Strategic Research Alliance funding"; National Science Foundation(grant numbers:ACI-1339756,CNS-1643351,OAC-1911229);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9001229"",""Machine learning";sparse matrix factorization;training acceleration;bipartite graph representation;"FPGA accelerator"",""Field programmable gate arrays";Acceleration;Training;System-on-chip;Optimization;Partitioning algorithms;"Bipartite graph"","""",""7"","""",""48"",""IEEE"",""18 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Achieving Flexible Global Reconfiguration in NoCs Using Reconfigurable Rings,""L. Wang"; L. Liu; J. Han; X. Wang; S. Yin;" S. Wei"",""Institute of Microelectronics, Tsinghua University, Beijing, China"; Institute of Microelectronics, Tsinghua University, Beijing, China; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada; School of Software Engineering, South China University of Technology, Guangzhou, China; Institute of Microelectronics, Tsinghua University, Beijing, China;" Institute of Microelectronics, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""611"",""622"",""The communication behaviors in NoCs of chip-multiprocessors exhibit great spatial and temporal variations, which introduce significant challenges for the reconfiguration in NoCs. Existing reconfigurable NoCs are still far from ideal reconfiguration scenarios, in which globally reconfigurable interconnects can be immediately reconfigured to provide bandwidths on demand for varying traffic flows. In this paper, we propose a hybrid NoC architecture that globally reconfigures the ring-based interconnect to adapt to the varying traffic flows with a high flexibility. The ring-based interconnect has the following advantages. First, it includes horizontal rings and vertical rings, which can be dynamically combined or split to provide low-latency channels for heavy traffic flows. Second, each combined ring connects a number of nodes, thereby improving both the utilization of each ring and the probability to reuse previous reconfigurable interconnects. Finally, the reconfiguration algorithm has a linear-time complexity and can be implemented using a low-overhead hardware design, making it possible to achieve a fast reconfiguration in NoCs. The experimental results show that compared to recent reconfigurable NoCs, the proposed NoC architecture can greatly improve the saturation throughput for synthetic traffic patterns, and reduce the packet latency over 40 percent for realistic benchmarks without incurring significant area and power overhead."",""1558-2183"","""",""10.1109/TPDS.2019.2940190"",""Ministry of Science and Technology of the People's Republic of China(grant numbers:2018ZX01028201)"; National Natural Science Foundation of China(grant numbers:61672317,61834002); Natural Science Foundation of Guangdong Province(grant numbers:2018A030313166); Research Grant of Guangdong Province(grant numbers:2017A050501003); Pearl River S and T Nova Program of Guangzhou(grant numbers:201806010038);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827943"",""Reconfiguration";NoCs;rings;"traffic flows"",""Mesh networks";Bandwidth;Complexity theory;Machine learning algorithms;Integrated circuit interconnections;Scalability;"Heuristic algorithms"","""",""7"","""",""29"",""IEEE"",""10 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Achieving Load-Balanced, Redundancy-Free Cluster Caching with Selective Partition,""Y. Yu"; W. Wang; R. Huang; J. Zhang;" K. B. Letaief"",""Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong"; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; Department of Electronic and Information Engineering, Hong Kong Polytechnic University, Hung Hom, Hong Kong;" Department of Electronic and Computer Engineering, Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""439"",""454"",""Data-intensive clusters increasingly rely on in-memory storages to improve I/O performance. However, the routinely observed file popularity skew and load imbalance create hot spots, which significantly degrade the benefits of in- memory caching. Common approaches to tame load imbalance include copying multiple replicas of hot files and creating parity chunks using storage codes. Yet, these techniques either suffer from high memory overhead due to cache redundancy or incur non-trivial encoding/decoding complexity. In this paper, we propose an effective approach to achieve load balancing without cache redundancy or encoding/decoding overhead. Our solution, termed SP-Cache, selectively partitions files based on the loads they contribute and evenly caches those partitions across the cluster. We develop an efficient algorithm to determine the optimal number of partitions for a hot file—too few partitions are incapable of mitigating hot spots, while too many are susceptible to stragglers. We have implemented SP-Cache atop Alluxio, a popular in-memory distributed storage system, and evaluated its performance through EC2 deployment and trace-driven simulations. SP-Cache can quickly react to the changing load by dynamically re-balancing cache servers. Compared to the state-of-the-art solution, SP-Cache reduces the file access latency by up to 40 percent in both the mean and the tail, using 40 percent less memory."",""1558-2183"","""",""10.1109/TPDS.2019.2931004"",""Kong ITF(grant numbers:ITS/391/15FX)"; RGC GRF(grant numbers:16200214);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772169"",""Cloud computing";cluster caching systems;load balancing;"selective partition"",""Servers";Load management;Redundancy;Production;Encoding;Bandwidth;"Computational modeling"","""",""7"","""",""58"",""IEEE"",""25 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Adaptive Alert Management for Balancing Optimal Performance among Distributed CSOCs using Reinforcement Learning,""A. Shah"; R. Ganesan; S. Jajodia; P. Samarati;" H. Cam"",""Center for Secure Information Systems, George Mason University, Fairfax, USA"; Center for Secure Information Systems, George Mason University, Fairfax, USA; Center for Secure Information Systems, George Mason University, Fairfax, USA; Computer Science Department, Università degli Studi di Milano, Milan, Italy;" U.S. Army Research Laboratory, Adelphi, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""16"",""33"",""Large organizations typically have Cybersecurity Operations Centers (CSOCs) distributed at multiple locations that are independently managed, and they have their own cybersecurity analyst workforce. Under normal operating conditions, the CSOC locations are ideally staffed such that the alerts generated from the sensors in a work-shift are thoroughly investigated by the scheduled analysts in a timely manner. Unfortunately, when adverse events such as increase in alert arrival rates or alert investigation rates occur, alerts have to wait for a longer duration for analyst investigation, which poses a direct risk to organizations. Hence, our research objective is to mitigate the impact of the adverse events by dynamically and autonomously re-allocating alerts to other location(s) such that the performances of all the CSOC locations remain balanced. This is achieved through the development of a novel centralized adaptive decision support system whose task is to re-allocate alerts from the affected locations to other locations. This re-allocation decision is non-trivial because the following must be determined: (1) timing of a re-allocation decision, (2) number of alerts to be reallocated, and (3) selection of the locations to which the alerts must be distributed. The centralized decision-maker (henceforth referred to as agent) continuously monitors and controls the level of operational effectiveness-LOE (a quantified performance metric) of all the locations. The agent's decision-making framework is based on the principles of stochastic dynamic programming and is solved using reinforcement learning (RL). In the experiments, the RL approach is compared with both rule-based and load balancing strategies. By simulating real-world scenarios, learning the best decisions for the agent, and applying the decisions on sample realizations of the CSOC's daily operation, the results show that the RL agent outperforms both approaches by generating (near-) optimal decisions that maintain a balanced LOE among the CSOC locations. Furthermore, the scalability experiments highlight the practicality of adapting the method to a large number of CSOC locations."",""1558-2183"","""",""10.1109/TPDS.2019.2927977"",""Army Research Office(grant numbers:W911NF-13-1-0421,W911NF-15-1-0576)"; Office of Naval Research(grant numbers:N00014-15-1-2007);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762232"",""Distributed cybersecurity operations center (CSOC)";centralized alert management;level of operational effectiveness;reinforcement learning;"adaptive resource allocation"",""Measurement";Organizations;Computer security;Monitoring;Sensors;Reinforcement learning;"Decision making"","""",""1"","""",""41"",""IEEE"",""15 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"aeSpTV: An Adaptive and Efficient Framework for Sparse Tensor-Vector Product Kernel on a High-Performance Computing Platform,""Y. Chen"; G. Xiao; M. T. Özsu; C. Liu; A. Y. Zomaya;" T. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Canada; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; School of Information Technologies, University of Sydney, Sidney, Australia;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 May 2020"",""2020"",""31"",""10"",""2329"",""2345"",""Multi-dimensional, large-scale, and sparse data, which can be neatly represented by sparse tensors, are increasingly used in various applications such as data analysis and machine learning. A high-performance sparse tensor-vector product (SpTV), one of the most fundamental operations of processing sparse tensors, is necessary for improving efficiency of related applications. In this article, we propose aeSpTV, an adaptive and efficient SpTV framework on Sunway TaihuLight supercomputer, to solve several challenges of optimizing SpTVon high-performance computing platforms. First, to map SpTV to Sunway architecture and tame expensive memory access latency and parallel writing conflict due to the intrinsic irregularity of SpTV, we introduce an adaptive SpTV parallelization. Second, to co-execute with the parallelization design while still ensuring high efficiency, we design a sparse tensor data structure named CSSoCR. Third, based on the adaptive SpTV parallelization with the novel tensor data structure, we present an autotuner that chooses the most befitting tensor partitioning method for aeSpTV using the variance analysis theory of mathematical statistics to achieve load balance. Fourth, to further leverage the computing power of Sunway, we propose customized optimizations for aeSpTV. Experimental results show that aeSpTV yields good sacalability on both thread-level and process-level parallelism of Sunway. It achieves a maximum GFLOPS of 195.69 on 128 processes. Additionally, it is proved that optimization effects of the partitioning autotuner and optimization techniques are remarkable."",""1558-2183"","""",""10.1109/TPDS.2020.2990429"",""National Key R&D Program of China(grant numbers:2018 YFB0203800)"; National Natural Science Foundation of China(grant numbers:61625202,61661146006,61751204,61772182,61860206011,61806077); Natural Sciences and Engineering Research Council of Canada;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9078890"",""Parallel";partition;sparse tensor data structure;sparse tensor-vector product;"Sunway architecture"",""Tensors";Sparse matrices;Data structures;Optimization;Kernel;"Parallel processing"","""",""17"","""",""40"",""IEEE"",""27 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"An Approximate Communication Framework for Network-on-Chips,""Y. Chen";" A. Louri"",""Department of Electrical and Computer Engineering, George Washington University, Washington, USA";" Department of Electrical and Computer Engineering, George Washington University, Washington, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Feb 2020"",""2020"",""31"",""6"",""1434"",""1446"",""Current multi-/many-core systems spend large amounts of time and power transmitting data across on-chip interconnects. This problem is aggravated when data-intensive applications, such as machine learning and pattern recognition, are executed in these systems. Recent studies show that some data-intensive applications can tolerate modest errors, thus opening a new design dimension, namely, trading result quality for better system performance. In this article, we explore application error tolerance and propose an approximate communication framework to reduce the power consumption and latency of network-on-chips (NoCs). The proposed framework incorporates a quality control method and a data approximation mechanism to reduce the packet size to decrease network power consumption and latency. The quality control method automatically identifies the error-resilient variables that can be approximated during transmission and calculates their error thresholds based on the quality requirements of the application by analyzing the source code. The data approximation method includes a lightweight lossy compression scheme, which significantly reduces packet size when the error-resilient variables are transmitted. This framework results in fewer flits in each data packet and reduces traffic in NoCs while guaranteeing the quality requirements of applications. Our cycle-accurate simulation using the AxBench benchmark suite shows that the proposed approximate communication framework achieves 62 percent latency reduction and 43 percent dynamic power reduction compared to previous approximate communication techniques while ensuring 95 percent result quality."",""1558-2183"","""",""10.1109/TPDS.2020.2968068"",""National Science Foundation(grant numbers:CCF-1812495,CCF-1740249,CCF-1513923)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8966491"",""Approximate communication";error control;power consumption;"network-on-chips (NoCs)"",""Power demand";Quality control;Multicore processing;Network interfaces;Approximate computing;Data compression;"Approximation methods"","""",""17"","""",""60"",""IEEE"",""22 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Attribute-Based Availability Model for Large Scale IaaS Clouds with CARMA,""H. Lv"; J. Hillston; P. Piho;" H. Wang"",""Department of Computer Science and Technology, Harbin Engineering University, Harbin, China"; University of Edinburgh, Edinburgh, United Kingdom; University of Edinburgh, Edinburgh, United Kingdom;" Department of Computer Science and Technology, Harbin Engineering University, Harbin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""733"",""748"",""High availability is one of the core properties of Infrastructure as a Service (IaaS) and ensures that users have anytime access to on-demand cloud services. However, significant variations of workflow and the presence of super-tasks, mean that heterogeneous workload can severely impact the availability of IaaS clouds. Although previous work has investigated global queues, VM deployment, and failure of PMs, two aspects are yet to be fully explored: one is the impact of task size and the other is the differing features across PMs such as the variable execution rate and capacity. To address these challenges we propose an attribute-based availability model of large scale IaaS developed in the formal modeling language CARMA. The size of tasks in our model can be a fixed integer value or follow the normal, uniform or log-normal distribution. Additionally, our model also provides an easy approach to investigating how to arrange the slack and normal resources in order to achieve availability levels. The two goals of our work are providing an analysis of the availability of IaaS and showing that the use of CARMA allows us to easily model complex phenomena that were not readily captured by other existing approaches."",""1558-2183"","""",""10.1109/TPDS.2019.2943339"",""National Natural Science Foundation of China(grant numbers:61402127)"; China Scholarship Council(grant numbers:201706685020); National Science and Technology Major Project of China(grant numbers:2016ZX03001023-005); Tianjin Key Laboratory of Advanced Networking (TANK); Tianjin University; Engineering and Physical Sciences Research Council(grant numbers:EP/L01503X/1);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8847381"",""Availability";cloud computing;formal model;CARMA;PM provisioning;super-tasks;"heterogeneous workload"",""Task analysis";Cloud computing;Analytical models;Computational modeling;Markov processes;Load modeling;"Containers"","""",""6"","""",""42"",""IEEE"",""24 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"An Event-Driven Approach to Serverless Seismic Imaging in the Cloud,""P. A. Witte"; M. Louboutin; H. Modzelewski; C. Jones; J. Selvage;" F. J. Herrmann"",""School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA"; School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA; Department of Earth, Ocean and Atmospheric Sciences, University of British Columbia, Vancouver, Canada; Osokey Ltd., Henley-on-Thames, United Kingdom; Osokey Ltd., Henley-on-Thames, United Kingdom;" School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Apr 2020"",""2020"",""31"",""9"",""2032"",""2049"",""Adapting the cloud for high-performance computing (HPC) is a challenging task, as software for HPC applications hinges on fast network connections and is sensitive to hardware failures. Using cloud infrastructure to recreate conventional HPC clusters is therefore in many cases an infeasible solution for migrating HPC applications to the cloud. As an alternative to the generic lift and shift approach, we consider the specific application of seismic imaging and demonstrate a serverless and event-driven approach for running large-scale instances of this problem in the cloud. Instead of permanently running compute instances, our workflow is based on a serverless architecture with high throughput batch computing and event-driven computations, in which computational resources are only running as long as they are utilized. We demonstrate that this approach is very flexible and allows for resilient and nested levels of parallelization, including domain decomposition for solving the underlying partial differential equations. While the event-driven approach introduces some overhead as computational resources are repeatedly restarted, it inherently provides resilience to instance shut-downs and allows a significant reduction of cost by avoiding idle instances, thus making the cloud a viable alternative to on-premise clusters for large-scale seismic imaging."",""1558-2183"","""",""10.1109/TPDS.2020.2982626"",""Georgia Research Alliance"; Georgia Institute of Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9044390"","""",""Cloud computing";Imaging;Mathematical model;Computational modeling;Numerical models;Propagation;"Benchmark testing"","""",""14"","""",""74"",""IEEE"",""23 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"An Integrated Indexing and Search Service for Distributed File Systems,""H. Sim"; A. Khan; S. S. Vazhkudai; S. -H. Lim; A. R. Butt;" Y. Kim"",""Oak Ridge National Laboratory, Oak Ridge, USA"; Sogang University, Seoul, South Korea; Oak Ridge National Laboratory, Oak Ridge, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Virginia Tech, Blacksburg, USA;" Sogang University, Seoul, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""15 May 2020"",""2020"",""31"",""10"",""2375"",""2391"",""Data services such as search, discovery, and management in scalable distributed environments have traditionally been decoupled from the underlying file systems, and are often deployed using external databases and indexing services. However, modern data production rates, looming data movement costs, and the lack of metadata, entail revisiting the decoupled file system-data services design philosophy. In this article, we present TagIt, a scalable data management service framework aimed at scientific datasets, which can be integrated into prevalent distributed file system architectures. A key feature of TagIt is a scalable, distributed metadata indexing framework, which facilitates a flexible tagging capability to support data discovery. Furthermore, the tags can also be associated with an active operator, for pre-processing, filtering, or automatic metadata extraction, which we seamlessly offload to file servers in a load-aware fashion. We have integrated TagIt into two popular distributed file systems, i.e., GlusterFS and CephFS. Our evaluation demonstrates that TagIt can expedite data search operation by up to 10× over the extant decoupled approach."",""1558-2183"","""",""10.1109/TPDS.2020.2990656"",""U.S. DOE's Scientific data management program"; National Science Foundation(grant numbers:CNS-1615411,CNS-1405697,CNS-1565314); National Research Foundation of Korea(grant numbers:2018R1A1A1A05079398); Oak Ridge Leadership Computing Facility(grant numbers:DE-AC05-00OR22725);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079563"",""Distributed systems";storage management;"scientific data management"",""Metadata";Servers;Indexing;Production;"Tagging"","""",""10"","""",""61"",""IEEE"",""27 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"An Optimal Locality-Aware Task Scheduling Algorithm Based on Bipartite Graph Modelling for Spark Applications,""Z. Fu"; Z. Tang; L. Yang;" C. Liu"",""College of Information Science and Engineering, and National Supercomputing Center in Changsha, Hunan University, China"; College of Information Science and Engineering, and National Supercomputing Center in Changsha, Hunan University, China; College of Computer and Communication Engineering, Changsha University of Science and Technology, Changsha, China;" College of Information Science and Engineering, and National Supercomputing Center in Changsha, Hunan University, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2020"",""2020"",""31"",""10"",""2406"",""2420"",""In the distributed computing framework of Spark, cross-node/rack data transfer produced by map tasks and reduce tasks are common problems resulting in performance degradation, such as prolonging of entire execution time and network congestion. To address these problems, this article utilizes the bipartite graph modelling to propose an optimal locality-aware task scheduling algorithm. By considering global optimality, the algorithm can generate the optimal scheduling solution for both the map tasks and the reduce tasks for data locality. Because of the different communication modes, this article uses a unified graph to model the map task scheduling and the reduce task scheduling respectively. Then, by calculating the communication cost matrix of tasks, we formulate an optimal task scheduling scheme to minimize overall communication cost and transform the problem as the well-known graph problem: minimum weighted bipartite matching (MWBM), which can be resolved by Kuhn-Munkres algorithm. In addition, this article proposes a locality-aware executor allocation strategy to improve the data locality further. We implement our algorithm and strategy in Spark-2.4.1 and evaluate its performance using several representative micro-benchmarks, macro-benchmarks, and HiBench benchmark suite. The experimental results verify that by reducing the network traffic and access latency, the proposed algorithm can improve the job performance substantially compared to some other task scheduling algorithms."",""1558-2183"","""",""10.1109/TPDS.2020.2992073"",""National Basic Research Program of China (973 Program)(grant numbers:2018YFB1701401,2018YFB0203804,2017YFB0202201)"; National Natural Science Foundation of China(grant numbers:61873090,L1824034,L1924056); Ministry of Education-China Mobile Research Fund Project(grant numbers:MCM20170506); China Knowledge Centre for Engineering Sciences and Technology(grant numbers:CKCEST-2018-1-13,CKCEST-2019-2-13); Science and Technology on Parallel and Distributed Processing Laboratory(grant numbers:WDZC20195500110);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9085890"",""Communication cost";data locality;task scheduling;weighted bipartite graph;"Spark"",""Task analysis";Optimal scheduling;Sparks;Scheduling;Scheduling algorithms;"Data transfer"","""",""19"","""",""40"",""IEEE"",""4 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Analyzing the Performance Trade-Off in Implementing User-Level Threads,""S. Iwasaki"; A. Amer; K. Taura;" P. Balaji"",""Department of Information and Communication Engineering, University of Tokyo, Tokyo, Japan"; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, USA; Department of Information and Communication Engineering, University of Tokyo, Tokyo, Japan;" Mathematical and Computer Science, Argonne National Laboratory, Lemont, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Mar 2020"",""2020"",""31"",""8"",""1859"",""1877"",""User-level threads have been widely adopted as a means of achieving lightweight concurrent execution without the costs of OS-level threads. Nevertheless, the costs of managing user-level threads represent a performance barrier that dictates how fine grained the concurrency exposed by an application can be without incurring significant overheads";" this in turn may translate into insufficient parallelism to exploit highly parallel systems. This article is a deep dive into the fundamental costs in implementing user-level threads. We first identify that one of the highest sources of fork-join overheads stems from deviations, events that incur context switching during the execution of a thread and disrupt a run-to-completion execution. We then conduct an in-depth investigation of a wide spectrum of methods with respect to how they handle deviations while covering both parent- and child-first scheduling policies. Our methodology involves a comprehensive instruction- and cache-level analysis of all methods on several modern CPU architectures. The primary finding of our evaluation is that dynamic promotion methods that assume the absence of deviation and dynamically provide context-switching support offer the best trade-off between performance and capability when the likelihood of deviation is low."",""1558-2183"","""",""10.1109/TPDS.2020.2976057"",""Exascale Computing(grant numbers:17-SC-20-SC)"; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Nuclear Security Administration;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018074"",""Multithreading";multitasking;scheduling;user-level threads;context switch;"task parallelism"",""Context";Instruction sets;Switches;Libraries;Runtime;Hardware;"Computer architecture"","""",""4"","""",""67"",""IEEE"",""28 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"APMigration: Improving Performance of Hybrid Memory Performance via An Adaptive Page Migration Method,""Y. Tan"; B. Wang; Z. Yan; W. Srisa-an; X. Chen;" D. Liu"",""College of Computer Science, Chongqing University, Chongqing, China"; College of Computer Science, Chongqing University, Chongqing, China; HewlettPackard Enterprise, Charlotte, USA; University of Nebraska Lincoln, Lincoln, USA; College of Computer Science, Chongqing University, Chongqing, China;" College of Computer Science, Chongqing University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""266"",""278"",""Byte-addressable, non-volatile memory (NVRAM) combines the benefits of DRAM and flash memory. However, due to its slower speed than DRAM, it is best to deploy it in combination with typical DRAM. In such Hybrid NVRAM systems, frequently accessed, hotpages can be stored in DRAM while other cold pages can reside in NVRAM, providing the benefits of both high performance (from DRAM) and lower power consumption and cost/performance (from NVRAM). While the idea seems beneficial, realizing an efficient hybrid NVRAM system requires careful page migration and accurate data temperature measurement. Existing solutions, however, often cause invalid migrations due to inaccurate data temperature accounting, because hot and cold pages are separately identified in DRAM and NVRAM regions. Moreover, since a new NVRAM frame is always allocated for each page swapped back NVRAM, a large amount of unnecessary NVRAM writes are generated during each page migration. Based on these observations, we propose APMigrate, an adaptive data migration approach for hybrid NVRAM systems. APMigrate consist of two parts, UIMigrate and LazyWriteback. UIMigrate focuses on eliminating invalid page migrations by considering data temperature in the entire DRAM-NVRAM space, while LazyWriteback focus on rewriting only dirty data back when the page is swapped back to NVRAM. Our experiments using SPEC 2006 show that APMigrate can reduce the number of migrations and improves performance by up to 90 percent compared to existing state-of-the-art approaches. For some workloads, LazyWriteback can reduce unnecessary NVRAM writes for existing page migrations by up to 75 percent."",""1558-2183"","""",""10.1109/TPDS.2019.2933521"",""Fundamental Research Funds for the Central Universities(grant numbers:2019CDJGFJSJ001,2018CDXYJSJ0026)"; National Natural Science Foundation of China(grant numbers:61402061,61672116,61802038); China Postdoctoral Science Foundation(grant numbers:2017M620412); Chongqing Postdoctoral Special Science Foundation(grant numbers:XmT2018003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789491"",""Hybrid memory system";page migration;unified temperature identification;"finer-grained access status"",""Random access memory";Nonvolatile memory;Memory management;Performance evaluation;Adaptive systems;Hybrid power systems;"Power demand"","""",""6"","""",""23"",""IEEE"",""6 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Approximate NoC and Memory Controller Architectures for GPGPU Accelerators,""V. Y. Raparti";" S. Pasricha"",""Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, USA";" Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Jan 2020"",""2020"",""31"",""5"",""25"",""39"",""High interconnect bandwidth is crucial for achieving better performance in many-core GPGPU architectures that execute highly data parallel applications. The parallel warps of threads running on shader cores generate a high volume of read requests to the main memory due to the limited size of data caches at the shader cores. This leads to a scenarios with rapid arrival of an even larger volume of reply data from the DRAM, which creates a bottleneck at memory controllers (MCs) that send reply packets back to the requesting cores over the network-on-chip (NoC). Coping with such high volumes of data requires intelligent memory scheduling and innovative NoC architectures. To mitigate memory bottlenecks in GPGPUs, we first propose a novel approximate memory controller architecture (AMC) that reduces the DRAM latency by opportunistically exploiting row buffer locality and bank level parallelism in memory request scheduling, and leverages approximability of the reply data from DRAM, to reduce the number of reply packets injected into the NoC. To further realize high throughput and low energy communication in GPGPUs, we propose a low power, approximate NoC architecture (Dapper) that increases the utilization of the available network bandwidth by using single cycle overlay circuits for the reply traffic between MCs and shader cores. Experimental results show that Dapper and AMC together increase NoC throughput by up to 21 percent";" and reduce NoC latency by up to 45.5 percent and energy consumed by the NoC and MC by up to 38.3 percent, with minimal impact on output accuracy, compared to state-of-the-art approximate NoC/MC architectures."",""1558-2183"","""",""10.1109/TPDS.2019.2958344"",""NSF(grant numbers:CCF-1813370)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8928966"",""GPGPU";approximate computing;network-on-chip;"memory controller"",""Computer architecture";Random access memory;Throughput;Approximate computing;Graphics processing units;Parallel processing;"Energy consumption"","""",""1"","""",""40"",""IEEE"",""9 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Architectural Support for NVRAM Persistence in GPUs,""S. Chen"; L. Liu; W. Zhang;" L. Peng"",""Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, USA"; SKLCA, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Software School, Fudan University, Shanghai, China;" Division of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1107"",""1120"",""Non-volatile Random Access Memories (NVRAM) have emerged in recent years to bridge the performance gap between the main memory and external storage devices, such as Solid State Drives (SSD). In addition to higher storage density, NVRAM provides byte-addressability, higher bandwidth, near-DRAM latency, and easier access compared to block devices such as traditional SSDs. This enables new programming paradigms taking advantage of durability and larger memory footprint. With the range and size of GPU workloads expanding, NVRAM will present itself as a promising addition to GPU's memory hierarchy. To utilize the non-volatility of NVRAMs, programs should allow durable stores, maintaining consistency through a power loss event. This is usually done through a logging mechanism that works in tandem with a transaction execution layer which can consist of a transactional memory or a locking mechanism. Together, this results in a transaction processing system that preserves the ACID properties. GPUs are designed with high throughput in mind, leveraging high degrees of parallelism. Transactional memory proposals enable fine-grained transactions at the GPU thread-level. However, with lower write bandwidths compared to that of DRAMs, using NVRAM as-is may yield sub-optimal overall system performance when threads experience long latency. To address this problem, we propose using Helper Warps to move persistence out of the critical path of transaction execution, alleviating the impact of latencies. Our mechanism achieves a speedup of 4.4 and 1.5 under bandwidth limits of 1.6 GB/s and 12 GB/s and is projected to maintain speed advantage even when NVRAM bandwidth gets as high as hundreds of GB/s in certain cases. Due to the speedup, our proposed method also results in reduction in overall energy consumption."",""1558-2183"","""",""10.1109/TPDS.2019.2960233"",""National Science Foundation(grant numbers:CCF-1422408,CNS-1527318)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935351"",""NVRAM";persistence;GPUs;"helper warps"",""Random access memory";Nonvolatile memory;Bandwidth;Graphics processing units;Hardware;Performance evaluation;"Instruction sets"","""",""3"","""",""49"",""IEEE"",""17 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Automated Fine-Grained CPU Cap Control in Serverless Computing Platform,""Y. K. Kim"; M. R. HoseinyFarahabady; Y. C. Lee;" A. Y. Zomaya"",""University of Sydney, Camperdown, Australia"; University of Sydney, Camperdown, Australia; Macquarie University, Macquarie Park, Australia;" University of Sydney, Camperdown, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2020"",""2020"",""31"",""10"",""2289"",""2301"",""Serverless computing has emerged as a new cloud computing execution model that liberates users and application developers from explicitly managing `physical' resources, leaving such a resource management burden to service providers. In this article, we study the problem of resource allocation for multi-tenant serverless computing platforms explicitly taking into account workload fluctuations including sudden surges. In particular, we investigate different root causes of performance degradation in these platforms where tenants (their applications) have different workload characteristics. To this end, we develop a fine-grained CPU cap control solution as a resource manager that dynamically adjusts CPU usage limit (or CPU cap) concerning applications with same/similar performance requirements, i.e., application groups. The adjustment of CPU caps applies primarily to co-located worker processes of serverless computing platforms to minimize resource contention, which is the major source of performance degradation. The actual adjustment decisions are made based on performance metrics (e.g., throttled time and queue length) using a group-aware scheduling algorithm. The extensive experimental results performed in our local cluster confirm that the proposed resource manager can effectively eliminate the burden of explicit reservation of computing capacity, even when fluctuations and sudden surges in the incoming workload exist. We measure the robustness of the proposed resource manager by comparing it with several heuristics which extensively used in practice, including the enhanced version of round robin and the least length queue scheduling policies, under various workload intensities driven by real-world scenarios. Notably, our resource manager outperforms other heuristics by decreasing skewness and average response time up to 44 and 94 percent, respectively, while it does not over-use the CPU resources."",""1558-2183"","""",""10.1109/TPDS.2020.2989771"",""Australian Research Council(grant numbers:DP190103710)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076860"",""Serverless computing";virtualized cloud platforms;operating system process management;dynamic CPU scheduling;"performance modeling"",""Interference";Round robin;Time factors;Resource management;Measurement;"Computer architecture"","""",""33"","""",""29"",""IEEE"",""23 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Automatic Generation of High-Performance FFT Kernels on Arm and X86 CPUs,""Z. Li"; H. Jia; Y. Zhang; T. Chen; L. Yuan;" R. Vuduc"",""SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China;" School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Apr 2020"",""2020"",""31"",""8"",""1925"",""1941"",""This article presents AutoFFT, a template-based code generation framework that can automatically generate high-performance FFT kernels for all natural-number radices. AutoFFT is based on the Cooley-Tukey FFT algorithm, which exploits the symmetric and periodic properties of the DFT matrix, as the outer parallelization framework. Because butterflies are the core operations of the Cooley-Tukey algorithm, we explore additional symmetric and periodic properties of the DFT matrix and formulate multiple optimized calculation templates to further reduce the number of floating-point operations for butterflies of arbitrary natural numbers. To fully exploit hardware resources, we encapsulate a series of optimizations in an assembly template optimizer. Given any DFT problem, AutoFFT automatically generates C FFT kernels using these calculation templates and converts them into efficient assembly kernels using the template optimizer. Through a series of experiments on Arm, Intel, and AMD processors, we show that AutoFFT-generated kernels can outperform those in Fastest Fourier Transform in the West (FFTW), the Arm Performance Libraries (ARMPL), and the Intel Math Kernel Library (MKL)."",""1558-2183"","""",""10.1109/TPDS.2020.2977629"",""National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2107YFB0202105,2016YFB0200803,2017YFB0202302)"; National Natural Science Foundation of China(grant numbers:61602443,61432018,61521092,61502450);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035643"",""AutoFFT";FFT;code generation;template;"DFT"",""Kernel";Libraries;Discrete Fourier transforms;Computer architecture;Optimization;Symmetric matrices;"Hardware"","""",""8"","""",""50"",""CCBY"",""13 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Bandwidth-Aware Dynamic Prefetch Configuration for IBM POWER8,""C. Navarro"; J. Feliu; S. Petit; M. E. Gómez;" J. Sahuquillo"",""Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain"; Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain; Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain; Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain;" Departamento de Informática de Sistemas y Computadores, Universitat Politècnica de València, Valencia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Apr 2020"",""2020"",""31"",""8"",""1970"",""1982"",""Advanced hardware prefetch engines are being integrated in current high-performance processors. Prefetching can boost the performance of most applications, however, the induced bandwidth consumption can lead the system to a high contention for main memory bandwidth, which is a scarce resource in current multicores. In such a case, the system performance can be severely damaged. This article characterizes the applications’ behavior in an IBM POWER8 machine, which presents many prefetch settings, varying the bandwidth contention. The study reveals that the best prefetch setting for each application depends on the main memory bandwidth availability, that is, it depends on the co-running applications. Based on this study, we propose Bandwidth-Aware Prefetch Configuration (BAPC) a scalable adaptive prefetching algorithm that improves the performance of multi-program workloads. BAPC increases the performance of the applications in a 12, 15, and 16 percent of 6-, 8-, and 10-application workloads over the IBM POWER8 default configuration. In addition, BAPC reduces bandwidth consumption in 39, 42, and 45 percent, respectively."",""1558-2183"","""",""10.1109/TPDS.2020.2982392"",""Ministerio de Ciencia, Innovación y Universidades and the European(grant numbers:RTI2018-098156-B-C51)"; Generalitat Valenciana(grant numbers:AICO/2019/317); Universitat Politècnica de València(grant numbers:SP20180140);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9044389"",""Prefetch engine";prefetch settings;"performance measures"",""Prefetching";Bandwidth;Interference;Hardware;Engines;"Memory management"","""",""5"","""",""29"",""IEEE"",""23 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Boosting the Performance of SSDs via Fully Exploiting the Plane Level Parallelism,""C. Gao"; L. Shi; K. Liu; C. J. Xue; J. Yang;" Y. Zhang"",""College of Computer Science, Chongqing University, Chongqing, P.R. China"; School of Computer Science and Technology, East China Normal University, Shanghai, P.R. China; College of Computer Science, Chongqing University, Chongqing, P.R. China; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong; University of Pittsburgh, Pittsburgh, USA;" University of Pittsburgh, Pittsburgh, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""5 May 2020"",""2020"",""31"",""9"",""2185"",""2200"",""Solid state drives (SSDs) are constructed with multiple level parallel organization, including channels, chips, dies, and planes. Among these parallel levels, plane level parallelism, which is the last level parallelism of SSDs, has the most strict restrictions. Only the same type of operations that access the same address in different planes can be processed in parallel. In order to maximize the access performance, several previous works have been proposed to exploit the plane level parallelism for host accesses and internal operations of SSDs. However, our preliminary studies show that the plane level parallelism is farfrom well utilized and should be further improved. The reason is that the strict restrictions of plane level parallelism are hard to be satisfied. In this article, a from plane to die parallel optimization framework is proposed to exploit the plane level parallelism through smartly satisfying the strict restrictions all the time. In order to achieve the objective, there are at least two challenges. First, due to that host access patterns are always complex, receiving multiple same-type requests to different planes at the same time is uncommon. Second, there are many internal activities, such as garbage collection (GC), which may destroy the restrictions. In order to solve above challenges, two schemes are proposed in the SSD controller: First, a die level write construction scheme is designed to make sure there are always N pages of data written by each write operation. Second, in a further step, a die level GC scheme is proposed to activate GC in the unit of all planes in the same die. Combing the die level write and die level GC, write accesses from both host write operations and GC induced valid page movements can be processed in parallel at all time. To further improve the performance of SSDs, host write operations blocked by GCs are suggested to be processed in parallel with GC induced valid page movements, bringing lesser waiting time cost of host write operations. As a result, the GC cost and average write latency can be significantly reduced. Experiment results show that the proposed framework is able to significantly improve the write performance without read performance impact."",""1558-2183"","""",""10.1109/TPDS.2020.2987894"",""National Natural Science Foundation of China(grant numbers:61772092,61872049)"; NSF(grant numbers:1910413,1617071,1725657,1718080); Research Grants Council of the Hong Kong Special Administrative Region, China(grant numbers:CityU 11219319); Frontier Interdisciplinary Research Funds for the Central Universities(grant numbers:2018CDQYJSJ0034);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068501"",""SSD";parallelism;storage;"scheduling, performance improvement"",""Parallel processing";Organizations;Random access memory;Computer science;Resource management;Optimization;"Solid state drives"","""",""9"","""",""55"",""IEEE"",""15 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Cartesian Partitioning Models for 2D and 3D Parallel SpGEMM Algorithms,""G. V. Demirci";" C. Aykanat"",""Department of Computer Engineering, Bilkent University, Ankara, Turkey";" Department of Computer Engineering, Bilkent University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Jun 2020"",""2020"",""31"",""12"",""2763"",""2775"",""The focus is distributed-memory parallelization of sparse-general-matrix-multiplication (SpGEMM). Parallel SpGEMM algorithms are classified under one-dimensional (1D), 2D, and 3D categories denoting the number of dimensions by which the 3D sparse workcube representing the iteration space of SpGEMM is partitioned. Recently proposed successful 2D- and 3D-parallel SpGEMM algorithms benefit from upper bounds on communication overheads enforced by 2D and 3D cartesian partitioning of the workcube on 2D and 3D virtual processor grids, respectively. However, these methods are based on random cartesian partitioning and do not utilize sparsity patterns of SpGEMM instances for reducing the communication overheads. We propose hypergraph models for 2D and 3D cartesian partitioning of the workcube for further reducing the communication overheads of these 2D- and 3D- parallel SpGEMM algorithms. The proposed models utilize two- and three-phase partitioning that exploit multi-constraint hypergraph partitioning formulations. Extensive experimentation performed on 20 SpGEMM instances by using upto 900 processors demonstrate that proposed partitioning models significantly improve the scalability of 2D and 3D algorithms. For example, in 2D-parallel SpGEMM algorithm on 900 processors, the proposed partitioning model respectively achieves 85 and 42 percent decrease in total volume and total number of messages, leading to 1.63 times higher speedup compared to random partitioning, on average."",""1558-2183"","""",""10.1109/TPDS.2020.3000708"",""Türkiye Bilimsel ve Teknolojik Araştirma Kurumu(grant numbers:EEEAG-115E512)"; National Center for High Performance Computing of Turkey(grant numbers:4005992019);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9110752"",""Sparse matrix-matrix multiplication";SpGEMM;sparse SUMMA SpGEMM;split-3D-SpGEMM;hypergraph partitioning;communication cost;bandwidth;"latency"",""Partitioning algorithms";Two dimensional displays;Three-dimensional displays;Solid modeling;Sparse matrices;Computational modeling;"Upper bound"","""",""4"","""",""40"",""IEEE"",""8 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"cCUDA: Effective Co-Scheduling of Concurrent Kernels on GPUs,""S. . -K. Shekofteh"; H. Noori; M. Naghibzadeh; H. Fröning;" H. S. Yazdi"",""Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran"; Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran; Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran; Institute of Computer Engineering, Ruprecht-Karls University of Heidelberg, Heidelberg, Germany;" Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""766"",""778"",""While GPUs are meantime omnipresent for many scientific and technical computations, they still continue to evolve as processors. An important recent feature is the ability to execute multiple kernels concurrently via queue streams. However, experiments show that different parameters including the behavior of kernels, the order of kernel launches and other execution configurations, e.g., the number of concurrent thread blocks, may result in different execution time for concurrent kernel execution. Since kernels may have different resource requirements, they can be classified into different classes, which are traditionally assumed as either memory-bound or compute-bound. However, a kernel may belong to the different classes on different hardware according to the hardware resources. In this paper, the definition of kernel mix intensity is introduced. Based on this, a scheduling framework called concurrent CUDA (cCUDA) is proposed to co-schedule the concurrent kernels more efficiently. It first profiles and ranks kernels with different execution behaviors and then takes the kernel resource requirements into account to partition thread blocks of different kernels and overlap them to better utilize the GPU resources. Experimental results on real hardware demonstrate performance improvement in terms of execution time of up to 1.86x, and an average speedup of 1.28x for a wide range of kernels. cCUDA is available at https://github.com/kshekofteh/cCUDA."",""1558-2183"","""",""10.1109/TPDS.2019.2944602"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8853389"",""Kernel";scheduling;concurrent kernel execution;stream;"resource management"",""Kernel";Graphics processing units;Benchmark testing;Hardware;Scheduling;"Analytical models"","""",""14"","""",""41"",""IEEE"",""30 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Coded Load Balancing in Cache Networks,""M. J. Siavoshani"; F. Parvaresh; A. Pourmiri;" S. P. Shariatpanahi"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Electrical Engineering, University of Isfahan, Isfahan, Iran; Department of Software Engineering, University of Isfahan, Isfahan, Iran;" School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""347"",""358"",""We consider load balancing problem in a cache network consisting of storage-enabled servers forming a distributed content delivery scenario. Previously proposed load balancing solutions cannot perfectly balance out requests among servers, which is a critical issue in practical networks. Therefore, in this paper, we investigate a coded cache content placement where coded chunks of original files are stored in servers based on the files popularity distribution. In our scheme, upon each request arrival at the delivery phase, by dispatching enough coded chunks to the request origin from the nearest servers, the requested file can be decoded. Here, we show that if n requests arrive randomly at n servers, the proposed scheme results in the maximum load of O(1) in the network. This result is shown to be valid under various assumptions for the underlying network topology. Our results should be compared to the maximum load of two baseline schemes, namely, nearest replica and power of two choices strategies, which are O(log n) and O(log log n), respectively. This finding shows that using coding, results in a considerable load balancing performance improvement, without compromising communications cost performance. This is confirmed by performing extensive simulation results, in non-asymptotic regimes as well."",""1558-2183"","""",""10.1109/TPDS.2019.2933839"",""IPM(grant numbers:95680425)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8792130"",""Distributed caching servers";content delivery networks;coded caching;request routing;load balancing;"communication cost"",""Servers";Load management;Measurement;Encoding;Network topology;Wireless communication;"Content distribution networks"","""",""2"","""",""35"",""IEEE"",""8 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Combinatorial Auctions for Temperature-Constrained Resource Management in Manycores,""H. Khdr"; M. Shafique; S. Pagani; A. Herkersdorf;" J. Henkel"",""Computer Science Department, Karlsruher Institut für Technologie,, Karlsruhe, Germany"; Department of Computer Engineering, Technische Universitat Wien, Wien, Austria; Central Engineering – Media, Arm Ltd., Cambridge, United Kingdom; Lehrstuhl für Integrierte Systeme, Technische Universität München, München, Germany;" Karlsruher Institut für Technologie, Karlsruhe, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Feb 2020"",""2020"",""31"",""7"",""1605"",""1620"",""Although manycore processors have plenty of cores, not all of them may run simultaneously at full speed and even some of them might need to be power-gated in order to keep the chip within safe temperature limits. Hence, a resource management technique, that allocates cores to application aiming at maximizing the system performance, will not be able to achieve its goal without taking into account the on-chip temperature and its impact on the availability of the chip's resources. However, considering a temperature constraint by the resource management will further increase its complexity, especially in manycores, and thus implementing it in a centralized scheme might lead to a computation bottleneck and a single point of failure. To avoid such scenarios, it is inevitable to distribute the computation required by the resource management technique throughout the chip. In this article, we propose a distributed resource management technique that considers temperature as an essential factor in allocating cores to applications and determining the power states of these cores and their voltage/frequency levels, while taking into account the performance models of the applications in order to maximize the overall system performance under a temperature constraint. Our proposed technique employs, for the first time, combinatorial auctions within an agent system to achieve the targeted goal in a distributed manner. The experimental evaluations show that our proposed technique achieves significant performance improvements with an average of 41% compared to several distributed resource management techniques."",""1558-2183"","""",""10.1109/TPDS.2020.2965523"",""Deutsche Forschungsgemeinschaft(grant numbers:146371743)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955960"",""Performance optimization";DVFS;temperature-aware design;system-level optimization;"runtime resource management"",""Resource management";System performance;Distributed management;Optimization;Thermal management;Steady-state;"Temperature"","""",""5"","""",""37"",""IEEE"",""10 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Combining Size-Based Load Balancing with Round-Robin for Scalable Low Latency,""J. Anselmi"",""INRIA Bordeaux Sud Ouest (Team: CQFD), Talence, France"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""886"",""896"",""When dispatching jobs to parallel servers, or queues, the highly scalable round-robin (RR) scheme reduces the variance of interarrival times at all queues to a great extent but has no impact on the variances of service processes. Contrariwise, size-interval task assignment (SITA) routing has little impact on the variances of interarrival times but makes the service processes as deterministic as possible. In this paper, we unify both `static' approaches to design a scalable load balancing framework able to control the variances of the arrival and service processes jointly. It turns out that the resulting combination significantly improves performance and is able to drive the mean job delay to zero in the large-system limit";" it is known that this property is not achieved when both approaches are considered separately. Within realistic parameters, we show that the optimal number of size intervals that partition the support of the job size distribution is small with respect to the system size. This enhances the applicability of the proposed load balancing scheme at a large scale. In fact, we find that adding a little bit of information about job sizes to a dispatcher operating under RR improves performance a lot. Under the optimal scaling of size intervals and assuming highly variable job sizes, numerical simulations indicate that the proposed algorithm is competitive with the (less scalable) join-the-shortest-workload algorithm even when the system size grows large."",""1558-2183"","""",""10.1109/TPDS.2019.2950621"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8889411"",""Dispatching policies";size-based routing;performance;"asymptotic optimality"",""Dispatching";Servers;Load management;Routing;Heuristic algorithms;Queueing analysis;"Task analysis"","""",""8"","""",""30"",""IEEE"",""31 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Comment on “A Tag Encoding Scheme Against Pollution Attack to Linear Network Coding”,""J. Chang"; B. Shao; Y. Ji;" G. Bian"",""School of Information and Control Engineering, Xi'An University of Architecture and Technology, Xi'An, P.R. China"; School of Management, Xi'An University of Architecture and Technology, Xi'An, P.R. China; School of Management, Xi'An University of Architecture and Technology, Xi'An, P.R. China;" School of Information and Control Engineering, Xi'An University of Architecture and Technology, Xi'An, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Jun 2020"",""2020"",""31"",""11"",""2618"",""2619"",""In 2014, Wu et al. proposed a tag encoding scheme, named KEPTE, to protect network coding against pollution attack. They also carefully analyzed the security of KEPTE based on the transmission of a data file through their key-pre-distributed network. In this article, we point out that their security analysis only holds for single data file transmitted in this network. If multiple files are multicasted though it, then any adversary may completely recover source node's signing key. A concrete example says that, after pre-distributing 90 keys to all the nodes in the network, it only allows to securely transmit (at most) 3 data files. More importantly, this scheme is completely insecure in standard security model for network model since the adversary is allowed to make polynomial times queries on any data files of its choice before outputting its final forgery. Finally, we also propose a twisted KEPTE scheme that is secure against any eavesdropping adversary no matter how many data files it has queried."",""1558-2183"","""",""10.1109/TPDS.2020.2999523"",""National Natural Science Foundation of China(grant numbers:61672416,61672059,61872284)"; Natural Science Research in Shaanxi(grant numbers:2018JM6105,2019JM118);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106826"",""Network coding";tag encoding;pollution attack;"KEPTE-scheme"",""Security";Network coding;Encoding;Eavesdropping;Pollution;Mathematical model;"Data models"","""",""6"","""",""5"",""IEEE"",""2 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Compression Ratio Modeling and Estimation across Error Bounds for Lossy Compression,""J. Wang"; T. Liu; Q. Liu; X. He; H. Luo;" W. He"",""Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA"; Department of Computer and Information Sciences, Temple University, Philadelphia, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA; Department of Computer and Information Sciences, Temple University, Philadelphia, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA;" Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Mar 2020"",""2020"",""31"",""7"",""1621"",""1635"",""Scientific simulations on high-performance computing (HPC) systems generate vast amounts of floating-point data that need to be reduced in order to lower the storage and I/O cost. Lossy compressors trade data accuracy for reduction performance and have been demonstrated to be effective in reducing data volume. However, a key hurdle to wide adoption of lossy compressors is that the trade-off between data accuracy and compression performance, particularly the compression ratio, is not well understood. Consequently, domain scientists often need to exhaust many possible error bounds before they can figure out an appropriate setup. The current practice of using lossy compressors to reduce data volume is, therefore, through trial and error, which is not efficient for large datasets which take a tremendous amount of computational resources to compress. This paper aims to analyze and estimate the compression performance of lossy compressors on HPC datasets. In particular, we predict the compression ratios of two modern lossy compressors that achieve superior performance, SZ and ZFP, on HPC scientific datasets at various error bounds, based upon the compressors' intrinsic metrics collected under a given base error bound. We evaluate the estimation scheme using twenty real HPC datasets and the results confirm the effectiveness of our approach."",""1558-2183"","""",""10.1109/TPDS.2019.2938503"",""National Science Foundation(grant numbers:CCF-1718297,CCF-1812861)"; Nanjing Institute of Technology; National Science Foundation(grant numbers:1828363,1813081);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821342"",""High-performance computing";lossy compression;data reduction;"performance modeling"",""Compressors";Data models;Measurement;Computational modeling;Estimation;Market research;"Predictive models"","""",""8"","""",""35"",""IEEE"",""30 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Concurrent Irrevocability in Best-Effort Hardware Transactional Memory,""R. Titos-Gil"; R. Fernández-Pascual; A. Ros;" M. E. Acacio"",""Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain"; Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain;" Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jan 2020"",""2020"",""31"",""6"",""1301"",""1315"",""Existing best-effort requester-wins implementations of transactional memory must resort to non-speculative execution to provide forward progress in the presence of transactions that exceed hardware capacity, experience page faults or suffer high-contention leading to livelocks. Current approaches to irrevocability employ lock-based synchronization to achieve mutual exclusion when executing a transaction non-speculatively, conservatively precluding concurrency with any other transactions in order to guarantee atomicity at the cost of degrading performance. In this article, we propose a new form of concurrent irrevocability whose goal is to minimize the loss of concurrency paid when transactions resort to irrevocability to complete. By enabling optimistic concurrency control also during non-speculative execution of a transaction, our proposal allows for higher parallelism than existing schemes. We describe the extensions to the instruction set to provide concurrent irrevocable transactions as well as the architectural extensions required to realize them on a best-effort HTM system without requiring any modification to the cache coherence protocol. Our evaluation shows that our proposal achieves an average reduction of 12.5 percent in execution time across the STAMP benchmarks, with 15.8 percent on average for highly contended workloads."",""1558-2183"","""",""10.1109/TPDS.2019.2963030"",""Spanish MCIU and AEI"; European Commission(grant numbers:RTI2018-098156-B-C53);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8945317"",""Parallel programming";multicore architectures;"transactional memory"",""Hardware";Proposals;Benchmark testing;Synchronization;Concurrent computing;Software;"Concurrency control"","""",""2"","""",""25"",""IEEE"",""30 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Congestion-Balanced and Welfare-Maximized Charging Strategies for Electric Vehicles,""Q. Tang"; K. Wang; K. Yang;" Y. -s. Luo"",""Hunan Provincial Key Laboratory of Intelligent Processing of Big Data on Transportation, School of Computer Science and Communication Engineering, Changsha University of Science and Technology, Changsha, China"; Department of Computer and Information Sciences, Northumbria University, Newcastle upon Tyne, United Kingdon; School of Computer Science and Electronic Engineering, University of Essex, Essex, United Kingdon;" Hunan Provincial Key Laboratory of Intelligent Processing of Big Data on Transportation, School of Computer Science and Communication Engineering, Changsha University of Science and Technology, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Jul 2020"",""2020"",""31"",""12"",""2882"",""2895"",""With the increase of the number of electric vehicles (EVs), it is of vital importance to develop the efficient and effective charging scheduling schemes for all the EVs. In this article, we aim to maximize the social welfare of all the EVs, charging stations (CSs) and power plant (PP), by taking into account the changing demand of each EV, the changing price, the capacity and the congestion balance between different CSs. To this end, two efficient scheduling algorithms, i.e., Centralized Charging Strategy (CCS) and Distributed Charging Strategy (DCS) are proposed. CCS has a slightly better performance than the DCS, as it takes all the information and make the decision in the central control unit. On the other hand, DCS dose not require the private information from EVs and can make decentralized decision. Extensive simulation are conducted to verify the effectiveness of the proposed algorithms, in terms of the performance, congestion balance, and computing complexity."",""1558-2183"","""",""10.1109/TPDS.2020.3003270"",""National Natural Science Foundation of China(grant numbers:61620106011,61772087)"; Zhongshan City Team Project(grant numbers:180809162197874); Outstanding Youth Project of Hunan Province Education Department(grant numbers:18B162); Changsha University of Science and Technology(grant numbers:2018IC23);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9120172"",""Social welfare maximization";congestion balance;charging strategy;"electric vehicle"",""Cascading style sheets";Optimization;Charging stations;Power grids;Pricing;Electric vehicles;"Stochastic processes"","""",""33"","""",""43"",""IEEE"",""18 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Cooperative Memory Expansion via OS Kernel Support for Networked Computing Systems,""P. Srinuan"; X. Yuan;" N. -F. Tzeng"",""School of Computing and Informatics, University of Louisiana, Lafayette, USA"; School of Computing and Informatics, University of Louisiana, Lafayette, USA;" School of Computing and Informatics, University of Louisiana, Lafayette, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Jun 2020"",""2020"",""31"",""11"",""2650"",""2667"",""The growing popularity of in-memory computing for bigdata analytics often causes performance bottlenecks to memory subsystem resided in operating systems (OS). This article purposes cooperative memory expansion (COMEX), an OS kernel extension. COMEX establishes a stable pool of memory collectively across nodes in a cluster and enhances OS's memory subsystem for memory aggregation from connected machines by allowing process's page table to track remote memory page frames without programmer effort or modifications to application codes. COMEX employs Remote Direct Memory Access (RDMA) for low-latency data transfer with destination kernel bypassed and does not rely on an old design of the I/O block subsystem usually adopted by all known remote paging. COMEX fits soundly in the emerging system design approach of resource disaggregation which breaks hard walls between server-centric machines into a new design paradigm of separated resource pools. The new architecture facilitates both system scaling-up and scaling-out, also eliminates imbalance resources existing in datacenters. We have implemented COMEX based on Linux kernel 3.10.87 and deployed on our 32 networked servers. Performance evaluation results under ten applications from two benchmark suites reveal the speedup of up to 170 times when application execution footprints are 10 times larger than available system memory."",""1558-2183"","""",""10.1109/TPDS.2020.2999507"",""National Science Foundation(grant numbers:CCF-1423302,CNS-1527051,III-1763620)"; Louisiana Board of Regents(grant numbers:LEQSF(2018-21)-RD-A-24);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9107499"",""I/O block devices";memory management;networked computer systems;operating systems (OS);page tables;"remote direct memory access (RDMA)"",""Memory management";Kernel;Servers;Random access memory;Slabs;"Performance evaluation"","""",""3"","""",""70"",""CCBY"",""3 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"COPA: Highly Cost-Effective Power Back-Up for Green Datacenters,""Y. Yin"; J. Wu; X. Zhou; L. Eeckhout; A. Qouneh; T. Li;" Z. Yu"",""Department of Computer Science, University of Science and Technology of China, Hefei, China"; Department of Computer Science, University of Science and Technology of China, Hefei, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Science, Sangfor Technologies Inc., Shenzhen, China; Ghent University, Gent, Belgium; New Western England University, Springfield, USA; Chinese Academy of Science, Shenzhen Institutes of Advanced Technology, Shenzhen, China;" Chinese Academy of Science, Shenzhen Institutes of Advanced Technology, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""967"",""980"",""Traditional datacenters employ costly diesel generators (DG) and uninterrupted power supplies (UPS) to back up power. However, some or even all racks of a green datacenter can still be powered by renewable energy during grid power outages. This makes the utilization of the DGs and UPSs in green datacenters significantly lower than in traditional datacenters. In this paper, we propose a highly cost-effective power back-up (COPA) approach for green datacenters by leveraging the availability characteristics of renewable energy as well as grid power outages. COPA contributes three new techniques. The first technique, called least UPS capacity planning, determines the least rated power capability and runtime of the UPSs to guarantee the normal operations of a green datacenter during grid power outages. The second technique, named cooperative UPS/renewable power supply, employs UPS and renewable energy at the same time to supply power to each rack when grid power fails. The last one, dubbed renewable-energy-aware dynamic power management, controls the power consumption dynamically based on the available capacity of renewable energy and UPS. We build an experimental cluster consisting of 10 servers, and use four representative benchmarks as well as verified data about the availability characteristics of solar and wind energy to evaluate COPA. The results show that COPA reduces 47 percent and 70 percent of the power back-up cost for a solar energy powered datacenter and a wind energy powered datacenter, respectively. Moreover, COPA guarantees the application's Service Level Agreement (SLA) for at least 20 minutes (over 79 percent outages) and 56 minutes on average while enabling the back-up power to last for at least 2 hours and for 3 hours on average, which cannot be achieved by other under-provisioning power back-up approaches."",""1558-2183"","""",""10.1109/TPDS.2019.2948336"",""National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000204)"; National Natural Science Foundation of China(grant numbers:61672511,61702495,61802384); China Postdoctoral Science Foundation(grant numbers:2017M622830); NSF of Guangdong province(grant numbers:2017A030310350); Chinese Academy of Sciences;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877859"",""Datacenter power backup";renewable energy;cooperative power distribution;"dynamic power management"",""Uninterruptible power systems";Renewable energy sources;Power system reliability;Green products;Power supplies;Power system management;"Switches"","""",""5"","""",""61"",""IEEE"",""21 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Correlation of Performance Optimizations and Energy Consumption for Stencil-Based Application on Intel Xeon Scalable Processors,""L. Szustak"; R. Wyrzykowski; T. Olas;" V. Mele"",""Department of Computer and Information Science, Czestochowa University of Technology, Częstochowa, Poland"; Department of Computer and Information Science, Czestochowa University of Technology, Częstochowa, Poland; Department of Computer and Information Science, Czestochowa University of Technology, Częstochowa, Poland;" Department of Mathematics and Applications R. Caccioppoli, University of Naples Federico II, Napoli, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jun 2020"",""2020"",""31"",""11"",""2582"",""2593"",""This article provides a comprehensive study of the impact of performance optimizations on the energy efficiency of a real-world CFD application called MPDATA, as well as an insightful analysis of performance-energy interaction of these optimizations with the underlying hardware that represents the first generation of Intel Xeon Scalable processors. Considering the MPDATA iterative application as a use case, we explore the fundamentals of energy and performance analysis for a memory-bound application when exposed to a set of optimization steps that increase the application performance, by improving the operational intensity of code and utilizing resources more efficiently. It is shown that for memory-bound applications, optimizing toward high performance could be a powerful strategy for improving the energy efficiency as well. In fact, for the considered performance optimizations, the energy gain is correlated with the performance gain but with varying degrees. As a result, these optimizations allow improving both performance and energy consumption radically, up to about 10.9 and 8.8 times, respectively. The impact of the Intel AVX-512 SIMD extension on the energy consumption and performance is demonstrated. Also, we discover limitations on the usability of CPU frequency scaling as a tool for balancing energy savings with admissible performance losses."",""1558-2183"","""",""10.1109/TPDS.2020.2996314"",""National Science Centre, Poland(grant numbers:UMO-2017/26/D/ST6/00687,UMO-2015/17/D/ST6/04059)"; Polish Minister of Science and Higher Education(grant numbers:020/RID/2018/19);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103117"",""CFD";MPDATA;intel xeon scalable;performance-energy trade-off;yokogawa WT310;"RAPL"",""Optimization";Energy consumption;Hardware;Program processors;Platinum;"Energy measurement"","""",""12"","""",""43"",""CCBY"",""28 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Cost-Aware Partitioning for Efficient Large Graph Processing in Geo-Distributed Datacenters,""A. C. Zhou"; B. Shen; Y. Xiao; S. Ibrahim;" B. He"",""National Engineering Lab for Big Data System Computing Technology, Shenzhen University, Shenzhen, China"; National Engineering Lab for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; National Engineering Lab for Big Data System Computing Technology, Shenzhen University, Shenzhen, China; IMT Atlantique, LS2N, Inria, Nantes, France;" National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Mar 2020"",""2020"",""31"",""7"",""1707"",""1723"",""Graph processing is an emerging computation model for a wide range of applications and graph partitioning is important for optimizing the cost and performance of graph processing jobs. Recently, many graph applications store their data on geo-distributed datacenters (DCs) to provide services worldwide with low latency. This raises new challenges to existing graph partitioning methods, due to the multi-level heterogeneities in network bandwidth and communication prices in geo-distributed DCs. In this article, we propose an efficient graph partitioning method named Geo-Cut, which takes both the cost and performance objectives into consideration for large graph processing in geo-distributed DCs. Geo-Cut adopts two optimization stages. First, we propose a cost-aware streaming heuristic and utilize the one-pass streaming graph partitioning method to quickly assign edges to different DCs while minimizing inter-DC data communication cost. Second, we propose two partition refinement heuristics which identify the performance bottlenecks of geo-distributed graph processing and refine the partitioning result obtained in the first stage to reduce the inter-DC data transfer time while satisfying the budget constraint. Geo-Cut can be also applied to partition dynamic graphs thanks to its lightweight runtime overhead. We evaluate the effectiveness and efficiency of Geo-Cut using real-world graphs with both real geo-distributed DCs and simulations. Evaluation results show that Geo-Cut can reduce the inter-DC data transfer time by up to 79 percent (42 percent as the median) and reduce the monetary cost by up to 75 percent (26 percent as the median) compared to state-of-the-art graph partitioning methods with a low overhead."",""1558-2183"","""",""10.1109/TPDS.2019.2955494"",""National Natural Science Foundation of China(grant numbers:61802260)"; Natural Science Foundation of Guangdong Province(grant numbers:2018A030310440,2019A1515012053); Shenzhen Science and Technology Foundation(grant numbers:JCYJ20180305125737520); Natural Science Foundation of SZU(grant numbers:827-000370,827-000175,860-000002110319); Guangdong Province Key Laboratory of Popular High Performance Computers(grant numbers:2017B030314073); ANR KerStream project(grant numbers:ANR-16-CE25-0014-01); MoE AcRF Tier 1(grant numbers:T1 251RES1610); National Natural Science Foundation of China(grant numbers:61929103);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911231"",""Graph processing";wide area network;"geo-distributed datacenters"",""Bandwidth";Wide area networks;Data transfer;Downlink;Internet;"Uplink"","""",""10"","""",""37"",""IEEE"",""25 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Countdown Slack: A Run-Time Library to Reduce Energy Footprint in Large-Scale MPI Applications,""D. Cesarini"; A. Bartolini; A. Borghesi; C. Cavazzoni; M. Luisier;" L. Benini"",""Department of SuperComputing Applications and Innovation, CINECA, Casalecchio di Reno (BO), Italy"; Department of Electrical, Electronic and Information Engineering “Guglielmo Marconi”, University of Bologna, Bologna, Italy; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Chief Technology and Innovation Officer, Leonardo S.p.A., Roma, Italy; Department of Information Technology and Electrical Engineering, Swiss Federal Institute of Technology in Zurich, Zurich, Switzerland;" Department of Electrical, Electronic and Information Engineering “Guglielmo Marconi”, University of Bologna, Bologna, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jun 2020"",""2020"",""31"",""11"",""2696"",""2709"",""The power consumption of supercomputers is a major challenge for system owners, users, and society. It limits the capacity of system installations, it requires large cooling infrastructures, and it is the cause of a large carbon footprint. Reducing power during application execution without changing the application source code or increasing time-to-completion is highly desirable in real-life high-performance computing scenarios. The power management run-time frameworks proposed in the last decade are based on the assumption that the duration of communication and application phases in an MPI application can be predicted and used at run-time to trade-off communication slack with power consumption. In this article, we first show that this assumption is too general and leads to mispredictions, slowing down applications, thereby jeopardizing the claimed benefits. We then propose a new approach based on (i) the separation of communication phases and slack during MPI calls and (ii) a timeout algorithm to cope with the hardware power management latency, which jointly makes it possible to achieve performance-neutral power saving in MPI applications without requiring labor-intensive and risky application source code modifications. We validate our approach in a tier-1 production environment with widely adopted scientific applications. Our approach has a time-to-completion overhead lower than 1 percent, while it successfully exploits slack in communication phases to achieve an average energy saving of 10 percent. If we focus on a large-scale application runs, the proposed approach achieves 22 percent energy saving with an overhead of only 0.4 percent. With respect to state-of-the-art approaches, COUNTDOWN Slack is the only that always leads to an energy saving with negligible overhead (<";" 3 percent)."",""1558-2183"","""",""10.1109/TPDS.2020.3000418"",""EU H2020(grant numbers:857191)"; EU H2020-INFRAEDI-2018-1 MaX(grant numbers:824143);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9109637"",""HPC";MPI;DVFS;power management;DVFS;P-states;energy saving;power saving;"reactive policy"",""Power system management";Hardware;Power demand;Cooling;Runtime;Task analysis;"Monitoring"","""",""6"","""",""60"",""IEEE"",""5 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Crocus: Enabling Computing Resource Orchestration for Inline Cluster-Wide Deduplication on Scalable Storage Systems,""P. Hamandawana"; A. Khan; C. -G. Lee; S. Park;" Y. Kim"",""Department of Computer Engineering, Ajou University, Suwon, Republic of Korea"; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea;" Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Mar 2020"",""2020"",""31"",""8"",""1740"",""1753"",""Inline deduplication dramatically improves storage space utilization. However, it degrades I/O throughput due to computeintensive deduplication operations such as chunking, fingerprinting or hashing of chunk content, and redundant lookup I/Os over the network in the I/O path. In particular, the fingerprint or hash generation of content contributes largely to the degraded I/O throughput and is computationally expensive. In this article, we propose CROCUS, a framework that enables compute resource orchestration to enhance cluster-wide deduplication performance. In particular, CROCUS takes into account all compute resources such as local and remote {CPU, GPU} by managing decentralized compute pools. An opportunistic Load-Aware Fingerprint Scheduler (LAFS), distributes and offloads compute-intensive deduplication operations in a load-aware fashion to compute pools. CROCUS is highly generic and can be adopted in both inline and offline deduplication with different storage tier configurations. We implemented CROCUS in Ceph scale-out storage system. Our extensive evaluation shows that CROCUS reduces the fingerprinting overhead by 86 percent with 4KB chunk size compared to Ceph with baseline deduplication while maintaining high disk-space savings. Our proposed LAFS scheduler, when tested in different internal and external contention scenarios also showed 54 percent improvement over a fixed or static scheduling approach."",""1558-2183"","""",""10.1109/TPDS.2020.2972882"",""National Research Foundation of Korea(grant numbers:NRF-2018R1A1A1A05079398)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8993857"",""Distributed file systems";scheduling;"storage management"",""Servers";Degradation;Graphics processing units;Computer architecture;Message systems;Metadata;"Throughput"","""",""9"","""",""47"",""IEEE"",""11 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Cross-Rack-Aware Updates in Erasure-Coded Data Centers: Design and Evaluation,""Z. Shen";" P. P. C. Lee"",""School of Informatics, Xiamen University, Xiamen, China";" Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""13 May 2020"",""2020"",""31"",""10"",""2315"",""2328"",""The update performance in erasure-coded data centers is often bottlenecked by the constrained cross-rack bandwidth. We propose CAU, a cross-rack-aware update mechanism that aims to mitigate the cross-rack update traffic in erasure-coded data centers. CAU builds on three design elements: (i) selective parity updates, which select the appropriate parity update approach based on the update pattern and the data layout to reduce the cross-rack update traffic"; (ii) data grouping, which relocates and groups updated data chunks in the same rack to further reduce the cross-rack update traffic;" and (iii) interim replication, which stores a specified number of temporary replicas for each newly updated data chunk. We evaluate CAU via trace-driven analysis, local cluster experiments, and Amazon EC2 experiments. We show that CAU enhances state-of-the-arts by mitigating the cross-rack update traffic as well as maintaining high update performance in both local cluster and geo-distributed environments."",""1558-2183"","""",""10.1109/TPDS.2020.2991021"",""Research Grants Council of the Hong Kong Special Administrative Region, China(grant numbers:GRF 14216316 and AoE/P-404/18)"; National Natural Science Foundation of China(grant numbers:61602120); Xidian University(grant numbers:ISN21-19);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080119"",""Erasure coding";data centers;"cross-rack-aware updates"",""Encoding";Bandwidth;Fault tolerance;Fault tolerant systems;Data centers;"Distributed databases"","""",""6"","""",""40"",""IEEE"",""28 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"cuPC: CUDA-Based Parallel PC Algorithm for Causal Structure Learning on GPU,""B. Zarebavani"; F. Jafarinejad; M. Hashemi;" S. Salehkaleybar"",""Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran"; Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran; Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran;" Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""530"",""542"",""The main goal in many fields in the empirical sciences is to discover causal relationships among a set of variables from observational data. PC algorithm is one of the promising solutions to learn underlying causal structure by performing a number of conditional independence tests. In this paper, we propose a novel GPU-based parallel algorithm, called cuPC, to execute an order-independent version of PC. The proposed solution has two variants, cuPC-E and cuPC-S, which parallelize PC in two different ways for multivariate normal distribution. Experimental results show the scalability of the proposed algorithms with respect to the number of variables, the number of samples, and different graph densities. For instance, in one of the most challenging datasets, the runtime is reduced from more than 11 hours to about 4 seconds. On average, cuPC-E and cuPC-S achieve 500X and 1300X speedup, respectively, compared to serial implementation on CPU."",""1558-2183"","""",""10.1109/TPDS.2019.2939126"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823064"",""Bayesian networks";causal discovery;CUDA;GPU;machine learning;parallel processing;"PC algorithm"",""Markov processes";Graphics processing units;Bayes methods;Graphical models;Parallel algorithms;"Scalability"","""",""16"","""",""43"",""IEEE"",""3 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"CURE: A High-Performance, Low-Power, and Reliable Network-on-Chip Design Using Reinforcement Learning,""K. Wang";" A. Louri"",""Department of Electrical and Computer Engineering, George Washington University, Washington, USA";" Department of Electrical and Computer Engineering, George Washington University, Washington, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""1 May 2020"",""2020"",""31"",""9"",""2125"",""2138"",""We propose CURE, a deep reinforcement learning (DRL)-based NoC design framework that simultaneously reduces network latency, improves energy-efficiency, and tolerates transient errors and permanent faults. CURE has several architectural innovations and a DRL-based hardware controller to manage design complexity and optimize trade-offs. First, in CURE, we propose reversible multi-function adaptive channels (RMCs) to reduce NoC power consumption and network latency. Second, we implement a new fault-secure adaptive error correction hardware in each router to enhance reliability for both transient errors and permanent faults. Third, we propose a router power-gating and bypass design that powers off NoC components to reduce power and extend chip lifespan. Further, for the complex dynamic interactions of these techniques, we propose using DRL to train a proactive control policy to provide improved fault-tolerance, reduced power consumption, and improved performance. Simulation using the PARSEC benchmark shows that CURE reduces end-to-end packet latency by 39 percent, improves energy efficiency by 92 percent, and lowers static and dynamic power consumption by 24 and 38 percent, respectively, over conventional solutions. Using mean-time-to-failure, we show that CURE is 7.7× more reliable than the conventional NoC design."",""1558-2183"","""",""10.1109/TPDS.2020.2986297"",""National Science Foundation(grant numbers:CCF-1420718,CCF1513606,CCF-1703013,CCF-1547034,CCF-1547035,CCF-1540736,CCF-1702980)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9061016"",""Computer architecture";network-on-chip(NoC);reliability;"deep reinforcement learning"",""Buffer storage";Hardware;Circuit faults;Reliability;Error correction;Power demand;"Transistors"","""",""16"","""",""45"",""IEEE"",""8 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Customer Perceived Value- and Risk-Aware Multiserver Configuration for Profit Maximization,""T. Wang"; J. Zhou; G. Zhang; T. Wei;" S. Hu"",""School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China"; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Technology, East China Normal University, Shanghai, China;" School of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1074"",""1088"",""Along with the wide deployment of infrastructures and the rapid development of virtualization techniques in cloud computing, more and more enterprises begin to adopt cloud services, inspiring the emergence of various cloud service providers. The goal of cloud service providers is to pursue profit maximization. To achieve this goal, cloud service providers need to have a good understanding of the economics of cloud computing. However, the existing pricing strategies rarely consider the interaction between user requests for services and the cloud service provider and hence cannot accurately reflect the supply and demand law of the cloud service market. In addition, few previous pricing strategies take into account the risk involved in the pricing contract. In this article, we first propose a dynamic pricing strategy that is developed based on the customer perceived value (CPV) and is able to accurately capture the real situation of supply and demand in marketing. The strategy is utilized to estimate the user's demand for cloud services. We then design a profit maximization scheme that is developed based on the CPV-aware dynamic pricing strategy and considers the risk in the pricing contract. The scheme is utilized to derive the optimal multiserver configuration for maximizing the profit. Extensive simulations are carried out to verify the proposed customer perceived value and risk-aware profit maximization scheme. As compared to two state of the art benchmarking methods, the proposed scheme gains 31.6 and 30.8 percent more profit on average, respectively."",""1558-2183"","""",""10.1109/TPDS.2019.2960024"",""National Natural Science Foundation of China(grant numbers:61802185,61272420)"; Natural Science Foundation of Jiangsu Province(grant numbers:BK20180470); Fundamental Research Funds for the Central Universities(grant numbers:30919011233); The Hubei Key Laboratory of Intelligent Geo-Information Processing(grant numbers:KLIGIP-2018A04); Natural Science Foundation of Shanghai(grant numbers:16ZR1409000);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933484"",""Cloud computing";customer perceived value;dynamic pricing;multiserver configuration;profit maximization;"risk"",""Cloud computing";Pricing;Servers;Contracts;Computational modeling;Task analysis;"Supply and demand"","""",""28"","""",""42"",""IEEE"",""16 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Customizable Scale-Out Key-Value Stores,""A. Anwar"; Y. Cheng; H. Huang; J. Han; H. Sim; D. Lee; F. Douglis;" A. R. Butt"",""IBM Research-Almaden, San Jose, USA"; George Mason University, Fairfax, USA; IBM Research–T.J. Watson, Ossining, USA; Virginia Tech, Blacksburg, USA; Oak Ridge National Laboratory, Oak Ridge, USA; Stony Brook University, Stony Brook, USA; Perspecta Labs, Basking Ridge, USA;" Virginia Tech, Blacksburg, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Apr 2020"",""2020"",""31"",""9"",""2081"",""2096"",""Enterprise KV stores are often not well suited for HPC applications, and thus cumbersome end-to-end KV design customization is required to meet the needs of modern HPC applications. To this end, in this article we present bespoKV, an adaptive, extensible, and scale-out KV store framework. bespoKV decouples the KV store design into the control plane for distributed management and the data plane for local data store. For the control plane, bespoKVprovides pre-built modules, called controlets, supporting common distributed functionalities (e.g., replication, consistency, and topology) and their various combinations. This decoupling allows bespoKV to take a user-provided single-server KV store, called a datalet, and transparently enables a scalable and fault-tolerant distributed KV store service. The resulting distributed stores are also adaptive to consistency or topology requirement changes and can be easily extended for new types of services. Such specializations enable innovative uses of KV stores in HPC applications, especially for emerging applications that utilize KV-friendly workloads. We evaluate bespoKV in a local testbed as well as in a public cloud settings. Experiments show that bespoKV-enabled distributed KV stores scale horizontally to a large number of nodes, and performs comparably and sometimes 1.2× to 2.6× better than the state-of-the-art systems."",""1558-2183"","""",""10.1109/TPDS.2020.2982640"",""National Science Foundation(grant numbers:CNS-1565314,CNS-1405697,CNS-1615411,CCF-1919075,CCF-1919113,CNS-1814430)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050561"",""Key-value stores";HPC KV stores;scale-out KV stores;"application tailored storage"",""Topology";Distributed databases;Peer-to-peer computing;Fault tolerance;Fault tolerant systems;Nonvolatile memory;"Cloud computing"","""",""4"","""",""81"",""IEEE"",""30 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"cuTensor-Tubal: Efficient Primitives for Tubal-Rank Tensor Learning Operations on GPUs,""T. Zhang"; X. -Y. Liu; X. Wang;" A. Walid"",""School of Computer Engineering and Science, Shanghai University, Shanghai, China"; Department of Electrical Engineering, Columbia University, New York, USA; Department of Electrical Engineering, Columbia University, New York, USA;" Nokia-Bell Labs, Murray Hill, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""595"",""610"",""Tensors are the cornerstone data structures in high-performance computing, big data analysis and machine learning. However, tensor computations are compute-intensive and the running time increases rapidly with the tensor size. Therefore, designing high-performance primitives on parallel architectures such as GPUs is critical for the efficiency of ever growing data processing demands. Existing GPU basic linear algebra subroutines (BLAS) libraries (e.g., NVIDIA cuBLAS) do not provide tensor primitives. Researchers have to implement and optimize their own tensor algorithms in a case-by-case manner, which is inefficient and error-prone. In this paper, we develop the cuTensor-tubal library of seven key primitives for the tubal-rank tensor model on GPUs: t-FFT, inverse t-FFT, t-product, t-SVD, t-QR, t-inverse, and t-normalization. cuTensor-tubal adopts a frequency domain computation scheme to expose the separability in the frequency domain, then maps the tube-wise and slice-wise parallelisms onto the single instruction multiple thread (SIMT) GPU architecture. To achieve good performance, we optimize the data transfer, memory accesses, and design the batched and streamed parallelization schemes for tensor operations with data-independent and data-dependent computation patterns, respectively. In the evaluations oft-product, t-SVD, t-QR, t-inverse and t-normalization, cuTensor-tubal achieves maximum 16.91x, 27.03x, 38.97x, 22.36x,15.43x speedups respectively over the CPU implementations running on dual 10-core Xeon CPUs. Two applications, namely, t-SVD-based video compression and low-tubal-rank tensor completion, are tested using our library and achieve maximum 9.80x and 269.26x speedups over multi-core CPU implementations."",""1558-2183"","""",""10.1109/TPDS.2019.2940192"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827946"",""Low-tubal-rank tensor decomposition";GPU;cuTensor-tubal library;t-SVD;"tensor completion"",""Graphics processing units";Libraries;Frequency-domain analysis;Matrix decomposition;Computational modeling;"Computer architecture"","""",""12"","""",""51"",""IEEE"",""10 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Data-Driven Derivation of an Analytic Model for Parallel Servers With Job Replication,""N. Bajunaid";" D. A. Menascé"",""King Abdulaziz University, Saudi Arabia";" Department of Computer Science, George Mason University, Fairfax, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""20 May 2020"",""2020"",""31"",""10"",""2435"",""2452"",""The job replication problem has been studied recently as a mechanism to improve performance and availability of systems with n parallel servers, each with its own queue. A dispatcher using some policy sends d (1 ≤ d ≤ n) copies of a job to d of the servers. Copies are eliminated from the system as soon as the first copy completes from any of the d servers. This article introduces a datadriven method to derive closed-form expressions for the average response time and other metrics of jobs as a function of the degree of replication d. This method consists of developing a simulator for the system in order to generate a very large number of datasets for a wide range of input parameters. A statistical and visualization analysis of the data provides the analytical models. It is important to emphasize the difference between using simulation methods to obtain the value of metrics (e.g., average response time) of a computer system given values of input parameters and using our data-driven method to obtain closed-form expressions that relate output metrics to input parameters. The latter is the focus of our approach. The analysis presented here covers results for homogeneous and heterogeneous servers with exponentially distributed service times and for homogeneous servers with hypo-exponentially and hyper-exponentially distributed service times. This article also presents a closed-form equation for the optimal replication degree for the case of homogeneous servers with hypo-exponentially distributed service times."",""1558-2183"","""",""10.1109/TPDS.2020.2992571"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086162"",""Parallel servers";job replication;data-driven analytic model derivation;"simulation"",""Servers";Analytical models;Computational modeling;Time factors;Measurement;Closed-form solutions;"Mathematical model"","""",""2"","""",""20"",""IEEE"",""4 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Data-Parallel Hashing Techniques for GPU Architectures,""B. Lessley";" H. Childs"",""Department of Computer and Information Science, University of Oregon, Eugene, USA";" Department of Computer and Information Science, University of Oregon, Eugene, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""237"",""250"",""Hash tables are a fundamental data structure for effectively storing and accessing sparse data, with widespread usage in domains ranging from computer graphics to machine learning. This study surveys the state-of-the-art research on data-parallel hashing techniques for emerging massively-parallel, many-core GPU architectures. This survey identifies key factors affecting the performance of different techniques and suggests directions for further research."",""1558-2183"","""",""10.1109/TPDS.2019.2929768"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765787"",""Graphics processors";hash tables;parallel algorithms;"search problems"",""Graphics processing units";Instruction sets;Parallel processing;Message systems;Hash functions;"Data structures"","""",""16"","""",""75"",""IEEE"",""18 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Decentralized Utility- and Locality-Aware Replication for Heterogeneous DHT-Based P2P Cloud Storage Systems,""Y. Hassanzadeh-Nazarabadi"; A. Küpçü;" O. Ozkasap"",""Department of Computer Engineering, Koç University, Istanbul, Turkey"; Department of Computer Engineering, Koç University, Istanbul, Turkey;" Department of Computer Engineering, Koç University, Istanbul, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1183"",""1193"",""As a Distributed Hash Table (DHT), Skip Graph routing overlays are exploited in several peer-to-peer (P2P) services, including P2P cloud storage. The fully decentralized replication algorithms that are applicable to the Skip Graph-based P2P cloud storage fail on improving the performance of the system with respect to both the availability of replicas as well as their response time. Additionally, they presume the system as homogeneous with respect to the nodes' latency distribution, availability behavior, and bandwidth, or storage. In this article, we propose Pyramid, which is the first fully decentralized utility- and locality-aware replication approach for Skip Graph-based P2P cloud storage systems. Pyramid considers the nodes as heterogeneous with respect to their latency distribution, availability behavior, bandwidth, and storage. Pyramid is utility-aware as it maximizes the average available bandwidth of replicas per time slot (e.g., per hour). Additionally, Pyramid is locality-aware as it minimizes the average latency between nodes and their closest replica. Our simulation results show that compared to the state-of-the-art solutions that either perform good in utility-awareness, or in locality-awareness, our proposed Pyramid improves both the utility- and locality-awareness of replicas with a gain of about 1.2 and 1.1 times at the same time, respectively."",""1558-2183"","""",""10.1109/TPDS.2019.2960018"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933479"",""P2P systems";cloud storage;distributed hash tables;skip graphs;replication;availability;"locality"",""Peer-to-peer computing";Cloud computing;Bandwidth;IP networks;Routing;Numerical models;"Protocols"","""",""18"","""",""47"",""IEEE"",""16 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Deep Learning Research and Development Platform: Characterizing and Scheduling with QoS Guarantees on GPU Clusters,""Z. Chen"; W. Quan; M. Wen; J. Fang; J. Yu; C. Zhang;" L. Luo"",""Department of Computer, National University of Defense Technology, Changsha, China"; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China; Department of Computer, National University of Defense Technology, Changsha, China;" Department of Computer, National University of Defense Technology, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""34"",""50"",""Deep learning (DL) has been widely adopted in various domains of artificial intelligence (AI), achieving dramatic developments in industry and academia. Besides giant AI companies, numerous small and medium-sized enterprises, institutes, and universities (EIUs) have focused on the research and development (R&D) of DL. Considering the high cost of datacenters and high performance computing (HPC) systems, EIUs prefer adopting off-the-shelf GPU clusters as a DL R&D platform for multiple users and developers to process diverse DL workloads. In such scenarios, the scheduling of multiple DL tasks on a shared GPU cluster is both significant and challenging in terms of efficiently utilizing limited resources. Existing schedulers cannot predict the resource requirements of diverse DL workloads, leading to the under-utilization of computing resources and a decline in user satisfaction. This paper proposes GENIE, a QoS-aware dynamic scheduling framework for a shared GPU cluster, which achieves users' QoS guarantee and high system utilization. In accordance with an exhaustive characterization, GENIE analyzes the key factors that affect the performance of DL tasks and proposes a prediction model derived from lightweight profiling to estimate the processing rate and response latency for diverse DL workloads. Based on the prediction models, we propose a QoS-aware scheduling algorithm to identify the best placements for DL tasks and schedule them on the shared cluster. Experiments on a GPU cluster and large-scale simulations demonstrate that GENIE achieves a QoS-guarantee percentage improvement of up to 67.4 percent and a makespan reduction of up to 28.2 percent, compared to other baseline schedulers."",""1558-2183"","""",""10.1109/TPDS.2019.2931558"",""National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000400,2018YFB0204300)"; National Natural Science Foundation of China(grant numbers:61872377,61802417,61802420); NUDT Science Foundation(grant numbers:ZK18-03-40);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8778770"",""DL research and development platform";characterizing;scheduling;QoS-aware;"GPU clusters"",""Task analysis";Graphics processing units;Research and development;Quality of service;Job shop scheduling;Training;"Predictive models"","""",""26"","""",""42"",""IEEE"",""29 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Designing Energy-Efficient MPSoC with Untrustworthy 3PIP Cores,""Y. Sun"; G. Jiang; S. -K. Lam;" F. Ning"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore"; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore;" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""51"",""63"",""The adoption of large-scale MPSoCs and the globalization of the IC design flow give rise to two major concerns: high power density due to continuous technology scaling and security due to the untrustworthiness of the third-party intellectual property (3PIP) cores. However, little work has been undertaken to consider these two critical issues jointly during the design stage. In this paper, we propose a design methodology that minimizes the energy consumption while simultaneously protecting the MPSoC against the effects of hardware trojans. The proposed methodology consists of three main stages: 1) Task scheduling to introduce core diversity in the MPSoC in order to detect the presence of malicious modifications in the cores, or mute their effects at runtime, 2) Vendor assignment to the cores using a novel heuristic that chooses vendor-specific cores with operating speed that minimizes the total energy consumption of the MPSoC, and 3) Explore optimization opportunities for further energy savings by minimizing idle periods on the cores, which are caused by the inter-task data dependencies. Experimental results show that our solutions consume only 1/3 energy of existing solutions without increasing schedule length while satisfying the security constraints."",""1558-2183"","""",""10.1109/TPDS.2019.2926721"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758837"",""3PIP cores";untrustworthy;energy efficient;scheduling;vendor assignment;"speed optimization"",""Trojan horses";Security;Task analysis;Energy consumption;Schedules;Hardware;"Optimization"","""",""2"","""",""47"",""IEEE"",""10 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Deterministic Data Distribution for Efficient Recovery in Erasure-Coded Storage Systems,""L. Xu"; M. Lyu; Z. Li; Y. Li;" Y. Xu"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2020"",""2020"",""31"",""10"",""2248"",""2262"",""Due to individual unreliable commodity components, failures are common in large-scale distributed storage systems. Erasure codes are widely deployed in practical storage systems to provide fault tolerance with low storage overhead. However, random data distribution (RDD), commonly used in erasure-coded storage systems, induces heavy cross-rack traffic, load imbalance, and random access, which adversely affects failure recovery. In this article, with orthogonal arrays, we define a Deterministic Data Distribution (D3) to uniformly distribute data/parity blocks among nodes, and propose an efficient failure recovery approach based on D3, which minimizes the cross-rack repair traffic against a single node failure. Thanks to the uniformity of D3, the proposed recovery approach balances the repair traffic not only among nodes within a rack but also among racks. We implement D3 over Reed-Solomon codes and Locally Repairable Codes in Hadoop Distributed File System (HDFS) with a cluster of 28 machines. Compared with RDD, our experiments show that D3 significantly speeds up the failure recovery up to 2.49 times for RS codes and 1.38 times for LRCs. Moreover, D3 supports front-end applications better than RDD in both of normal and recovery states."",""1558-2183"","""",""10.1109/TPDS.2020.2987837"",""National Key R&D Program of China(grant numbers:2018YFB1003204)"; National Natural Science Foundation of China(grant numbers:61832011,61772486);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9069296"",""Distributed storage system";erasure coding;traffic;orthogonal array;"load balance"",""Arrays";Bandwidth;Maintenance engineering;Layout;Fault tolerance;"Fault tolerant systems"","""",""14"","""",""44"",""IEEE"",""16 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Distributed Graph Computation Meets Machine Learning,""W. Xiao"; J. Xue; Y. Miao; Z. Li; C. Chen; M. Wu; W. Li;" L. Zhou"",""Alibaba Group, Hangzhou, China"; Microsoft Research, China; Microsoft Research, China; Google, Mountain View, USA; ByteDance, Beijing, China; Conflux Foundation, Singapore; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China;" Microsoft Research, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Feb 2020"",""2020"",""31"",""7"",""1588"",""1604"",""TuX2 is a new distributed graph engine that bridges graph computation and distributed machine learning. TuX2 inherits the benefits of elegant graph computation model, efficient graph layout, and balanced parallelism to scale to billion-edge graphs, while extended and optimized for distributed machine learning to support heterogeneity in data model, Stale Synchronous Parallel in scheduling, and a new Mini-batch, Exchange, GlobalSync, and Apply (MEGA) model for programming. TuX2 further introduces a hybrid vertex-cut graph optimization and supports various consistency models in fault tolerance for machine learning. We have developed a set of representative distributed machine learning algorithms in TuX2, covering both supervised and unsupervised learning. Compared to the implementations on distributed machine learning platforms, writing those algorithms in TuX2 takes only about 25 percent of the code: our graph computation model hides the detailed management of data layout, partitioning, and parallelism from developers. The extensive evaluation of TuX2, using large datasets with up to 64 billion of edges, shows that TuX2 outperforms PowerGraph/PowerLyra, the state-of-the-art distributed graph engines, by an order of magnitude, while beating two state-of-the-art distributed machine learning systems by at least 60 percent."",""1558-2183"","""",""10.1109/TPDS.2020.2970047"",""National Natural Science Foundation of China(grant numbers:61472009)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8974443"",""Graph computing";distributed machine learning;heterogeneity;stale synchronous parallel;"MEGA model"",""Machine learning";Data models;Computational modeling;Engines;Machine learning algorithms;Layout;"Parallel processing"","""",""8"","""",""83"",""IEEE"",""29 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Distributed Training of Deep Learning Models: A Taxonomic Perspective,""M. Langer"; Z. He; W. Rahayu;" Y. Xue"",""BOSS ZhiPin Career Science Lab (CSL), Beijing, China"; La Trobe University, Bundoora, Australia; La Trobe University, Bundoora, Australia;" BOSS ZhiPin Career Science Lab (CSL), Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Jul 2020"",""2020"",""31"",""12"",""2802"",""2818"",""Distributed deep learning systems (DDLS) train deep neural network models by utilizing the distributed resources of a cluster. Developers of DDLS are required to make many decisions to process their particular workloads in their chosen environment efficiently. The advent of GPU-based deep learning, the ever-increasing size of datasets, and deep neural network models, in combination with the bandwidth constraints that exist in cluster environments require developers of DDLS to be innovative in order to train high-quality models quickly. Comparing DDLS side-by-side is difficult due to their extensive feature lists and architectural deviations. We aim to shine some light on the fundamental principles that are at work when training deep neural networks in a cluster of independent machines by analyzing the general properties associated with training deep learning models and how such workloads can be distributed in a cluster to achieve collaborative model training. Thereby we provide an overview of the different techniques that are used by contemporary DDLS and discuss their influence and implications on the training process. To conceptualize and compare DDLS, we group different techniques into categories, thus establishing a taxonomy of distributed deep learning systems."",""1558-2183"","""",""10.1109/TPDS.2020.3003307"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9120226"",""Survey";machine learning;deep learning;distributed systems;stochastic gradient descent;"big data"",""Deep learning";Training;Computational modeling;Parallel processing;Taxonomy;Data models;"Biological system modeling"","""",""36"","""",""51"",""IEEE"",""18 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Dynamic Undervolting to Improve Energy Efficiency on Multicore X86 CPUs,""P. Koutsovasilis"; K. Parasyris; C. D. Antonopoulos; N. Bellas;" S. Lalis"",""Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece"; University of Thessaly; Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece; Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece;" Department of Electrical and Computer Engineering, University of Thessaly, Volos, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jul 2020"",""2020"",""31"",""12"",""2851"",""2864"",""Chip manufacturers introduce redundancy at various levels of CPU design to guarantee correct operation, even for worst-case combinations of non-idealities in process variation and system operating conditions. This redundancy is implemented partly in the form of voltage margins. However, for a wide range of real-world execution scenarios these margins are excessive and merely translate to increased power consumption, hindering the effort towards higher-energy efficiency in both HPC and general purpose computing. Our study on the x86-64 Haswell and Skylake multicore microarchitectures reveals-wide voltage margins, which vary across different microarchitectures, different chip parts of the same microarchitecture, and across different workloads. We find that it is necessary to quantify-voltage margins using multi-threaded and multi-instance workloads, as characterization with single-threaded and single-instance workloads that do not stress the CPU to its full capacity typically identifies overly optimistic margins that lead to errors when applied in realistic program execution scenarios. In addition, we introduce, deploy and evaluate a run-time governor that dynamically reduces the supply voltage of modern multicore x86-64 CPUs. Our governor employs a model that takes as input a set of performance metrics which are directly measurable via performance monitoring counters and have high predictive value for the minimum tolerable supply voltage (Vmin), to predict and apply the appropriate reduction for the workload at hand. Compared with the conventional DVFS governor, our approach achieves up to 42 percent energy savings for the Skylake family and 34 percent for the Haswell family for complex, real-world applications."",""1558-2183"","""",""10.1109/TPDS.2020.3004383"",""European Commission(grant numbers:688540)"; U.S. Department of Energy(grant numbers:DEAC52-07NA2734 (LLNL-JRNL-809714));" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9123555"",""Energy efficiency";undervolting;low-energy and low-power technologies measurement;"modeling"",""Benchmark testing";Central Processing Unit;Multicore processing;Monitoring;Stress;Computer crashes;"Workstations"","""",""4"","""",""54"",""IEEE"",""23 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"EEPC: A Framework for Energy-Efficient Parallel Control of Connected Cars,""M. Shen"; G. Luo;" N. Xiao"",""School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China"; Center for Energy-Efficient Computing and Applications, School of Electronics Engineering and Computer Science, Peking University, Beijing, China;" School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""64"",""79"",""With the advanced communication sensors are deployed into the modern connected vehicles (CVs), large amounts of traffic information can be collected in real-time, which gives the chance to explore the various techniques to control the routing of CVs in a ground traffic network. However, the control of CVs often suffers from energy inefficiency due to the constant changes of network capacity and traffic demand. In this paper, we propose a cost-based iterative framework, named EEPC, to explore the energy-efficient parallel control of connected vehicles. EEPC enables the control of CVs to iteratively generate a feasible solution, where the control of each vehicle is guided in an energy-efficient way routing on its own trajectory. EEPC eliminates the conflicts between CVs with a limited number of iterations and in each iteration, EEPC enables each vehicle to coordinate with other vehicles for a same road resource of the traffic network, further determining which vehicle needs the resource most. Note that at each iteration, the imposed cost is updated to guide the coordination between CVs while the energy is always used to guide the control of CVs in EEPC. In addition, we also explore the parallel control of CVs to improve the real-time performance of EEPC. We provide two parallel approaches, one is fine grain and the other is coarse grain. The fine grain performs the parallel control of single-vehicle routing while the coarse grain performs the parallel control of multi-vehicle routing. Note that fine grain adopts multi-threading techniques and coarse grain adopts MPI techniques. The simulation results show that the proposed EEPC can generate a feasible control solution. Notably, we also demonstrate that the generated solution is effective in eliminating the resource conflicts between CVs and in suggesting an energy-efficient route to each vehicle. To the best of our knowledge, this is the first work to explore energy-efficient parallel control of CVs."",""1558-2183"","""",""10.1109/TPDS.2019.2930500"",""National Natural Science Foundation of China(grant numbers:61433019,61802446)"; Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); Guangdong Basic and Application Basic Research Teams(grant numbers:2018B030312002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770106"",""Cyber-physical system applications";connected vehicles;control and parallel control;energy-efficient control;"real-time control"",""Energy consumption";Roads;Control systems;Real-time systems;Routing;"Task analysis"","""","""","""",""19"",""IEEE"",""23 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Algorithms for Delay-Aware NFV-Enabled Multicasting in Mobile Edge Clouds With Resource Sharing,""H. Ren"; Z. Xu; W. Liang; Q. Xia; P. Zhou; O. F. Rana; A. Galis;" G. Wu"",""Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China"; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China; Research School of Computer Science, Australian National University, Canberra, Australia; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, International School of Information Science and Engineering, Dalian University of Technology, Dalian, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; Cardiff University, Cardiff, United Kingdom; University College London, London, United Kingdom;" Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Apr 2020"",""2020"",""31"",""9"",""2050"",""2066"",""Stringent delay requirements of many mobile applications have led to the development of mobile edge clouds, to offer low latency network services at the network edges. Most conventional network services are implemented via hardware-based network functions, including firewalls and load balancers, to guarantee service security and performance. However, implementing hardware-based network functions usually incurs both a high capital expenditure (CAPEX) and operating expenditure (OPEX). Network Function Virtualization (NFV) exhibits a potential to reduce CAPEX and OPEX significantly, by deploying software-based network functions in virtual machines (VMs) on edge-clouds. We consider a fundamental problem of NFV-enabled multicasting in a mobile edge cloud, where each multicast request has both service function chain and end-to-end delay requirements. Specifically, each multicast request requires chaining of a sequence of network functions (referred to as a service function chain) from a source to a set of destinations within specified end-to-end delay requirements. We devise an approximation algorithm with a provable approximation ratio for a single multicast request admission if its delay requirement is negligible";" otherwise, we propose an efficient heuristic. Furthermore, we also consider admissions of a given set of the delay-aware NFV-enabled multicast requests, for which we devise an efficient heuristic such that the system throughput is maximized, while the implementation cost of admitted requests is minimized. We finally evaluate the performance of the proposed algorithms in a real test-bed, and experimental results show that our algorithms outperform other similar approaches reported in literature."",""1558-2183"","""",""10.1109/TPDS.2020.2983918"",""National Natural Science Foundation of China(grant numbers:61802048,61802047)"; Fundamental Research Funds for the Central Universities(grant numbers:DUT17RC(3)061,DUT17RC(3)070,DUT19RC(4)035,DUT19GJ204); Australian Research Council(grant numbers:DP200101985); National Natural Science Foundation of China(grant numbers:61972448);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050847"",""Mobile edge clouds";network function virtualization;multicasting;approximation algorithms;"algorithm design"",""Multicast communication";Delays;Multicast algorithms;Approximation algorithms;Cloud computing;Optical switches;"Network function virtualization"","""",""47"","""",""51"",""IEEE"",""30 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Efficient and Portable Workgroup Size Tuning,""C. -L. Yu";" S. -L. Tsao"",""Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan";" Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""455"",""469"",""The performance of an OpenCL program is strongly influenced by both hardware and software attributes. To achieve superior performance, developers may leverage automatic performance tuning techniques to determine the optimal parameters on the target device. Although existing approaches have shown promising tuning results in their target scenarios, other requirements such as efficiency, portability, and usability should also be considered because of the rapid growth of heterogeneous computing applications and platforms. In this paper, we re-examine the workgroup size tuning problem and propose a novel approach to meet the aforementioned requirements. We abstract the architectural details into a set of hardware parameters so that the proposed approach can be applied without the presence of target devices, which makes it more accessible to developers. The proposed approach is evaluated on 20 OpenCL kernels and six devices, including both CPUs and GPUs. Experimental results demonstrate that, with negligible overhead, our approach filters out 88.6 percent of the possible workgroup sizes on average. Among all the workgroup size candidates, the bestand worst-performing candidates can achieve average performance of 95.5 and 92.1 percent, respectively, compared with the optimal workgroup size."",""1558-2183"","""",""10.1109/TPDS.2019.2937295"",""Ministry of Science and Technology of the People's Republic of China(grant numbers:108-2321-B-009-004-,108-2218-E-009-052-,107-2622-8-009-019-TA,107-2218-E-009-001)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812929"",""OpenCL";workgroup size selection;automatic performance tuning;"microbenchmarking"",""Tuning";Performance evaluation;Kernel;Hardware;Indexes;Computational modeling;"Graphics processing units"","""",""1"","""",""36"",""IEEE"",""26 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Compute-Intensive Job Allocation in Data Centers via Deep Reinforcement Learning,""D. Yi"; X. Zhou; Y. Wen;" R. Tan"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore"; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore;" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Feb 2020"",""2020"",""31"",""6"",""1474"",""1485"",""Reducing the energy consumption of the servers in a data center via proper job allocation is desirable. Existing advanced job allocation algorithms, based on constrained optimization formulations capturing servers' complex power consumption and thermal dynamics, often scale poorly with the data center size and optimization horizon. This article applies deep reinforcement learning to build an allocation algorithm for long-lasting and compute-intensive jobs that are increasingly seen among today's computation demands. Specifically, a deep Q-network is trained to allocate jobs, aiming to maximize a cumulative reward over long horizons. The training is performed offline using a computational model based on long short-term memory networks that capture the servers' power and thermal dynamics. This offline training approach avoids slow online convergence, low energy efficiency, and potential server overheating during the agent's extensive state-action space exploration if it directly interacts with the physical data center in the usually adopted online learning scheme. At run time, the trained Q-network is forward-propagated with little computation to allocate jobs. Evaluation based on eight months' physical state and job arrival records from a national supercomputing data center hosting 1,152 processors shows that our solution reduces computing power consumption by more than 10 percent and processor temperature by more than 4°C without sacrificing job processing throughput."",""1558-2183"","""",""10.1109/TPDS.2020.2968427"",""Nation Research Foundation, Prime Minister's Office(grant numbers:NRF2015ENC-GBICRD001-012)"; Green Data Centre Research(grant numbers:NRF2015ENC-GDCR01001-003); Alibaba Group(grant numbers:M4062352);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8964275"",""Job allocation";data center;energy efficiency;"deep reinforcement learning"",""Servers";Data centers;Training;Resource management;Computational modeling;Optimization;"Power demand"","""",""26"","""",""35"",""IEEE"",""22 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Efficient Method for Parallel Computation of Geodesic Transformation on CPU,""D. Žlaus";" D. Mongus"",""Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia";" Faculty of Electrical Engineering and Computer Science, University of Maribor, Maribor, Slovenia"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""935"",""947"",""This article introduces a fast Central Processing Unit (CPU) implementation of geodesic morphological operations using stream processing. In contrast to the current state-of-the-art, that focuses on achieving insensitivity to the filter sizes with efficient data structures, the proposed approach achieves efficient computation of long chains of elementary $3 \times 3$3×3 filters using multicore and Single Instruction Multiple Data (SIMD) processing. In comparison to the related methods, up to 100 times faster computation of common geodesic operators is achieved in this way, allowing for real-time processing (with over 30 FPS) of up to 1500 filters long chains, applied on $1024\times 1024$1024×1024 images. In addition, the proposed approach outperformed GPGPU, and proved to be more efficient than the comparable streaming method for the computation of morphological erosions and dilations with window sizes up to $183\times 183$183×183 in the case of using char and $27\times 27$27×27 when using double data types."",""1558-2183"","""",""10.1109/TPDS.2019.2953057"",""Javna Agencija za Raziskovalno Dejavnost RS(grant numbers:P2-0041,J2-8176)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8907491"",""Geodesic operators";mathematical morphology;SIMD;parallel processing;"stream processing"",""Field programmable gate arrays";Streaming media;Parallel processing;Pipelines;Central Processing Unit;Multicore processing;"Kernel"","""",""1"","""",""44"",""IEEE"",""20 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Parallelism of Post-Quantum Signature Scheme SPHINCS,""S. Sun"; R. Zhang;" H. Ma"",""State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China;" State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""29 May 2020"",""2020"",""31"",""11"",""2542"",""2555"",""SPHINCS was recently proposed as a stateless, quantum-resilient hash-based signature scheme. However, one possible limitation of SPHINCS is its signing speed, namely, the best known implementation merely produces a few hundred of signatures per second, which is not good enough, e.g., for a social website with a huge amount of users. Aiming at improving the singing throughput, we present highly parallel and optimized implementations of SPHINCS, which can be deployed on various multi-core platforms. As a first step, we give an elementary implementation on ×86/64 processors, which proves the effectiveness and correctness of our implementations. To obtain a significantly higherthroughput, we implement SPHINCS on Graphics Processing Units (GPUs). Furthermore, we develop a few general and hardware-specific techniques to take full advantage of the computing power of targeted platforms. We instantiate the underlying hash functions with three primitives. Our comprehensive benchmark shows that our work outperforms all the state-of-the-art implementations of SPHINCS regarding throughput with reasonable latency, and has scalability on multiple cores and multiple GPU cards. For instance, forthe key generation algorithm instantiated with ChaCha running on a GeForce GTX 1080, we obtain 5152 signatures per second which is 7.88x speedup fasterthan a recent FPGA implementation. When upgrade to TITAN Xp, 6,651 signatures are generated in one second. With four TITAN Xp GPUs, the obtained throughput satisfies vast majority scenarios."",""1558-2183"","""",""10.1109/TPDS.2020.2995562"",""National Natural Science Foundation of China(grant numbers:61772520,61802392,61972094,61472416,61632020)"; Key Research and Development Project of Zhejiang Province(grant numbers:2017C01062,2020C01078); Beijing Municipal Science and Technology Commission(grant numbers:Z191100007119007,Z191100007119002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9095410"",""Post-quantum cryptography";parallel computation;stateless hash-based signature schemes;SPHINCS;"multi-core platforms"",""Throughput";Graphics processing units;Multicore processing;Field programmable gate arrays;"Cryptography"","""",""14"","""",""55"",""IEEE"",""18 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Performance Estimation and Work-Group Size Pruning for OpenCL Kernels on GPUs,""X. Wang"; X. Qian; A. Knoll;" K. Huang"",""Department of Informatics, Chair of Robotics, Artificial Intelligence and Real-time Systems, Technical University of Munich, Garching, Germany"; Ming Hsieh Department of Electrical Engineering, Department of Computer Science, University of Southern California, Los Angeles, USA; Department of Informatics, Chair of Robotics, Artificial Intelligence and Real-time Systems, Technical University of Munich, Garching, Germany;" Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, School of Data and Computer Science, Sun Yat-sen University, Guangzhou, P. R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1089"",""1106"",""Graphic Processing Units (GPUs) play a vital role in state-of-the-art high-performance scientific computing realm and research work towards its performance analysis is crucial but nontrivial. Extant GPU performance models are far from practical use, while fine-grained GPU simulation requires a considerably large time cost. Moreover, massive amounts of designs with various program inputs and parameter settings pose a challenge for efficient performance estimation and tuning of parallel GPU applications. To this end, this article presents a hybrid framework for the efficient performance estimation and work-group size pruning of OpenCL workloads on GPUs. The framework contains a static module used to extract the kernel execution trace from the high-level source code and a dynamical module used to mimic the kernel execution flow to estimate the runtime performance. For the design space pruning, an extra analysis is performed to filter out the redundant work-group sizes with duplicated execution traces and inferior pipelines. The proposed framework does not require any program runs to estimate the performance and find the optimal or near-optimal designs. Experiments on four Commercial Off-The-Shelf (COTS) Nvidia GPUs show that the framework can predict the runtime performance with an average error of 17.04 percent and reduce the program design space by an average of 78.47 percent."",""1558-2183"","""",""10.1109/TPDS.2019.2958343"",""China Scholarship Council(grant numbers:201506270152)"; National Natural Science Foundation of China(grant numbers:61872393); National Science Foundation(grant numbers:NSF-CCF-1657333,NSF-CCF-1717754,NSF-CNS-1717984,NSF-CCF-1750656);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8928962"",""GPU";performance estimation;OpenCL;work-group size;"performance tuning"",""Kernel";Graphics processing units;Measurement;Runtime;Estimation;Hardware;"Analytical models"","""",""2"","""",""50"",""IEEE"",""9 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Efficient SSD Cache for Cloud Block Storage via Leveraging Block Reuse Distances,""K. Zhou"; Y. Zhang; P. Huang; H. Wang; Y. Ji; B. Cheng;" Y. Liu"",""Wuhan National Laboratory for Optoelectronics and School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; Wuhan National Laboratory for Optoelectronics and School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics and School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics and School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Tencent Corporation, Shenzhen, China; Tencent Corporation, Shenzhen, China;" Tencent Corporation, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 May 2020"",""2020"",""31"",""11"",""2496"",""2509"",""Solid State Drives (SSDs) are popularly used for caching in large scale cloud storage systems nowadays. Traditionally, most cache algorithms make replacement upon each miss when cache space is full. However, we observe that in a typical Cloud Block Storage (CBS) system, there is a great percentage of blocks with large reuse distances, which would result in large number of blocks being evicted out of the cache before they ever have a chance to be referenced while they are cached, significantly jeopardizing the cache efficiency. In this article, we propose LEA, Lazy Eviction cache Algorithm, for cloud block storage to efficiently remedy the cache inefficiencies caused by cache blocks with large reuse distances. LEA mainly employs two lists, Lazy Eviction List (LEL) and Block Identity List (BIL), which keep track of two types of victim blocks respectively based on their cache duration when replacements occur, to improve cache efficiency. When a cache miss happens, if the victim block has not resided in cache for longer than its reuse distance, LEA inserts the missed block identity into BIL. Otherwise, it inserts the missed block entry into LEL. We have evaluated LEA by using IO traces collected from Tencent, one of the largest network service providers in the world, and several open source traces. Experimental results show that LEA not only outperforms most of the state-of-the-art cache algorithms in hit ratio, but also greatly reduces the number of SSD writes."",""1558-2183"","""",""10.1109/TPDS.2020.2994075"",""National Natural Science Foundation of China(grant numbers:61821003)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091319"",""Cloud block storage";cache algorithm;SSD;"reuse distance"",""Cloud computing";Servers;Routing;Distributed databases;Indexes;Virtual machine monitors;"Reliability"","""",""6"","""",""53"",""IEEE"",""11 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Enabling Encrypted Boolean Queries in Geographically Distributed Databases,""X. Yuan"; X. Yuan; Y. Zhang; B. Li;" C. Wang"",""School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA"; Faculty of Information Technology, Monash University, Clayton, Australia; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada;" Department of Computer Science, City University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""634"",""646"",""The persistent growth of big data applications has being raising new challenges in managing large volumes of datasets with high scalability, confidentiality protection, and flexible types of search queries. In this paper, we propose a secure design to disassemble the private dataset with the aim to store them across geographically distributed servers while supporting secure multi-client Boolean queries. In this design, the data owner encrypts the private database with the searchable index attributes. The encrypted dataset will be disassembled and distributed evenly across multiple servers by leveraging the property of a distributed index framework. By constructing an encryption structure, generating search tokens, and enabling parallel query, we show how the proposed design performs the secure while efficient Boolean search. These queries are not only limited to those initiated by the data owner but also can be extended to support multiple authorized clients, where each client is allowed to access a necessary part of the private database. In this stage, we advocate a non-interactive authorization scheme where data owner is not required to stay online to process the query request. Moreover, the query operation can be executed in parallel, which significantly improves the search efficiency. We formally characterize the leakage profile, which allow us to follow the existing security analysis method to demonstrate that our system can guarantee data confidentiality and query privacy. To validate our protocol, we implement a system prototype and evaluate the efficiency of our construction. Through experimental results, we demonstrate the effectiveness of our protocol in terms of data outsourcing time and Boolean query time."",""1558-2183"","""",""10.1109/TPDS.2019.2940945"",""Louisiana Board of Regents(grant numbers:LEQSF(2018-21)-RD-A-24)"; Data61-Monash Collaborative Research Project; Research Grants Council of Hong Kong(grant numbers:CityU 11212717,CityU C1008-16G); National Natural Science Foundation of China(grant numbers:61572412);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8834804"",""Searchable symmetric encryption";multi-client data access;key-value stores;"Boolean query"",""Servers";Distributed databases;Cloud computing;Indexes;Encryption;"Data privacy"","""",""11"","""",""33"",""IEEE"",""12 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Enabling Runtime SpMV Format Selection through an Overhead Conscious Method,""W. Zhou"; Y. Zhao; X. Shen;" W. Chen"",""Computer Science, North Carolina State University College of Engineering, Raleigh, USA"; Department of Computer, North Carolina State University, Raleigh, USA; Computer Science, North Carolina State University, Raleigh, USA;" CAS, IBM Canada Ltd, Markham, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""80"",""93"",""Sparse matrix-vector multiplication (SpMV) is an important kernel and its performance is critical for many applications. Storage format selection is to select the best format to store a sparse matrix";" it is essential for SpMV performance. Prior studies have focused on predicting the format that helps SpMV run fastest, but have ignored the runtime prediction and format conversion overhead. This work shows that the runtime overhead makes the predictions from previous solutions frequently sub-optimal and sometimes inferior regarding the end-to-end time. It proposes a new paradigm for SpMV storage selection, an overhead-conscious method. Through carefully designed regression models and neural network-based time series prediction models, the method captures the influence imposed on the overall program performance by the overhead and the benefits of format prediction and conversions. The method employs a novel two-stage lazy-and-light scheme to help control the possible negative effects of format predictions, and at the same time, maximize the overall format conversion benefits. Experiments show that the technique outperforms previous techniques significantly. It improves the overall performance of applications by 1.21X to 1.53X, significantly larger than the 0.83X to 1.25X upper-bound speedups overhead-oblivious methods could give."",""1558-2183"","""",""10.1109/TPDS.2019.2932931"",""DOE Early Career Award(grant numbers:DE-SC0013700)"; National Science Foundation(grant numbers:CCF-1455404,CCF-1525609,CNS-1717425,CCF-1703487); IBM;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8787872"",""SpMV";high performance computing;program optimization;sparse matrix format;"prediction model"",""Sparse matrices";Runtime;Predictive models;Matrix converters;Buildings;Kernel;"Time series analysis"","""",""7"","""",""53"",""IEEE"",""5 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Endpoint-Flexible Coflow Scheduling Across Geo-Distributed Datacenters,""W. Li"; X. Yuan; K. Li; H. Qi; X. Zhou;" R. Xu"",""Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong"; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China;" Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 May 2020"",""2020"",""31"",""10"",""2466"",""2481"",""Over the last decade, we have witnessed growing data volumes generated and stored across geographically distributed datacenters. Processing such geo-distributed datasets may suffer from significant slowdown as the underlying network flows have to go through the inter-datacenter networks with relatively low and highly heterogeneous available link bandwidth. Thus, optimizing the transmissions of inter-datacenter flows, especially coflows that capture application-level semantics, is important for improving the communication performance of such geo-distributed applications. However, prior solutions on coflow scheduling have significant limitations: they schedule coflows with already-fixed endpoints of flows, making them insufficient to optimize the coflow completion time (CCT). In this article, we focus on the problem of jointly considering endpoint placement and coflow scheduling to minimize the average CCT of coflows across geo-distributed datacenters. To solve this problem without any prior knowledge of coflow arrivals, we present a coflow-aware optimization framework called SmartCoflow. In SmartCoflow, we first apply an approximate algorithm to obtain the endpoint placement and scheduling decisions for a single coflow. Based on the single-coflow solution, we then develop an efficient online algorithm to handle the dynamically arrived coflows. Through rigorous theoretical analysis, we prove that SmartCoflow has a non-trivial competitive ratio. We also extend SmartCoflow to incorporate various design choices or requirements of applications and operators, such as enforcing an inter-datacenter bandwidth usage budget and considering coflow deadline. Through experimental results from testbed implementation and trace-driven simulations, we demonstrate that SmartCoflow can reduce the average CCT, lower bandwidth usage, and improve coflow deadline meet rate, when compared to the state-of-the-art scheduling-only method."",""1558-2183"","""",""10.1109/TPDS.2020.2992615"",""NSFC General Technology Basic Research Joint Funds(grant numbers:U1836214)"; National Natural Science Foundation of China(grant numbers:61832013); Artificial Intelligence Science and Technology Major Project of Tianjin(grant numbers:18ZXZNGX00190); National Key R&D Program of China(grant numbers:2019QY1302); National Natural Science Foundation of China(grant numbers:61672379); National Key R&D Program of China(grant numbers:2019YFB2102404); NSFC-Guangdong Joint Funds(grant numbers:U1701263); Natural Science Foundation of Tianjin City(grant numbers:18ZXZNGX00040); National Key R&D Program of China(grant numbers:2018YFB1004700); National Natural Science Foundation of China(grant numbers:61872265,61672131); Key research and Development Program for Guangdong Province(grant numbers:2019B010136001); National Natural Science Foundation of China(grant numbers:61772112); Science Innovation Foundation of Dalian(grant numbers:2019J12GX037);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086773"",""Inter-datacenter";coflow scheduling;CCT;deadline;"endpoint flexibility"",""Task analysis";Bandwidth;Scheduling;Heuristic algorithms;Distributed databases;Approximation algorithms;"Data models"","""",""11"","""",""67"",""IEEE"",""5 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"Energy and Task-Aware Partitioning on Single-ISA Clustered Heterogeneous Processors,""A. Suyyagh";" Z. Zilic"",""Department of Electrical and Computer Engineering, McGill University, Montreal, Canada";" Department of Electrical and Computer Engineering, McGill University, Montreal, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""306"",""317"",""Heterogeneous multi-core processing is increasingly adopted in embedded systems. Heterogeneous platforms can provide energy consumption reduction by employing longstanding techniques like Dynamic Voltage and Frequency Scaling (DVFS) and Dynamic Power Management (DPM). An effective energy-management strategy simultaneously exploits hardware-and software-level energy-reduction techniques. Energy-efficient partitioning is one software-level method where task allocation to heterogeneous clusters directly impacts the total system energy. In this paper, we couple the problem of energy-efficient partitioning on single-ISA heterogeneous platforms with task-aware scheduling. Tasks differ in their instruction mix, cache, memory and I/O access, execution path, and active processing and SoC circuitry. This affects their power demand. We make further use of underlying hardware frequency scaling to reduce the system energy. We propose four variants of our Task and Cluster Heterogeneity Aware Partitioning (TCHAP) targeting ARM big.LITTLE platforms, and show that our algorithms achieve up to 30 percent energy-reduction on average compared to a state-of-the-art scheme."",""1558-2183"","""",""10.1109/TPDS.2019.2937029"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812914"",""Energy-aware scheduling";task partitioning;real-time systems;heterogeneous multicores;"ARM big.LITTLE"",""Task analysis";Program processors;Multicore processing;Real-time systems;Clustering algorithms;Energy consumption;"Hardware"","""",""10"","""",""38"",""IEEE"",""26 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Energy-Aware Application Placement in Mobile Edge Computing: A Stochastic Optimization Approach,""H. Badri"; T. Bahreini; D. Grosu;" K. Yang"",""Department of Industrial & Systems Engineering, Wayne State University, Detroit, USA"; Department of Computer Science, Wayne State University, Detroit, USA; Department of Computer Science, Wayne State University, Detroit, USA;" Department of Industrial & Systems Engineering, Wayne State University, Detroit, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""909"",""922"",""The Quality of Service (QoS) in Mobile Edge Computing (MEC) systems is significantly dependent on the application offloading and placement decisions. Due to the movement of users in MEC networks, an optimal application placement might turn into the least efficient placement in few minutes. Thus, it is crucial to take the dynamics of the system into account when designing application placement mechanisms. On the other hand, energy consumption of servers is a significant component of the cost of services in MEC systems and must also be considered in the design of the mechanisms. In this article, we model the problem of energy-aware application placement in edge computing systems as a multi-stage stochastic program. The objective is to maximize the QoS of the system while taking into account the limited energy budget of the edge servers. To solve the problem, we design a novel parallel Sample Average Approximation (SAA) algorithm. We conduct an extensive experimental analysis to evaluate the performance of the proposed algorithm using real-world trace data."",""1558-2183"","""",""10.1109/TPDS.2019.2950937"",""National Science Foundation(grant numbers:IIS-1724227)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8897679"",""Mobile edge computing";energy-aware application placement;quality of service;multi-stage stochastic programming;sample average approximation;"parallel algorithms"",""Servers";Quality of service;Stochastic processes;Energy consumption;Cloud computing;Optimization;"Computational modeling"","""",""72"","""",""55"",""IEEE"",""13 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Energy-Efficient Parallel Real-Time Scheduling on Clustered Multi-Core,""A. Bhuiyan"; D. Liu; A. Khan; A. Saifullah; N. Guan;" Z. Guo"",""Department of Electrical and Computer Engineering, University of Central Florida, Orlando, USA"; Yunnan University, Kunming, China; Brainco Inc., Somerville, USA; Department of Computer Science, Wayne State University, Detroit, USA; Department of Computing, Hong Kong Polytechnic University, Hong Kong;" Department of Electrical and Computer Engineering, University of Central Florida, Orlando, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Apr 2020"",""2020"",""31"",""9"",""2097"",""2111"",""Energy-efficiency is a critical requirement for computation-intensive real-time applications on multi-core embedded systems. Multi-core processors enable intra-task parallelism, and in this work, we study energy-efficient real-time scheduling of constrained deadline sporadic parallel tasks, where each task is represented as a directed acyclic graph (DAG). We consider a clustered multi-core platform where processors within the same cluster run at the same speed at any given time. A new concept named speed-profile is proposed to model per-task and per-cluster energy-consumption variations during run-time to minimize the expected long-term energy consumption. To our knowledge, no existing work considers energy-aware real-time scheduling of DAG tasks with constrained deadlines, nor on a clustered multi-core platform. The proposed energy-aware real-time scheduler is implemented upon an ODROID XU-3 board to evaluate and demonstrate its feasibility and practicality. To complement our system experiments in large-scale, we have also conducted simulations that demonstrate a CPU energy saving of up to 67 percent through our proposed approach compared to existing methods."",""1558-2183"","""",""10.1109/TPDS.2020.2985701"",""National Science Foundation(grant numbers:CNS-1850851,CNS-1742985)"; National Natural Science Foundation of China(grant numbers:61801418);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066868"",""Parallel task";real-time scheduling;energy minimization;cluster-based platform;"heterogeneous platform"",""Task analysis";Real-time systems;Program processors;Processor scheduling;Power demand;Multicore processing;"Energy consumption"","""",""26"","""",""52"",""IEEE"",""14 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"ERA-LSTM: An Efficient ReRAM-Based Architecture for Long Short-Term Memory,""J. Han"; H. Liu; M. Wang; Z. Li;" Y. Zhang"",""Institute of Microelectronics, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jan 2020"",""2020"",""31"",""6"",""1328"",""1342"",""Processing-in-memory (PIM) architecture based on resistive random access memory (ReRAM) crossbars is a promising solution to the memory bottleneck that long short-term memory (LSTM) faces. Based on the dataflow analysis of the LSTM computing paradigm, this article proposes to adopt the ReRAM-based analog approximate computing to conduct the LSTM-specific element-wise computation. Combined with the dot-product computation implemented with ReRAM crossbars, a new LSTM processing tile is designed to significantly reduce the demand for analog-to-digital converters (ADCs), which is the major part of power consumption of existing designs. Next, we elaborate on a mapping scheme to efficiently deploy large-scale LSTM onto multiple processing tiles. Finally, an architecture enhancement is proposed to support crossbar-friendly LSTM pruning to further improve efficiency. This overall design, named ERA-LSTM, is presented. Our evaluation shows that it can outperform two state-of-the-art FPGA-based LSTM accelerators by 103.6 and 35.9 times, respectively";" compared with a state-of-the-art ReRAM-based LSTM accelerator with digital element-wise computation, it is 6.1 times more efficient. Moreover, our experiments demonstrate that the impact of hardware constraints and approximation errors on the inference accuracy can be effectively reduced by the proposed fine-tuning scheme and by optimizing the design of the approximator."",""1558-2183"","""",""10.1109/TPDS.2019.2962806"",""Beijing Academy of Artificial Intelligence"; Beijing Innovation Center for Future Chip; Tsinghua University; Science and Technology Innovation Special Zone Project, China; Tsinghua University(grant numbers:2018Z05JDX005); China Postdoctoral Science Foundation(grant numbers:2019M650030);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8944023"",""Long short-term memory (LSTM)";resistive random-access memory (ReRAM);processing in memory (PIM);approximate computing;"accelerator"",""Computer architecture";Logic gates;Artificial neural networks;Microprocessors;Hardware;Training;"Field programmable gate arrays"","""",""26"","""",""64"",""IEEE"",""27 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Errata to “Exploring Fault-Tolerant Erasure Codes for Scalable All-Flash Array Clusters”,""S. Koh"; J. Zhang; M. Kwon; J. Yoon; D. Donofrio; N. S. Kim;" M. Jung"",NA"; NA; NA; NA; NA; NA;" NA,""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2020"",""2020"",""31"",""6"",""1460"",""1460"",""Presents corrections to author affiliation information in the above mentioned article."",""1558-2183"","""",""10.1109/TPDS.2020.2971074"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8995737"","""",""Fault tolerance";Fault tolerant systems;Computer architecture;Convergence;"Telecommunications"","""",""1"","""",""1"",""IEEE"",""12 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Errata to “On-Edge Multi-Task Transfer Learning: Model and Practice With Data-Driven Task Allocation”,""Q. Chen"; Z. Zheng; C. Hu; D. Wang;" F. Liu"",""Key Laboratory of Services Computing Technology and System, Ministry of Education, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China"; Edge Cloud Innovation Lab, Technical Innovation Department, Cloud BU, Huawei Technologies Company, Ltd., Shenzhen, China; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong;" Key Laboratory of Services Computing Technology and System, Ministry of Education, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Jun 2020"",""2020"",""31"",""11"",""2569"",""2569"",""Presents corrections to author information for the above named paper."",""1558-2183"","""",""10.1109/TPDS.2020.2997321"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106908"","""",""Task analysis";Resource management;Technological innovation;Computational modeling;Big Data;Service computing;"Computer science"","""","""","""",""1"",""IEEE"",""2 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"ESetStore: An Erasure-Coded Storage System With Fast Data Recovery,""C. Liu"; Q. Wang; X. Chu; Y. -W. Leung;" H. Liu"",""College of Big Data and Internet, Shenzhen Technology University, Shenzhen, China"; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong;" Department of Computing, Hang Seng University of Hong Kong, Siu Lek Yuen, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Apr 2020"",""2020"",""31"",""9"",""2001"",""2016"",""Erasure codes have been used extensively in large-scale storage systems to reduce the storage overhead of triplication-based storage systems. One key performance issue introduced by erasure codes is the long time needed to recover from a single failure, which occurs constantly in large-scale storage systems. We present ESetStore, a prototype erasure-coded storage system that aims to achieve fast recovery from failures. ESetStore is novel in the following aspects. We proposed a data placement algorithm named ESet for our ESetStore that can aggregate adequate I/O resources from available storage servers to recover from each single failure. We designed and implemented efficient read and write operations on our erasure-coded storage system via effective use of available I/O and computation resources. We evaluated the performance of ESetStore with extensive experiments on a cluster with 50 storage servers. The evaluation results demonstrate that our recovery performance can obtain linear performance growth by harvesting available I/O resources. With our defined parameter recovery I/O parallelism under some mild conditions, we can achieve optimal recovery performance, in which ESet enables minimal recovery time. Rather than being an alternative to improve recovery performance, our work can be an enhancement for existing solutions, such as Partial-parallel-repair (PPR), to further improve recovery performance."",""1558-2183"","""",""10.1109/TPDS.2020.2983411"",""Hong Kong Innovation and Technology(grant numbers:ITS/443/16FX)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9051846"",""ESetStore";ESet;Erasure coded storage systems;"Fast data recovery"",""Servers";Reliability;Encoding;Distributed databases;Performance evaluation;Containers;"Bandwidth"","""",""12"","""",""44"",""IEEE"",""31 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch and GPUDirect,""A. Li"; S. L. Song; J. Chen; J. Li; X. Liu; N. R. Tallent;" K. J. Barker"",""High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA"; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA; Computer Science and Mathematics Department, Oak Ridge National Laboratory, Oak Ridge, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA; Computer Science Department, College of William and Mary, Williamsburg, USA; High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA;" High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""94"",""110"",""High performance multi-GPU computing becomes an inevitable trend due to the ever-increasing demand on computation capability in emerging domains such as deep learning, big data and planet-scale simulations. However, the lack of deep understanding on how modern GPUs can be connected and the real impact of state-of-the-art interconnect technology on multi-GPU application performance become a hurdle. In this paper, we fill the gap by conducting a thorough evaluation on five latest types of modern GPU interconnects: PCIe, NVLink-V1, NVLink-V2, NVLink-SLI and NVSwitch, from six high-end servers and HPC platforms: NVIDIA P100-DGX-1, V100-DGX-1, DGX-2, OLCF's SummitDev and Summit supercomputers, as well as an SLI-linked system with two NVIDIA Turing RTX-2080 GPUs. Based on the empirical evaluation, we have observed four new types of GPU communication network NUMA effects: three are triggered by NVLink's topology, connectivity and routing, while one is caused by PCIe chipset design issue. These observations indicate that, for an application running in a multi-GPU node, choosing the right GPU combination can impose considerable impact on GPU communication efficiency, as well as the application's overall performance. Our evaluation can be leveraged in building practical multi-GPU performance models, which are vital for GPU task allocation, scheduling and migration in a shared environment (e.g., AI cloud and HPC centers), as well as communication-oriented performance tuning."",""1558-2183"","""",""10.1109/TPDS.2019.2928289"",""Exascale Computing Project(grant numbers:17-SC-20-SC)"; U.S. Department of Energy; National Nuclear Security Administration; U.S. Department of Energy(grant numbers:66150); Pacific Northwest National Laboratory; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); U.S. Department of Energy(grant numbers:DE-AC05-76RL01830);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8763922"",""Performance evaluation";GPU;interconnect;NUMA;PCIe;NVLink;NVSwitch;SLI;GPUDirect;RDMA;"NCCL"",""Graphics processing units";Bandwidth;Topology;Peer-to-peer computing;Network topology;Switches;"Routing"","""",""81"","""",""60"",""IEEE"",""15 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;
"Evaluation of Stream Processing Frameworks,""G. van Dongen";" D. Van den Poel"",""Ghent University, Ghent, Belgium";" Ghent University, Ghent, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Mar 2020"",""2020"",""31"",""8"",""1845"",""1858"",""The increasing need for real-time insights in data sparked the development of multiple stream processing frameworks. Several benchmarking studies were conducted in an effort to form guidelines for identifying the most appropriate framework for a use case. In this article, we extend this research and present the results gathered. In addition to Spark Streaming and Flink, we also include the emerging frameworks Structured Streaming and Kafka Streams. We define four workloads with custom parameter tuning. Each of these is optimized for a certain metric or for measuring performance under specific scenarios such as bursty workloads. We analyze the relationship between latency, throughput, and resource consumption and we measure the performance impact of adding different common operations to the pipeline. To ensure correct latency measurements, we use a single Kafka broker. Our results show that the latency disadvantages of using a micro-batch system are most apparent for stateless operations. With more complex pipelines, customized implementations can give event-driven frameworks a large latency advantage. Due to its micro-batch architecture, Structured Streaming can handle very high throughput at the cost of high latency. Under tight latency SLAs, Flink sustains the highest throughput. Additionally, Flink shows the least performance degradation when confronted with periodic bursts of data. When a burst of data needs to be processed right after startup, however, micro-batch systems catch up faster while event-driven systems output the first events sooner."",""1558-2183"","""",""10.1109/TPDS.2020.2978480"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025240"",""Apache spark";structured streaming;apache flink;apache kafka;kafka streams;distributed computing;stream processing frameworks;benchmarking;"big data"",""Benchmark testing";Sparks;Pipelines;Throughput;Storms;Microsoft Windows;"Measurement"","""",""45"","""",""22"",""IEEE"",""5 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Exact Distributed Load Centrality Computation: Algorithms, Convergence, and Applications to Distance Vector Routing,""L. Maccari"; L. Ghiro; A. Guerrieri; A. Montresor;" R. L. Cigno"",""University of Venice, Venice, Italy"; University of Trento, Trento, Italy; SpazioDati srl, Cascina, Italy; University of Trento, Trento, Italy;" University of Brescia, Brescia, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Mar 2020"",""2020"",""31"",""7"",""1693"",""1706"",""Many optimization techniques for networking protocols take advantage of topological information to improve performance. Often, the topological information at the core of these techniques is a centrality metric such as the Betweenness Centrality (BC) index. BC is, in fact, a centrality metric with many well-known successful applications documented in the literature, from resource allocation to routing. To compute BC, however, each node must run a centralized algorithm and needs to have the global topological knowledge";" such requirements limit the feasibility of optimization procedures based on BC. To overcome restrictions of this kind, we present a novel distributed algorithm that requires only local information to compute an alternative similar metric, called Load Centrality (LC). We present the new algorithm together with a proof of its convergence and the analysis of its time complexity. The proposed algorithm is general enough to be integrated with any distance vector (DV) routing protocol. In support of this claim, we provide an implementation on top of Babel, a real-world DV protocol. We use this implementation in an emulation framework to show how LC can be exploited to reduce Babel's convergence time upon node failure, without increasing control overhead. As a key step towards the adoption of centrality-based optimization for routing, we study how the algorithm can be incrementally introduced in a network running a DV routing protocol. We show that even when only a small fraction of nodes participate in the protocol, the algorithm accurately ranks nodes according to their centrality."",""1558-2183"","""",""10.1109/TPDS.2020.2973960"",""European Commission(grant numbers:688768)"; Horizon 2020 Framework Programme(grant numbers:645274); IEEE Smart Cities Initiative;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999507"",""Multi-hop networks";mesh networks;ad-hoc networks;bellman-ford;load centrality;distributed algorithms;"failure recovery"",""Routing protocols";Measurement;Routing;Indexes;Distributed algorithms;"Network topology"","""",""4"","""",""47"",""IEEE"",""14 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Exploiting Parallelism and Vectorisation in Breadth-First Search for the Intel Xeon Phi,""M. Paredes"; G. Riley;" M. Luján"",""School of Computer Science, Kilburn Building, University of Manchester, Manchester, United Kingdom"; School of Computer Science, Kilburn Building, University of Manchester, Manchester, United Kingdom;" School of Computer Science, Kilburn Building, University of Manchester, Manchester, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""111"",""128"",""Modern applications generate massive amounts of data that is challenging to process or analyse. Graph algorithms have emerged as a solution for the analysis of such data because they can represent the entities participating in the generation of large-scale datasets in terms of vertices and their relationships in terms of edges. Graph analysis algorithms are used for finding patterns within these relationships, aiming to extract information to be further analysed. The breadth-first search (BFS) is one of the main graph search algorithms used for graph analysis and its optimisation has been widely researched using different parallel computers. However, the parallelisation of BFS has been shown to be challenging because of its inherent characteristics, including irregular memory access patterns, data dependencies and workload imbalance, that limit its scalability. This paper investigates the optimisation of the BFS on the Xeon Phi (Knights Corner), a modern parallel architecture provided with an advanced vector processor supporting the AVX-512 instruction set, using a bespoke development framework integrated with the Graph 500 benchmark. In addition, to demonstrate portability, we show results for a direct port of the algorithms to a more recent version of the Xeon Phi (Knights Landing) and to a Skylake CPU which supports most of the AVX-512 instruction set. Optimised parallel versions of two high-level algorithms for BFS were created using vectorisation, starting with the conventional top-down BFS algorithm and, building on this, a hybrid BFS algorithm. On the KNC our best implementations result in speedups of 1.37x (top-down) and 1.37x (hybrid), for a one million vertices graph, compared to the state-of-the-art. On the KNL and Skylake, the performance is higher than on KNC. In addition, we show results of our best hybrid algorithm on real-world graphs from the SNAP datasets with speedups up to 1.3x on KNC. Performance on KNL and Skylake is again higher, demonstrating the robustness and portability of our algorithm. The hybrid BFS algorithm can be further used to speed up other graph analysis algorithms and the lessons learned from vectorisation can be applied to other algorithms targeting existing and future models of the Xeon Phi and other advanced vector architectures."",""1558-2183"","""",""10.1109/TPDS.2019.2927451"",""Engineering and Physical Sciences Research Council(grant numbers:EP/L000725/1,PAMELA EP/K008730/1)"; The National Council for Science and Technology of Mexico; Royal Society University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758397"",""Breadth-first search";graph algorithms;hybrid BFS;vectorisation;parallel architecture;Graph 500;"Xeon Phi"",""Distance measurement";Optical transmitters;Optical receivers;Light emitting diodes;Avalanche photodiodes;"Optical noise"","""",""2"","""",""27"",""CCBY"",""9 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Exploring New Opportunities to Defeat Low-Rate DDoS Attack in Container-Based Cloud Environment,""Z. Li"; H. Jin; D. Zou;" B. Yuan"",""Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Big Data Security Engineering Research Center, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China"; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Big Data Security Engineering Research Center, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Big Data Security Engineering Research Center, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China;" Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Big Data Security Engineering Research Center, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""695"",""706"",""DDoS attacks are rampant in cloud environments and continually evolve into more sophisticated and intelligent modalities, such as low-rate DDoS attacks. But meanwhile, the cloud environment is also developing in constant. Now container technology and microservice architecture are widely applied in cloud environment and compose container-based cloud environment. Comparing with traditional cloud environments, the container-based cloud environment is more lightweight in virtualization and more flexible in scaling service. Naturally, a question that arises is whether these new features of container-based cloud environment will bring new possibilities to defeat DDoS attacks. In this paper, we establish a mathematical model based on queueing theory to analyze the strengths and weaknesses of the container-based cloud environment in defeating low-rate DDoS attack. Based on this, we propose a dynamic DDoS mitigation strategy, which can dynamically regulate the number of container instances serving for different users and coordinate the resource allocation for these instances to maximize the quality of service. And extensive simulations and testbed-based experiments demonstrate our strategy can make the limited system resources be utilized sufficiently to maintain the quality of service acceptable and defeat DDoS attack effectively in the container-based cloud environment."",""1558-2183"","""",""10.1109/TPDS.2019.2942591"",""National Key Research & Development (R&D) Plan of China(grant numbers:2017YFB0802205)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845616"",""Container";microservice;DDoS attack;mitigation;"cloud computing"",""Computer crime";Cloud computing;Containers;Mathematical model;Computer architecture;"Resource management"","""",""30"","""",""60"",""CCBY"",""20 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Exploring Token-Oriented In-Network Prioritization in Datacenter Networks,""K. Liu"; B. Tian; C. Tian; B. Li; Q. Wang; J. Zheng; J. Sun; Y. Gao; W. Wang; G. Chen; W. Dou; Y. Jiang; H. Zhou; J. Jiang; F. Zhang;" G. Zhang"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Huawei Technologies Company, Ltd, Shenzhen, China; Huawei Technologies Company, Ltd, Shenzhen, China;" Huawei Technologies Company, Ltd, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1223"",""1238"",""In memory computing and high-end distributed storage demand low latency, high throughput, and zero data loss simultaneously from datacenter networks. Existing reactive congestion control approaches cannot both minimize queuing latency and ensure zero data loss. A token-oriented proactive approach can achieve them together by controlling congestion even before sending data packets. However, state-of-the-art token-oriented approaches only strive to optimize network-level metrics: maximizing throughput while achieving flow-level fairness. This article answers the question of how to support objective-aware traffic scheduling in token-oriented approaches. The novelty of Token-Oriented in-network Prioritization (TOP) is that it prioritizes tokens instead of data packets. We make three contributions. Via simulations over a hypothetical TOP system, our first contribution is demonstrating the potential performance gain that can be brought by TOP. Second, we investigate the applicability of TOP. Although the overhead of enabling necessary TOP features in switches is trivial, we find that mainstream commodity datacenter switches do not support them. We hence propose a readily-deployable remedy to achieve in-network prioritization by pushing both switch and end-host hardware capacity to an extreme end. Lastly, we implement a running TOP system with Linux hosts and commodity switches, and evaluate TOP in testbeds and with large-scale simulations for various scenarios."",""1558-2183"","""",""10.1109/TPDS.2019.2958899"",""National Key R&D Program of China(grant numbers:2018YFB1003505)"; National Natural Science Foundation of China(grant numbers:61772265,61802172); Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930303"",""Token-oriented proactive congestion control";"datacenters"",""Receivers";Delays;Throughput;Control systems;Distributed databases;"Hardware"","""",""2"","""",""60"",""IEEE"",""10 Dec 2019"","""","""",""IEEE"",""IEEE Journals"""
"Fast and Accurate Traffic Measurement With Hierarchical Filtering,""H. Wang"; H. Xu; L. Huang;" Y. Zhai"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 May 2020"",""2020"",""31"",""10"",""2360"",""2374"",""Sketches have been widely used to record traffic statistics using sub-linear space data structure. Most sketches focus on the traffic estimation of elephant flows (i.e., heavy hitters) due to their importance to many network optimization tasks, e.g., traffic engineering and load balancing. In fact, the information of aggregate mice flows (e.g., all the mice flows with the same source IP) is also crucial to many security-associated tasks, e.g., DDoS detection and network scan detection. However, the previous solutions, e.g., measuring each individual flow or using multiple sketches for independent measurement tasks, will result in worse estimation error or higher computational overhead. To conquer the above disadvantages, we propose an accurate traffic measurement framework with multiple filters, called Sketchtree, to efficiently measure both elephant flows and aggregate mice flows. These filters in Sketchtree are organized in a hierarchical manner, and help to alleviate the hash collision and improve the measurement accuracy, as the number of flows through hierarchical filters in turn will be decreased gradually. We also design some mechanisms to improve the resource utilization efficiency. To validate our proposal, we have implemented Sketchtree and conducted experimental evaluation using real campus traffic traces. The experimental results show that Sketchtree can increase the processing speed by 100 percent, and reduce the measurement error by over 30 percent compared with state-of-the-art sketches."",""1558-2183"","""",""10.1109/TPDS.2020.2991007"",""National Natural Science Foundation of China(grant numbers:61822210,61936015,U1709217)"; Anhui Initiative in Quantum Information Technologies(grant numbers:AHY150300);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080095"",""Network measurement";heavy hitters;hierarchical filtering;sketch;"attribute"",""Mice";Aggregates;Task analysis;Computer crime;Size measurement;Data structures;"Denial-of-service attack"","""",""5"","""",""56"",""IEEE"",""28 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Faster Parallel Core Maintenance Algorithms in Dynamic Graphs,""Q. -S. Hua"; Y. Shi; D. Yu; H. Jin; J. Yu; Z. Cai; X. Cheng;" H. Chen"",""National Engineering Research Center-Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China"; National Engineering Research Center-Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China; School of Computer Science and Technology, Shandong University, Qingdao, P.R. China; National Engineering Research Center-Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China; School of Computer Science and Technology, Shandong Academy of Sciences, Jinan, Shandong, P.R. China; Department of Computer Science, Georgia State University, Atlanta, USA; School of Computer Science and Technology, Shandong University, Qingdao, P.R. China;" National Engineering Research Center-Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jan 2020"",""2020"",""31"",""6"",""1287"",""1300"",""This article studies the core maintenance problem for dynamic graphs which requires to update each vertex's core number with the insertion/deletion of vertices/edges. Previous algorithms can either process one edge associated with a vertex in each iteration or can only process one superior edge associated with the vertex (an edge 〈u";" v〉 is a superior edge of vertex u if v' core number is no less than u's core number) in each iteration. Thus for high superior-degree vertices (the vertices associated with many superior edges) insertions/deletions, previous algorithms become very inefficient. In this article, we discovered a new structure called joint edge set whose insertions/deletions make each vertex's core number change at most one. The joint edge set mainly contains all the superior edges associated with the high superior-degree vertices as long as these vertices are 3+-hop independent. Based on this discovery, faster parallel algorithms are devised to solve the core maintenance problems. In our algorithms, we can process all edges in the joint edge set in one iteration and thus can greatly increase the parallelism and reduce the processing time. The results of extensive experiments conducted on various types of real-world, temporal, and synthetic graphs illustrate that the proposed algorithms achieve good efficiency, stability and scalability. Specifically, the new algorithms can outperform the single-edge processing algorithms by up to four orders of magnitude. Compared with the matching based algorithm and the superior edge based algorithm, our algorithms show a significant speedup up to 60× in the processing time."",""1558-2183"","""",""10.1109/TPDS.2019.2960226"",""National Basic Research Program of China (973 Program)(grant numbers:2018YFB1003203)"; National Natural Science Foundation of China(grant numbers:61572216,61832006,61971269,61672321,61832012);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935160"",""Graph analysis";core maintenance problem;"parallel algorithm"",""Maintenance engineering";Heuristic algorithms;Multicore processing;Parallel algorithms;Computer science;Indexes;"Stability analysis"","""",""42"","""",""26"",""IEEE"",""17 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Fault-Tolerant Routing Mechanism in 3D Optical Network-on-Chip Based on Node Reuse,""P. Guo"; W. Hou; L. Guo; W. Sun; C. Liu; H. Bao; L. H. K. Duong;" W. Liu"",""School of Computer Science and Engineering, Northeastern University, Shenyang, China"; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore;" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""547"",""564"",""The three-dimensional Network-on-Chips (3D NoCs) has become a mature multi-core interconnection architecture in recent years. However, the traditional electrical lines have very limited bandwidth and high energy consumption, making the photonic interconnection promising for future 3D Optical NoCs (ONoCs). Since existing solutions cannot well guarantee the fault-tolerant ability of 3D ONoCs, in this paper, we propose a reliable optical router (OR) structure which sacrifices less redundancy to obtain more restore paths. Moreover, by using our fault-tolerant routing algorithm, the restore path can be found inside the disabled OR under the deadlock-free condition, i.e., fault-node reuse. Experimental results show that the proposed approach outperforms the previous related works by maximum 81.1 percent and 33.0 percent on average for throughput performance under different synthetic and real traffic patterns. It can improve the system average optical signal to noise ratio (OSNR) performance by maximum 26.92 percent and 12.57 percent on average, and it can improve the average energy consumption performance by 0.3 percent to 15.2 percent under different topology types/sizes, failure rates, OR structures, and payload packet sizes."",""1558-2183"","""",""10.1109/TPDS.2019.2939240"",""National Natural Science Foundation of China(grant numbers:61501104,61775033,61771120,61801105)"; Fundamental Research Funds for the Central Universities(grant numbers:N161604004,N161608001,N171602002); China Postdoctoral Science Foundation(grant numbers:2018T110210); Nanjing University(grant numbers:KFKT2018B04); Nanyang Technological University(grant numbers:NAP M4082282,SUG M4082087);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8823063"",""3D optical network-on-chip";fault-tolerant routing mechanism;"node reuse"",""Three-dimensional displays";Fault tolerance;Fault tolerant systems;Optical interconnections;Routing;Silicon;"Photonics"","""",""41"","""",""60"",""IEEE"",""3 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"FeatherCNN: Fast Inference Computation with TensorGEMM on ARM Architectures,""H. Lan"; J. Meng; C. Hundt; B. Schmidt; M. Deng; X. Wang; W. Liu; Y. Qiao;" S. Feng"",""Tencent AI Lab, Shenzhen, China"; Tencent AI Lab, Shenzhen, China; Parallel and Distributed Architectures Group, Institute of Computer Science, Johannes Gutenberg University Mainz, Mainz, Germany; Parallel and Distributed Architectures Group, Institute of Computer Science, Johannes Gutenberg University Mainz, Mainz, Germany; Tencent AI Lab, Shenzhen, China; Tencent AI Lab, Shenzhen, China; Shandong University, Jinan, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China;" Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""580"",""594"",""Deep Learning is ubiquitous in a wide field of applications ranging from research to industry. In comparison to timeconsuming iterative training of convolutional neural networks (CNNs), inference is a relatively lightweight operation making it amenable to execution on mobile devices. Nevertheless, lower latency and higher computation efficiency are crucial to allow for complex models and prolonged battery life. Addressing the aforementioned challenges, we propose FeatherCNN- a fast inference library for ARM CPUs - targeting the performance ceiling of mobile devices. FeatherCNN employs three key techniques: 1) A highly efficient TensorGEMM (generalized matrix multiplication) routine is applied to accelerate Winograd convolution on ARM CPUs, 2) General layer optimization based on custom high performance kernels improves both the computational efficiency and locality of memory access patterns for non-Winograd layers. 3) The framework design emphasizes joint layer-wise optimization using layer fusion to remove redundant calculations and memory movements. Performance evaluation reveals that FeatherCNN significantly outperforms state-ofthe-art libraries. A forward propagation pass of VGG-16 on a 64-core ARM server is 48, 14, and 12 times faster than Caffe using OpenBLAS, Caffe2 using Eigen, and NNPACK, respectively. In addition, FeatherCNN is 3.19 times faster than the recently released TensorFlow Lite library on an iPhone 7 plus. In terms of GEMM performance, FeatherCNN achieves 14.8 and 39.0 percent higher performance than Apple's Accelerate framework on an iPhone 7 plus and Eigen on a Samsung Galaxy S8, respectively. The source code of FeatherCNN library is publicly available at https://github.com/tencent/feathercnn."",""1558-2183"","""",""10.1109/TPDS.2019.2939785"",""National Natural Science Foundation of China(grant numbers:61702494,U1813203)"; National High Technology Research and Development Program of China(grant numbers:2015AA020109,2016YFB0201305); Shenzhen Fundamental Research Fund(grant numbers:JCYJ20160331190123578,GGFW2017073114031767); Shenzhen Discipline Construction Project for Urban Computing and Data Intelligence; Youth Innovation Promotion Association of the Chinese Academy of Sciences;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8826372"",""Convolutional neural networks";ARM architecture;inference computation;"tensorGEMM"",""Convolution";Performance evaluation;Optimization;Computer architecture;Acceleration;Mobile handsets;"Libraries"","""",""20"","""",""45"",""IEEE"",""6 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Fireplug: Efficient and Robust Geo-Replication of Graph Databases,""R. Neiheiser"; L. Rech; M. Bravo; L. Rodrigues;" M. Correia"",""Departamento de Engenharia de Automação e Sistemas, Universidade Federal de Santa Catarina, Florianópolis, Brazil"; Departamento de Informática e Estatística, Universidade Federal de Santa Catarina, Florianópolis, Brazil; IMDEA Software Institute, Madrid, Spain; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal;" INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2020"",""2020"",""31"",""8"",""1942"",""1953"",""Although graph-databases have been assuming an increasing relevance in applications that exhibit strong dependability requirements, including tolerance to malicious faults, few works have addressed Byzantine fault tolerance in this particular context, and previous attempts suffer from lack of flexibility and poor performance. This article describes and evaluates Fireplug, a flexible architecture to build robust geo-replicated graph databases. Fireplug can be configured to tolerate from crash to Byzantine faults, both within and across different datacenters. Furthermore, Fireplug is robust to bugs in existing graph database implementations, as it allows to combine multiple graph database instances in a cohesive manner. Thus, Fireplug can support many different deployments, according to the performance/robustness trade-offs imposed by the target application. Our evaluation shows that Fireplug is able implement Byzantine fault tolerance without penalty when compared to the built-in replication mechanism of Neo4j, which only supports crash faults. Additionally, performance optimizations introduced by Fireplug improve the overall performance by up to 900 percent in geo-replicated scenarios."",""1558-2183"","""",""10.1109/TPDS.2020.2981019"",""FCT(grant numbers:PTDC/ EEI-SCR/ 1741/ 2014 (Abyss),UIDB/ 50021/ 2020)"; CNPq/Brasil(grant numbers:401364/2014-3);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9040450"",""Graph databases";geo-replication;N-version programming;"byzantine faults"",""Databases";Computer crashes;Fault tolerance;Fault tolerant systems;Software;Protocols;"Programming"","""",""2"","""",""44"",""IEEE"",""18 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"FULL-KV: Flexible and Ultra-Low-Latency In-Memory Key-Value Store System Design on CPU-FPGA,""Y. Qiu"; J. Xie; H. Lv; W. Yin; W. -S. Luk; L. Wang; B. Yu; H. Chen; X. Ge; Z. Liao;" X. Shi"",""State Key Laboratory of ASIC and System, Fudan University, Shanghai, China"; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; Huawei Chengdu Research Institute, Chengdu, China; Huawei Chengdu Research Institute, Chengdu, China; Huawei Chengdu Research Institute, Chengdu, China; Huawei Chengdu Research Institute, Chengdu, China;" Huawei Chengdu Research Institute, Chengdu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Mar 2020"",""2020"",""31"",""8"",""1828"",""1444"",""In-memory key-value store (IMKVS) has gained great popularity in data centers. However, big data brings great challenges in performance and power consumption because of the general-purpose Von Neumann computer architecture. Remote direct memory access (RDMA) technology supporting zero-copy networking could partly alleviate the problem but is still not efficient for KVS. To overcome this problem, we present a flexible and ultra-low-latency IMKVS system named FULL-KV, based on a CPU-FPGA heterogeneous architecture. The FPGA serves as a KVS accelerator that can bypass the CPU and implement both the network stacks and the KVS processing with a highly parallel hardware architecture. The system latency of FULL-KV can achieve as low as 1.5μs/2.2μs for the PUT/GET operation, which is 3.0x/1.5x faster than current state-of-the-art hardware-based KVS systems. Besides, FULL-KV can support 4x larger values (up to 4M bytes). Given a total Ethernet bandwidth of 20Gbps, the peak throughput of the single-node FULL-KV can reach 26.0 million key-value operations per second (Mops). In the two-node test system with a commercial Ethernet switch, the peak throughput can reach 52Mops, manifesting the system scalability and practicability."",""1558-2183"","""",""10.1109/TPDS.2020.2973965"",""National Natural Science Foundation of China(grant numbers:61971143)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8999599"",""CPU-FPGA heterogeneous architecture";Hardware accelerator;In-memory key-value store;"ultra-low-latency performance"",""Random access memory";Throughput;Computer architecture;Field programmable gate arrays;Data centers;Power demand;"Flash memories"","""",""8"","""",""39"",""IEEE"",""14 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Fully Homomorphic based Privacy-Preserving Distributed Expectation Maximization on Cloud,""A. Alabdulatif"; I. Khalil; A. Y. Zomaya; Z. Tari;" X. Yi"",""Department of Computer Science, Qassim University, Buraydah, Saudi Arabia"; Department of Distributed Systems and Networking, Royal Melbourne Institute of Technology (RMIT) University, Melbourne, Australia; School of Information Technologies, University of Sydney, Camperdown, Australia; School of Computer Science and IT, RMIT University, Melbourne, Australia;" School of Computer Science and IT, RMIT University, Melbourne, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Jun 2020"",""2020"",""31"",""11"",""2668"",""2681"",""Expectation maximization (EM) is a clustering-based machine learning algorithm that is widely used in many areas of science (e.g., bioinformatics and computer vision) to find maximum likelihood and maximum a posteriori estimates for models with latent variables. To deploy such an algorithm in cloud environments, security and privacy issues need be considered to avoid data breaches or abuses by external malicious parties or even by cloud service providers. However, the processing performance of the EM algorithm poses a challenge in terms of building a secure environment. This article describes an innovative and practical privacy-preserving EM algorithm for cloud systems that addresses this challenge, and estimates the EM parameters in an accurate and secure manner. Fully homomorphic encryption (FHE) is used to ensure the privacy of both the EM algorithm computations and the users' sensitive data in the cloud. A distributed-based approach is also proposed to overcome the overheads of FHE computations and ensure a fast convergence of the EM algorithm. The conducted experiments demonstrate a significant improvement in the convergence time of the distributed EM algorithm, while achieving a high level of accuracy and reducing the associated computational FHE overheads."",""1558-2183"","""",""10.1109/TPDS.2020.2999407"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106834"",""Expectation maximization";distributed analytics;data privacy;fully homomorphic encryption;"cloud computing"",""Cloud computing";Data models;Computational modeling;Data privacy;Analytical models;Signal processing algorithms;"Clustering algorithms"","""",""8"","""",""37"",""IEEE"",""2 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"GA-Par: Dependable Microservice Orchestration Framework for Geo-Distributed Clouds,""Z. Wen"; T. Lin; R. Yang; S. Ji; R. Ranjan; A. Romanovsky; C. Lin;" J. Xu"",""Newcastle University, Newcastle upon Tyne, United Kingdom"; EPFL, Lausanne, Switzerland; University of Leeds, Leeds, United Kingdom; Zhejiang University, Hangzhou Shi, China; Newcastle University, Newcastle upon Tyne, United Kingdom; Newcastle University, Newcastle upon Tyne, United Kingdom; Zhejiang University, Hangzhou Shi, China;" University of Leeds, Leeds, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""129"",""143"",""Recent advances in composing Cloud applications have been driven by deployments of inter-networking heterogeneous microservices across multiple Cloud datacenters. System dependability has been of the upmost importance and criticality to both service vendors and customers. Security, a measurable attribute, is increasingly regarded as the representative example of dependability. Literally, with the increment of microservice types and dynamicity, applications are exposed to aggravated internal security threats and externally environmental uncertainties. Existing work mainly focuses on the QoS-aware composition of native VM-based Cloud application components, while ignoring uncertainties and security risks among interactive and interdependent container-based microservices. Still, orchestrating a set of microservices across datacenters under those constraints remains computationally intractable. This paper describes a new dependable microservice orchestration framework GA-Par to effectively select and deploy microservices whilst reducing the discrepancy between user security requirements and actual service provision. We adopt a hybrid (both whitebox and blackbox based) approach to measure the satisfaction of security requirement and the environmental impact of network QoS on system dependability. Due to the exponential grow of solution space, we develop a parallel Genetic Algorithm framework based on Spark to accelerate the operations for calculating the optimal or near-optimal solution. Large-scale real world datasets are utilized to validate models and orchestration approach. Experiments show that our solution outperforms the greedy-based security aware method with 42.34 percent improvement. GA-Par is roughly 4× faster than a Hadoop-based genetic algorithm solver and the effectiveness can be constantly guaranteed under different application scales."",""1558-2183"","""",""10.1109/TPDS.2019.2929389"",""National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000103)"; National Natural Science Foundation of China(grant numbers:61421003); Beijing Advanced Innovation Center for Big Data and Brain Computing (BDBC);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8766876"",""Service orchestration";dependability;"microservice"",""Security";Quality of service;Uncertainty;Optimization;Cloud computing;Genetic algorithms;"Silicon"","""",""22"","""",""53"",""IEEE"",""19 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Generalized Cost-Based Job Scheduling in Very Large Heterogeneous Cluster Systems,""W. R. KhudaBukhsh"; S. Kar; B. Alt; A. Rizk;" H. Koeppl"",""Mathematical Biosciences Institute, The Ohio State University, Columbus, USA"; Multimedia Communications Lab (KOM), Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany; Bioinspired Communication Systems Lab (BCS), Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany; Institute of Measurement, Control and Microtechnology, Universität Ulm, Ulm, Germany;" Bioinspired Communication Systems Lab (BCS), Department of Electrical Engineering and Information Technology, Technische Universität Darmstadt, Darmstadt, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jun 2020"",""2020"",""31"",""11"",""2594"",""2604"",""We study job assignment in large, heterogeneous resource-sharing clusters of servers with finite buffers. This load balancing problem arises naturally in today's communication and big data systems, such as Amazon Web Services, Network Service Function Chains, and Stream Processing. Arriving jobs are dispatched to a server, following a load balancing policy that optimizes a performance criterion such as job completion time. Our contribution is a randomized Cost-Based Scheduling (CBS) policy in which the job assignment is driven by general cost functions of the server queue lengths. Beyond existing schemes, such as the Join the Shortest Queue (JSQ), the power of d or the SQ(d) and the capacity-weighted JSQ, the notion of CBS yields new application-specific policies such as hybrid locally uniform JSQ. As today's data center clusters have thousands of servers, exact analysis of CBS policies is tedious. In this article, we derive a scaling limit when the number of servers grows large, facilitating a comparison of various CBS policies with respect to their transient as well as steady state behavior. A byproduct of our derivations is the relationship between the queue filling proportions and the server buffer sizes, which cannot be obtained from infinite buffer models. Finally, we provide extensive numerical evaluations and discuss several applications including multi-stage systems."",""1558-2183"","""",""10.1109/TPDS.2020.2997771"",""Deutsche Forschungsgemeinschaft"; Ohio State University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9099971"",""Job scheduling";performance evaluation;"mean-field limit"",""Scheduling";Load management;Load modeling;Cost function;Data centers;"Performance evaluation"","""",""5"","""",""43"",""IEEE"",""26 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"gMig: Efficient vGPU Live Migration with Overlapped Software-Based Dirty Page Verification,""Q. Lu"; X. Zheng; J. Ma; Y. Dong; Z. Qi; J. Yao; B. He;" H. Guan"",""Shanghai Jiao Tong University, Shanghai, China"; Alibaba Group, Hangzhou, China; University of Michigan, Ann Arbor, USA; Intel Corporation; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; National University of Singapore, Singapore;" Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1209"",""1222"",""This paper introduces gMig, an open-source and practical vGPU live migration solution for full virtualization. Taking the advantage of the dirty pattern of GPU workloads, gMig presents the One-Shot Pre-Copy mechanism combined with the hashing based Software Dirty Page technique to achieve efficient vGPU live migration. Particularly, we propose three core techniques for gMig: 1) Dynamic Graphics Address Remapping, which parses and manipulates GPU commands to adjust the address mapping and adapt to a different environment after migration, 2) Software Dirty Page, which utilizes a hashing based approach with sampling pre-filtering to detect page modification, overcomes the commodity GPU's hardware limitation, and speeds up the migration by only sending the dirtied pages, 3) Overlapped Migration Process, which significantly compresses the hanging overhead by overlapping the dirty page verification and transmission concurrently. Our evaluation shows that gMig achieves GPU live migration with an average downtime of 302 ms on Windows and 119 ms on Linux. With the help of Software Dirty Page, the number of GPU pages transferred during the downtime is effectively reduced by up to 80.0 percent . The design of sampling filter and overlapped processing can bring about further 30.0 and 10.0 percent improvements in page processing."",""1558-2183"","""",""10.1109/TPDS.2019.2947521"",""National Key Research & Development Program of China(grant numbers:2016YFB1000502)"; National Natural Science Foundation of China(grant numbers:61672344,61525204,61572322,61732010); Shanghai Key Laboratory of Scalable Computing and Systems;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869867"",""GPU";virtualization;"migration"",""Graphics processing units";Virtualization;Hardware;Cloud computing;Computer architecture;"Performance evaluation"","""",""4"","""",""39"",""IEEE"",""16 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"GPGPU Performance Estimation With Core and Memory Frequency Scaling,""Q. Wang";" X. Chu"",""Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong";" Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jul 2020"",""2020"",""31"",""12"",""2865"",""2881"",""Contemporary graphics processing units (GPUs) support dynamic voltage and frequency scaling to balance computational performance and energy consumption. However, accurate and straightforward performance estimation for a given GPU kernel under different frequency settings is still lacking for real hardware, which is essential to determine the best frequency configuration for energy saving. In this article, we reveal a fine-grained analytical model to estimate the execution time of GPU kernels with both core and memory frequency scaling. Compared to the cycle-level simulators, which are too slow to apply on real hardware, our model only needs simple and one-off micro-benchmarks to extract a set of hardware parameters and kernel performance counters without any source code analysis. Our experimental results show that the proposed performance model can capture the kernel performance scaling behaviors under different frequency settings and achieve decent accuracy (average errors of 3.85, 8.6, 8.82, and 8.83 percent on a set of 20 GPU kernels with four modern Nvidia GPUs)."",""1558-2183"","""",""10.1109/TPDS.2020.3004623"",""Hong Kong RGC GRF(grant numbers:HKBU 12200418)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9124659"",""Graphics processing units";dynamic voltage and frequency scaling;"GPU performance modeling"",""Graphics processing units";Kernel;Hardware;Frequency estimation;Time-frequency analysis;Estimation;"Analytical models"","""",""23"","""",""55"",""IEEE"",""24 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GPU-Accelerated Real-Time Stereo Estimation With Binary Neural Network,""G. Chen"; H. Meng; Y. Liang;" K. Huang"",""School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China"; School of Computer Science and Engineering, Northeastern University, Shenyang, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China;" School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Jul 2020"",""2020"",""31"",""12"",""2896"",""2907"",""Depth estimation from stereo images is essential to many applications such as robotics and autonomous vehicles, most of which ask for the real-time response, high energy and storage efficiency. Recent work has shown deep neural networks (DNN) perform extremely well for stereo estimation. However, these state-of-the-art DNN based algorithms are challenging to be deployed into real-world applications due to the high computational complexities of DNNs. Most of them are too slow for real-time inference and require several seconds of GPU computation to process image frames. In this article, we address the problem of fast stereo estimation and propose an efficient and light-weighted stereo matching system, called StereoBit, to produce a disparity map in a real-time manner while achieving close to state-of-the-art accuracy. To achieve this goal, we propose a binary neural network to generate weighted Hamming distance for an efficient similarity join in stereo estimation. In addition, we propose a novel approximation approach to derive StereoBit network directly from the well-trained network with the cosine similarity. Our approximation strategies enable a significant speedup while maintaining almost the same accuracy compared to the network with the cosine similarity. Furthermore, we present an optimization framework for fully exploiting the computing power of StereoBit. The framework provides a significant speedup of stereo estimation routines, and at the same time, reduces the memory usage for storing parameters. The effectiveness of StereoBit is evaluated by comprehensive experiments. StereoBit can achieve 60 frames per second on an NVIDIA TITAN Xp GPU on KITTI 2012 benchmark while achieving 3-pixel non-occluded stereo error 3.56 percent."",""1558-2183"","""",""10.1109/TPDS.2020.3006238"",""National Natural Science Foundation of China(grant numbers:61702085)"; Shenzhen Basic Research Grants(grant numbers:JCYJ20180507182508857); Science and Technology Planning Project of Guangzhou city of China(grant numbers:202007050004);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9130887"",""GPU acceleration";stereo estimation;"binary neural network"",""Estimation";Neural networks;Real-time systems;Graphics processing units;Computational modeling;Optimization;"Convolution"","""",""25"","""",""30"",""IEEE"",""1 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"gQoS: A QoS-Oriented GPU Virtualization with Adaptive Capacity Sharing,""Q. Lu"; J. Yao; H. Guan;" P. Gao"",""Shanghai Jiao Tong University, Shanghai, China"; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China;" Tencent Corporation, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""843"",""855"",""Currently, the virtualization technologies for cloud computing infrastructures supporting extra devices, such as GPU, require additional development and refinement. This requirement is particularly evident in the area of resource sharing and allocation under some performance constraints, like the quality of service (QoS) guarantee, in light of the closed GPU platform. This deficiency significantly limits the applicability range of the cloud platform, which aims to support the efficient and fluent execution of business and academic workloads. This paper introduces gQoS, an adaptive virtualized GPU resource capacity sharing system under the QoS target, which can share and allocate the virtualized GPU resource among workloads adaptively, guaranteeing the QoS level with stability and accuracy. We evaluate the workloads and compare our gQoS strategy with other allocation strategies. The experiments show that our strategy guarantees much better accuracy and stability in QoS control and that the total GPU resource utilization under gQoS can be rewarded with at most a 25.85 percent reduction compared with other strategies."",""1558-2183"","""",""10.1109/TPDS.2019.2948753"",""National Key Research & Development Program of China(grant numbers:2018YFB1003603)"; National Natural Science Foundation of China(grant numbers:61772339,61572322,61525204);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8880518"",""GPU virtualization";QoS control;resource scheduling;"cloud computing"",""Graphics processing units";Quality of service;Virtualization;Resource management;Cloud computing;Hardware;"Virtual machining"","""",""7"","""",""30"",""IEEE"",""24 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"GRP-HEFT: A Budget-Constrained Resource Provisioning Scheme for Workflow Scheduling in IaaS Clouds,""H. R. Faragardi"; M. R. Saleh Sedghpour; S. Fazliahmadi; T. Fahringer;" N. Rasouli"",""Institute of Computer Sceience, University of Innsbruck, Innsbruck, Austria"; Iran University of Science and Technology, Tehran, Iran; Iran University of Science and Technology, Tehran, Iran; Institute of Computer Sceience, University of Innsbruck, Innsbruck, Austria;" Department of Computer Engineering, Technical and Vocational University, Alborz, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""6"",""1239"",""1254"",""In Infrastructure as a Service (IaaS) Clouds, users are charged to utilize cloud services according to a pay-per-use model. If users intend to run their workflow applications on cloud resources within a specific budget, they have to adjust their demands for cloud resources with respect to this budget. Although several scheduling approaches have introduced solutions to optimize the makespan of workflows on a set of heterogeneous IaaS cloud resources within a certain budget, the hourly-based cost model of some well-known cloud providers (e.g., Amazon EC2 Cloud) can easily lead to a higher makespan and some schedulers may not find any feasible solution. In this article, we propose a novel resource provisioning mechanism and a workflow scheduling algorithm, named Greedy Resource Provisioning and modified HEFT (GRP-HEFT), for minimizing the makespan of a given workflow subject to a budget constraint for the hourly-based cost model of modern IaaS clouds. As a resource provisioning mechanism, we propose a greedy algorithm which lists the instance types according to their efficiency rate. For our scheduler, we modified the HEFT algorithm to consider a budget limit. GRP-HEFT is compared against state-of-the-art workflow scheduling techniques, including MOACS (MultiObjective Ant Colony System), PSO (Particle Swarm Optimization), and GA (Genetic Algorithm). The experimental results demonstrate that GRP-HEFT outperforms GA, PSO, and MOACS for several well-known scientific workflow applications for different problem sizes on average by 13.64, 19.77, and 11.69 percent, respectively. Also in terms of time complexity, GRP-HEFT outperforms GA, PSO and MOACS."",""1558-2183"","""",""10.1109/TPDS.2019.2961098"",""The Austrian Research Promotion Agency(grant numbers:868018)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8937813"",""Cloud computing";workflow scheduling;resource provisioning;"budget-constrained scheduling"",""Scheduling";Cloud computing;Task analysis;Genetic algorithms;Schedules;"Scheduling algorithms"","""",""97"","""",""33"",""IEEE"",""20 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Heterogeneous Edge Offloading With Incomplete Information: A Minority Game Approach,""M. Hu"; Z. Xie; D. Wu; Y. Zhou; X. Chen;" L. Xiao"",""School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China"; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; Department of Computing, Faculty of Science and Engineering, Macquarie University, Sydney, Australia; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China;" Department of Communication Engineering, Xiamen University, Xiamen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""1 May 2020"",""2020"",""31"",""9"",""2139"",""2154"",""Task offloading is one of key operations in edge computing, which is essential for reducing the latency of task processing and boosting the capacity of end devices. However, the heterogeneity among tasks generated by various users makes it challenging to design efficient task offloading algorithms. In addition, the assumption of complete information for offloading decision-making does not always hold in a distributed edge computing environment. In this article, we formulate the problem of heterogeneous task offloading in a distributed environment as a minority game (MG), in which each player must make decisions independently in each turn and the players who end up on the minority side win. The multi-player MG incentivizes players to cooperate with each other in the scenarios with incomplete information, where players don't have full information about other players (e.g., the number of tasks, the required resources). To address the challenges incurred by task heterogeneity and the divergence of naive MG approaches, we propose an MG based scheme, in which tasks are divided into subtasks and instructed to form into a set of groups as possible, and the left ones are scheduled to perform decision adjustment in a probabilistic manner. We prove that our proposed algorithm can converge to a near-optimal point, and also investigate its stability and price of anarchy in terms of task processing time. Finally, we conduct a series of simulations to evaluate the effectiveness of our proposed scheme and the results indicate that our scheme can achieve around 30% reduction of task processing time compared with other approaches. Moreover, our proposed scheme can converge to a near-optimal point, which cannot be guaranteed by naive MG approaches."",""1558-2183"","""",""10.1109/TPDS.2020.2988161"",""National Natural Science Foundation of China(grant numbers:61802452,U1911201,61972432,61971366,61872420)"; Guangdong Special Support Program(grant numbers:2017TX04X148); Natural Science Foundation of Guangdong Province(grant numbers:2018A030310079); Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X355); Pearl River Talent Recruitment Program(grant numbers:2017GC010465);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9070220"",""Heterogeneous edge offloading";incomplete information;"minority game"",""Task analysis";Servers;Edge computing;Computational modeling;Cameras;Games;"Analytical models"","""",""34"","""",""37"",""IEEE"",""17 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"HeteroYARN: A Heterogeneous FPGA-Accelerated Architecture Based on YARN,""R. Li"; Q. Yang; Y. Li; X. Gu; W. Xiao;" K. Li"",""School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Electrical and Computer Engineering, Virginia Commonwealth University, Richmond, USA;" Department of Computer Science, State University of New York, New Paltz, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2020"",""2020"",""31"",""12"",""2968"",""2980"",""In recent years, the heterogeneous distributed platform integrating with FPGAs to accelerate computation tasks has been widely studied to deal with the deluge of data. However, most of current works suffer from poor universality and low resource utilization that run specific algorithms with the highly customized structure. Moreover, there are still many challenges, such as data curation, task scheduling, and resource management, which further limit the scalability of a CPU-FPGA distributed platform. In this paper, we present HeteroYARN, an FPGA-accelerated heterogeneous architecture based on YARN platform, which provides resource management and programming support for computing-intensive applications using FPGAs. In particular, the HeteroYARN abstracts FPGA accelerators as general resources and provides programming APIs to utilize those accelerators easily. Our HeteroYARN simplifies the request and usage of FPGA resources to enhance the efficiency of the heterogeneous framework while maintaining previous workflow unchanged. Experimental results using two representative algorithms, K-means and Naive Bayes classifier, which are accelerated by FPGAs, demonstrate the usability of the HeteroYARN framework and show performance speedup improvement by 7.5x (K-means) and 2.3x (Naive Bayes) respectively compared to conventional CPU-only applications provided by Mahout."",""1558-2183"","""",""10.1109/TPDS.2019.2905201"",""National Key Research and Development Program of China(grant numbers:2016YFB0800402,2016QY01W0202)"; National Natural Science Foundation of China(grant numbers:U1836204,61572221,61433006,U1401258,61572222,61502185); National Social Science Fund of China(grant numbers:16ZDA092);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667646"",""Heterogeneous system";heterogeneous FPGA architecture;FPGA-accelerated computing;data-intensive computing;"YARN"",""Field programmable gate arrays";Yarn;Resource management;Task analysis;Scheduling;Computer architecture;"Processor scheduling"","""",""2"","""",""40"",""IEEE"",""15 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"High Performance GPU Tensor Completion With Tubal-Sampling Pattern,""T. Zhang"; X. -Y. Liu;" X. Wang"",""School of Computer Engineering and Science, Shanghai University, Shanghai, China"; Department of Electrical Engineering, Columbia University, New York, USA;" Department of Electrical Engineering, Columbia University, New York, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Mar 2020"",""2020"",""31"",""7"",""1724"",""1739"",""Data completion is a problem of filling missing or unobserved elements of partially observed datasets. Data completion algorithms have received wide attention and achievements in diverse domains including data mining, signal processing, and computer vision. We observe a ubiquitous tubal-sampling pattern in big data and Internet of Things (IoT) applications, which is introduced by many reasons such as high data acquisition cost, downsampling for data compression, sensor node failures, and packet losses in low-power wireless transmissions. To meet the time and accuracy requirements of applications, data completion methods are expected to be accurate as well as fast. However, the existing methods for data completion with the tubal-sampling pattern are either accurate or fast, but not both. In this article, we propose high-performance graphics processing unit (GPU) tensor completion for data completion with the tubal-sampling pattern. First, by exploiting the convolution theorem, we split a tensor least-squares minimization problem into multiple least-squares sub-problems in the frequency domain. In this way, massive parallelisms are exposed for many-core GPU architectures while still preserving high recovery accuracy. Second, we propose computing slice-level and tube-level tasks in batches to improve GPU utilization. Third, we reduce the data transfer cost by eliminating the accesses to the CPU memory inside algorithm loop structures. The experimental results show that the proposed tensor completion is both fast and accurate. Using synthetic data of varying sizes, the proposed GPU tensor completion achieves maximum 248.18×, 7, 403.27×, and 33.27× speedups over the CPU MATLAB implementation, GPU element-sampling tensor completion in the cuTensor-tubal library, and GPU high-performance matrix completion, respectively. With a 50 percent sampling rate, the proposed GPU tensor completion achieves a recovery error of 1.40e-5, which is comparable with that of the GPU element-sampling tensor completion and three orders of magnitude better than that of the GPU high-performance matrix completion. To utilize multiple GPUs in servers, we design a multi-GPU scheme for tubal-sampling tensor completion. The multi-GPU tensor completion achieves maximum 1.89× speedup on two GPUs versus on a single GPU for medium or big tensors. We further evaluate the performance of the proposed GPU tensor completion in three real applications, namely, video transmission in wireless camera networks, RF fingerprint-based indoor localization, and seismic data completion, and it achieves maximum speedups of 448.68×, 24.63×, and 311.54×, respectively. We integrate this high-performance GPU tensor completion implementation into the cuTensor-tubal library to support various applications."",""1558-2183"","""",""10.1109/TPDS.2020.2975196"",""Natural Science Foundation of Shanghai(grant numbers:17ZR1409800)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9003201"",""Tensor completion";GPU;low-tubal-rank tensor model;tubal-sampling;"alternating minimization"",""Tensile stress";Graphics processing units;Libraries;Wireless communication;Wireless sensor networks;Cameras;"Big Data"","""",""8"","""",""43"",""IEEE"",""19 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"High Performance Simulation of Spiking Neural Network on GPGPUs,""P. Qu"; Y. Zhang; X. Fei;" W. Zheng"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 May 2020"",""2020"",""31"",""11"",""2510"",""2523"",""Spiking neural network (SNN) is the most commonly used computational model for neuroscience and neuromorphic computing communities. It provides more biological reality and possesses the potential to achieve high computational power and energy efficiency. Because existing SNN simulation frameworks on general-purpose graphics processing units (GPGPUs) do not fully consider the biological oriented properties of SNNs, like spike-driven, activity sparsity, etc., they suffer from insufficient parallelism exploration, irregular memory access, and load imbalance. In this article, we propose specific optimization methods to speed up the SNN simulation on GPGPU. First, we propose a fine-grained network representation as a flexible and compact intermediate representation (IR) for SNNs. Second, we propose the cross-population/-projection parallelism exploration to make full use of GPGPU resources. Third, sparsity aware load balance is proposed to deal with the activity sparsity. Finally, we further provide dedicated optimization to support multiple GPGPUs. Accordingly, BSim, a code generation framework for high-performance simulation of SNN on GPGPUs is also proposed. Tests show that, compared to a state-of-the-art GPU-based SNN simulator GeNN, BSim achieves 1.41× - 9.33× speedup for SNNs with different configurations";" it outperforms other simulators much more."",""1558-2183"","""",""10.1109/TPDS.2020.2994123"",""Beijing Academy of Artificial Intelligence(grant numbers:BAAI2019ZD0503)"; Beijing National Research Center for Information Science and Technology; Tsinghua University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9091320"",""Spiking neural network";SNN simulation;GPGPU;load balance;"computational neuroscience"",""Neurons";Computational modeling;Synapses;Biological system modeling;Parallel processing;Biological neural networks;"Mathematical model"","""",""11"","""",""51"",""IEEE"",""11 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"High-Quality Shared-Memory Graph Partitioning,""Y. Akhremtsev"; P. Sanders;" C. Schulz"",""Google, Zurich, Switzerland"; Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany;" Faculty of Computer Science, University of Vienna, Vienna, Austria"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jun 2020"",""2020"",""31"",""11"",""2710"",""2722"",""Partitioning graphs into blocks of roughly equal size such that few edges run between blocks is a frequently needed operation in processing graphs. Recently, size, variety, and structural complexity of these networks has grown dramatically. Unfortunately, previous approaches to parallel graph partitioning have problems in this context since they often show a negative trade-off between speed and quality. We present an approach to multi-level shared-memory parallel graph partitioning that produces balanced solutions, shows high speedups for a variety of large graphs and yields very good quality independently of the number of cores used. For example, in an extensive experimental study, at 79 cores, one of our closest competitors is faster but fails to meet the balance criterion in the majority of cases and another is mostly slower and incurs about 13 percent larger cut size. Important ingredients include parallel label propagation for both coarsening and refinement, parallel initial partitioning, a simple yet effective approach to parallel localized local search, and fast locality preserving hash tables."",""1558-2183"","""",""10.1109/TPDS.2020.3001645"",""DFG(grant numbers:SA 933/10-2,SCHU 2567/1-2)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115228"",""Parallel graph partitioning";shared-memory parallelism;local search;"label propagation"",""Partitioning algorithms";Clustering algorithms;Program processors;Complex networks;Contracts;"Parallel algorithms"","""",""18"","""",""54"",""IEEE"",""11 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Hotspot-Aware Hybrid Memory Management for In-Memory Key-Value Stores,""H. Jin"; Z. Li; H. Liu; X. Liao;" Y. Zhang"",""Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China"; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China;" Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""779"",""792"",""Emerging Non-Volatile Memory (NVM) technologies promise much higher memory density and energy efficiency than DRAM, at the expense of higher read/write latency and limited write endurance. Hybrid memory systems composed of DRAM and NVM have the potential to provide very large capacity of main memory for in-memory key-value (K-V) stores. However, there remains challenges to directly deploy DRAM-based K-V stores in hybrid memory systems. The performance and energy efficiency of K-V stores on hybrid memory systems have not been fully explored yet. In this paper, we propose HMCached, an in-memory K-V store built on a hybrid DRAM/NVM system. HMCached utilizes an application-level data access counting mechanism to identify frequently-accessed (hotspot) objects (i.e., K-V pairs) in NVM, and migrates them to fast DRAM to reduce the costly NVM accesses. We also propose an NVM-friendly index structure to store the frequently-updated portion of object metadata in DRAM, and thus further mitigate the NVM accesses. Moreover, we propose a benefit-aware memory reassignment policy to address the slab calcification problem in slab-based K-V store systems, and significantly improve the benefit gain from the DRAM. We implement the proposed schemes with Memcached and evaluate it with Zipfian-like workloads. Experiment results show that HMCached significantly reduces NVM accesses by 70 percent compared to the vanilla Memcached running on a DRAM/NVM hybrid memory system without any optimizations, and improves application performance by up to 50 percent. Moreover, compared to a DRAM-only system, HMCached achieves 90 percent of performance and 46 percent reduction of energy consumption for realistic (read-intensive) workloads while significantly reducing the DRAM usage by 75 percent."",""1558-2183"","""",""10.1109/TPDS.2019.2945315"",""National Basic Research Program of China (973 Program)(grant numbers:2017YFB1001603)"; National Natural Science Foundation of China(grant numbers:61672251,61732010,61825202);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859283"",""In-memory key-value store";non-volatile memory;"hybrid memory system"",""Random access memory";Nonvolatile memory;Slabs;Memory management;Metadata;Resource management;"Indexes"","""",""13"","""",""41"",""CCBY"",""4 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"HPPT-NoC: A Dark-Silicon Inspired Hierarchical TDM NoC with Efficient Power-Performance Trading,""S. Hesham"; D. Goehringer;" M. A. Abd El Ghany"",""German University in Cairo, Cairo, Egypt"; Technische Universität Dresden, Dresden, Germany;" German University in Cairo, Cairo, Egypt"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""675"",""694"",""Networks-on-chip (NoCs) acquired substantial advancements as the typical solution for a modular, flexible and high performance communication infrastructure coping with the scalable Multi-/Manycores technology. However, the increasing chip complexity heading towards thousand cores, together with the approaching dark-silicon era, puts energy efficiency as an integral design key for future NoC-based multicores, where NoCs are significantly contributing to the total chip power. In this paper, we propose HPPT-NoC, a dark-silicon inspired energy-efficient hierarchical TDM NoC with online distributed setup-scheme. The proposed network makes use of the dim silicon parts of the chip to hierarchically connect quad-routers units. Normal routers operate at full-chip-frequency at high supply level, and hierarchical routers operate at half-chip-frequency and lower supply voltage with adequate synchronization. Routers follow a proposed TDM architecture that separates the datapath from the control-setup planes. This allows separate clocking and operating supplies between data and control and to keep the control-setup as a single-slot-cycle design independent of the datapath slot size. The proposed NoC architecture is evaluated versus a base NoC from the state-of-the-art in terms of performance and hardware results using Synopsys VCS and Synopsys Design Compiler for SAED90nm and SAED32nm technologies. The obtained results highlight the power-frequency-trading feature supported by the proposed hierarchical NoC through the configurable data-control clock relation and maintained over the different technology nodes. With the same power budget of the base NoC, the proposed architecture provides up to 74% setup latency enhancement, 32% increased NoC saturation load, and 21% higher success rates, offering up to 78% improved power delay product. On the other hand, with 38% power savings, the proposed NoC provides up to 37% enhanced latency and 15% higher success rates, with 72% enhanced power delay product. The proposed design consumes almost double the area of the base NoC, however with an average of 56% under-clocked (dim) silicon area operating at half to quarter the maximum chip frequency. This results in reduced power density as a main concern in the dark-silicon era down to 24% of the base NoC."",""1558-2183"","""",""10.1109/TPDS.2019.2942589"",""Deutsche Forschungsgemeinschaft(grant numbers:SFB/TRR 196)"; Alexander von Humboldt Foundation-Research Group Linkage Programme;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8845671"",""Dark-Silicon";multi-cores;networks-on-chip;power;synthesis;"time-division-multiplexing"",""Time division multiplexing";Multicore processing;Energy efficiency;Resource management;Silicon;"Routing"","""",""8"","""",""44"",""IEEE"",""20 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"HRHS: A High-Performance Real-Time Hardware Scheduler,""D. Derafshi"; A. Norollah; M. Khosroanjam;" H. Beitollahi"",""Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran"; Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran; Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran;" Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""897"",""908"",""This article represents an on-line time-predictable distributed hardware scheduler solution, suitable for many-core systems. We have partitioned the Main scheduler into uniform Partial schedulers to achieve a significant gain in term of performance and scalability, while software scheduling solutions impose excessive delays (in order of thousands of clock cycles) to a system. Although we have considered the implementation of the Earliest Deadline First (EDF) algorithm for each Partial scheduler, one can use customized scheduling policies, as needed. Designers can also modify different parts of our proposed architecture to obtain more suitable hardware for their design. HRHS outperforms conventional schedulers, in terms of resource utilization (LUT, register), delay and energy consumption by 36.83, 22.93, 46.36 and 59.26 percent on average, respectively. It also overpowers clustering solutions by circumventing their intrinsic off-line characteristics. The presented designs are also implemented in ASIC with 45-nanometer technology, in which the HRHS design excels in power, area and critical path length by 49.33, 50.67, and 53.33 percent on average, respectively, over other designs implemented in this article."",""1558-2183"","""",""10.1109/TPDS.2019.2952136"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894365"",""FPGA";hardware accelerator;hardware scheduler;hard real-time scheduling;many-core;multi-core;"real-time system"",""Task analysis";Hardware;Scheduling;Real-time systems;Computer architecture;Software;"Processor scheduling"","""",""17"","""",""38"",""IEEE"",""8 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"iCELIA: A Full-Stack Framework for STT-MRAM-Based Deep Learning Acceleration,""H. Yan"; H. R. Cherian; E. C. Ahn; X. Qian;" L. Duan"",""Samsung, Austin, USA"; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, USA; Department of Electrical and Computer Engineering, University of Texas at San Antonio, San Antonio, USA; Ming Hsieh Department of Electrical Engineering and the Department of Computer Science, University of Southern California, Los Angeles, USA;" Computing Technology Lab, Alibaba DAMO Academy, Sunnyvale, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""408"",""422"",""A large variety of applications rely on deep learning to process big data, learn sophisticated features, and perform complicated tasks. Utilizing emerging non-volatile memory (NVM)'s unique characteristics, including the crossbar array structure and gray-scale cell resistances, to perform neural network (NN) computation is a well-studied approach in accelerating deep learning applications. Compared to other NVM technologies, STT-MRAM has its unique advantages in performing NN computation. However, the state-of-the-art research have not utilized STT-MRAM for deep learning acceleration due to its device and architecture-level challenges. Consequently, this paper enables STT-MRAM, for the first time, as an effective and practical deep learning accelerator. In particular, it proposes a full-stack framework iCELIA spanning multiple design levels, including device-level fabrication, circuit-level enhancements, architecture-level synaptic weight quantization, and system-level accelerator design. The primary contributions of iCELIA over our prior work CELIA include a new non-uniform weight quantization scheme and much enhanced accelerator system design. The proposed framework significantly mitigates the model accuracy loss due to reduced data precision in a cohesive manner, constructing a comprehensive STT-MRAM accelerator system for fast NN computation with high energy efficiency and low cost."",""1558-2183"","""",""10.1109/TPDS.2019.2937517"",""National Science Foundation(grant numbers:CCF-1566158)"; University of Texas System; UTSA Office of Vice President for Research, Economic Development and Knowledge Enterprise;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812913"",""STT-MRAM";deep learning acceleration;processing-in-memory;"device and architecture co-design"",""Deep learning";Nonvolatile memory;Computer architecture;Acceleration;Artificial neural networks;Resistance;"Microprocessors"","""",""12"","""",""50"",""IEEE"",""26 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Improving MPI Collective I/O for High Volume Non-Contiguous Requests With Intra-Node Aggregation,""Q. Kang"; S. Lee; K. Hou; R. Ross; A. Agrawal; A. Choudhary;" W. -k. Liao"",""Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA"; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA; Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA;" Department of Electrical and Computer Engineering, Northwestern Univeristy, Evanston, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Jun 2020"",""2020"",""31"",""11"",""2682"",""2695"",""Two-phase I/O is a well-known strategy for implementing collective MPI-IO functions. It redistributes I/O requests among the calling processes into a form that minimizes the file access costs. As modern parallel computers continue to grow into the exascale era, the communication cost of such request redistribution can quickly overwhelm collective I/O performance. This effect has been observed from parallel jobs that run on multiple compute nodes with a high count of MPI processes on each node. To reduce the communication cost, we present a new design for collective I/O by adding an extra communication layer that performs request aggregation among processes within the same compute nodes. This approach can significantly reduce inter-node communication contention when redistributing the I/O requests. We evaluate the performance and compare it with the original two-phase I/O on Cray XC40 parallel computers (Theta and Cori) with Intel KNL and Haswell processors. Using I/O patterns from two large-scale production applications and an I/O benchmark, we show our proposed method effectively reduces the communication cost and hence maintains the scalability for a large number of processes."",""1558-2183"","""",""10.1109/TPDS.2020.3000458"",""Exascale Computing Project(grant numbers:17-SC-20-SC)"; U.S. Department of Energy; National Nuclear Security Administration; DOE Office of Advanced Scientific Computing Research; DOE(grant numbers:DE-SC0014330,DE-SC0019358); DOE Office of Science User Facility(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9109678"",""Parallel I/O";MPI collective I/O;two-phase I/O;"non-contiguous I/O"",""Performance evaluation";Benchmark testing;Program processors;Libraries;Production;Aggregates;"Writing"","""",""6"","""",""39"",""IEEE"",""5 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Improving Overall Performance of TLC SSD by Exploiting Dissimilarity of Flash Pages,""W. Zhang"; Q. Cao; H. Jiang;" J. Yao"",""Wuhan National Laboratory for Optoeletronics, Huazhong University of Science and Technology, Wuhan, China"; Wuhan National Laboratory for Optoeletronics, Huazhong University of Science and Technology, Wuhan, China; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, USA;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""332"",""346"",""TLC flash has three types of pages to accommodate the three bits in each TLC physical cell exhibiting very different program latencies. This paper proposes PA-SSD to effectively improve the overall performance by exploiting the dissimilarity of TLC pages on program latency throughout the write request handling workflow. The main idea behind PA-SSD is to coordinately allocate the same type of pages for sub-requests of any given user write request, to mitigate the potential program latency imbalance among the sub-requests, and to schedule sub-requests according to their page-types. We achieve the PA-SSD design goal by answering three key research questions: (1) how to properly determine page-type for each user write request? (2) how to actually allocate a physical page for each sub-request with an assigned page-type from (1)? (3) how to effectively schedule the sub-requests in the chips queues when their page-types are judiciously allocated from (2)? To answer the first question, we propose seven page-type specifying schemes to investigate their effects under different workloads. We answer the second question by redesigning the page allocation strategy in TLC SSD to uniformly and sequentially determine physical pages for allocation following the internal programming process of TLC flash. Lastly, a page-type aware scheduling policy is presented to reorder the sub-requests within chips' queues. Our experiments show that PA-SSD can accelerate both the write and read performance. Particularly, our proposed queue-depth based page-type specifying scheme improves write performance by 2.6 times and read performance by 1.5 times over the conventional TLC SSD."",""1558-2183"","""",""10.1109/TPDS.2019.2934458"",""National Natural Science Foundation of China(grant numbers:61872156)"; Science Fund for Creative Research Groups(grant numbers:61821003); National Basic Research Program of China (973 Program)(grant numbers:2018YFA0701804); Fundamental Research Funds for the Central Universities(grant numbers:2018KFYXKJC037); Alibaba Innovative Research; National Science Foundation(grant numbers:CCF-1704504,CCF-1629625);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798696"",""TLC SSD";diverse program latencies;TSU scheduling;write performance;"page-type aware"",""Resource management";Time factors;Schedules;Registers;Programming;"Proposals"","""",""9"","""",""25"",""IEEE"",""14 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Improving Restore Performance for In-Line Backup System Combining Deduplication and Delta Compression,""Y. Zhang"; Y. Yuan; D. Feng; C. Wang; X. Wu; L. Yan; D. Pan;" S. Wang"",""School of Computer Science, Hubei University of Technology, Wuhan, China"; School of Computer Science, Hubei University of Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, and Division of Data Storage System, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China;" School of Computer Science, Hubei University of Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 May 2020"",""2020"",""31"",""10"",""2302"",""2314"",""Data deduplication, though being efficient in removing duplicate chunks, introduces chunk fragmentation which decreases restore performance. Rewriting algorithms are proposed to reduce the chunk fragmentation. Delta compression is often used as a complement for data deduplication to further improve storage efficiency. We observe that delta compression introduces a new type of chunk fragmentation stemming from improper delta compression for chunks of which the base chunks are fragmented. The new type of chunk fragmentation severely decreases restore performance and cannot be addressed by existing rewriting algorithms. To address this problem, we propose SDC, a scheme performing post-deduplication delta compression only for the chunks of which the bases can be directly found in the restore cache to eliminate additional disk reads for base chunks, thus avoiding the new type of chunk fragmentation. In addition, self-referenced chunks can be fragmented, which decrease restore performance, and these fragmented chunks can serve as bases to decrease the restore performance repeatedly. We propose a hybrid rewriting scheme for SDC to rewrite such fragmented chunks. Experimental results show that SDC improves the restore performance of the approach that directly performs delta compression after data deduplication by 2.9-16.9x, and achieves more than 95 percent of its compression gains."",""1558-2183"","""",""10.1109/TPDS.2020.2991030"",""National Natural Science Foundation of China(grant numbers:61821003,61832007,61772222,U1705261,61772180,61902116)"; technological innovation project of Hubei Province(grant numbers:2019(2019 AAA047)); scientific research fund of Hubei Provincial Department of Education(grant numbers:B2017042); Hubei University of Technology(grant numbers:BSQD2019025,BSQD2019022,BSQD2019020,BSQD2019026);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080096"",""Data deduplication";delta compression;storage system;chunk fragmentation;"restore performance"",""Containers";Acceleration;Computer science;Indexes;Measurement;Data storage systems;"Redundancy"","""",""8"","""",""34"",""IEEE"",""28 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Improving Restore Performance of Packed Datasets in Deduplication Systems via Reducing Persistent Fragmented Chunks,""Y. Zhang"; M. Fu; X. Wu; F. Wang; Q. Wang; C. Wang; X. Dong;" H. Han"",""School of Computer Science, Hubei University of Technology, Wuhan, China"; Sangfor Technologies Inc., Shenzhen, China; School of Computer Science, Hubei University of Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Division of Data Storage System, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Science and Technology, Division of Data Storage System, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China; School of Computer Science, Hubei University of Technology, Wuhan, China;" School of Computer Science, Hubei University of Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Mar 2020"",""2020"",""31"",""7"",""1651"",""1664"",""Data deduplication, though being efficient for redundancy elimination in storage systems, introduces chunk fragmentation which severely decreases restore performance. Rewriting algorithms are proposed to reduce the chunk fragmentation. Typically, the backup software aggregates files into larger “tar” type files for storage. We observe that, in tar type datasets, a large number of Persistent Fragmented Chunks (PFCs) are repeatedly rewritten by state-of-the-art rewriting algorithms in every backup, which severely impacts restore performance. We found that the existence of PFCs is due to the traditional strategy of storing PFCs along with other chunks in the containers to preserve the stream locality, rendering them always stored in the containers with low utilization. We propose DePFC to reduce PFCs. DePFC identifies and removes PFCs from the containers preserving the stream locality, and groups them together, to increase the utilization of containers holding them for the subsequent backup, thus preventing them from being rewritten again. We further propose an FC Buffer to avoid mistaken rewrites of PFCs and grouping PFCs that cause restore cache thrashing together. Experimental results demonstrate that DePFC improves restore performance of state-of-the-art rewriting algorithms by 44.24-89.42 percent, while attaining comparable deduplication efficiency, and FC Buffer further improves restore performance."",""1558-2183"","""",""10.1109/TPDS.2020.2972898"",""National Natural Science Foundation of China(grant numbers:61902116,61772180,61832020,61772216)"; National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB10033005); The Scientific Research Fund of Hubei Provincial Department of Education(grant numbers:B2017042); Research Foundation for Advanced Talents of Hubei University of Technology(grant numbers:BSQD2019025,BSQD2019022,BSQD2019020,BSQD2019026);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8994084"",""Data deduplication";storage system;chunk fragmentation;"restore performance"",""Containers";Metadata;Redundancy;Software;Software algorithms;Indexes;"Electromagnetic compatibility"","""",""8"","""",""33"",""IEEE"",""11 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Integrating Task Duplication in Optimal Task Scheduling With Communication Delays,""M. Orr";" O. Sinnen"",""Department of Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand";" Department of Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2020"",""2020"",""31"",""10"",""2277"",""2288"",""Task scheduling with communication delays is an NP-hard problem. Some previous attempts at finding optimal solutions to this problem have used branch-and-bound state-space search, with promising results. Duplication is an extension to the task scheduling model which allows tasks to be executed multiple times within a schedule, providing benefits to schedule length where this allows a reduction in communication costs. This article proposes the first approach to state-space search for optimal task scheduling with task duplication. Also presented are new definitions for important standard bounding metrics in the context of duplication. An extensive empirical evaluation shows that the use of duplication significantly increases the difficulty of optimal scheduling, but the proposed approach also gives certainty that a large proportion of task graphs can be scheduled more effectively when duplication is allowed, and permits to quantify the exact advantage."",""1558-2183"","""",""10.1109/TPDS.2020.2989767"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076867"",""Scheduling";parallel systems;graph and tree search strategies;"optimization"",""Task analysis";Program processors;Schedules;Optimal scheduling;Processor scheduling;Delays;"Search problems"","""",""7"","""",""25"",""IEEE"",""23 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Interval Job Scheduling With Machine Launch Cost,""R. Ren"; Y. Zhu; C. Li;" X. Tang"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore"; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Southeast University, Nanjing, China;" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Jun 2020"",""2020"",""31"",""12"",""2776"",""2788"",""We study an interval job scheduling problem in distributed systems. We are given a set of interval jobs, with each job specified by a size, an arrival time and a processing length. Once a job arrives, it must be placed on a machine immediately and run for a period of its processing length without interruption. The homogeneous machines to run jobs have the same capacity limits such that at anytime, the total size of the jobs running on any machine cannot exceed its capacity. Launching each machine incurs a fixed cost. After launch, a machine is charged a constant cost per time unit until it is terminated. The problem targets to minimize the total cost incurred by the machines for processing the given set of interval jobs. We focus on the algorithmic aspects of the problem in this article. For the special case where all the jobs have a unit size equal to the machine capacity, we propose an optimal offline algorithm and an optimal 2-competitive online algorithm. For the general case where jobs can have arbitrary sizes, we establish a non-trivial lower bound on the optimal solution. Based on this lower bound, we propose a 5-approximation algorithm in the offline setting. In the non-clairvoyant online setting, we design a O(μ)-competitive Modified First-Fit algorithm which is near optimal (μ is the max/min job processing length ratio). In the clairvoyant online setting, we propose an asymptotically optimal O(√log μ)-competitive algorithm based on our Modified First-Fit strategy."",""1558-2183"","""",""10.1109/TPDS.2020.3002786"",""Singapore Ministry of Education Academic Research Fund Tier 1(grant numbers:2019-T1-002-042)"; National Natural Science Foundation of China(grant numbers:61902063); Natural Science Foundation of Jiangsu Province(grant numbers:BK20190342);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9119143"",""Job scheduling";online algorithm;"approximation algorithm"",""Servers";Virtual machining;Computational modeling;Cloud computing;Approximation algorithms;"Job shop scheduling"","""",""7"","""",""32"",""IEEE"",""16 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Large-Scale Automatic K-Means Clustering for Heterogeneous Many-Core Supercomputer,""T. Yu"; W. Zhao; P. Liu; V. Janjic; X. Yan; S. Wang; H. Fu; G. Yang;" J. Thomson"",""University of St Andrews, St Andrews, United Kingdom"; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; University of St Andrews, St Andrews, United Kingdom; University of California, Berkeley, Berkeley, USA; Wellcome Trust Sanger Institute, Saffron Walden, United Kingdom; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China;" University of St Andrews, St Andrews, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Jan 2020"",""2020"",""31"",""5"",""997"",""1008"",""This article presents an automatic k-means clustering solution targeting the Sunway TaihuLight supercomputer. We first introduce a multilevel parallel partition approach that not only partitions by dataflow and centroid, but also by dimension, which unlocks the potential of the hierarchical parallelism in the heterogeneous many-core processor and the system architecture of the supercomputer. The parallel design is able to process large-scale clustering problems with up to 196,608 dimensions and over 160,000 targeting centroids, while maintaining high performance and high scalability. Furthermore, we propose an automatic hyper-parameter determination process for k-means clustering, by automatically generating and executing the clustering tasks with a set of candidate hyper-parameter, and then determining the optimal hyper-parameter using a proposed evaluation method. The proposed autoclustering solution can not only achieve high performance and scalability for problems with massive high-dimensional data, but also support clustering without sufficient prior knowledge for the number of targeted clusters, which can potentially increase the scope of k-means algorithm to new application areas."",""1558-2183"","""",""10.1109/TPDS.2019.2955467"",""Engineering and Physical Sciences Research Council(grant numbers:EP/P020631/1)"; ABC: Adaptive Brokerage(grant numbers:EP/R010528/1); National Key R&D Program of China(grant numbers:2017YFE0123600); China Postdoctoral Science Foundation(grant numbers:2018M641359); Center for High Performance Computing and System Simulation of Pilot National Laboratory for Marine Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911258"",""Supercomputer";heterogeneous many-core processor;data partitioning;clustering;scheduling;"AutoML"",""Supercomputers";Task analysis;Clustering algorithms;Parallel processing;Partitioning algorithms;Scalability;"Graphics processing units"","""",""8"","""",""44"",""IEEE"",""25 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Local-Density Subspace Distributed Clustering for High-Dimensional Data,""Y. -a. Geng"; Q. Li; M. Liang; C. -Y. Chi; J. Tan;" H. Huang"",""Beijing Key Lab of Transportation Data Analysis and Mining, Beijing Jiaotong University, Beijing, China"; Beijing Key Lab of Transportation Data Analysis and Mining, Beijing Jiaotong University, Beijing, China; WeiXin Group, Tencent Company Limited, Beijing, China; Institute of Communications Engineering, National Tsing Hua University, Hsinchu, Taiwan; Department of Business Administration, Beijing Technology and Business University, Beijing, China;" Department of Electrical and Computer Engineering, University of Pittsburgh, Pittsburgh, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Mar 2020"",""2020"",""31"",""8"",""1799"",""1814"",""Distributed clustering is emerging along with the advent of the era of big data. However, most existing established distributed clustering methods focus on problems caused by a large amount of data rather than caused by the large dimension of data. Consequently, they suffer the “curse” of dimensionality (e.g., poor performance and heavy network overhead) when high-dimensional (HD) data are clustered. In this article, we propose a distributed algorithm, referred to as Local Density Subspace Distributed Clustering (LDSDC) algorithm, to cluster large-scale HD data, motivated by the idea that a local dense region of a HD dataset is usually distributed in a low-dimensional (LD) subspace. LDSDC follows a local-global-local processing structure, including grouping of local dense regions (atom clusters) followed by subspace Gaussian model (SGM) fitting (flexible and scalable to data dimension) at each sub-site, merging of atom clusters at every sub-site according to the merging result broadcast from the global site. Moreover, we propose a fast method to estimate the parameters of SGM for HD data, together with its convergence proof. We evaluate LDSDC on both synthetic and real datasets and compare it with four state-of-the-art methods. The experimental results demonstrate that the proposed LDSDC yields best overall performance."",""1558-2183"","""",""10.1109/TPDS.2020.2975550"",""National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2017YFC1501503)"; Natural Science Foundation of Beijing Municipality(grant numbers:L191016); National Social Science Foundation of China(grant numbers:18CSH019); Beijing Municipal Education Commission Research Program(grant numbers:SM20191001107,PXM2019_014213_000007); China Scholarship Council(grant numbers:201907090062); Ministry of Science and Technology of the People's Republic of China(grant numbers:109-2221-E-007-024);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9007481"",""High-dimensional clustering";distributed clustering;density-base clustering;"subspace Gaussian model"",""Clustering algorithms";Distributed databases;Principal component analysis;Data models;Clustering methods;Big Data;"Kernel"","""",""5"","""",""62"",""IEEE"",""24 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Location-Aware and Budget-Constrained Service Deployment for Composite Applications in Multi-Cloud Environment,""T. Shi"; H. Ma; G. Chen;" S. Hartmann"",""School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand"; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand;" Department of Informatics, Clausthal University of Technology, Clausthal-Zellerfeld, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2020"",""2020"",""31"",""8"",""1954"",""1969"",""Enterprise application providers are increasingly moving their workloads to the cloud for technical and economic benefits. Multi-cloud environment makes it possible to orchestrate multiple cloud resources. With the increasing number of available cloud resources provided by multiple cloud providers at different locations with different prices, application providers face the challenge to select proper cloud resources to deploy their applications in the form of a workflow of component service units. Existing studies usually consider minimizing execution time or/and deployment cost. From the perspective of application providers, however, they also pay huge attention to application response time, including particularly network latency between deployed services and users. Meanwhile, application deployment is often subject to stringent budgetary control to ensure financial viability. This article studies a new type of composite application deployment problem that jointly considers both the performance optimization and budget control in multi-cloud at the global scale. To find solutions with minimal response time without running into the risk of over-spending, we propose a hybrid GA-based approach, featuring new design of domain-tailored service clustering, repair algorithm, solution representation, population initialization, and genetic operators. Extensive experiments using the real-world dataset demonstrate that our proposed hybrid GA approach outperforms some recently proposed approaches."",""1558-2183"","""",""10.1109/TPDS.2020.2981306"",""New Zealand Marsden Fund(grant numbers:VUW1510)"; Royal Society of New Zealand;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9039753"",""Location";budget;service deployment;multi-cloud;composite applications;genetic algorithm;clustering;"repair algorithm"",""Cloud computing";Maintenance engineering;Optimization;Genetic algorithms;Time factors;Clustering algorithms;"Business"","""",""31"","""",""56"",""IEEE"",""17 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Lock-Free Parallelization for Variance-Reduced Stochastic Gradient Descent on Streaming Data,""Y. Peng"; Z. Hao;" X. Yun"",""Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China;" Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 May 2020"",""2020"",""31"",""9"",""2220"",""2231"",""Stochastic Gradient Descent (SGD) is an iterative algorithm for fitting a model to the training dataset in machine learning problems. With low computation cost, SGD is especially suited for learning from large datasets. However, the variance of SGD tends to be high because it uses only a single data point to determine the update direction at each iteration of gradient descent, rather than all available training data points. Recent research has proposed variance-reduced variants of SGD by incorporating a correction term to approximate full-data gradients. However, it is difficult to parallelize such variants with high performance and accuracy, especially on streaming data. As parallelization is a crucial requirement for large-scale applications, this article focuses on the parallel setting in a multicore machine and presents LFS-STRSAGA, a lock-free approach to parallelizing variance-reduced SGD on streaming data. LFS-STRSAGA embraces a lock-free data structure to process the arrival of streaming data in parallel, and asynchronously maintains the essential information to approximate full-data gradients with low cost. Both our theoretical and empirical results show that LFS-STRSAGA matches the accuracy of the state-of-the-art variance-reduced SGD on streaming data under sparsity assumption (common in machine learning problems), and that LFS-STRSAGA reduces the model update time by over 98 percent."",""1558-2183"","""",""10.1109/TPDS.2020.2987867"",""National Natural Science Foundation of China(grant numbers:61702499)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068443"",""Gradient descent methods";machine learning;parallel computing;multicore;"streaming data"",""Computational modeling";Machine learning;Data models;Training data;Multicore processing;Training;"Mathematical model"","""",""6"","""",""30"",""IEEE"",""15 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Low-Cost Datacenter Load Balancing With Multipath Transport and Top-of-Rack Switches,""E. Dong"; X. Fu; M. Xu;" Y. Yang"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Institute of Computer Science, University of Goettingen, Goettingen, Germany; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2020"",""2020"",""31"",""10"",""2232"",""2247"",""Load balancing in datacenter networks (DCNs) is an important and challenging task for datacenter managers. A number of sophisticated technologies have been proposed to improve load balancing performance in a complicated circumstance, i.e., with various traffic characteristics. Many approaches need a high cost to implement, such as changing switch hardware. The efficiency problem has not been well addressed. MPTCP was proposed as a low-cost approach to improve data transmission in DCNs, which uses subflows to balance workloads across multiple paths. However, current MPTCP is not satisfying, especially when there are rack-local flows or many-to-one short flows. In this article, we propose DCMPTCP to improve the efficacy of MPTCP. We gradually develop three mechanisms. First, DCMPTCP identifies rack-local traffic and eliminates unnecessary subflows to reduce the overhead. Second, DCMPTCP estimates flow length and establishes subflows in a smarter way. Third, DCMPTCP strengthens explicit congestion notification to improve the congestion control performance on inter-rack many-to-one short flows. We have implemented DCMPTCP in both the Linux kernel and ns-3 simulator. Our comprehensive testbed experiments and simulations show that DCMPTCP outperforms MPTCP in both 1 Gbps testbed, and 10 Gbps large-scale simulation network."",""1558-2183"","""",""10.1109/TPDS.2020.2989441"",""National Natural Science Foundation of China(grant numbers:61625203,61832013)"; National Key R&D Program of China(grant numbers:2017YFB0801701);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9076326"",""Datacenter networks";multipath TCP;load balancing;network protocols;"network communications"",""Load management";Throughput;Computer science;Hardware;Data communication;Servers;"Bandwidth"","""",""7"","""",""55"",""IEEE"",""22 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Making Application-Level Crash Consistency Practical on Flash Storage,""D. H. Kang"; C. Min; S. -W. Lee;" Y. I. Eom"",""Department of Computer Engineering, Dongguk University-Gyeongju, Gyeongju, South Korea"; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, USA; College of Computing, Sungkyunkwan University, Suwon, South Korea;" College of Computing, Sungkyunkwan University, Suwon, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1009"",""1020"",""We present the design, implementation, and evaluation of a new file system, called ACCFS, supporting application-level crash consistency as its first-class citizen functionality. With ACCFS, application data can be correctly recovered in the event of system crashes without any complex update protocol at the application level. With the help of the SHARE interface supporting atomic address remapping at the flash storage layer, ACCFS can easily and efficiently achieve crash consistency as well as single-write journaling. We prototyped ACCFS by slightly modifying the full data journal mode in ext4, implemented the SHARE interface as firmware in a commercial SSD available in the market, and carried out various experiments by running ACCFS on top of the SSD. Our preliminary experimental results are very promising. For instance, the performance of an OLTP benchmark using MySQL/InnoDB engine can be boosted by more than 2-6x by offloading the responsibility of guaranteeing the atomic write of MySQL data pages from the InnoDB engine's own journaling mechanism to ACCFS. This impressive performance gain is in part due to the single-write journaling in ACCFS and in part comes from the fact that the frequent fsync() calls caused by the complex update protocol at the application level can be avoided. ACCFS is a practical solution for the crash consistency problem in that (1) the SHARE interface can be, like the TRIM command, easily supported by commercial SSDs, (2) it can be embodied with a minor modification on the existing ext4 file system, and (3) the existing applications can be made crash consistent simply by opening files in O_ATOMIC mode while the legacy applications can be run without any change."",""1558-2183"","""",""10.1109/TPDS.2019.2959305"",""Institute for Information and communications Technology Promotion(grant numbers:IITP-2015-0-00284)"; NVRam Based High Performance Open Source DBMS Development(grant numbers:2015-0-00314);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936358"",""File system-level consistency";application-level consistency;flash storage device;"address remapping"",""Computer crashes";Engines;Databases;Protocols;Atomic layer deposition;Benchmark testing;"Metadata"","""",""3"","""",""39"",""IEEE"",""18 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Massively Scaling Seismic Processing on Sunway TaihuLight Supercomputer,""Y. Hu"; H. Yang; Z. Luan; L. Gan; G. Yang;" D. Qian"",""State Key Laboratory of Software Development Environment, Beihang University, Beijing, China"; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; Sino-German Joint Software Institute, School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Sino-German Joint Software Institute, School of Computer Science and Engineering, Beihang University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1194"",""1208"",""Common Midpoint (CMP) and Common Reflection Surface (CRS) are widely used methods for improving the signal-to-noise ratio in the field of seismic processing. These methods are computationally intensive and require high-performance computing. This article optimizes these methods on the Sunway many-core architecture and implements large-scale seismic processing on the Sunway Taihulight supercomputer. We propose the following three optimization techniques: 1) we propose a software cache method to reduce the overhead of memory accesses, and share data among CPEs via the register communication"; 2) we re-design the semblance calculation procedure to further reduce the overhead of memory accesses;" 3) we propose a vectorization method to improve the performance when processing the small volume of data within short loops. The experimental results show that our implementations of CMP and CRS methods on Sunway achieve 3.50× and 3.01× speedup on average compared to the-state-of-the-art implementations on CPU. In addition, our implementation is capable to run on more than one million cores of Sunway TaihuLight with good scalability."",""1558-2183"","""",""10.1109/TPDS.2019.2962395"",""National Key R&D Program of China(grant numbers:2016YFB1000503,2016YFB0200100)"; National Natural Science Foundation of China(grant numbers:61502019,61732002); State Key Laboratory of Software Development Environment(grant numbers:SKLSDE-2018ZX-19); Center for High Performance Computing and System Simulation; Pilot National Laboratory for Marine Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943329"",""Many-core architecture";sunway taihulight;seismic processing;common midpoint;common reflection surface;"performance optimization"",""Computer architecture";Software;Surface treatment;Stacking;Acceleration;Supercomputers;"Optimization"","""",""7"","""",""31"",""IEEE"",""25 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"MEMPHA: Model of Exascale Message-Passing Programs on Heterogeneous Architectures,""S. Z. Koohi"; N. A. W. A. Hamid; M. Othman;" G. Ibragimov"",""Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Universiti Putra Malaysia, Seri Kembangan, Malaysia"; Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Institute for Mathematical Research, Universiti Putra Malaysia, Seri Kembangan, Malaysia; Department of Communication Technology and Network, Faculty of Computer Science and Information Technology, Institute for Mathematical Research, Universiti Putra Malaysia, Seri Kembangan, Malaysia;" Department of Mathematics, Faculty of Science, Universiti Putra Malaysia, Seri Kembangan, Malaysia"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Jun 2020"",""2020"",""31"",""11"",""2570"",""2581"",""Delivering optimum performance on a parallel computer is highly dependant on the efficiency of the scheduling and mapping procedure. If the composition of the parallel application is known a prior, the mapping can be accomplished statically on the compilation time. The mapping algorithm uses the model of the parallel application and maps its tasks to processors in a way to minimize the total execution time. In this article, current modeling approaches have discussed. Later, a new modeling schema named Model of Exascale Message-Passing Programs on Heterogeneous Architectures (MEMPHA) has proposed. A comparative study has been performed between MEMPHA and existing models. To exhibit the efficiency of the MEMPHA, experiments have performed on a set of data-set hypergraphs. The results obtained from the experiments show that deploying the MEMPHA helps to optimize metrics, including the congestion, total communication volume and maximum volume of data being sent or received. These improvements vary from 76 to 1 percent, depending on the metric and benchmark model. Moreover, MEMPHA supports the modeling of applications with multiple producers for a single data transmission, where the rest of the approaches fail."",""1558-2183"","""",""10.1109/TPDS.2020.2995867"",""Universiti Putra Malaysia(grant numbers:GP/2017/9569600)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096589"",""Parallel models";scheduling and task partitioning;heterogeneous (hybrid) systems;"modelling and prediction"",""Task analysis";Computational modeling;Mathematical model;Computer architecture;Program processors;Parallel processing;"Measurement"","""",""2"","""",""41"",""IEEE"",""19 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Millimeter-Scale and Billion-Atom Reactive Force Field Simulation on Sunway Taihulight,""P. Gao"; X. Duan; T. Zhang; M. Zhang; B. Schmidt; X. Zhang; H. Sun; W. Zhang; L. Gan; W. Xue; H. Fu; W. Liu;" G. Yang"",""School of Software, Shandong University, Jinan, China"; School of Software, Shandong University, Jinan, China; School of Software, Shandong University, Jinan, China; School of Software, Shandong University, Jinan, China; Institute for Computer Science, Johannes Gutenberg University, Mainz, Germany; Materials Genome Institute, Shanghai University, Shanghai, China; Materials Genome Institute, Shanghai University, Shanghai, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Earth System Science, Ministry of Education Key Laboratory for Earth System Modeling, Tsinghua University, Beijing, China; School of Software, Shandong University, Jinan, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jul 2020"",""2020"",""31"",""12"",""2954"",""2967"",""Large-scale molecular dynamics (MD) simulations on supercomputers play an increasingly important role in many research areas. With the capability of simulating charge equilibration (QEq), bonds and so on, Reactive force field (ReaxFF) enables the precise simulation of chemical reactions. Compared to the first principle molecular dynamics (FPMD), ReaxFF has far lower requirements on computational resources so that it can achieve higher efficiencies for large-scale simulations. In this article, we present our efforts on scaling ReaxFF on the Sunway TaihuLight Supercomputer (TaihuLight). We have carefully redesigned the force analysis and neighbor list building steps. By applying fine-grained optimizations we gain better single process performance. For the many-body interactions, we propose an isolated computation and update strategy and implement inverse trigonometric functions. For QEq, we implement a pipelined conjugate gradient (CG) approach to achieving better scalability. Furthermore, we reorganize the data layout and implement the update operation based on data locality in ReaxFF. Our experiments show that this approach can simulate chemical reactions with 1,358,954,496 atoms using 4,259,840 cores with a performance of 0.015 ns/day. To our best knowledge, this is the first realization of chemical reaction simulation with a millimeter-scale force field."",""1558-2183"","""",""10.1109/TPDS.2020.3008499"",""National Natural Science Foundation of China(grant numbers:61972231,U1806205,51761135015,U1839206)"; Key Project of Joint Fund of Shandong Province(grant numbers:ZR2019LZH007); Shenzhen Basic Research Fund(grant numbers:JCYJ20180507182818013); Center for High Performance Computing and System Simulation, Pilot National Laboratory for Marine Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9138781"",""High performance computing";molecular dynamics;computational science;"Sunway TaihuLight Supercomputer (TaihuLight)"",""Computational modeling";Force;Supercomputers;Chemicals;Dynamics;Mathematical model;"Multicore processing"","""",""12"","""",""33"",""IEEE"",""10 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;
"Minimizing Tardiness for Data-Intensive Applications in Heterogeneous Systems: A Matching Theory Perspective,""K. Xu"; L. Lv; T. Li; M. Shen; H. Wang;" K. Yang"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Protocol Research Lab, Huawei, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Department of Computer Science and Engineering, University of Minnesota at Duluth, Duluth, USA;" School of Computer Science and Electronic Engineering, University of Essex, Colchester, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""144"",""158"",""The increasing data requirements of Internet applications have driven a dramatic surge in developing new programming paradigms and complex scheduling algorithms to handle data-intensive workloads. Due to the expanding volume and the variety of such flows, their raw data are often processed on Intermediate Processing Nodes (IPNs) before being sent to servers. However, the intermediate processing constraint is rarely considered in existing flow computing models. This paper aims to minimize the tardiness of data-intensive applications in the presence of intermediate processing constraint. Motivating cases show that the tardiness is affected by both IPN locations and flow dispatching strategies. Based on the observation that dispatching flows to IPNs is essentially building a matching between flows and IPNs, a novel solution is proposed based on matching theory. In the deployment phase, a tardiness-aware deferred acceptance algorithm is developed to optimize IPN locations. In the operation phase, the Power-of-D paradigm and matching theory are combined together to dispatch flows efficiently. Evaluation results show that our solution effectively minimizes the total tardiness of data-intensive applications in heterogeneous systems."",""1558-2183"","""",""10.1109/TPDS.2019.2930992"",""National Key R&D Program of China(grant numbers:2018YFB0803405)"; China National Funds for Distinguished Young Scientists(grant numbers:61825204); National Natural Science Foundation of China(grant numbers:61602039); Natural Science Foundation of Beijing Municipality(grant numbers:4192050); CCF-Tencent Open Fund WeBank; UK EPSRC Project NIRVANA(grant numbers:EP/L026031/1);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8772187"",""Heterogeneous system";data-intensive application;matching theory;"power-of-D"",""Routing";Delays;Dispatching;Cameras;Relays;"Job shop scheduling"","""",""11"","""",""69"",""IEEE"",""25 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Minority Disk Failure Prediction Based on Transfer Learning in Large Data Centers of Heterogeneous Disk Systems,""J. Zhang"; K. Zhou; P. Huang; X. He; M. Xie; B. Cheng; Y. Ji;" Y. Wang"",""Key Laboratory of Information Storage System, Intelligent Cloud Storage Joint Research Center, Wuhan National Laboratory for Optoelectronics (Huazhong University of Science and Technology), Huazhong University of Science and Technology, Wuhan, China"; Key Laboratory of Information Storage System, Intelligent Cloud Storage Joint Research Center, Wuhan National Laboratory for Optoelectronics (Huazhong University of Science and Technology), Huazhong University of Science and Technology, Wuhan, China; Temple University, Philadelphia, USA; Temple University, Philadelphia, USA; Tencent Inc., Shenzhen, China; Tencent Inc., Shenzhen, China; Tencent Inc., Shenzhen, China;" Tencent Inc., Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 May 2020"",""2020"",""31"",""9"",""2155"",""2169"",""The storage system in large scale data centers is typically built upon thousands or even millions of disks, where disk failures constantly happen. A disk failure could lead to serious data loss and thus system unavailability or even catastrophic consequences if the lost data cannot be recovered. While replication and erasure coding techniques have been widely deployed to guarantee storage availability and reliability, disk failure prediction is gaining popularity as it has the potential to prevent disk failures from occurring in the first place. Recent trends have turned toward applying machine learning approaches based on disk SMART attributes for disk failure predictions. However, traditional machine learning (ML) approaches require a large set of training data in order to deliver good predictive performance. In large-scale storage systems, new disks enter gradually to augment the storage capacity or to replace failed disks, leading storage systems to consist of small amounts of new disks from different vendors and/or different models from the same vendor as time goes on. We refer to this relatively small amount of disks as minority disks. Due to the lack of sufficient training data, traditional ML approaches fail to deliver satisfactory predictive performance in evolving storage systems which consist of heterogeneous minority disks. To address this challenge and improve the predictive performance for minority disks in large data centers, we propose a minority disk failure prediction model named TLDFP based on a transfer learning approach. Our evaluation results in two realistic datasets have demonstrated that TLDFP can deliver much more precise results and lower additional maintenance cost, compared to four popular prediction models based on traditional ML algorithms and two state-of-the-art transfer learning methods."",""1558-2183"","""",""10.1109/TPDS.2020.2985346"",""National Natural Science Foundation of China(grant numbers:61821003)"; National Basic Research Program of China (973 Program)(grant numbers:2016YFB0800402); National Science Foundation(grant numbers:CCF-1717660,CCF-1813081);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9057467"",""Disk failure";machine learning;transfer learning;cloud computing;"data center"",""Data centers";Servers;Data models;Predictive models;Training data;Support vector machines;"Reliability"","""",""20"","""",""52"",""IEEE"",""6 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Modeling Analysis and Cost-Performance Ratio Optimization of Virtual Machine Scheduling in Cloud Computing,""B. Wan"; J. Dang; Z. Li; H. Gong; F. Zhang;" S. Oh"",""Department of Computer Science and Technology, Xidian University, Xi'an, China"; Department of Computer Science and Technology, Xidian University, Xi'an, China; Key Laboratory of Hunan Province for Internet of Things and Information Security and College of Information Engineering, Xiangtan University, Xiangtan, China; College of Mathematics and Statistics, Changsha University of Science and Technology, Changsha, China; DEKE Lab, School of Information, Renmin University of China, Beijing, China;" Department of Computer and Information Engineering, Ajou University, Suwon, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Feb 2020"",""2020"",""31"",""7"",""1518"",""1532"",""As an essential feature of cloud computing, dynamic scalability enables the cloud system to dynamically expand or shrink resources according to user needs at runtime. Effectively predicting and optimizing the cost and performance of cloud computing platforms have become one of the key research challenges in the field of cloud computing. In this article, to quantitatively predict the cost and performance of cloud computing platforms, we propose a cloud computing resource analysis model considering both hot/cold startup and hot/cold shutdown of virtual machines (VMs), and use the M/M/N/oo queuing model to analyze cloud computing platform and acquire accurate performance indicators, such as elasticity indicators, cost indicators, performance indicators, cost-performance ratios, etc. In addition, we establish a multi-objective optimization model to optimize both performance and cost of cloud computing platform. Then the optimal stopping and cost-performance optimization algorithm are applied to obtain the optimal configurations, including the number of hot startup VMs, the system service rate, the hot/cold startup rate of VMs, and the hot/cold shutdown rate. By comparing with existing optimization methods, we demonstrate the superiority of our cost-performance ratio optimization method."",""1558-2183"","""",""10.1109/TPDS.2020.2968913"",""Science and Technology Projects of Xi'an, China(grant numbers:201809170CX11JC12)"; National Natural Science Foundation of China(grant numbers:61972302); Key Research and Development Program of Shanxi Provence(grant numbers:2017ZDXM-GY-002); Hunan Provincial Natural Science Foundation of China for Distinguished Young Scholars(grant numbers:2018JJ1025); Hunan Science and Technology Planning(grant numbers:2019RS3019); National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB1003702);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967018"",""Cloud computing";queuing system;"cost-performance ratio optimization"",""Cloud computing";Optimization;Computational modeling;Analytical models;Task analysis;Queueing analysis;"Elasticity"","""",""15"","""",""43"",""IEEE"",""23 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"NVGraph: Enforcing Crash Consistency of Evolving Network Analytics in NVMM Systems,""S. Lim"; T. Coy; Z. Lu; B. Ren;" X. Zhang"",""School of Computer Science and Engineering, Washington State University, Vancouver, USA"; School of Computer Science and Engineering, Washington State University, Vancouver, USA; School of Computer Science and Engineering, Washington State University, Vancouver, USA; Department of Computer Science, College of William and Mary, Williamsburg, USA;" School of Computer Science and Engineering, Washington State University, Vancouver, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jan 2020"",""2020"",""31"",""6"",""1255"",""1269"",""Many complex networks can be modeled as evolving graphs. Existing in-memory graph data structures are designed for DRAM. Thus, they cannot effectively exploit the current and ongoing adoption of emerging non-volatile main memory (NVMM) for two reasons. (1) Ephemeral graph data structures are not crash-consistent for NVMM. (2) NVMM writes and reads and may incur higher latency than DRAM. In this article, we propose a novel persistent evolving graph data structure, named NVGRAPH, for both computing and in-memory storage of evolving graphs in NVMM. We devise NVGRAPH as a multi-version data structure, wherein a minimum of one version of its data is stored in NVMM to provide the desired durability at runtime for failure recovery, and another version is stored in both DRAM and NVMM to reduce the NVMM-induced memory latency. NVGRAPH is also a partitioned data structure. We dynamically transform the layout of NVGRAPH (e.g., changing the size of its partition in DRAM) exploiting network properties and data access patterns of workloads. For the evaluation of NVGRAPH, we implement four representative real-world graph applications: pagerank, BFS, influence maximization, and rumor source detection. The experimental results show that the performance of NVGRAPH is comparable to other in-memory data structure (e.g., CSR and LLAMA) while using 70 percent less DRAM. It scales well up to 10 billion edges and 201 snapshots and supports crash consistency. It offers up to the 21X speedup of execution time compared to the scale-up graph computation approaches (e.g., GraphChi and X-stream)."",""1558-2183"","""",""10.1109/TPDS.2020.2965452"",""National Science Foundation(grant numbers:CNS-1906541)"; DoE Electricity Industry Technology and Practices Innovation Challenge Award; WSU Vancouver Research;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8955949"",""Evolving graphs";crash consistency;non-volatile main memory;"graph layout transformation"",""Nonvolatile memory";Random access memory;Computer crashes;Runtime;Layout;"Arrays"","""",""3"","""",""75"",""IEEE"",""10 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"On Fault-Tolerant Bin Packing for Online Resource Allocation,""C. Li";" X. Tang"",""MOE Key Laboratory of Computer Network and Information Integration, School of Computer Science and Engineering, Southeast University, Nanjing, China";" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""817"",""829"",""We study an online fault-tolerant bin packing problem that models reliable resource allocation. In this problem, each item is replicated and has f + 1 replicas including one primary and f standbys. The packing of items is required to tolerate up to f faulty bins, i.e., to guarantee that at least one correct replica of each item is available regardless of which f bins turn to be faulty. Any feasible packing algorithm must satisfy an exclusion constraint and a space constraint. The exclusion constraint is generalized from the fault tolerance requirement and the space constraint comes from the capacity planning. The target of bin packing is to minimize the number of bins used. We first derive a lower bound on the number of bins needed by any feasible packing algorithm. We then study three heuristic algorithms named mirroring, shifting and mixing under a particular setting where all items have the same size. The mirroring algorithm has a low utilization of the bin capacity. Compared with the mirroring algorithm, the shifting algorithm requires fewer bins. However, in online packing, the process of opening bins by the shifting algorithm is not smooth. It turns out that even for packing a few items, the shifting algorithm needs to quickly open a large number of bins. The mixing algorithm adopts a dual average strategy to gradually open new bins for incoming items. We prove that the mixing algorithm is feasible and show that it balances the number of bins used and the process of opening bins. Finally, to pack items with different sizes, we extend the mirroring algorithm by adopting the First-Fit strategy and extend both the shifting and mixing algorithms by involving the harmonic strategy. The asymptotic competitive ratios of the three extended algorithms are analyzed respectively."",""1558-2183"","""",""10.1109/TPDS.2019.2948327"",""Singapore MOE Academic Research Fund(grant numbers:2013-T1-002-123,2018-T1-002-063)"; National Natural Science Foundation of China(grant numbers:61902063); Natural Science Foundation of Jiangsu Province(grant numbers:BK20190342);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8877781"",""Fault-tolerance";bin packing;heuristic;"online"",""Fault tolerance";Fault tolerant systems;Servers;Resource management;Heuristic algorithms;"Switches"","""",""7"","""",""21"",""IEEE"",""21 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"On-Edge Multi-Task Transfer Learning: Model and Practice With Data-Driven Task Allocation,""Q. Chen"; Z. Zheng; C. Hu; D. Wang;" F. Liu"",""National Engineering Research Center for Big Data Technology and System, Key Laboratory of Services Computing Technology and System, Ministry of Education, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; Edge Cloud Innovation Lab, Technical Innovation Department, Cloud BU, Huawei Technologies Co., Ltd., Shenzhen, China; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong;" National Engineering Research Center for Big Data Technology and System, Key Laboratory of Services Computing Technology and System, Ministry of Education, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Jan 2020"",""2020"",""31"",""6"",""1357"",""1371"",""On edge devices, data scarcity occurs as a common problem where transfer learning serves as a widely-suggested remedy. Nevertheless, transfer learning imposes heavy computation burden to the resource-constrained edge devices. Existing task allocation works usually assume all submitted tasks are equally important, leading to inefficient resource allocation at a task level when directly applied in Multi-task Transfer Learning (MTL). To address these issues, we first reveal that it is crucial to measure the impact of tasks on overall decision performance improvement and quantify task importance. We then show that task allocation with task importance for MTL (TATIM) is a variant of NP-complete Knapsack problem, where the complicated computation to solve this problem needs to be conducted repeatedly under varying contexts. To solve TATIM with high computational efficiency, we propose a Data-driven Cooperative Task Allocation (DCTA) approach. Finally, we evaluate the performance of DCTA by not only a trace-driven simulation, but also a new comprehensive real-world AIOps case study which bridges model and practice via a new architecture and main components design within AIOps system. Extensive experiments show that our DCTA reduces 3.24 times of processing time, and saves 48.4 percent energy consumption compared with the state-of-the-art when solving TATIM."",""1558-2183"","""",""10.1109/TPDS.2019.2962435"",""National Natural Science Foundation of China(grant numbers:61722206,61761136014,61520106005)"; NSFC-DFG(grant numbers:392046569); National Key Research and Development (R&D)(grant numbers:2017YFB1001703); Fundamental Research Funds for the Central Universities(grant numbers:2017KFKJXX009,3004210116); RGC GRF(grant numbers:PolyU 15210119,CRF C5026-18G,ITF UIM/363,ITF ITS/070/19FP,PolyU 1-ZVPZ); Huawei Collaborative;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943178"",""Edge computing";transfer learning;data-driven task allocation;"real-world application"",""Task analysis";Resource management;Image edge detection;Data models;Machine learning;Performance evaluation;"Computational modeling"","""",""42"","""",""69"",""IEEE"",""25 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Online Deadline-Aware Task Dispatching and Scheduling in Edge Computing,""J. Meng"; H. Tan; X. -Y. Li; Z. Han;" B. Li"",""LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China"; LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China; LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China; LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China;" LINKE Lab, School of Computer Science and Technology, University of Science and Technology of China (USTC), Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jan 2020"",""2020"",""31"",""6"",""1270"",""1286"",""In this article, we study online deadline-aware task dispatching and scheduling in edge computing. We jointly considerthe management of the networking and computing resources to meet the maximum number of deadlines. We propose an online algorithm, named Dedas, which greedily schedules newly arriving tasks and considers whether to replace some existing tasks in order to make the new deadlines satisfied. We derive a non-trivial competitive ratio of Dedas theoretically, and our analysis is asymptotically tight. Besides, we implement a distributed approximation D - Dedas with a better scalability and less than 10 percent performance loss compared with the centralized algorithm Dedas. We then build DeEdge, an edge computing testbed installed with typical latency-sensitive applications such as IoT sensor monitoring and face matching. We adopt a real-world data trace from the Google cluster for large-scale emulations. Extensive testbed experiments and simulations demonstrate that the deadline miss ratio of Dedas is stable for online tasks, which is reduced by up to 60 percent compared with state-of-the-art methods. Moreover, Dedas performs well in minimizing the average task completion time."",""1558-2183"","""",""10.1109/TPDS.2019.2961905"",""National Key R&D Program of China(grant numbers:2018YFB0803400)"; China National Funds for Distinguished Young Scientists(grant numbers:61625205); National Natural Science Foundation of China(grant numbers:61772489,61751211); Key Research Program of Frontier Sciences(grant numbers:QYZDY-SSW-JSC002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8941266"",""Edge computing";task dispatching and scheduling;deadline-aware tasks;"online algorithm"",""Task analysis";Servers;Bandwidth;Processor scheduling;Computational modeling;Cloud computing;"Edge computing"","""",""72"","""",""31"",""IEEE"",""24 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Online Placement and Scaling of Geo-Distributed Machine Learning Jobs via Volume-Discounting Brokerage,""X. Li"; R. Zhou; L. Jiao; C. Wu; Y. Deng;" Z. Li"",""School of Computer Science, Wuhan University, Wuhan, China"; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Department of Computer and Information Science, University of Oregon, Eugene, USA; University of Hong Kong, Kowloon, Hong Kong; School of Computer Science, Wuhan University, Wuhan, China;" School of Computer Science, Wuhan University, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""4"",""948"",""966"",""Geo-distributed machine learning (ML) often uses large geo-dispersed data collections produced over time to train global models, without consolidating the data to a central site. In the parameter server architecture, “workers” and “parameter servers” for a geo-distributed ML job should be strategically deployed and adjusted on the fly, to allow easy access to the datasets and fast exchange of the model parameters at anytime. Despite many cloud platforms now provide volume discounts to encourage the usage of their ML resources, different geo-distributed ML jobs that run in the clouds often rent cloud resources separately and respectively, thus rarely enjoying the benefit of discounts. We study an ML broker service that aggregates geo-distributed ML jobs into cloud data centers for volume discounts via dynamic online placement and scaling of workers and parameter servers in individual jobs for long-term cost minimization. To decide the number and the placement of workers and parameter servers, we propose an efficient online algorithm which first decomposes the online problem into a series of one-shot optimization problems solvable at each individual time slot by the technique of regularization, and afterwards round the fractional decisions to the integer ones via a carefully-designed dependent rounding method. We prove a parameterized-constant competitive ratio for our online algorithm as the theoretical performance analysis, and also conduct extensive simulation studies to exhibit its close-to-offline-optimum practical performance in realistic settings."",""1558-2183"","""",""10.1109/TPDS.2019.2955935"",""National Natural Science Foundation of China(grant numbers:61502504)"; Technological Innovation Major Projects of Hubei Province(grant numbers:2017AAA125); Science and Technology Program of Wuhan City(grant numbers:2018010401011288); WHU-Xiaomi AI Lab, Hong Kong RGC GRF HKU(grant numbers:17204715,17225516,17204619,C7036-15G (CRF),C5026-18G (CRF));" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8913598"",""Geo-distributed machine learning";online placement;"volume discount brokerage"",""Data centers";Data models;Servers;Cloud computing;Machine learning;Optimization;"Minimization"","""",""6"","""",""52"",""IEEE"",""26 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Online Scheduling of Task Graphs on Heterogeneous Platforms,""L. -C. Canon"; L. Marchal; B. Simon;" F. Vivien"",""FEMTO-ST Institute – Université de Bourgogne Franche-Comté, Besançon, France"; FEMTO-ST Institute – Université de Bourgogne Franche-Comté, Besançon, France; University of Bremen, Bibliothekstr, Germany;" FEMTO-ST Institute – Université de Bourgogne Franche-Comté, Besançon, France"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""721"",""732"",""Modern computing platforms commonly include accelerators. We target the problem of scheduling applications modeled as task graphs on hybrid platforms made of two types of resources, such as CPUs and GPUs. We consider that task graphs are uncovered dynamically, and that the scheduler has information only on the available tasks, i.e., tasks whose predecessors have all been completed. Each task can be processed by either a CPU or a GPU, and the corresponding processing times are known. Our study extends a previous 4√m/k-competitive online algorithm by Amaris et al. [1], where mis the number of CPUs and k the number of GPUs (m≥k). We prove that no online algorithm can have a competitive ratio smaller than √m/k . We also study how adding flexibility on task processing, such as task migration or spoliation, or increasing the knowledge of the scheduler by providing it with information on the task graph, influences the lower bound. We provide a (2√m/k+1)-competitive algorithm as well as a tunable combination of a system-oriented heuristic and a competitive algorithm";" this combination performs well in practice and has a competitive ratio in Θ(√m/k). We also adapt all our results to the case of multiple types of processors. Finally, simulations on different sets of task graphs illustrate how the instance properties impact the performance of the studied algorithms and show that our proposed tunable algorithm performs the best among the online algorithms in almost all cases and has even performance close to an offline algorithm."",""1558-2183"","""",""10.1109/TPDS.2019.2942909"",""LABEX MILYON(grant numbers:ANR-10-LABX-0070)"; Université de Lyon(grant numbers:ANR-11-IDEX- 0007); Agence Nationale de la Recherche;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8846007"",""Scheduling";heterogeneous computing;task graphs;"online algorithms"",""Task analysis";Graphics processing units;Runtime;Heuristic algorithms;Schedules;"Processor scheduling"","""",""10"","""",""24"",""IEEE"",""23 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Optimized Block-Based Algorithms to Label Connected Components on GPUs,""S. Allegretti"; F. Bolelli;" C. Grana"",""Dipartimento di Ingegneria Enzo Ferrari, Università degli Studi di Modena e Reggio Emilia, Modena, Italy"; Dipartimento di Ingegneria Enzo Ferrari, Università degli Studi di Modena e Reggio Emilia, Modena, Italy;" Dipartimento di Ingegneria Enzo Ferrari, Università degli Studi di Modena e Reggio Emilia, Modena, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""423"",""438"",""Connected Components Labeling (CCL) is a crucial step of several image processing and computer vision pipelines. Many efficient sequential strategies exist, among which one of the most effective is the use of a block-based mask to drastically cut the number of memory accesses. In the last decade, aided by the fast development of Graphics Processing Units (GPUs), a lot of data parallel CCL algorithms have been proposed along with sequential ones. Applications that entirely run in GPU can benefit from parallel implementations of CCL that allow to avoid expensive memory transfers between host and device. In this paper, two new eight-connectivity CCL algorithms are proposed, namely Block-based Union Find (BUF) and Block-based Komura Equivalence (BKE). These algorithms optimize existing GPU solutions introducing a block-based approach. Extensions for three-dimensional datasets are also discussed. In order to produce a fair comparison with previously proposed alternatives, YACCLAB, a public CCL benchmarking framework, has been extended and made suitable for evaluating also GPU algorithms. Moreover, three-dimensional datasets have been added to its collection. Experimental results on real cases and synthetically generated datasets demonstrate the superiority of the new proposals with respect to state-of-the-art, both on 2D and 3D scenarios."",""1558-2183"","""",""10.1109/TPDS.2019.2934683"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798895"",""Parallel processing";connected components labeling;GPU;"CUDA"",""Vegetation";Graphics processing units;Labeling;Indexes;Data structures;Three-dimensional displays;"Lattices"","""",""22"","""",""58"",""IEEE"",""14 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Geo-Distributed Data Analytics with Coordinated Task Scheduling and Routing,""L. Zhao"; Y. Yang; A. Munir; A. X. Liu; Y. Li;" W. Qu"",""Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China"; Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China; Department of Computer Science, Michigan State University, East Lansing, USA; Department of Computer Science, Michigan State University, East Lansing, USA; Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China;" Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""279"",""293"",""Recent trends show that cloud computing is growing to span more and more globally distributed datacenters. For geo-distributed datacenters, there is an increasingly need for scheduling algorithms to place tasks across datacenters, by jointly considering WAN traffic and computation. This scheduling must deal with situations such as wide-area distributed data, data sharing, WAN bandwidth costs and datacenter capacity limits, while also minimizing makespan. However, this scheduling problem is NP-hard. We propose a new resource allocation algorithm called HPS+, an extension to Hypergraph Partition-based Scheduling. HPS+ models the combined task-data dependencies and data-datacenter dependencies as an augmented hypergraph, and adopts an improved hypergraph partition technique to minimize WAN traffic. It further uses a coordination mechanism to allocate network resources closely following the guidelines of task requirements, for minimizing the makespan. Evaluation across the real China-Astronomy-Cloud model and Google datacenter model show that HPS+ saves the amount of data transfers by upto 53 percent and reduces the makespan by 39 percent compared to existing algorithms."",""1558-2183"","""",""10.1109/TPDS.2019.2938164"",""National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000205)"; National Natural Science Foundation of China(grant numbers:61402325,61872265,U1836214,61672131); Generation of Artificial Intelligence Science and Technology Major Project of Tianjin(grant numbers:18ZXZNGX00190); Key research and Development Program for Guangdong Province(grant numbers:2019B010136001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818672"",""Data Analytics";Task Scheduling;Routing;"Geo-distributed Cloud"",""Task analysis";Data transfer;Wide area networks;Bandwidth;Processor scheduling;Scheduling;"Cloud computing"","""",""18"","""",""46"",""IEEE"",""28 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Optimizing Streaming Parallelism on Heterogeneous Many-Core Architectures,""P. Zhang"; J. Fang; C. Yang; C. Huang; T. Tang;" Z. Wang"",""National University of Defense Technology, Changsha, China"; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China;" University of Leeds, Leeds, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Apr 2020"",""2020"",""31"",""8"",""1878"",""1896"",""As many-core accelerators keep integrating more processing units, it becomes increasingly more difficult for a parallel application to make effective use of all available resources. An effective way of improving hardware utilization is to exploit spatial and temporal sharing of the heterogeneous processing units by multiplexing computation and communication tasks - a strategy known as heterogeneous streaming. Achieving effective heterogeneous streaming requires carefully partitioning hardware among tasks, and matching the granularity of task parallelism to the resource partition. However, finding the right resource partitioning and task granularity is extremely challenging, because there is a large number of possible solutions and the optimal solution varies across programs and datasets. This article presents an automatic approach to quickly derive a good solution for hardware resource partition and task granularity for task-based parallel applications on heterogeneous many-core architectures. Our approach employs a performance model to estimate the resulting performance of the target application under a given resource partition and task granularity configuration. The model is used as a utility to quickly search for a good configuration at runtime. Instead of hand-crafting an analytical model that requires expert insights into low-level hardware details, we employ machine learning techniques to automatically learn it. We achieve this by first learning a predictive model offline using training programs. The learned model can then be used to predict the performance of any unseen program at runtime. We apply our approach to 39 representative parallel applications and evaluate it on two representative heterogeneous many-core platforms: a CPU-XeonPhi platform and a CPU-GPU platform. Compared to the single-stream version, our approach achieves, on average, a 1.6x and 1.1x speedup on the XeonPhi and the GPU platform, respectively. These results translate to over 93 percent of the performance delivered by a theoretically perfect predictor."",""1558-2183"","""",""10.1109/TPDS.2020.2978045"",""National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB0204301)"; National Natural Science Foundation of China(grant numbers:61972408,61602501,61872294);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9022909"",""Heterogeneous computing";parallelism;performance tuning;"machine learning"",""Task analysis";Graphics processing units;Hardware;Parallel processing;Runtime;Machine learning;"Predictive models"","""",""8"","""",""69"",""IEEE"",""3 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"P-PFC: Reducing Tail Latency with Predictive PFC in Lossless Data Center Networks,""C. Tian"; B. Li; L. Qin; J. Zheng; J. Yang; W. Wang; G. Chen;" W. Dou"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Feb 2020"",""2020"",""31"",""6"",""1447"",""1459"",""Remote Direct Memory Access(RDMA) technology rapidly changes the landscape of nowadays datacenter applications. Congestion control for RDMA networking is a critical challenge. As an end-to-end layer 3 congestion control mechanism, Datacenter QCN (DCQCN) alleviates the unfairness and head-of-the-line blocking problems of Priority-based Flow Control (PFC). However, a lossless network does not guarantee low latency even with DCQCN enabled. When network congestion happens, switch queues still build-up due to the response latency of end-to-end solutions. In this article, we propose Predictive PFC (P-PFC) to reduce tail latency in RDMA networks. P-PFC monitors the derivative of buffer occupation, predicts the happening of PFC trigger in the future, and proactively triggers PFC pause in advance. The benefit is that buffer usage can be maintained at a low level, hence the tail latency can be controlled. Preliminary evaluation results demonstrate that P-PFC can reduce tail latency by more than half of that in standard PFC in many scenarios, without hurting the throughput and average latency. P-PFC can also protect innocent flows compared with standard PFC according to our experiments. To our best knowledge, this is the first work of using derivative to improve PFC in lossless RDMA networks."",""1558-2183"","""",""10.1109/TPDS.2020.2969182"",""National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB1003505)"; National Natural Science Foundation of China(grant numbers:61772265,61802172); Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship (Shuangchuang) Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8967139"",""Data center networks";congestion control;PFC;"RDMA"",""Switches";Standards;Data centers;Throughput;Monitoring;Receivers;"Prediction algorithms"","""",""12"","""",""37"",""IEEE"",""23 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Pache: A Packet Management Scheme of Cache in Data Center Networks,""T. Chen"; X. Gao; T. Liao;" G. Chen"",""Shanghai Key Laboratory of Scalable and Computing and Systems, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Shanghai Key Laboratory of Scalable and Computing and Systems, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Scalable and Computing and Systems, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China;" Shanghai Key Laboratory of Scalable and Computing and Systems, Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""253"",""265"",""The communication traffic among servers is a principal bottleneck for data center networks (DCNs), in which redundant traffic has a significant impact on the performance of DCNs. In this paper, we propose Pache, a distributed and efficient cache scheme to achieve traffic redundancy elimination in DCNs. In Pache, nodes cache packets to reduce bandwidth consumption and gain performance revenue. Each cache node manages the local cache and improves cache efficiency with counting bloom filter, hash table and Trie. Each cache node broadcasts the local cache information by a bloom filter to other servers, and servers have cache information tables (cache-table) to record them, which achieves a cache sharing mechanism. Moreover, Pache employs false positive information table (fp-table) to reduce the false positive rate caused by bloom filters. A server determines an original packet or the corresponding fingerprint packet to send by querying cache-table and fp-table. Cache node placement mechanism is also designed to select which node to cache a packet. For evaluating the performance of Pache, we execute extensive simulations and conduct a case study on Amazon EC2 platform from different aspects. The results show that Pache can eliminate about 40 percent redundant traffic on average, and only increase 10 percent runtime, and it is also a scalable and feasible cache scheme in DCNs."",""1558-2183"","""",""10.1109/TPDS.2019.2931905"",""National Natural Science Foundation of China(grant numbers:61872238,61672353)"; Shanghai Science and Technology Fund(grant numbers:17510740200); Huawei Innovation Research Program(grant numbers:HO2018085286); State Key Laboratory of Air Traffic Management System and Technology(grant numbers:SKLATM20180X); CCF-Huawei Database System Innovation Research Plan(grant numbers:CCF-Huawei DBIR2019002A);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8781932"",""Data center network";cache;traffic redundancy elimination;counting bloom filter;hash table;"trie"",""Servers";Redundancy;Peer-to-peer computing;Topology;Data centers;Bandwidth;"Internet"","""",""4"","""",""40"",""IEEE"",""30 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"PALE: Time Bounded Practical Agile Leader Election,""B. Sidik"; R. Puzis; P. Zilberman;" Y. Elovici"",""Telekom Innovation Laboratories, Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel"; Telekom Innovation Laboratories, Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel; Telekom Innovation Laboratories, Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel;" Telekom Innovation Laboratories, Department of Software and Information Systems Engineering, Ben-Gurion University of the Negev, Beer-Sheva, Israel"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""470"",""485"",""Many tasks executed in dynamic distributed systems, such as sensor networks or enterprise environments with bring-your-own-device policy, require central coordination by a leader node. In the past it has been proven that distributed leader election in dynamic environments with constant changes and asynchronous communication is not possible. Thus, state-of-the-art leader election algorithms are not applicable in asynchronous environments with constant network changes. Some algorithms converge only after the network stabilizes (an unrealistic requirement in many dynamic environments). Other algorithms reach consensus in the presence of network changes but require a global clock or some level of communication synchrony. Determining the weakest assumptions, under which bounded leader election is possible, remains an unresolved problem. In this study we present a leader election algorithm that operates in the presence of changes and under weak (realistic) assumptions regarding message delays and regarding the clock drifts of the distributed nodes. The proposed algorithm is self-sufficient, easy to implement and can be extended to support multiple regions, self-stabilization, and mobile ad-hoc networks. We prove the algorithm's correctness and provide a complexity analysis of the time, space, and number of messages required to elect a leader."",""1558-2183"","""",""10.1109/TPDS.2019.2933620"",""Lockheed Martin and RSA EMC";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8789472"",""Leader election";dynamic;partial synchrony;timed;"distributed algorithms"",""Heuristic algorithms";Voting;Delays;Clocks;Task analysis;"Stability criteria"","""",""2"","""",""50"",""IEEE"",""6 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Partitioning Tree-Shaped Task Graphs for Distributed Platforms With Limited Memory,""C. Gou"; A. Benoit;" L. Marchal"",""East China Normal University, Shanghai, China"; Univ. Lyon, CNRS, ENS de Lyon, Inria, Université Claude-Bernard Lyon 1, LIP UMR5668, Lyon, France;" Univ. Lyon, CNRS, ENS de Lyon, Inria, Université Claude-Bernard Lyon 1, LIP UMR5668, Lyon, France"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Feb 2020"",""2020"",""31"",""7"",""1533"",""1544"",""Scientific applications are commonly modeled as the processing of directed acyclic graphs of tasks, and for some of them, the graph takes the special form of a rooted tree. This tree expresses both the computational dependencies between tasks and their storage requirements. The problem of scheduling/traversing such a tree on a single processor to minimize its memory footprint has already been widely studied. The present article considers the parallel processing of such a tree and studies how to partition it for a homogeneous multiprocessor platform, where each processor is equipped with its own memory. We formally state the problem of partitioning the tree into subtrees, such that each subtree can be processed on a single processor (i.e., it must fit in memory), and the goal is to minimize the total resulting processing time. We prove that this problem is NP-complete, and we design polynomial-time heuristics to address it. An extensive set of simulations demonstrates the usefulness of these heuristics."",""1558-2183"","""",""10.1109/TPDS.2020.2971200"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8979173"",""Scheduling";tree partitioning;memory-aware;makespan minimization;"parallel computing"",""Task analysis";Memory management;Computational modeling;Processor scheduling;Scheduling;"Schedules"","""",""6"","""",""23"",""IEEE"",""3 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Pattern-Based Dynamic Compilation System for CGRAs With Online Configuration Transformation,""L. Liu"; X. Man; J. Zhu; S. Yin;" S. Wei"",""Institute of Microelectronics, Tsinghua University, Beijing, China"; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China;" Institute of Microelectronics, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jul 2020"",""2020"",""31"",""12"",""2981"",""2994"",""Prevailing data-intensive applications, such as artificial intelligence and internet of things, demand considerable compute capability. Coarse-grained reconfigurable architectures (CGRAs) can meet this demand via providing abundant compute resources. However, compilation has become an essential problem because the increasing resources need to be orchestrated efficiently. Static compilation is insufficient due to conservative resource allocation and exponentially increasing time cost while state-of-the-art dynamic compilation still performs poorly in both generality and efficiency. This article proposes a dynamic compilation system for CGRAs through online pattern-based configuration transformation, which enables virtualization to improve resource utilization and flexibility. It utilizes statically-generated patterns to straightforwardly determine dynamic placement of registers and operations so that the transformation algorithm has a low complexity. Domain-specific features are extracted by a k-means clustering algorithm to help improve the quality of patterns. The experimental results show that statically compiled applications can be transformed onto arbitrary resources at runtime, reserving 73.5 (22.8-163.3 percent) of the original performance/resource on average, 9.1 (0-52.9 percent) better than the state-of-theart non-general methods."",""1558-2183"","""",""10.1109/TPDS.2020.3007492"",""National Natural Science Foundation of China(grant numbers:61672317,61834002)"; National Science Technology Major Project of China(grant numbers:2018ZX01028-201);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9134964"",""CGRA";dynamic compilation;"configuration transformation"",""Dynamic compiler";Heuristic algorithms;Registers;Dynamic scheduling;Kernel;"Resource management"","""",""3"","""",""26"",""IEEE"",""7 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Performance Analysis of Trial and Error Algorithms,""J. Gaveau"; C. J. Le Martret;" M. Assaad"",""Waveform Design Laboratory, Thales SIX GTS France, Gennevilliers, France"; Waveform Design Laboratory, Thales SIX GTS France, Gennevilliers, France;" Laboratoire des Signaux et Systèmes (L2S), CentraleSupelec & Université Paris-Saclay, Gif sur Yvette, France"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jan 2020"",""2020"",""31"",""6"",""1343"",""1356"",""Model-free decentralized optimizations and learning are receiving increasing attention from theoretical and practical perspectives. In particular, two fully decentralized learning algorithms, namely Trial and Error Learning (TEL) and Optimal Dynamical Learning (ODL), are very appealing for a broad class of games. Indeed, ODL has the property to spend a high proportion of time in an optimum state that maximizes the sum of the utilities of all players, whereas, TEL has the property to spend a high proportion of time in an optimum state that maximizes the sum of the utilities of all players if there is a pure Nash equilibrium, otherwise, it spends a high proportion of time in a state that maximizes a trade-off between the sum of the utilities of the players and a predefined stability function. On the other hand, estimating the mean fraction of time spent in the optimum state (as well as the mean time duration to reach it) is challenging due to the high complexity and dimension of the inherent Markov chains. In this article, under some specific system model, an evaluation of the above performance metrics is provided by proposing an approximation of the considered Markov chains, which allows overcoming the problem of high dimensionality. A comparison between the two algorithms is then performed which allows a better understanding of their performance."",""1558-2183"","""",""10.1109/TPDS.2020.2964256"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8950102"",""Model-free optimization";markov chain;multi-agent system;game theory;"trial and error"",""Games";Convergence;Markov processes;Optimization;Approximation algorithms;Mathematical model;"Resource management"","""",""2"","""",""20"",""IEEE"",""6 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Performance Modeling of Parallel Loops on Multi-Socket Platforms Using Queueing Systems,""Y. Cho"; S. Oh;" B. Egger"",""School of Computer Science and Engineering, Seoul National University, Seoul, Korea"; School of Computer Science and Engineering, Seoul National University;" School of Computer Science and Engineering, Seoul National University, Seoul, Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""318"",""331"",""Predicting the performance of parallel loops on modern shared-memory multi-socket multi-core systems in dependence of the allocated resources is an important means to achieve better system utilization. Previous prediction techniques are tied to specific architectures and do not allow for purely online performance predictions without requiring an offline analysis of the parallel program. This paper presents a practical approach based on queueing theory to model the performance of parallel programs in dependence of the number of allocated core resources. Based on the key insight that scalability of scientific parallel loops is limited by memory performance, a hierarchically constructed M/M/1/N/N queue system is used to analytically compute the response time at the different congestion points in the memory system of modern NUMA architectures. After automatically tuning the model to a specific architecture by executing a number of micro-benchmarks, the required parameter values are obtained at runtime from hardware performance counters present in modern commodity AMD and Intel processors. Evaluated with 24 OpenMP parallel loops on a 64-core AMD and a 72-core Intel multi-socket platform, the presented queueing system is able to accurately predict the speedup of parallel loops with a mean absolute percentage error of 8.3 percent on the AMD system and 6.7 percent on the Intel platform."",""1558-2183"","""",""10.1109/TPDS.2019.2938172"",""National Research Foundation of Korea"; Korean government(grant numbers:NRF-2015K1A3A1A14021288,2016R1A2B4009193); Dept. of Computer Science and Engineering, SNU(grant numbers:21A20151113068); Seoul National University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818668"",""Performance modeling";parallel loop;queueing system;multi-socket system;OpenMP;"NUMA"",""Computational modeling";Servers;Predictive models;Time factors;Multicore processing;"Dynamic scheduling"","""",""8"","""",""35"",""IEEE"",""28 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Performance Optimization for Relative-Error-Bounded Lossy Compression on Scientific Data,""X. Zou"; T. Lu; W. Xia; X. Wang; W. Zhang; H. Zhang; S. Di; D. Tao;" F. Cappello"",""Harbin Institute of Technology, Shenzhen, China"; Marvell Technology Group, Santa Clara, USA; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Argonne National Laboratory, Lemont, USA; University of Alabama, Tuscaloosa, USA;" Argonne National Laboratory, Lemont, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Mar 2020"",""2020"",""31"",""7"",""1665"",""1680"",""Scientific simulations in high-performance computing (HPC) environments generate vast volume of data, which may cause a severe I/O bottleneck at runtime and a huge burden on storage space for postanalysis. Unlike traditional data reduction schemes such as deduplication or lossless compression, not only can error-controlled lossy compression significantly reduce the data size but it also holds the promise to satisfy user demand on error control. Pointwise relative error bounds (i.e., compression errors depends on the data values) are widely used by many scientific applications with lossy compression since error control can adapt to the error bound in the dataset automatically. Pointwise relative-error-bounded compression is complicated and time consuming. We develop efficient precomputation-based mechanisms based on the SZ lossy compression framework. Our mechanisms can avoid costly logarithmic transformation and identify quantization factor values via a fast table lookup, greatly accelerating the relative-error-bounded compression with excellent compression ratios. In addition, we reduce traversing operations for Huffman decoding, significantly accelerating the decompression process in SZ. Experiments with eight well-known real-world scientific simulation datasets show that our solution can improve the compression and decompression rates (i.e., the speed) by about 40 and 80 p, respectively, in most of cases, making our designed lossy compression strategy the best-in-class solution in most cases."",""1558-2183"","""",""10.1109/TPDS.2020.2972548"",""National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2018YFB1003800,2018YFB1003805)"; National Natural Science Foundation of China(grant numbers:61972441,61832004,61972112,61672186,61872110); Wuhan National Laboratory for Optoelectronics(grant numbers:2018WNLOKF008); Key R&D Program for Guangdong Province(grant numbers:2019B010136001); Shenzhen Science and Technology Program(grant numbers:JCYJ20170413105929681,JCYJ20170811161545863); U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:1619253);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8989806"",""Lossy compression";high-performance computing;scientific data;"compression rate"",""Compressors";Quantization (signal);Data models;Distributed databases;Error correction;Decoding;"Acceleration"","""",""8"","""",""33"",""IEEE"",""10 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Performance-Aware Speculative Resource Oversubscription for Large-Scale Clusters,""R. Yang"; C. Hu; X. Sun; P. Garraghan; T. Wo; Z. Wen; H. Peng; J. Xu;" C. Li"",""School of Computing, University of Leeds, Leeds, United Kingdom"; Beihang University, Beijing, China; School of Computing, University of Leeds, Leeds, United Kingdom; Lancaster University, Lancaster, United Kingdom; Beihang University, Beijing, China; Newcastle University, Newcastle upon Tyne, United Kingdom; Beihang University, Beijing, China; School of Computing, University of Leeds, Leeds, United Kingdom;" Alibaba Group, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Feb 2020"",""2020"",""31"",""7"",""1499"",""1517"",""It is a long-standing challenge to achieve a high degree of resource utilization in cluster scheduling. Resource oversubscription has become a common practice in improving resource utilization and cost reduction. However, current centralized approaches to oversubscription suffer from the issue with resource mismatch and fail to take into account other performance requirements, e.g., tail latency. In this article we present ROSE, a new resource management platform capable of conducting performance-aware resource oversubscription. ROSE allows latency-sensitive long-running applications (LRAs) to co-exist with computation-intensive batch jobs. Instead of waiting for resource allocation to be confirmed by the centralized scheduler, job managers in ROSE can independently request to launch speculative tasks within specific machines according to their suitability for oversubscription. Node agents of those machines can however, avoid any excessive resource oversubscription by means of a mechanism for admission control using multi-resource threshold control and performance-aware resource throttle. Experiments show that in case of mixed co-location of batch jobs and latency-sensitive LRAs, the CPU utilization and the disk utilization can reach 56.34 and 43.49 percent, respectively, but the 95th percentile of read latency in YCSB workloads only increases by 5.4 percent against the case of executing the LRAs alone."",""1558-2183"","""",""10.1109/TPDS.2020.2970013"",""Alibaba Group"; National Key Research and Development Program of China Stem Cell and Translational Research(grant numbers:2016YFB1000503); National Natural Science Foundation of China(grant numbers:61421003); Engineering and Physical Sciences Research Council(grant numbers:EP/T01461X/1); Beijing Advanced Innovation Center for Big Data and Brain Computing;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8972553"",""Resource scheduling";oversubscription;cluster utilization;resource throttling;"QoS"",""Task analysis";Resource management;Quality of service;Heart beat;Yarn;"Decision making"","""",""16"","""",""53"",""IEEE"",""28 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Phase-Aware Cache Partitioning to Target Both Turnaround Time and System Performance,""L. Pons"; J. Sahuquillo; V. Selfa; S. Petit;" J. Pons"",""Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain"; Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain; Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain; Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain;" Department of Computer Engineering, Universitat Politècnica de València, Valencia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""29 May 2020"",""2020"",""31"",""11"",""2556"",""2568"",""The Last Level Cache (LLC) plays a key role in the system performance of current multi-cores by reducing the number of long latency main memory accesses. The inter-application interference at this shared resource, however, can lead the system to undesired situations regarding performance and fairness. Recent approaches have successfully addressed fairness and turnaround time (TT) in commercial processors. Nevertheless, these approaches must face sustaining system performance, which is challenging. This work makes two main contributions. LLC behaviors regarding cache performance, data reuse and cache occupancy, that adversely impact on the final performance are identified. Second, based on these behaviors, we propose the Critical-Phase Aware Partitioning Approach (CPA), which reduces TT while sustaining (and even improving) IPC by making an effective use of the LLC space. Experimental results show that CPA outperforms CA, Dunn and KPart state-of-the-art approaches, and improves TT (over 40 percent in some workloads) over Linux default behavior while sustaining or even improving IPC by more than 3 percent in several mixes."",""1558-2183"","""",""10.1109/TPDS.2020.2996031"",""Ministerio de Ciencia, Innovación y Universidades"; European ERDF(grant numbers:RTI2018-098156-B-C51); Generalitat Valenciana(grant numbers:AICO/2019/317);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9097450"",""Cache memories";multi-core multiprocessors;memory structures;memory hierarchy;"performance"",""System performance";Interference;Throughput;Measurement;Heuristic algorithms;Program processors;"Multicore processing"","""",""10"","""",""27"",""IEEE"",""20 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Power Guarantee for Electric Systems Using Real-Time Scheduling,""E. Kim"; Y. Lee; L. He; K. G. Shin;" J. Lee"",""Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA"; Department of Robotics, Hanyang University, Ansan-si, Korea; Department of Computer Science and Engineering, University of Colorado Denver, Denver, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, USA;" Department of Computer Science and Engineering, Sungkyunkwan University (SKKU), Suwon-si, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Mar 2020"",""2020"",""31"",""8"",""1783"",""1798"",""Modern electric systems, such as electric vehicles, mobile robots, nano satellites, and drones, require to support various power-demand operations for user applications and system maintenance. This, in turn, calls for advanced power management that jointly considers power demand by the operations and power supply from various sources, such as batteries, solar panels, and supercapacitors. In this article, we develop a power scheduling framework for a reliable energy storage system with multiple power-supply sources and multiple power-demand operations. Specifically, we develop offline power-supply guarantee analysis and online power management. The former provides an offline power-supply guarantee such that every power-demand operation completes its execution in time while the sum of power required by individual operations does not exceed the total power supplied by the entire energy storage system at any time"; to this end, we develop a plain power-supply analysis as well as its improved version using real-time scheduling techniques. On the other hand, the latter efficiently utilizes the surplus power available at runtime for improving system performance;" we propose two approaches, depending on whether future scheduling information of power-demanding tasks is available or not. For evaluation, we perform simulations to evaluate both the plain and improved analyses for offline power guarantee under various synthetic power-demand operations. In addition, we have built a simulation model and demonstrated that the proposed framework with the offline analysis and online management not only guarantees the required power-supply, but also enhances system performance by up to 56.49 percent."",""1558-2183"","""",""10.1109/TPDS.2020.2977041"",""National Science Foundation(grant numbers:CNS-1446117,CNS-1739577)"; Office of Naval Research(grant numbers:N00014-18-1-2141); LG Chem Ltd.; National Research Foundation of Korea(grant numbers:2019R1A2B5B02001794,2017H1D8A2031628); Grand Information Technology Research Center support program(grant numbers:IITP-2020-2015-0-00742);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9018173"",""Offline power-supply guarantee";online power management;real-time scheduling;"electric systems"",""Batteries";Power system management;Real-time systems;Power demand;"Analytical models"","""",""3"","""",""37"",""IEEE"",""28 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Power-Aware Allocation of Graph Jobs in Geo-Distributed Cloud Networks,""S. Hosseinalipour"; A. Nayak;" H. Dai"",""Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, USA"; Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, USA;" Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""749"",""765"",""In the era of big-data, the jobs submitted to the clouds exhibit complicated structures represented by graphs, where the nodes denote the sub-tasks each of which can be accommodated at a slot in a server, while the edges indicate the communication constraints among the sub-tasks. We develop a framework for efficient allocation of graph jobs in geo-distributed cloud networks (GDCNs), explicitly considering the power consumption of the datacenters (DCs). We address the following two challenges arising in graph job allocation: i) the allocation problem belongs to NP-hard nonlinear integer programming";" ii) the allocation requires solving the NP-complete sub-graph isomorphism problem, which is particularly cumbersome in large-scale GDCNs. We develop a suite of efficient solutions for GDCNs of various scales. For small-scale GDCNs, we propose an analytical approach based on convex programming. For medium-scale GDCNs, we develop a distributed allocation algorithm exploiting the processing power of DCs in parallel. Afterward, we provide a novel low-complexity (decentralized) sub-graph extraction method, based on which we introduce cloud crawlers aiming to extract allocations of good potentials for large-scale GDCNs. Given these suggested strategies, we further investigate strategy selection under both fixed and adaptive DC pricing schemes, and propose an online learning algorithm for each."",""1558-2183"","""",""10.1109/TPDS.2019.2943457"",""National Science Foundation(grant numbers:ECCS-1444009,CNS-1824518)"; Army Research Office(grant numbers:W911NF-17-1-0087);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8847383"",""Big-data";graph jobs;geo-distributed cloud networks;datacenter power consumption;job allocation;integer programming;convex optimization;"online learning"",""Resource management";Cloud computing;Power demand;Servers;Task analysis;Twitter;"Distributed algorithms"","""",""12"","""",""46"",""IEEE"",""24 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Preemptive and Low Latency Datacenter Scheduling via Lightweight Containers,""W. Chen"; X. Zhou;" J. Rao"",""Department of Computer Science, University of Colorado Colorado Springs, USA"; Department of Computer Science, University of Colorado Colorado Springs, USA;" Department of Computer Science, University of Texas, Arlington, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 Jun 2020"",""2020"",""31"",""12"",""2749"",""2762"",""Datacenters are evolving to host heterogeneous workloads on shared clusters to reduce the operational cost and achieve higher resource utilization. However, it is challenging to schedule heterogeneous workloads with diverse resource requirements and QoS constraints. On one hand, latency-critical jobs need to be scheduled as soon as they are submitted to avoid any queuing delays. On the other hand, best-effort long jobs should be allowed to occupy the cluster when there are idle resources to improve cluster utilization. The challenge lies in how to minimize the queuing delays of short jobs while maximizing cluster utilization. In this article, we propose and develop BIG-C, a container-based resource management framework for data-intensive cluster computing. The key design is to leverage lightweight virtualization, a.k.a, containers, to make tasks preemptable in cluster scheduling. We devise two types of preemption strategies: immediate and graceful preemptions and show their effectiveness and tradeoffs with loosely-coupled MapReduce workloads as well as iterative, in-memory Spark workloads. Based on the mechanisms for task preemption, we further develop job-level and task-level preemptive policies as well as a preemptive fair share cluster scheduler. Our implementation on Yarn and evaluation with synthetic and production workloads show that low job latency and high resource utilization can be both attained when scheduling heterogeneous workloads on a contended cluster."",""1558-2183"","""",""10.1109/TPDS.2019.2957754"",""National Science Foundation(grant numbers:SHF-1816850,CNS-1422119)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924761"",""Datacenter scheduling";OS lightweight virtualization;preemption;multi-tenancy;"heterogeneous workloads"",""Task analysis";Sparks;Yarn;Containers;Delays;Resource management;"Processor scheduling"","""",""10"","""",""40"",""IEEE"",""5 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Probabilistic Consistency Guarantee in Partial Quorum-Based Data Store,""X. Yao";" C. -L. Wang"",""Department of Computer Science, University of Hong Kong, Hong Kong";" Department of Computer Science, University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Mar 2020"",""2020"",""31"",""8"",""1815"",""1827"",""Many NoSQL databases support quorum-based protocols, which require a subset of replicas (called a quorum) to respond to each write/read operation. These systems configure the quorum size to tune the operation latency and adopt multiple consistency levels. Some recent works illustrate that using probability models to quantify the chance of reading the last update is important because it could avoid returning stale values under eventual consistency. There are two challenging issues: (1) from inconsistent replicas, how to determine the minimum quorum size (i.e., the lowest access latency) to read the newest data at a specified probability";" (2) node failure frequently happens in large-scale systems, how to guarantee the probability-based consistent reads. This article presents Probabilistic Consistency Guarantee (PCG), which is the first dynamic quorum decision and failure-aware quantification model. PCG model respectively quantifies the server-side consistency after the latest write, which reflects the object's time-varying update progress, and the possibility of reading this update when responding to the end-users. Our theoretical analysis derives several formulas to determine the quorum size of a read quorum and the consensus result selected from this quorum is the data updated by the last write at the user-specified probability. When some replicas are unavailable, our model knows how to rescale the quorum and read values from surviving replicas could reduce the stale reads caused by node failures. The experimental results in Cassandra demonstrate that the PCG model can achieve up to 77.7 percent more accurate predictions and reduce up to 48.9 percent read latency than those of the previous model."",""1558-2183"","""",""10.1109/TPDS.2020.2973619"",""Hong Kong RGC Collaborative Research Fund(grant numbers:E-RB29)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8998160"",""Quorum-replicated database";eventual consistency;"probabilistic consistency model"",""Probability";Probabilistic logic;Distributed databases;Data models;Predictive models;Cloud computing;"Synchronization"","""",""6"","""",""43"",""IEEE"",""13 Feb 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Proofs of Physical Reliability for Cloud Storage Systems,""L. Li";" L. Lazos"",""Department of Electrical and Computer Engineering, The University of Arizona, Tucson, USA";" Department of Electrical and Computer Engineering, The University of Arizona, Tucson, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1048"",""1065"",""Cloud service providers (CSPs) promise to reliably store repositories outsourced by clients. Unfortunately, once files have left the client's control, he has no means to verify their redundant storage. In this article, we develop Proof of Physical Reliability (PoPR) auditing mechanisms that prove that a CSP stores an outsourced repository across multiple physical storage nodes. A PoPR complements the existing proof-of-retrievability (PoR) and proof-of-data possession (PDP) methods that are concerned with file retrievability, but without any verification of the fault-tolerance to physical storage nodes failures. A PoPR goes beyond retrievability by verifying that a file is redundantly stored across multiple physical storage nodes according to a pre-agreed layout and can, therefore, survive node failures. The verification mechanism relies on a combination of storage integrity and timing tests on the simultaneous retrieval of a collection of file symbols from multiple storage nodes. Compared to the state-of-the-art, our approach accommodates CSPs with heterogeneous storage devices (hard disks, SSDs, etc.) and does not assume constant data processing nor network delays. Instead, it can operate under any delayvariance, because it relies only on (loose) delay bounds. We analytically prove the security of our construction and experimentally validate its success in heterogeneous storage settings."",""1558-2183"","""",""10.1109/TPDS.2019.2958919"",""National Science Foundation(grant numbers:CNS1813401)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8930269"",""Proof of reliability";fault tolerance;data integrity;data security and privacy;storage reliability;"retrievability"",""Delays";Protocols;Security;Layout;Data centers;"Fault tolerance"","""",""1"","""",""43"",""IEEE"",""10 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Quantum Game Analysis on Extrinsic Incentive Mechanisms for P2P Services,""S. Wang"; W. Sun; L. Ma; W. Lv;" X. Cheng"",""College of Information Science and Technology, Beijing Normal University, Beijing, P.R.China"; Department of Computer Science, Columbia University, New York, USA; Department of Computer Science, Texas Christian University, Fort Worth, USA; School of Computer Science and Engineering, Beihang University, Beijing, China;" Department of Computer Science, George Washington University, Washington, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""159"",""170"",""Peer-to-peer (P2P) services such as mobile P2P transmissions and resource sharing, provide efficient methods to deliver data without the deployment of any central server. Nevertheless, the free-riding phenomenon inherit in such services presses a need for incentive mechanisms to stimulate contributions of data transmissions or sharing. As a result, it is imperative to answer the following questions: whether, and if so to what extent, an incentive mechanism can invoke such contributions? To answerthese questions, we employ an n-player continuous quantum game model to analyze the general extrinsic incentive mechanisms as well as the reputation-based incentive mechanisms, a typical class of extrinsic incentive mechanisms. We focus on studying the extrinsic incentive mechanisms in this paper due to their wide scope of applications stemming from the fact that they promote cooperative behaviors by offering rewards rather than depending on the internal bounds (e.g., social ties) among peers, which may not always exist between any pair of peers. To the best of our knowledge, we are the first to analyze the extrinsic incentive mechanisms for P2P services from a quantum game perspective. Such a perspective is adopted because the extended strategy space in the quantum game broadens the range for searching optimal strategies and the introduction of entanglement makes the proposed analytical frameworks more practical due to the consideration of the peers' relationships imposed by the rewards in extrinsic incentive mechanisms. Our quantum game-based analytical framework is generic because it is compatible with classic game-based schemes. The analytical results can provide a straightforward insight on evaluating the potential of the extrinsic incentive mechanisms and can serve as important references for designing new extrinsic incentive mechanisms."",""1558-2183"","""",""10.1109/TPDS.2019.2933416"",""National Natural Science Foundation of China(grant numbers:61772080,U1811463)"; National Science Foundation(grant numbers:OAC-1829553,CNS-1912755,IIS- 1741279,CNS-1704397);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8788575"",""P2P services";incentive mechanisms;"quantum game"",""Games";Peer-to-peer computing;Quantum entanglement;Game theory;Resource management;"Analytical models"","""",""6"","""",""33"",""IEEE"",""5 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Quantum Supremacy Circuit Simulation on Sunway TaihuLight,""R. Li"; B. Wu; M. Ying; X. Sun;" G. Yang"",""Department of Computer Science & Technology, Tsinghua University, Beijing, China"; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Centre for Quantum Software and Information, University of Technology Sydney, Sydney, Australia; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China;" Department of Computer Science & Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""805"",""816"",""With the rapid progress made by industry and academia, quantum computers with dozens of qubits or even larger size are being realized. However, the fidelity of existing quantum computers often sharply decreases as the circuit depth increases. Thus, an ideal quantum circuit simulator on classical computers, especially on high-performance computers, is needed for benchmarking and validation. We design a large-scale simulator of universal random quantum circuits, often called “quantum supremacy circuits”, and implement it on Sunway TaihuLight. The simulator can be used to accomplish the following two tasks: 1) Computing a complete output state-vector";" 2) Calculating one or a few amplitudes. We target the simulation of 49-qubit circuits. For task 1), we successfully simulate such a circuit of depth 39, and for task 2) we reach the 55-depth level. To the best of our knowledge, both of the simulation results reach the largest depth for 49-qubit quantum supremacy circuits."",""1558-2183"","""",""10.1109/TPDS.2019.2947511"",""National Key R&D Program of China(grant numbers:2018YFA0306701)"; National Supercomputing Center in Wuxi; National Natural Science Foundation of China(grant numbers:61832015); Chinese Academy of Sciences(grant numbers:XDB28000000);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8869942"",""Quantum computing";quantum circuit simulation;"Sunway TaihuLight"",""Qubit";Task analysis;Logic gates;Supercomputers;"Circuit simulation"","""",""25"","""",""30"",""IEEE"",""16 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"QWEB: High-Performance Event-Driven Web Architecture With QAT Acceleration,""J. Li"; X. Hu; D. Qian; C. Wei; G. McFadden; B. Will; P. Yu; W. Li;" H. Guan"",""Shanghai Jiao Tong University, Shanghai, China"; Shanghai Jiao Tong University, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China; Intel Corporation, Shanghai, China;" Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jun 2020"",""2020"",""31"",""11"",""2633"",""2649"",""Hardware accelerators have been a promising solution to reduce the cost of cloud datacenters. This article investigates the acceleration of an important datacenter workload: the web server (or proxy) that faces high computational consumption originated from SSL/TLS processing and HTTP compression. Our study reveals that for the widely-deployed event-driven web architecture, the straight offloading of SSL/TLS or compression tasks suffers from frequent blockings in the offload I/O, leading to the underutilization of both CPU and accelerator resources. To achieve efficient acceleration, we propose QWEB, a comprehensive offload solution based on Intel QuickAssist Technology (QAT). QWEB introduces an asynchronous offload mode for SSL/TLS processing and a pipelining offload mode for HTTP compression, both allowing concurrent offload tasks from a single application process/thread. With these two novel offload modes, the blocking penalty is amortized or even eliminated, and the utilization rate of the parallel computation engines inside the QAT accelerator is greatly increased. The evaluation shows that QWEB provides up to 9x handshake performance with TLS-RSA (2048-bit) over the software baseline. Additionally, the secure data transfer throughput is enhanced by 2x for the SSL/TLS offloading only, 3.5x for the compression offloading only and 5x for the combined offloading."",""1558-2183"","""",""10.1109/TPDS.2020.2999353"",""National Natural Science Foundation of China(grant numbers:61972245)"; National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000502); National Science Fund for Distinguished Young Scholars(grant numbers:61525204);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9106846"",""Accelerator";event-driven web architecture;SSL/TLS;HTTP compression;Offload I/O;"concurrency"",""Computer architecture";Service-oriented architecture;Servers;Elliptic curve cryptography;Pipeline processing;"Acceleration"","""",""2"","""",""77"",""IEEE"",""2 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Random Priority-Based Thrashing Control for Distributed Shared Memory,""Y. -W. Ci"; M. R. Lyu; Z. Zhang; D. -C. Zuo;" X. -Z. Yang"",""Institute of Software, Chinese Academy of Sciences, Beijing, China"; Department of Computer Sciences and Engineering, The Chinese University of Hong Kong (CUHK), Hong Kong; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China;" School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""663"",""674"",""Shared memory is widely used for inter-process communication. The shared memory abstraction allows computation to be decoupled from communication, which offers benefits, including portability and ease of programming. To enable shared memory access by processes that are on different machines, distributed shared memory (DSM) can be employed. However, DSM systems can suffer from thrashing: while different processes update certain hot data items, the largest amount of effort is spent on data synchronization, and little progress is made by each process. To avoid interference between processes during data updating while providing shared memory at page granularity, more time is reserved for a writer to hold a page in a traditional manner. In this paper, we report on complex thrashing, which can explain why extending the time of holding a page might not be sufficient to control thrashing. To increase the throughput, we propose a thrashing control mechanism that allows each process to update a set of pages during a period of time, where the pages compose a logical area. Because of the isolation of areas, updates on different areas can be performed concurrently. To allow the areas to be fairly well used, each process is assigned with a random priority for thrashing control. The thrashing control mechanism is implemented on a Linux-based DSM system. Performance results show that the execution time of the applications that are apt to cause system thrashing can be significantly reduced by our approach."",""1558-2183"","""",""10.1109/TPDS.2019.2942302"",""Society of Hong Kong Scholars"; Hong Kong Special Administrative Region, China(grant numbers:CUHK 14210717); Microsoft Research;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844812"",""Distributed shared memory";inter process communication;"thrashing control"",""Synchronization";Process control;Memory management;Programming;Distributed databases;Frequency synchronization;"Message passing"","""","""","""",""33"",""IEEE"",""19 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"REACT: Scalable and High-Performance Regular Expression Pattern Matching Accelerator for In-Storage Processing,""W. S. Jeong"; C. Lee; K. Kim; M. K. Yoon; W. Jeon; M. Jung;" W. W. Ro"",""School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea"; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical Engineering, Korea Advanced Institute of Science and Technology, South Korea;" School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1137"",""1151"",""This article proposes REACT, a regular expression matching accelerator, which can be embedded in a modern Solid-State Drive (SSD) and a novel data access scheduling algorithm for high matching throughput. Specifically, REACT, including our data access scheduling algorithm, increases the utilization of SSD and the degree of internal memory parallelism for pattern matching processes. While the low-level flash exhibits long latency, modern SSDs in practice achieve high I/O performance by utilizing the massive internal parallelism at the system-level. However, exploiting the parallelism is limited for pattern matching since the subblocks, which constitute an input data and can be placed in multiple flash pages, should be tested in a sequence to process the input correctly. This limitation can induce low utilization of the accelerator. To address this challenge, the proposed REACT simultaneously processes multiple input streams with a parallel processing architecture to maximize matching throughput by hiding the long and irregular latency. The scheduling algorithm finds a data stream which requires a sub-block in closest time and prioritizes the access request to reduce the data stall of REACT. REACT achieves maximum 22.6 percent of matching throughput improvement on a 16channel high-performance SSD compared to the accelerator without the proposed scheduling algorithm."",""1558-2183"","""",""10.1109/TPDS.2019.2953646"",""National Research Foundation of Korea(grant numbers:NRF-2018R1A2A2A05018941)"; Memory Division of Samsung Electronics Co., Ltd.(grant numbers:NRF 2016R1C1B2015312,NRF 2017R1A4A1015498);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8901167"",""In-storage processing (ISP)";regular expression matching;accelerator;"solid-state drive"",""Delays";Pattern matching;Scheduling algorithms;Throughput;Computer architecture;"Performance evaluation"","""",""7"","""",""45"",""IEEE"",""14 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Reconciling Time Slice Conflicts of Virtual Machines With Dual Time Slice for Clouds,""T. Kim"; C. H. Park; J. Huh;" J. Ahn"",""Department of Software and Computer Engineering, Ajou University, Suwon, South Korea"; Department of Information Technology, Uppsala University, Uppsala, Sweden; School of Computing, Korea Advanced Institute of Science and Technology, Daejeon, South Korea;" Department of Software and Computer Engineering, Ajou University, Suwon, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""20 May 2020"",""2020"",""31"",""10"",""2453"",""2465"",""The proliferation of system virtualization poses a new challenge for the coarse-grained time sharing techniques for consolidation, since operating systems are running on virtual CPUs. The current system stack was designed under the assumption that operating systems can seize CPU resources at any moment. However, for the guest operating system on a virtual machine (VM), such assumption cannot be guaranteed, since virtual CPUs of VMs share a limited number of physical cores. Due to the time-sharing of physical cores, the execution of a virtual CPU is not contiguous, with a gap between the virtual and real time spaces. Such a virtual time discontinuity problem leads to significant inefficiency for lock and interrupt handling, which rely on the immediate availability of CPUs whenever the operating system requires computation. To reduce scheduling latencies of virtual CPUs, shortening time slices can be a straightforward strategy, but it may lead to the increased overhead of context switching costs across virtual machines for some workloads. It is challenging to determine a single time slice to satisfy all the VMs. In this article, we propose to have dual time slice to resolve the time slice conflict problem occurred in different types of virtual machines."",""1558-2183"","""",""10.1109/TPDS.2020.2993252"",""National Research Foundation of Korea(grant numbers:NRF-2013R1A2A2A01015514,NRF-2019R1C1C1005166)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9090316"",""Virtualization";virtual time discontinuity;"dual time slice"",""Kernel";Virtual machine monitors;Virtual machining;Throughput;Real-time systems;"Context"","""",""4"","""",""41"",""IEEE"",""8 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Reduce Operations: Send Volume Balancing While Minimizing Latency,""M. O. Karsavuran"; S. Acer;" C. Aykanat"",""Computer Engineering Department, Bilkent University, Ankara, Turkey"; Sandia National Laboratories, Center for Computing Research, Albuquerque, USA;" Computer Engineering Department, Bilkent University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Feb 2020"",""2020"",""31"",""6"",""1461"",""1473"",""Communication hypergraph model was proposed in a two-phase setting for encapsulating multiple communication cost metrics (bandwidth and latency), which are proven to be important in parallelizing irregular applications. In the first phase, computational-task-to-processor assignment is performed with the objective of minimizing total volume while maintaining computational load balance. In the second phase, communication-task-to-processor assignment is performed with the objective of minimizing total number of messages while maintaining communication-volume balance. The reduce-communication hypergraph model suffers from failing to correctly encapsulate send-volume balancing. We propose a novel vertex weighting scheme that enables part weights to correctly encode send-volume loads of processors for send-volume balancing. The model also suffers from increasing the total communication volume during partitioning. To decrease this increase, we propose a method that utilizes the recursive bipartitioning framework and refines each bipartition by vertex swaps. For performance evaluation, we consider column-parallel SpMV, which is one of the most widely known applications in which the reduce-task assignment problem arises. Extensive experiments on 313 matrices show that, compared to the existing model, the proposed models achieve considerable improvements in all communication cost metrics. These improvements lead to an average decrease of 30 percent in parallel SpMV time on 512 processors for 70 matrices with high irregularity."",""1558-2183"","""",""10.1109/TPDS.2020.2964536"",""National Center for High Performance Computing of Turkey(grant numbers:4005072018)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951280"",""Communication hypergraph";communication cost;maximum communication volume;communication volume;latency;recursive bipartitioning;hypergraph partitioning;sparse matrix;"sparse matrix-vector multiplication"",""Program processors";Task analysis;Computational modeling;Load modeling;Sparse matrices;Measurement;"Solid modeling"","""",""1"","""",""21"",""IEEE"",""7 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Reducing the Impact of Intensive Dynamic Memory Allocations in Parallel Multi-Threaded Programs,""D. Langr";" M. Kočička"",""Department of Computer Systems, Faculty of Information Technology, Czech Technical University in Prague, Praha, Czech Republic";" Department of Computer Systems, Faculty of Information Technology, Czech Technical University in Prague, Praha, Czech Republic"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1152"",""1164"",""Frequent dynamic memory allocations (DyMAs) can significantly hinder the scalability of parallel multi-threaded programs. As the number of threads grows, DyMAs can even become the main performance bottleneck. We introduce modern tools and methods for evaluating the impact of DyMAs and present techniques for its reduction, which include scalable heap implementations, small buffer optimization, and memory pooling. Additionally, we provide a survey of state-of-the-art implementations of these techniques and study them experimentally by using a benchmark program, server simulator software, and a real-world high-performance computing application. As a result, we show that relatively small modifications in parallel program's source code or a way of its execution may substantially reduce the runtime overhead associated with the use of dynamic data structures."",""1558-2183"","""",""10.1109/TPDS.2019.2960514"",""Ministerstvo Školství, Mládeže a Tělovýchovy(grant numbers:CZ.02.1.01/0.0/0.0/16_019/0000765)"; Ministry of Education, Youth and Science; NSF(grant numbers:OCI-0725070,ACI-1238993); state of Illinois;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936486"",""Dynamic memory allocation";memory pooling;multi-threading;parallel program;scalable heap implementation;shared memory;"small buffer optimization"",""Benchmark testing";Message systems;Data structures;C++ languages;Runtime;Libraries;"Dynamic scheduling"","""","""","""",""38"",""IEEE"",""18 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Reliability Aware Energy Optimized Scheduling of Non-Preemptive Periodic Real-Time Tasks on Heterogeneous Multiprocessor System,""N. Kumar"; J. Mayank;" A. Mondal"",""Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, India"; Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, India;" Department of Computer Science and Engineering, Indian Institute of Technology Patna, Patna, India"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""871"",""885"",""Higher reliability and lower energy consumption are conflicting, yet among the most important design objectives for the real-time systems. Moreover, in the domain of real-time systems, non-preemptive scheduling is relatively unexplored with objectives such as reliability and energy. Thus we propose an active replication based framework to schedule a set of periodic real-time tasks in the non-preemptive heterogeneous environment such that the given reliability and timing constraints are satisfied whereas the energy consumption is minimized. First, we formulate the problem as a constraint optimization problem that provides an optimal solution";" however, it does not scale well. Thus, we also propose heuristics which apply reservation of processors and reallocation of jobs, to compute suboptimal solution efficiently in terms of energy consumption as well as schedulability. Heuristics make use of the interplay of task-level reliability target, reliability of replicas, number of replicas, reliability of tasks, and energy consumption. We perform an experimental study on the test cases generated by extending UUnisort algorithm [1] and observe the effect of various simulation parameters on energy consumption and schedulability."",""1558-2183"","""",""10.1109/TPDS.2019.2950251"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8886607"",""Real-time systems";scheduling;non-preemptive;energy consumption;"reliability"",""Reliability";Task analysis;Energy consumption;Real-time systems;Program processors;Processor scheduling;"Timing"","""",""23"","""",""41"",""IEEE"",""30 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Reliability-Aware Network Service Provisioning in Mobile Edge-Cloud Networks,""J. Li"; W. Liang; M. Huang;" X. Jia"",""Research School of Computer Science, The Australian National University, Canberra, Australia"; Research School of Computer Science, The Australian National University, Canberra, Australia; Research School of Computer Science, The Australian National University, Canberra, Australia;" Department of Computer Science, City University of Hong Kong, Hong Kong, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Feb 2020"",""2020"",""31"",""7"",""1545"",""1558"",""The Mobile Edge-Cloud (MEC) network has emerged as a promising networking paradigm to address the conflict between increasing computing-intensive applications and resource-constrained mobile Internet-of-Thing (IoT) devices with portable size and storage. In MEC environments, Virtualized Network Functions (VNFs) are deployed for provisioning network services to users to reduce the service cost on top of dedicated hardware infrastructures. However, VNFs may suffer from failures and malfunctions while network service providers have to guarantee continuously reliable services to their consumers to meet the ever-growing service demands of users, thereby securing their revenues for the service. In this article, we focus on reliable VNF service provisioning in MECs, by placing primary and backup VNF instances to cloudlets in an MEC network to meet the service reliability requirements of users. We first formulate a novel VNF service reliability problem with the aim to maximize the revenue collected by admitting as many as user requests while meeting their different reliability requirements, assuming that requests arrive into the system one by one without the knowledge of future arrivals, and the admission or rejection decision must be made immediately. We then develop two efficient online algorithms for the problem under two different backup schemes: the on-site (local) and off-site (remote) schemes, by adopting the primal-dual updating technique. Both algorithms achieve provable competitive ratios with bounded moderate resource capacity violations. We finally evaluate the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms are promising, compared with existing baseline algorithms."",""1558-2183"","""",""10.1109/TPDS.2020.2970048"",""Australian Research Council(grant numbers:DP200101985)"; Research Grants Council of Hong Kong(grant numbers:CityU11214316);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8974390"",""Mobile edge computing (MEC)";virtualized network function (VNF);virtualized service functions (VNFs);revenue maximization;reliability-aware service provisioning;Fault-tolerance;software failure;cloudlet failure;online algorithms;the primal-dual dynamic updating technique;mobile edge-cloud networks;competitive ratio analysis;"resource allocation and optimization"",""Cloud computing";Software reliability;Heuristic algorithms;Computer network reliability;Software;"Software algorithms"","""",""43"","""",""25"",""IEEE"",""29 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Replica Exchange MCMC Hardware With Automatic Temperature Selection and Parallel Trial,""K. Dabiri"; M. Malekmohammadi; A. Sheikholeslami;" H. Tamura"",""Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto, Toronto, Canada"; Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto, Toronto, Canada; Edward S. Rogers Sr. Department of Electrical & Computer Engineering, University of Toronto, Toronto, Canada;" Fujitsu Laboratories Ltd., Kawasaki, Kanagawa, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Mar 2020"",""2020"",""31"",""7"",""1681"",""1692"",""A replica exchange Markov Chain Monte Carlo (MCMC) engine is developed with automatic temperature adjustment for solving combinatorial optimization problems by minimizing the energy of the Ising model. The automatic temperature adjustment scheme ensures that the MCMC process is optimized at every stage of the execution. This approach is performed by dynamically adjusting temperatures of all replicas, based on the properties of any given problem, in addition to the capability of automatically inserting new replicas or removing any existing replicas to achieve the best possible resource efficiency and execution time. The proposed algorithm is integrated with parallel evaluation of energy increment and update scheme. The engine is implemented on the FPGA platform with a capacity of running up to 42 replicas in pipeline, each running 1024 fully-connected Ising spins in parallel. The performance of the hardware is examined with three different classes of problems, Vertex Cover, Maximum-Cut, and Travelling Salesman using the engine in three modes, simulated annealing, with replica exchange while the adjustments are turned on or off. Up to 16x speedup is observed by turning on the replica exchange capability in addition to the advantage of eliminating the challenging process of finding an optimal annealing schedule for simulated annealing process."",""1558-2183"","""",""10.1109/TPDS.2020.2972359"",""Fujitsu Laboratories Ltd."; Natural Sciences and Engineering Research Council of Canada;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025708"",""Replica exchange";combinatorial optimization;Markov chain Monte Carlo;hardware implementation;ising model;FPGA;parallel computing;simulated annealing;Boltzmann machine;neural network;"openCL"",""Hardware";Engines;Monte Carlo methods;Field programmable gate arrays;Simulated annealing;"Probability distribution"","""",""11"","""",""39"",""IEEE"",""5 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Resource Management for Power-Constrained HEVC Transcoding Using Reinforcement Learning,""L. Costero"; A. Iranfar; M. Zapater; F. D. Igual; K. Olcoz;" D. Atienza"",""Departamento de Arquitectura de Computadores y Automática, Universidad Complutense de Madrid, Madrid, Spain"; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Lausanne, Switzerland; Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Lausanne, Switzerland; Departamento de Arquitectura de Computadores y Automática, Universidad Complutense de Madrid, Madrid, Spain; Departamento de Arquitectura de Computadores y Automática, Universidad Complutense de Madrid, Madrid, Spain;" Embedded Systems Laboratory (ESL), Swiss Federal Institute of Technology Lausanne (EPFL), Lausanne, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jul 2020"",""2020"",""31"",""12"",""2834"",""2850"",""The advent of online video streaming applications and services along with the users' demand for high-quality contents require High Efficiency Video Coding (HEVC), which provides higher video quality and more compression at the cost of increased complexity. On one hand, HEVC exposes a set of dynamically tunable parameters to provide trade-offs among Quality-of-Service (QoS), performance, and power consumption of multi-core servers on the video providers' data center. On the other hand, resource management of modern multi-core servers is in charge of adapting system-level parameters, such as operating frequency and multithreading, to deal with concurrent applications and their requirements. Therefore, efficient multi-user HEVC streaming necessitates joint adaptation of application-and system-level parameters. Nonetheless, dealing with such a large and dynamic design space is challenging and difficult to address through conventional resource management strategies. Thus, in this work, we develop a multi-agent Reinforcement Learning framework to jointly adjust application-and system-level parameters at runtime to satisfy the QoS of multi-user HEVC streaming in power-constrained servers. In particular, the design space, composed of all design parameters, is split into smaller independent sub-spaces. Each design sub-space is assigned to a particular agent so that it can explore it faster, yet accurately. The benefits of our approach are revealed in terms of adaptability and quality (with up to to 4× improvements in terms of QoS when compared to a static resource management scheme), and learning time (6× fasterthan an equivalent mono-agent implementation). Finally, we show that the power-capping techniques formulated outperform the hardware-based power capping with respect to quality."",""1558-2183"","""",""10.1109/TPDS.2020.3004735"",""EU (FEDER) and Spanish MINECO(grant numbers:RTI2018-093684-B-I00,MECD (FPU15/02050),CM(S2018/TCS-4423),UCM (PR65/19-22445)"; ERC Consolidator(grant numbers:725657); H2020 RECIPE(grant numbers:801137); H2020 DeepHealth project(grant numbers:825111);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9124674"",""Resource management";DVFS;power capping;reinforcement learning;Q-learning;HEVC;"self-adaptation"",""Quality of service";Resource management;Transcoding;Servers;Streaming media;Throughput;"Power demand"","""",""7"","""",""41"",""IEEE"",""24 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Resource-Constrained Replication Strategies for Hierarchical and Heterogeneous Tasks,""W. C. Ao";" K. Psounis"",""Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, USA";" Ming Hsieh Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""793"",""804"",""In large-scale cloud computing systems, a task is often divided into multiple subtasks which can be executed in parallel in different machines. As a result, the task completion time is constrained by the completion time of the slowest subtask. To reduce the task completion time, the strategy of replicating the straggling subtasks has been employed in cloud computing frameworks such as MapReduce and Hadoop. Analyzing mathematically the performance of such replication strategies has recently received great attention. However, most of the analytical work focuses on the case where the completion times of the subtasks are identically distributed. This assumption may not hold in practice due to the modularization and encapsulation of the computation of a task, resulting in different service requirements for different subtasks. In this paper, we consider the case where the completion times of the subtasks of a task are drawn from heterogeneous/empirical distributions. Furthermore, we consider the case where jobs consist of hierarchical tasks that are required to be executed in a specific order described by a task precedence graph. We propose a novel framework to investigate how to allocate replication resources among the subtasks such that the overall task completion time is minimized. Specifically, we devise a Lagrange multiplier-based method and a water-filling-like algorithm for integer programs. We show via analysis and simulations the optimality and efficiency of our proposed algorithms, and explore the tradeoff between cost and latency from introducing replications in a task graph."",""1558-2183"","""",""10.1109/TPDS.2019.2945294"",""NSF(grant numbers:ECCS-1444060)"; Cisco Research Center;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8859329"",""Task replication";latency;resource allocation;task graph;"parallel computing"",""Task analysis";Resource management;Optimization;Cloud computing;Power system reliability;Probability;"Time factors"","""",""8"","""",""25"",""IEEE"",""4 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"RIVA: Robust Integrity Verification Algorithm for High-Speed File Transfers,""B. Charyyev";" E. Arslan"",""Stevens Institute of Technology, Hoboken, USA";" University of Nevada, Reno, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Jan 2020"",""2020"",""31"",""6"",""1387"",""1399"",""End-to-end integrity verification is designed to protect file transfers against silent data corruption by comparing checksum of files at source and destination end points using cryptographic hash functions such as MD5 and SHA1. However, existing implementations of end-to-end integrity verification for file transfers fall short to detect undetected disk errors that causes inconsistency between disk and cache memory. In this article, we propose Robust Integrity Verification Algorithm (RIVA) to strengthen the integrity of file transfers by forcing checksum computation tasks to read files directly from disk. RIVA achieves this by invalidating memory mappings of file pages after their transfer such that when the file is read again for checksum calculation, it will be fetched from disk and silent disk errors will be captured. We design and conduct extensive fault resilience experiments to evaluate the robustness of integrity verification algorithms against undetected disk write errors. The results indicate that while the state-of-the-art integrity verification algorithms fail to detect the injected errors for almost all file sizes, RIVA captures all of them with the help of cache invalidation. We further run statistical analysis to assess the probability of missing silent disk errors and find that RIVA reduces the likelihood by 10 to 15 orders of magnitude compared to the existing approaches. Finally, enforcing disk read in integrity verification introduces an inevitable overhead in exchange of increased robustness against silent disk errors, but RIVA keeps its overhead below 15 percent in most cases by running transfer, cache invalidation, and checksum computation processes concurrently for different portions of the same file."",""1558-2183"","""",""10.1109/TPDS.2020.2966616"",""National Science Foundation(grant numbers:OAC-1850353)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8959149"",""End-to-end integrity verification";file transfers;high performance networks;undetected disk errors;"silent data corruption"",""Receivers";Servers;Disk drives;Data transfer;Target tracking;Robustness;"Distributed databases"","""",""8"","""",""45"",""IEEE"",""14 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"RMWPaxos: Fault-Tolerant In-Place Consensus Sequences,""J. Skrzypczak"; F. Schintke;" T. Schütt"",""Department of Distributed Algorithms, Zuse Institute Berlin, Berlin, Germany"; Department of Distributed Algorithms, Zuse Institute Berlin, Berlin, Germany;" Department of Distributed Algorithms, Zuse Institute Berlin, Berlin, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2020"",""2020"",""31"",""10"",""2392"",""2405"",""Building consensus sequences based on distributed, fault-tolerant consensus, as used for replicated state machines, typically requires a separate distributed state for every new consensus instance. Allocating and maintaining this state causes significant overhead. In particular, freeing the distributed, outdated states in a fault-tolerant way is not trivial and adds further complexity and cost to the system. In this article, we propose an extension to the single-decree Paxos protocol that can learn a sequence of consensus decisions `in-place', i.e., with a single set of distributed states. Our protocol does not require dynamic log structures and hence has no need for distributed log pruning, snapshotting, compaction, or dynamic resource allocation. The protocol builds a fault-tolerant atomic register that supports arbitrary read-modify-write operations. We use the concept of consistent quorums to detect whether the previous consensus still needs to be consolidated or is already finished so that the next consensus value can be safely proposed. Reading a consolidated consensus is done without state modifications and is thereby free of concurrency control and demand for serialisation. A proposer that is not interrupted reaches agreement on consecutive consensus decisions within a single message round-trip per decision by preparing the acceptors eagerly with the previous request."",""1558-2183"","""",""10.1109/TPDS.2020.2981891"",""Deutsche Forschungsgemeinschaft(grant numbers:RE 1389)"; DFG priority program(grant numbers:SPP 2037);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9050852"",""Consensus";Paxos;atomic register;consistent quorum;fault-tolerance;"data management"",""Registers";Fault tolerance;Fault tolerant systems;Protocols;Safety;Proposals;"Computer crashes"","""",""6"","""",""51"",""CCBY"",""30 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"SaberLDA: Sparsity-Aware Learning of Topic Models on GPUs,""K. Li"; J. Chen; W. Chen;" J. Zhu"",""Department of Computer Sciences and Technology, Tsinghua University, Beijing, China"; Department of Computer Sciences and Technology, Tsinghua University, Beijing, China; Department of Computer Sciences and Technology, Tsinghua University, Beijing, China;" Department of Computer Sciences and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Apr 2020"",""2020"",""31"",""9"",""2112"",""2124"",""Latent Dirichlet Allocation (LDA) is a popular tool for analyzing discrete count data such as text and images, which are required to model datasets and a large number of topics, e.g., tens of thousands of topics for industry scale applications. Although distributed CPU systems have been used to address this problem, they are slow and resource inefficient. GPU-based systems have emerged as a promising alternative because of their high computational power and memory bandwidth. However, existing GPU-based LDA systems can only learn thousands of topics, because they use dense data structures, and have linear time complexity to the number of topics. In this article, we propose SaberLDA, a GPU-based LDA system that implements a sparsity-aware algorithm to achieve sublinear time complexity to learn a large number of topics. To address the challenges introduced by sparsity, we propose a novel data layout, a warp-based sampling kernel, an efficient sparse matrix counting method, and a fine-grained load balancing strategy. SaberLDA achieves linear speedup on 4 GPUs and is 6-10 times faster than existing GPU systems in thousands of topics. It can learn 40,000 topics from a dataset of billions of tokens in two hours, which was previously only achievable using clusters of tens of CPU servers."",""1558-2183"","""",""10.1109/TPDS.2020.2979702"",""National Natural Science Foundation of China(grant numbers:61525202)"; Beijing Academy of Artificial Intelligence;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9061024"",""GPU acceleration";latent dirichlet allocation;topic models;"machine learning"",""Graphics processing units";Sparse matrices;Data models;Computational modeling;Time complexity;Resource management;"Vocabulary"","""","""","""",""42"",""CCBY"",""8 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Safety Enhancement for Real-Time Parallel Applications in Distributed Automotive Embedded Systems: A Stable Stopping Approach,""G. Xie"; G. Zeng;" R. Li"",""Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, China"; Graduate School of Engineering, Nagoya University, Nagoya, Japan;" Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Apr 2020"",""2020"",""31"",""9"",""2067"",""2080"",""In distributed automotive embedded systems, safety issues run through the entire life cycle, and safety mechanisms for error handling are desirable for risk control. This article focuses on safety enhancement (i.e., safety mechanisms for error handling) for a safety-critical automotive application within its deadline. A stable stopping approach used for safety enhancement for an automotive application is proposed based on the static recovery mechanism provided in ISO 26262. The Stable Stopping-based Safety Enhancement (SSSE) approach is proposed by combining known backward recovery, proposed forward recovery, and proposed forward-and-backward recovery through primary-backup repetition. The stable stopping (i.e., SSSE) approach is a convergence algorithm, which means that when the reliability value reaches a steady state and the algorithm can stop. Experimental results reveal that the exposure level defined in ISO 26262 drops from E3 to E1 after using SSSE, and such improvement enables a safety guarantee of higher level."",""1558-2183"","""",""10.1109/TPDS.2020.2984719"",""National Natural Science Foundation of China(grant numbers:61702172,61932010,61672217,61972139)"; CCF-Tencent Open Fund(grant numbers:CCF-TecentRAGR20190119); Northeastern University(grant numbers:PAL-N201803); Natural Science Foundation of Hunan Province(grant numbers:2018JJ3076); Fundamental Research Funds for the Central Universities;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055151"",""Distributed automotive embedded systems";safety enhancement;"stable stopping"",""Safety";Task analysis;Reliability;Automotive applications;Real-time systems;"Time factors"","""",""2"","""",""30"",""IEEE"",""2 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Scalable and Adaptive Data Replica Placement for Geo-Distributed Cloud Storages,""K. Liu"; J. Peng; J. Wang; W. Liu; Z. Huang;" J. Pan"",""School of Computer Science and Engineering, Central South University, Changsha, China"; School of Computer Science and Engineering, Central South University, Changsha, China; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada; School of Computer Science and Engineering, Central South University, Changsha, China; School of Automation, Central South University, Changsha, China;" Department of Computer Science, University of Victoria, Victoria, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Feb 2020"",""2020"",""31"",""7"",""1575"",""1587"",""In geo-distributed cloud storage systems, data replication has been widely used to serve the ever more users around the world for high data reliability and availability. How to optimize the data replica placement has become one of the fundamental problems to reduce the inter-node traffic and the system overhead of accessing associated data items. In the big data era, traditional solutions may face the challenges of long running time and large overheads to handle the increasing scale of data items with time-varying user requests. Therefore, novel offline community discovery and online community adjustment schemes are proposed to solve the replica placement problem in a scalable and adaptive way. The offline scheme can find a replica placement solution based on the average read/write rates for a certain period of time. The scalability can be achieved as 1) the computation complexity is linear to the amount of data items and 2) the data-node communities can evolve in parallel for a distributed replica placement. Furthermore, the online scheme is adaptive to handle the bursty data requests, without the need to completely override the existing replica placement. Driven by real-world data traces, extensive performance evaluations demonstrate the effectiveness of our design to handle large-scale datasets."",""1558-2183"","""",""10.1109/TPDS.2020.2968321"",""National Natural Science Foundation of China(grant numbers:61672537,61672539,61873353)"; China Postdoctoral Science Foundation; Natural Sciences and Engineering Research Council of Canada; CFI; British Columbia Knowledge Development Fund;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8964316"",""Geo-distributed storage system";data replica placement;scalability;adaptivity;"community discovery"",""Distributed databases";Cloud computing;Scalability;Computer science;Adaptive systems;Reliability;"Face"","""",""16"","""",""36"",""IEEE"",""22 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Scalable, Multi-Constraint, Complex-Objective Graph Partitioning,""G. M. Slota"; C. Root; K. Devine; K. Madduri;" S. Rajamanickam"",""Computer Science Department, Rensselaer Polytechnic Institute, Troy, USA"; Computer Science Department, Rensselaer Polytechnic Institute, Troy, USA; Scalable Algorithms Department, Sandia National Laboratories, Albuquerque, USA; Electrical Engineering and Computer Science Department, The Pennsylvania State University, University Park, USA;" Scalable Algorithms Department, Sandia National Laboratories, Albuquerque, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Jun 2020"",""2020"",""31"",""12"",""2789"",""2801"",""We introduce XtraPuLP, a distributed-memory graph partitioner designed to process irregular trillion-edge graphs. XtraPuLP is based on the scalable label propagation community detection technique, which has been demonstrated in various prior works as a viable means to produce high quality partitions of skewed and small-world graphs with minimal computation time. Our XtraPuLP implementation can also be generalized to compute partitions with an arbitrary number of constraints, and it can compute partitions with balanced communication load across all parts. On a collection of large sparse graphs, we show that XtraPuLP partitioning is considerably faster than state-of-the-art partitioning methods, while also demonstrating that XtraPuLP can produce partitions of real-world graphs with billion+ vertices and over a hundred billion edges in minutes. Additionally, we demonstrate XtraPuLP on a variety of applications, including large-scale graph analytics and sparse matrix-vector multiplication."",""1558-2183"","""",""10.1109/TPDS.2020.3002150"",""National Science Foundation(grant numbers:OCI-0725070,ACI-1238993,ACI-1444747)"; U.S. Department of Energy; Rensselaer Polytechnic Institute and Sandia National Laboratories(grant numbers:DE-AC02-05CH11231); National Science Foundation(grant numbers:ACI-1253881,CCF-1439057);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115834"",""Graph partitioning";load balancing;label propagation;"graph analysis"",""Partitioning algorithms";Scalability;Sparse matrices;Computational modeling;Rats;Indexes;"Social networking (online)"","""",""18"","""",""49"",""IEEE"",""12 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Scheduling Parallel Real-Time Tasks on the Minimum Number of Processors,""H. Cho"; C. Kim; J. Sun; A. Easwaran; J. -D. Park;" B. -C. Choi"",""Department of Computer and Information Science, Korea University, Sejong, South Korea"; Department of Computer and Information Science, Korea University, Sejong, South Korea; Department of Computer and Information Science, Korea University, Sejong, South Korea; School of Computer Engineering, Nanyang Technological University, Singapore; ETRI, Daejeon, South Korea;" ETRI, Daejeon, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""171"",""186"",""Recently, several parallel frameworks have emerged to utilize the increasing computational capacity of multiprocessors. Parallel tasks are distinguished from traditional sequential tasks in that the subtasks contained in a single parallel task can simultaneously execute on multiple processors. In this study, we consider the scheduling problem of minimizing the number of processors on which the parallel real-time tasks feasibly run. In particular, we focus on scheduling sporadic parallel real-time tasks, in which precedence constraints between subtasks of each parallel task are expressed using a directed acyclic graph (DAG). To address the problem, we formulate an optimization problem that aims to minimize the maximum processing capacity for executing the given tasks. We then suggest a polynomial solution consisting of three steps: (1) transform each parallel real-time task into a series of multithreaded segments, while respecting the precedence constraints of the DAG"; (2) selectively extend the segment lengths;" and (3) interpret the problem as a flow network to balance the flows on the terminal edges. We also provide the schedulability bound of the proposed solution: it has a capacity augmentation bound of 2. Our experimental results show that the proposed approach yields higher performance than one developed in a recent study."",""1558-2183"","""",""10.1109/TPDS.2019.2929048"",""National Research Foundation of Korea"; Ministry of Education(grant numbers:NRF-2015R1D1A1A01057018,NRF-2018R1D1A1B07049078);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8770295"",""Real-time scheduling";multicores;multiprocessors;linear programming;flow networks;maximum flow problem;"minimum cost flow problem"",""Task analysis";Processor scheduling;Real-time systems;Program processors;Computational modeling;Scheduling;"Multicore processing"","""",""15"","""",""51"",""IEEE"",""23 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Scheduling Periodical Multi-Stage Jobs With Fuzziness to Elastic Cloud Resources,""J. Zhu"; X. Li; R. Ruiz; W. Li; H. Huang;" A. Y. Zomaya"",""School of Computer Science & Technology, Nanjing University of Posts & Telecommunications, Nanjing, China"; School of Computer Science and Engineering, Southeast University, Nanjing, China; Grupo de Sistemas de Optimización Aplicada, Universitat Politècnica de València, València, Spain; Centre of Distributed and High Performance Computing, School of Computer Science, The University of Sydney, Camperdown, Australia; School of Computer Science & Technology, Nanjing University of Posts & Telecommunications, Nanjing, China;" Centre of Distributed and High Performance Computing, School of Computer Science, The University of Sydney, Camperdown, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Jul 2020"",""2020"",""31"",""12"",""2819"",""2833"",""We investigate a workflow scheduling problem with stochastic task arrival times and fuzzy task processing times and due dates. The problem is common in many real-time and workflow-based applications, where tasks with fixed stage number and linearly dependency are executed on scalable cloud resources with multiple price options. The challenges lie in proposing effective, stable, and robust algorithms under stochastic and fuzzy tasks. A triangle fuzzy number-based model is formulated. Two metrics are explored: the cost and the degree of satisfaction. An iterated heuristic framework is proposed to periodically schedule tasks, which consists of a task collection and a fuzzy task scheduling phases. Two task collection strategies are presented and two task prioritization strategies are employed. In order to achieve a high satisfaction degree, deadline constraints are defined at both job and task levels. By designing delicate experiments and applying sophisticated statistical techniques, experimental results show that the proposed algorithm is more effective and robust than the two existing methods."",""1558-2183"","""",""10.1109/TPDS.2020.3004134"",""National Basic Research Program of China (973 Program)(grant numbers:2017YFB1400800)"; National Natural Science Foundation of China(grant numbers:61672297,61872077,61832004); Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:18KJB520039); National Science Foundation for Post-doctoral Scientists of China(grant numbers:2018M640510); Spanish Ministry of Science, Innovation, and Universities(grant numbers:RTI2018-094940-B-I00);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9122581"",""Job scheduling";fuzzy processing times;fuzzy deadlines;"cloud computing"",""Task analysis";Dynamic scheduling;Job shop scheduling;Cloud computing;Schedules;"Heuristic algorithms"","""",""13"","""",""31"",""IEEE"",""22 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"SF-Sketch: A Two-Stage Sketch for Data Streams,""L. Liu"; Y. Shen; Y. Yan; T. Yang; M. Shahzad; B. Cui;" G. Xie"",""Shaanxi Key Laboratory of Network and System Security, Xidian University, Xi'an, China"; Shaanxi Key Laboratory of Network and System Security, Xidian University, Xi'an, China; Department of Computer and Science, Peking University, Beijing, China; Department of Computer and Science, Peking University, Beijing, China; Department of Computer Science, North Carolina State University, Raleigh, USA; Department of Computer and Science, Peking University, Beijing, China;" Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2020"",""2020"",""31"",""10"",""2263"",""2276"",""Sketches are probabilistic data structures designed for recording frequencies of items in a multi-set. They are widely used in various fields, especially for gathering Internet statistics from distributed data streams in network measurements. In a distributed streaming application with high data rates, a sketch in each monitoring node “fills up” very quickly and then its content is transferred to a remote collector responsible for answering queries. Thus, the size of the contents transferred must be kept as small as possible while meeting the desired accuracy requirement. To obtain significantly higher accuracy while keeping the same update and query speed as the best prior sketches, in this article, we propose a new sketch - the Slim-Fat (SF) sketch. The key idea behind the SF-sketch is to maintain two separate sketches: a larger sketch, the Fat-subsketch, and a smaller sketch, the Slim-subsketch. The Fat-subsketch is used for updating and periodically producing the Slim-subsketch, which is then transferred to the remote collector for answering queries quickly and accurately. We also present the error bound as well as an accurate model of the correct rate of the SF-sketch, and verify their correctness through experiments. We implemented and extensively evaluated the SF-sketch along with several prior sketches. Our results show that when the size of our Slim-subsketch and of the widely used Count-Min (CM) sketch are kept the same, our SF-sketch outperforms the CM-sketch by up to 33.1 times in terms of accuracy (when the ratio of the sizes of the Fat-subsketch and the Slim-subsketch is 16:1). We have made all source codes publicly available at Github [“Source code of SF sketches,” [Online]. Available: https://github.com/paper2017/SF-sketch]."",""1558-2183"","""",""10.1109/TPDS.2020.2987609"",""National Basic Research Program of China (973 Program)(grant numbers:2018YFE0207600,2018YFB2100403)"; National Natural Science Foundation of China(grant numbers:61672061,U1736216); National Science Foundation(grant numbers:CNS-1616317,CNS-1616273);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9068427"",""Network measurements";sketch;distributed monitoring;multiset;"frequent items"",""Distributed databases";Monitoring;Bars;Frequency measurement;Registers;Fats;"Hash functions"","""",""12"","""",""37"",""IEEE"",""15 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Simplified Workflow Simulation on Clouds based on Computation and Communication Noisiness,""R. Mathá"; S. Ristov; T. Fahringer;" R. Prodan"",""Institute of Computer Science, University of Innsbruck, Innsbruck, Austria"; Institute of Computer Science, University of Innsbruck, Innsbruck, Austria; Institute of Computer Science, University of Innsbruck, Innsbruck, Austria;" Institute of Information Technology, University of Klagenfurt, Klagenfurt"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Feb 2020"",""2020"",""31"",""7"",""1559"",""1574"",""Many researchers rely on simulations to analyze and validate their researched methods on Cloud infrastructures. However, determining relevant simulation parameters and correctly instantiating them to match the real Cloud performance is a difficult and costly operation, as minor configuration changes can easily generate an unreliable inaccurate simulation result. Using legacy values experimentally determined by other researchers can reduce the configuration costs, but is still inaccurate as the underlying public Clouds and the number of active tenants are highly different and dynamic in time. To overcome these deficiencies, we propose a novel model that simulates the dynamic Cloud performance by introducing noise in the computation and communication tasks, determined by a small set of runtime execution data. Although the estimating method is apparently costly, a comprehensive sensitivity analysis shows that the configuration parameters determined for a certain simulation setup can be used for other simulations too, thereby reducing the tuning cost by up to 82.46 percent, while declining the simulation accuracy by only 1.98 percent on average. Extensive evaluation also shows that our novel model outperforms other state-of-the-art dynamic Cloud simulation models, leading up to 22 percent lower makespan inaccuracy."",""1558-2183"","""",""10.1109/TPDS.2020.2967662"",""European Commission(grant numbers:801091)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8964294"",""Cloud computing";simulation;workflow applications;burstable instances;"performance instability and noisiness"",""Task analysis";Cloud computing;Computational modeling;Analytical models;Data models;"Sensitivity analysis"","""",""9"","""",""32"",""CCBY"",""22 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Simultaneous Management of Peak-Power and Reliability in Heterogeneous Multicore Embedded Systems,""M. Ansari"; J. Saber-Latibari; M. Pasandideh;" A. Ejlali"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""623"",""633"",""Analysis of reliability, power, and performance at hardware and software levels due to heterogeneity is a crucial requirement for heterogeneous multicore embedded systems. Escalating power densities have led to thermal issues for heterogeneous multicore embedded systems. This paper proposes a peak-power-aware reliability management scheme to meet power constraints through distributing power density on the whole chip such that reliability targets are satisfied. In this paper, we consider peak power consumption as a system-level power constraint to prevent system failure. To balance the power consumption, we also employ a Dynamic Frequency Scaling (DFS) method to further reduce peak power consumption and satisfy thermal constraints on the chip. We illustrate the benefits of our scheme by comparing it with state-of-the-art schemes, resulting in average in 26.5 percent less peak power consumption (up to 54.3 percent)."",""1558-2183"","""",""10.1109/TPDS.2019.2940631"",""Sharif University of Technology(grant numbers:G930827)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8827950"",""Power consumption";reliability;embedded systems;dynamic frequency scaling;thermal safe power;"thermal design power"",""Multicore processing";Reliability;Task analysis;Power demand;Embedded systems;Real-time systems;"Timing"","""",""16"","""",""44"",""IEEE"",""10 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Single Restart with Time Stamps for Parallel Task Processing with Known and Unknown Processors,""J. P. Champati";" B. Liang"",""Division of Information Science and Engineering, School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden";" Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""187"",""200"",""We study the problem of scheduling n tasks on m + m' parallel processors, where the processing times on m processors are known while those on the remaining m' processors are not known a priori. This semi-online model is an abstraction of certain heterogeneous computing systems, e.g., with them known processors representing local CPU cores and the unknown processors representing remote servers with uncertain availability of computing cycles. Our objective is to minimize the makespan of all tasks. We initially focus on the case m' = 1 and propose a semi-online algorithm termed Single Restart with Time Stamps (SRTS), which has time complexity O(nlogn). We derive its competitive ratio in comparison with the optimal offline solution. If the unknown processing times are deterministic, the competitive ratio of SRTS is shown to be either always constant or asymptotically constant in practice, respectively in cases where the processing times are independent and dependent on m. A similar result is obtained when the unknown processing times are random. Furthermore, extending the ideas of SRTS, we propose a heuristic algorithm termed SRTS-Multiple (SRTS-M) for the case m' > 1. Finally, where tasks arrive dynamically with unknown arrival times, we extend SRTS to Dynamic SRTS (DSRTS) and find its competitive ratio. Besides the proven competitive ratios, simulation results further suggest that SRTS and SRTS-M give superior performance on average over randomly generated task processing times, substantially reducing the makespan over the best known alternatives. Interestingly, the performance gain is more significant for task processing times sampled from heavy-tailed distributions."",""1558-2183"","""",""10.1109/TPDS.2019.2929173"",""Natural Sciences and Engineering Research Council of Canada";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765409"",""Computational offloading";edge computing;mobile cloud computing;opportunistic computing;unknown processing times;task restart;"semi-online algorithms"",""Program processors";Task analysis;Processor scheduling;Scheduling;Heuristic algorithms;Servers;"Schedules"","""",""6"","""",""36"",""IEEE"",""17 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"SLEEF: A Portable Vectorized Library of C Standard Mathematical Functions,""N. Shibata";" F. Petrogalli"",""Graduate School of Information Science, Nara Institute of Science and Technology, Nara, Japan";" ARM 110, Cambridge, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Jan 2020"",""2020"",""31"",""6"",""1316"",""1327"",""In this article, we present techniques used to implement our portable vectorized library of C standard mathematical functions written entirely in C language. In order to make the library portable while maintaining good performance, intrinsic functions of vector extensions are abstracted by inline functions or preprocessor macros. We implemented the functions so that they can use sub-features of vector extensions such as fused multiply-add, mask registers, and extraction of mantissa. In order to make computation with SIMD instructions efficient, the library only uses a small number of conditional branches, and all the computation paths are vectorized. We devised a variation of the Payne-Hanek argument reduction for trigonometric functions and a floating point remainder, both of which are suitable for vector computation. We compare the performance with our library to Intel SVML."",""1558-2183"","""",""10.1109/TPDS.2019.2960333"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936472"",""Parallel and vector implementations";SIMD processors;elementary functions;"floating-point arithmetic"",""Libraries";Registers;Standards;Program processors;Optimization;Open source software;"Computer architecture"","""",""11"","""",""53"",""CCBY"",""18 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Spatially Bursty I/O on Supercomputers: Causes, Impacts and Solutions,""J. Yu"; W. Yang; F. Wang; D. Dong; J. Feng;" Y. Li"",""Computational Aerodynamics Institute, China Aerodynamics Research and Development Center, Mianyang, China"; College of Computer, National University of Defense Technology, Changsha, China; Computational Aerodynamics Institute, China Aerodynamics Research and Development Center, Mianyang, China; College of Computer, National University of Defense Technology, Changsha, China; National Supercomputer Centre in Tianjin, Tianjin, China;" National Supercomputer Centre in Tianjin, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Jul 2020"",""2020"",""31"",""12"",""2908"",""2922"",""Understanding the I/O characteristics of supercomputers is crucial for grasping accurate I/O workloads and uncovering potential I/O inefficiency. We collect and analyze I/O traces from two production supercomputers, and find that the I/O traffic peaks in the system not only occur in short periods of time but also originate from a minority of adjacent compute nodes, which we call spatially bursty I/O. Since modern supercomputers widely adopt I/O forwarding architecture, in which an I/O node performs I/O on behalf of a subset of compute nodes in the vicinity, spatially bursty I/O will cause significant load imbalance and underutilization on the I/O nodes. To address such problems, we quantitatively analyze the two causes of spatially bursty I/O, including uneven I/O distribution on job's processes and uneven job nodes distribution on the system. Two different solutions are proposed to mobilize more I/O nodes to participate in job's I/O activity. (1) We change the I/O node mapping, making adjacent compute nodes use different I/O nodes instead of a same one. (2) According to the job's I/O characteristics extracted from history I/O traces, we distribute the compute nodes of data-intensive jobs more sparsely to utilize more I/O nodes. Extensive evaluations of both solutions show that they can further exploit the potential of I/O forwarding layer. We have deployed the proposed I/O node mapping on a production supercomputer for 11 months. Our experience finds that it can effectively promote I/O performance, balance loads, and alleviate I/O interference."",""1558-2183"","""",""10.1109/TPDS.2020.3005572"",""National Numerical Windtunnel Project of China";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9127806"",""Parallel I/O";bursty I/O;I/O forwarding;I/O node mapping;"resource allocation"",""Supercomputers";Aerodynamics;Computer architecture;Probability distribution;Entropy;Production;"History"","""",""8"","""",""51"",""IEEE"",""29 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"System Error Prediction for Business Support Systems in Telecommunications Networks,""E. -H. Yeh"; P. Lin; X. -X. Lin; J. -Y. Jeng;" Y. Fang"",""Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan"; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan; Information System Department General Headquarters, Chunghwa Telecom Company, Ltd., Taipei, Taiwan;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 Jun 2020"",""2020"",""31"",""11"",""2723"",""2733"",""Reliability and stability have been treated as the major requirements for the Business Support System (BSS) in telecommunications networks. It is crucial and essential for service providers to maintain good operating state of the BSS. In this article, we aim at system error prediction for a BSS, i.e., we predict occurrences of the abnormal state or behavior of the BSS. Because the occurrences of system errors are rare events in the BSS (i.e., the dataset of system status is highly imbalanced), it is highly challenging to use machine learning or deep learning algorithms to predict system error for the BSS. To address this challenge, we propose a machine learning-based framework for the system error prediction and a Frequency-based Feature Creation (FFC) algorithm to create new features to improve prediction. By adding the time-series information created by the existing features, the proposed FFC can amplify the effects of important features. Our experimental results show that the FFC significantly improves the prediction performance for the Random Forest algorithm."",""1558-2183"","""",""10.1109/TPDS.2020.3001593"",""Ministry of Science and Technology, Taiwan(grant numbers:MOST 108-2823-8-002-007-,MOST 107-2221-E-002-042-MY3,MOST 106-2923-E-002-005-MY3)"; Chunghwa Telecom in Taiwan;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9115291"",""System error prediction";business support system;machine learning;"telecommunications network"",""Machine learning algorithms";Monitoring;Prediction algorithms;Databases;Training;Anomaly detection;"Business"","""",""3"","""",""44"",""IEEE"",""11 Jun 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"T-BASIR: Finding Shutdown Bugs for Cloud-Based Applications in Cloud Spot Markets,""A. Alourani"; A. D. Kshemkalyani;" M. Grechanik"",""Department of Computer Science, University of Illinois at Chicago, Chicago, USA"; Department of Computer Science, University of Illinois at Chicago, Chicago, USA;" Department of Computer Science, University of Illinois at Chicago, Chicago, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Apr 2020"",""2020"",""31"",""8"",""1912"",""1924"",""One of the major advantages of cloud spot instances in cloud computing is to allow stakeholders to economically deploy their applications at much lower costs than that of other types of cloud instances. In exchange, spot instances are often exposed to revocations (i.e., terminations) by cloud providers. With spot instances becoming pervasive, terminations have become a part of the normal behavior of cloud-based applications";" thus, these applications may be left in an incorrect state leading to certain bugs. Unfortunately, these applications are not designed or tested to deal with this behavior in the cloud environment, and as a result, the advantages of cloud spot instances could be significantly minimized or even entirely negated. We propose a novel solution to automatically find these bugs and locate their causes in the source code. We evaluate our solution using 10 popular open-source applications. The results show that our solution not only finds more instances and different types of these bugs compared to the random approach, but it also locates the causes of these bugs to help developers improve the design of the shutdown process and is more efficient in finding instances of these bugs since it interposes at the system call layer."",""1558-2183"","""",""10.1109/TPDS.2020.2980265"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9035645"",""Cloud computing";cloud spot markets;shutdown bugs of cloud-based applications;kernel modules;irregular terminations of cloud-based applications;"spot instance revocations"",""Computer bugs";Cloud computing;Testing;Radio access technologies;Open source software;Kernel;"Hardware"","""",""2"","""",""47"",""IEEE"",""13 Mar 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"T-Caching: Enhancing Feasibility of In-Network Caching in ICN,""S. Lee"; I. Yeom;" D. Kim"",""Department of Computer Engineering, Sungkyunkwan University, Seoul, South Korea"; Department of Computer Engineering, Sungkyunkwan University, Seoul, South Korea;" Department of Computer Science, Kangwon National University, Chuncheon-si, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2020"",""2020"",""31"",""7"",""1486"",""1498"",""In Information-Centric Networking (ICN), in-network caching is one of the core functions to implement efficient content distribution. As content is served from the network cache, redundant transmission across the same link is reduced and better quality services are provided to the user. However, caching operations (locating cached content, validating/writing content to the cache) may be a large burden on routers. In this article, we substantially reduce the caching overhead of routers by introducing T-Caching, a new in-network caching model in which routers selectively cache some of the most popular content recommended by content providers. In T-Caching, tokens are used 1) to enable routers to maximize caching benefits by simply controlling the amount of content inserted into the cache";" 2) to allow content providers to recommend the most popular content as much as routers would actually cache. Our simulation and empirical studies using synthetic data, as well as real-world traces, show that T-Caching allows routers to insert much less than 1 percent of the content into the cache, compared to typical caching mechanisms. Nevertheless, the cache-hit ratio is improved by up to 2~9 times depending on the cache size. We believe that T-Caching can significantly contribute to improving feasibility and performance of in-network caching in ICN."",""1558-2183"","""",""10.1109/TPDS.2020.2970702"",""National Research Foundation of Korea(grant numbers:NRF-2018R1C1B4A01022931,NRF-2016M3C4A7952587)"; MSIT, Korea(grant numbers:IITP-2019-2018-0-01431);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8975984"",""Information-centric networking";network caching;feasibility;caching overhead;"caching performance"",""Routing";Servers;Computer architecture;Topology;Network topology;Mathematical model;"Logic gates"","""",""24"","""",""36"",""IEEE"",""30 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Task Scheduling for Energy Consumption Constrained Parallel Applications on Heterogeneous Computing Systems,""Z. Quan"; Z. -J. Wang; T. Ye;" S. Guo"",""College of Information Science and Engineering, Hunan University, Changsha, China"; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; College of Information Science and Engineering, Hunan University, Changsha, China;" Department of Computing, Hong Kong Polytechnic University, Kowloon, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1165"",""1182"",""Power-aware task scheduling on processors has been a research hotspot in computing systems. Given an application G containing a set N of tasks {n1,...,n|N|}, and a system containing a set U of processors {u1,..., u|U|}, the power-aware task scheduling generally refers to finding the appropriate processor and frequency for each task ni, so as to make sure that all the tasks can be finished efficiently and the overall energy consumption is guaranteed. In this article, we study the problem of minimizing the schedule length for energy consumption constrained parallel applications on heterogeneous computing systems, where the schedule length refers to the time interval between starting the first task and finishing the last task. For this problem, existing work adopts a policy that preassigns the minimum energy consumption for each unassigned task. Nevertheless, our analysis reveals that, such a pre-assignment policy could be unfair for the low priority tasks, and it may not achieve an optimistic schedule length. Thereby, we propose a new task scheduling algorithm that suggests a weight-based mechanism to preassign energy consumption for unassigned tasks, and we provide the rigorous proof to show its feasibility. Further, we show that this idea can be extended to solve reliability maximization problems with energy consumption constraint or with both deadline and energy consumption constraints, where the reliability refers to the probability of executing application G without failures, and the deadline constraint refers to the “allowable” maximum schedule length. We have conducted extensive experiments based on real parallel applications. The experimental results consistently demonstrate that our proposed algorithms can achieve favourable performance, compared to state-of-the-art algorithms."",""1558-2183"","""",""10.1109/TPDS.2019.2959533"",""National Key R&D Program of China(grant numbers:2018YFB0204100)"; National Natural Science Foundation of China(grant numbers:61472124,61472453,61602166,61702320,U1401256,U1501252,U1611264,U1711261,U1711262,U1811264,61972425);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936469"",""heterogeneous systems";energy consumption;parallel application;task scheduling;"reliability"",""Task analysis";Energy consumption;Reliability;Program processors;Schedules;Processor scheduling;"Scheduling"","""",""40"","""",""55"",""IEEE"",""18 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"The Design of Fast Content-Defined Chunking for Data Deduplication Based Storage Systems,""W. Xia"; X. Zou; H. Jiang; Y. Zhou; C. Liu; D. Feng; Y. Hua; Y. Hu;" Y. Zhang"",""Harbin Institute of Technology, Shenzhen, China"; Harbin Institute of Technology, Shenzhen, China; Department of Computer Science and Engineering, University of Texas at Arlington, USA; Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China; Harbin Institute of Technology, Shenzhen, China; Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China;" Wuhan National Laboratory for Optoelectronics, School of Computer Sci.&Tech., Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Apr 2020"",""2020"",""31"",""9"",""2017"",""2031"",""Content-Defined Chunking (CDC) has been playing a key role in data deduplication systems recently due to its high redundancy detection ability. However, existing CDC-based approaches introduce heavy CPU overhead because they declare the chunk cut-points by computing and judging the rolling hashes of the data stream byte by byte. In this article, we propose FastCDC, a Fast and efficient Content-Defined Chunking approach, for data deduplication-based storage systems. The key idea behind FastCDC is the combined use of five key techniques, namely, gear based fast rolling hash, simplifying and enhancing the Gear hash judgment, skipping sub-minimum chunk cut-points, normalizing the chunk-size distribution in a small specified region to address the problem of the decreased deduplication ratio stemming from the cut-point skipping, and last but not least, rolling two bytes each time to further speed up CDC. Our evaluation results show that, by using a combination of the five techniques, FastCDC is 3-12X faster than the state-of-the-art CDC approaches, while achieving nearly the same and even higher deduplication ratio as the classic Rabin-based CDC. In addition, our study on the deduplication throughput of FastCDC-based Destor (an open source deduplication project) indicates that FastCDC helps achieve 1.2-3.0X higher throughput than Destor based on state-of-the-art chunkers."",""1558-2183"","""",""10.1109/TPDS.2020.2984632"",""National Natural Science Foundation of China(grant numbers:61972441,61872110,61872414,61772212,61821003,61772222,61832007)"; National Science and Technology of China(grant numbers:2017ZX01032-101); Wuhan National Laboratory for Optoelectronics(grant numbers:2018WNLOKF008); Shenzhen Science and Technology Program(grant numbers:JCYJ20190806143405318); Key R&D Program for Guangdong Province(grant numbers:2019B010136001); National Science Foundation(grant numbers:CCF-1704504,CCF-1629625);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9055082"",""Data deduplication";content-defined chunking;storage system;"performance evaluation"",""Microsoft Windows";Gears;Power capacitors;Redundancy;Acceleration;Throughput;"Distributed databases"","""",""40"","""",""53"",""IEEE"",""2 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"The Existence of Completely Independent Spanning Trees for Some Compound Graphs,""X. -W. Qin"; R. -X. Hao;" J. -M. Chang"",""Department of Mathematics, Beijing Jiaotong University, Beijing, P.R. China"; Department of Mathematics, Beijing Jiaotong University, Beijing, P.R. China;" Institute of Information and Decision Sciences, National Taipei University of Business, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""201"",""210"",""Given two regular graphs G and H such that the vertex degree of G is equal to the number of vertices in H, the compound graph G(H) is constructed by replacing each vertex of G by a copy of Hand replacing each edge of G by an additional edge connecting random vertices in two corresponding copies of H, respectively, under the constraint that each vertex in G(H) is incident with only one additional edge, exactly. L-HSDCm is a compound graph G(H), where G is a hypercube Qm and H is a complete graph Km, which is defined by focusing on the connected relation between servers in the novel data center network HSDCm proposed in [30]. A set of k spanning trees in a graph G are called completely independent spanning trees (CISTs for short) if the paths joining every pair of vertices x and yin any two trees have neither vertex nor edge in common, except for x and y. In this paper, we give a sufficient condition for the existence of k CISTs in a kind of compound graph. Furthermore, a specific construction algorithm is provided. As corollaries of the main results, the existences of two CISTs form m ≥ 4";" three CISTs form m ≥ 8 and four CISTs form m ≥ 10 in L-HSDCm(m) are gotten directly."",""1558-2183"","""",""10.1109/TPDS.2019.2931904"",""National Natural Science Foundation of China(grant numbers:11731002)"; 111 Project of China(grant numbers:B16002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8781860"",""Completely independent spanning tree";data center network architecture;compound graph;"fault-tolerance"",""Compounds";Data centers;Hypercubes;Servers;Network architecture;Bipartite graph;"Routing"","""",""26"","""",""30"",""IEEE"",""30 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"The Impact of Event Processing Flow on Asynchronous Server Efficiency,""S. Zhang"; Q. Wang; Y. Kanemasa; H. Shan;" L. Hu"",""Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, USA"; Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, USA; Software Laboratory, FUJITSU LABORATORIES LTD, Kawasaki, Japan; JD.com American Technologies Corporation, Mountain View, USA;" Computing and Information Sciences, Florida International University, Miami, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""565"",""579"",""Asynchronous event-driven server architecture has been considered as a superior alternative to the thread-based counterpart due to reduced multithreading overhead. In this paper, we conduct empirical research on the efficiency of asynchronous Internet servers, showing that an asynchronous server may perform significantly worse than a thread-based one due to two design deficiencies. The first one is the widely adopted one-event-one-handler event processing model in current asynchronous Internet servers, which could generate frequent unnecessary context switches between event handlers, leading to significant CPU overhead of the server. The second one is a write-spin problem (i.e., repeatedly making unnecessary I/O system calls) in asynchronous servers due to some specific runtime workload and network conditions (e.g., large response size and non-trivial network latency). To address these two design deficiencies, we present a hybrid solution by exploiting the merits of different asynchronous architectures so that the server is able to adapt to dynamic runtime workload and network conditions in the cloud. Concretely, our hybrid solution applies a lightweight runtime request checking and seeks for the most efficient path to process each request from clients. Our results show that the hybrid solution can achieve from 10 to 90 percent higher throughput than all the other types of servers under the various realistic workload and network conditions in the cloud."",""1558-2183"","""",""10.1109/TPDS.2019.2938500"",""National Science Foundation(grant numbers:1566443)"; Louisiana Board of Regents(grant numbers:LEQSF(2015-18)-RD-A-11); Fujitsu;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8825817"",""Asynchronous";event-driven;thread-based;internet servers;"efficiency"",""Servers";Instruction sets;Connectors;Runtime;Internet;Context;"Message systems"","""","""","""",""61"",""IEEE"",""5 Sep 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"The Network-Integrated Storage System,""I. Kettaneh"; A. Alquraan; H. Takruri; S. Yang; A. C. Arpaci-Dusseau; R. H. Arpaci-Dusseau;" S. Al-Kiswany"",""Cheriton School of Computer Science, University of Waterloo,, Waterloo, Canada"; Cheriton School of Computer Science, University of Waterloo,, Waterloo, Canada; Cheriton School of Computer Science, University of Waterloo,, Waterloo, Canada; Computer Sciences Department, University of Wisconsin-Madison, Madison, USA; Computer Sciences Department, University of Wisconsin-Madison, Madison, USA; Computer Sciences Department, University of Wisconsin-Madison, Madison, USA;" Cheriton School of Computer Science, University of Waterloo,, Waterloo, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""486"",""500"",""We present NICE, a key-value storage system design that leverages new software-defined network capabilities to build cluster-based network-efficient storage system. NICE presents novel techniques to co-design network routing and multicast with storage replication, consistency, and load balancing to achieve higher efficiency, performance, and scalability. We implement the NICEKV prototype. NICEKV follows the NICE approach in designing four essential network-centric storage mechanisms: request routing, replication, consistency, and load balancing. Our evaluation shows that the proposed approach brings significant performance gains compared with the current systems design: up to 7× put/get performance improvement, up to 2× reduction in network load, 3× to 9× load reduction on the storage nodes, and the elimination of scalability bottlenecks present in current designs."",""1558-2183"","""",""10.1109/TPDS.2019.2938158"",""NSERC Discovery"; NSERC Engage; Canada Foundation for Innovation; NSF(grant numbers:CNS-1419199,CNS-1421033,CNS-1319405,CNS-1218405); NetApp VTC, Canada;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8818663"",""Key-value storage";software-defined networks;network-integrated design;network-system co-design;"distributed storage"",""Peer-to-peer computing";Routing;Control systems;Standards;Protocols;System analysis and design;"Load management"","""",""3"","""",""63"",""IEEE"",""28 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"The Workflow Trace Archive: Open-Access Data From Public and Private Computing Infrastructures,""L. Versluis"; R. Mathá; S. Talluri; T. Hegeman; R. Prodan; E. Deelman;" A. Iosup"",""Computer Science, Vrije Universiteit Amsterdam, HV Amsterdam, Netherlands"; Institute of Computer Science, Universitat Innsbruck, Innsbruck, Austria; Computer Science, Vrije Universiteit Amsterdam, HV Amsterdam, Netherlands; Computer Science, Vrije Universiteit Amsterdam, HV Amsterdam, Netherlands; Institute of Software Technology, University of Klagenfurt, Klagenfurt am, Austria; Information Sciences Institute, University of Southern California, Los Angeles, USA;" Computer Science, Vrije Universiteit Amsterdam, HV Amsterdam, Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""4 May 2020"",""2020"",""31"",""9"",""2170"",""2184"",""Realistic, relevant, and reproducible experiments often need input traces collected from real-world environments. In this work, we focus on traces of workflows—common in datacenters, clouds, and HPC infrastructures. We show that the state-of-the-art in using workflow-traces raises important issues: (1) the use of realistic traces is infrequent and (2) the use of realistic, open-access traces even more so. Alleviating these issues, we introduce the Workflow Trace Archive (WTA), an open-access archive of workflow traces from diverse computing infrastructures and tooling to parse, validate, and analyze traces. The WTA includes ${>}48$>48 million workflows captured from ${>}10$>10 computing infrastructures, representing a broad diversity of trace domains and characteristics. To emphasize the importance of trace diversity, we characterize the WTA contents and analyze in simulation the impact of trace diversity on experiment results. Our results indicate significant differences in characteristics, properties, and workflow structures between workload sources, domains, and fields."",""1558-2183"","""",""10.1109/TPDS.2020.2984821"",""Vidi MagnaData"; European Union's Horizon 2020 Research and Innovation Programme(grant numbers:801091); National Science Foundation(grant numbers:1664162);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066946"",""Workflow";open-source;open-access;traces;characterization;archive;survey;"simulation"",""Cloud computing";Open source software;Testing;Labeling;Computational modeling;Task analysis;"Tools"","""",""14"","""",""60"",""CCBY"",""14 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Thread Isolation to Improve Symbiotic Scheduling on SMT Multicore Processors,""J. Feliu"; J. Sahuquillo; S. Petit;" L. Eeckhout"",""Department of Computer Engineering, Universitat Politécnica de Valéncia, Valéncia, Spain"; Department of Computer Engineering, Universitat Politécnica de Valéncia, Valéncia, Spain; Department of Computer Engineering, Universitat Politécnica de Valéncia, Valéncia, Spain;" Department of Electronics and Information Systems, Ghent University, Gent, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""359"",""373"",""Resource sharing is a critical issue in simultaneous multithreading (SMT) processors as threads running simultaneously on an SMT core compete for shared resources. Symbiotic job scheduling, which co-schedules applications with complementary resource demands, is an effective solution to maximize hardware utilization and improve overall system performance. However, symbiotic job scheduling typically distributes threads evenly among cores, i.e., all cores get assigned the same number of threads, which we find to lead to sub-optimal performance. In this paper, we show that asymmetric schedules (i.e., schedules that assign a different number of threads to each SMT core) can significantly improve performance compared to symmetric schedules. To leverage this finding, we propose thread isolation, a technique that turns symmetric schedules into asymmetric ones yielding higher overall system performance. Thread isolation identifies SMT-adverse applications and schedules them in isolation on a dedicated core to mitigate their sharp performance degradation under SMT. Our experimental results on an IBM POWER8 processor show that thread isolation improves system throughput by up to 5.5 percent compared to a state-of-the-art symmetric symbiotic job scheduler."",""1558-2183"","""",""10.1109/TPDS.2019.2934955"",""Generalitat Valenciana(grant numbers:APOSTD/2017/052)"; Ministerio de Ciencia, Innovación y Universidades and the European ERDF(grant numbers:RTI2018-098156-B-C51); Universitat Politècnica de València(grant numbers:SP20180140); FWO(grant numbers:G.0434.16N,G.0144.17N); H2020 European Research Council(grant numbers:741097);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8798706"",""Simultaneous multithreading (SMT)";symbiotic job scheduling;"thread isolation"",""Schedules";Message systems;Symbiosis;Program processors;Degradation;Resource management;"Throughput"","""",""2"","""",""29"",""IEEE"",""14 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Thread-Level Locking for SIMT Architectures,""L. Gao"; Y. Xu; R. Wang; Z. Luan; Z. Yu;" D. Qian"",""Beijing Key Laboratory of Electronic System Reliability and Prognostics, College of Information Engineering, Capital Normal University, Beijing, China"; Xi'an Jiaotong University; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China;" School of Computer Science and Engineering, Beihang University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1121"",""1136"",""As more emerging applications are moving to GPUs, thread-level synchronization has become a requirement. However, GPUs only provide warp-level and thread-block-level rather than thread-level synchronization. Moreover, it is highly possible to cause live-locks by using CPU synchronization mechanisms to implement thread-level synchronization for GPUs. In this article, we first propose a software-based thread-level synchronization mechanism called lock stealing for GPUs to avoid live-locks. We then describe how to implement our lock stealing algorithm in mutual exclusive locks and readers-writer locks with high performance. Finally, by putting it all together, we develop a thread-level locking library (TLLL) for commercial GPUs. To evaluate TLLL and show its general applicability, we use it to implement six widely used programs. We compare TLLL against the state-of-the-art ad-hoc GPU synchronization, GPU software transactional memory (STM), and CPU hardware transactional memory (HTM), respectively. The results show that, compared with the ad-hoc GPU synchronization for Delaunay mesh refinement (DMR), TLLL improves the performance by 22 percent on average on a GTX970 GPU, and shows up to 11 percent of performance improvement on a Volta V100 GPU. Moreover, it significantly reduces the required memory size. Such low memory consumption enables DMR to successfully run on the GTX970 GPU with the 10-million mesh size, and the V100 GPU with the 40-million mesh size, with which the ad-hoc synchronization can not run successfully. In addition, TLLL outperforms the GPU STM by 65 percent, and the CPU HTM (running on a Xeon E5-2620 v4 CPU with 16 hardware threads) by 43 percent on average."",""1558-2183"","""",""10.1109/TPDS.2019.2955705"",""National Key R&D Program of China(grant numbers:2017YFB0202202,2018YFB0203901,2017YFC0820100,2016YFB1000204)"; National Natural Science Foundation of China(grant numbers:61732002,61672511,61702495,61772350); Shenzhen Technology Research Project(grant numbers:JSGG20160510154636747); outstanding technical talent program of CAS; Common Information System Equipment Research Program(grant numbers:JZX2017-0988/Y300); Beijing Nova program(grant numbers:Z181100006218093); Beijing Innovation Center for Future Chip(grant numbers:KYJJ2018008);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8911260"",""Deadlocks";parallelism and concurrency;runtime environments;SIMD processors;"synchronization"",""Graphics processing units";Synchronization;Message systems;System recovery;Instruction sets;Hardware;"Computer architecture"","""",""2"","""",""38"",""IEEE"",""25 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Throughput Maximization of NFV-Enabled Multicasting in Mobile Edge Cloud Networks,""Y. Ma"; W. Liang; J. Wu;" Z. Xu"",""Research School of Computer Science, The Australian National University, Canberra, Australia"; Research School of Computer Science, The Australian National University, Canberra, Australia; Department of Computer and Information Sciences, Temple University, Philadelphia, USA;" School of Software, Dalian University of Technology, Dalian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""393"",""407"",""Mobile Edge Computing (MEC) reforms the cloud paradigm by bringing unprecedented computing capacity to the vicinity of end users at the mobile network edge. This provides end users with swift and powerful computing and storage capacities, energy efficiency, and mobility- and context-awareness support. Furthermore, Network Function Virtualization (NFV) is another promising technique that implements various network functions for many applications as pieces of software in servers or cloudlets in MEC networks. The provisioning of virtualized network services in MEC can improve user service experiences, simplify network service deployment, and ease network resource management. However, user requests arrive dynamically and different users demand different amounts of resources, while the resources in MEC are dynamically occupied or released by different services. It thus poses a significant challenge to optimize the performance of MEC through efficient computing and communication resource allocations to meet ever-growing resource demands of users. In this paper, we study NFV-enabled multicasting that is a fundamental routing problem in an MEC network, subject to resource capacities on both its cloudlets and links. Specifically, we first devise an approximation algorithm for the cost minimization problem of admitting a single NFV-enabled multicast request. We then develop an efficient algorithm for the throughput maximization problem for the admissions of a given set of NFV-enabled multicast requests. We third devise an online algorithm with a provable competitive ratio for the online throughput maximization problem when NFV-enabled multicast requests arrive one by one without the knowledge of future request arrivals. We finally evaluate the performance of the proposed algorithms through experimental simulations. Simulation results demonstrate that the proposed algorithms are promising."",""1558-2183"","""",""10.1109/TPDS.2019.2937524"",""National Natural Science Foundation of China(grant numbers:61802048,61802047)"; fundamental research funds for the central universities in China(grant numbers:DUT17RC(3)061,DUT17RC(3)070); Xinghai Scholar Program in Dalian University of Technology, China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812911"",""Mobile edge-cloud networks (MEC)";distributed resource allocation and provisioning;NFV-enabled multicast requests;virtualized network function (VNF);VNF instance placement and sharing;service function chains (SFCs);throughput maximization;Steiner tree problems;"online algorithms"",""Cloud computing";Multicast algorithms;Heuristic algorithms;Approximation algorithms;Throughput;Task analysis;"Bandwidth"","""",""38"","""",""29"",""IEEE"",""26 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Time-Optimal Leader Election in Population Protocols,""Y. Sudo"; F. Ooshita; T. Izumi; H. Kakugawa;" T. Masuzawa"",""Osaka University, Osaka, Japan"; Nara Institute of Science and Technology, Ikoma, Japan; Nagoya Institute of Technology, Nagoya, Japan; Ryukoku University, Shiga, Japan;" Osaka University, Osaka, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Jun 2020"",""2020"",""31"",""11"",""2620"",""2632"",""In this article, we present the first leader election protocol in the population protocol model that stabilizes within O(logn) parallel time in expectation with O(logn) states per agent, where n is the number of agents. Given a rough knowledge m of lg n such that m ≥ lg n and m = O(logn), the proposed protocol guarantees that exactly one leader is elected and the unique leader is kept forever thereafter. This protocol is time-optimal because it was recently proven that any leader election protocol requires Ω(logn) parallel time."",""1558-2183"","""",""10.1109/TPDS.2020.2991771"",""JSPS KAKENHI(grant numbers:17K19977,18K11167,18K18000,19H04085,19K11826,20H04140)"; JST SICORP(grant numbers:JPMJSC1606);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9086071"",""Population protocols";leader election;"stabilization time"",""Protocols";Voting;Sociology;Statistics;Phase locked loops;Schedules;"Epidemics"","""",""8"","""",""19"",""CCBY"",""4 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Toward Designing Cost-Optimal Policies to Utilize IaaS Clouds with Online Learning,""X. Wu"; P. Loiseau;" E. Hyytiä"",""Fondazione Bruno Kessler, Povo, Italy"; Inria, CNRS, Grenoble INP, LIG, Université Grenoble Alpes, Grenoble, France;" University of Iceland, Reykjavik, Iceland"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2020"",""2020"",""31"",""3"",""501"",""514"",""Many businesses possess a small infrastructure that they can use for their computing tasks, but also often buy extra computing resources from clouds. Cloud vendors such as Amazon EC2 offer two types of purchase options: on-demand and spot instances. As tenants have limited budgets to satisfy their computing needs, it is crucial for them to determine how to purchase different options and utilize them (in addition to possible self-owned instances) in a cost-effective manner while respecting their response-time targets. In this paper, we propose a framework to design policies to allocate self-owned, on-demand and spot instances to arriving jobs. In particular, we propose a near-optimal policy to determine the number of self-owned instances and an optimal policy to determine the number of on-demand instances to buy and the number of spot instances to bid for at each time unit. Our policies rely on a small number of parameters and we use an online learning technique to infer their optimal values. Through numerical simulations, we show the effectiveness of our proposed policies, in particular that they achieve a cost reduction of up to 64.51 percent when spot and on-demand instances are considered and of up to 43.74 percent when self-owned instances are considered, compared to previously proposed or intuitive policies."",""1558-2183"","""",""10.1109/TPDS.2019.2935199"",""French National Research Agency"; American Friends of the Alexander von Humboldt Foundation; European Union's Horizon 2020 research and innovation programme(grant numbers:754514); Academy of Finland(grant numbers:296206);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8821399"",""On-demand instances";spot instances;cost efficiency;"online learning"",""Task analysis";Cloud computing;Resource management;Pricing;Parallel processing;Rendering (computer graphics);"Cleaning"","""",""6"","""",""26"",""IEEE"",""30 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Towards Accurate Prediction for High-Dimensional and Highly-Variable Cloud Workloads with Deep Learning,""Z. Chen"; J. Hu; G. Min; A. Y. Zomaya;" T. El-Ghazawi"",""Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter"; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter; School of Computer Science, The University of Sydney, Camperdown, Australia;" Department of Electrical and Computer Engineering, The George Washington University, Washington, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2020"",""2020"",""31"",""4"",""923"",""934"",""Resource provisioning for cloud computing necessitates the adaptive and accurate prediction of cloud workloads. However, the existing methods cannot effectively predict the high-dimensional and highly-variable cloud workloads. This results in resource wasting and inability to satisfy service level agreements (SLAs). Since recurrent neural network (RNN) is naturally suitable for sequential data analysis, it has been recently used to tackle the problem of workload prediction. However, RNN often performs poorly on learning long-term memory dependencies, and thus cannot make the accurate prediction of workloads. To address these important challenges, we propose a deep Learning based Prediction Algorithm for cloud Workloads (L-PAW). First, a top-sparse auto-encoder (TSA) is designed to effectively extract the essential representations of workloads from the original high-dimensional workload data. Next, we integrate TSA and gated recurrent unit (GRU) block into RNN to achieve the adaptive and accurate prediction for highly-variable workloads. Using real-world workload traces from Google and Alibaba cloud data centers and the DUX-based cluster, extensive experiments are conducted to demonstrate the effectiveness and adaptability of the L-PAW for different types of workloads with various prediction lengths. Moreover, the performance results show that the L-PAW achieves superior prediction accuracy compared to the classic RNN-based and other workload prediction methods for high-dimensional and highly-variable real-world cloud workloads."",""1558-2183"","""",""10.1109/TPDS.2019.2953745"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8902077"",""Cloud computing";workload prediction;resource provisioning;sequential data analysis;"deep learning"",""Cloud computing";Feature extraction;Recurrent neural networks;Predictive models;Data mining;Deep learning;"Data centers"","""",""90"","""",""36"",""IEEE"",""15 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Towards Distributed SDN: Mobility Management and Flow Scheduling in Software Defined Urban IoT,""D. Wu"; X. Nie; E. Asmare; D. I. Arkhipov; Z. Qin; R. Li; J. A. McCann;" K. Li"",""Department of Computer Engineering, Hunan University, Changsha, China"; Department of Computer Engineering, Hunan University, Changsha, China; Department of Computing, Imperial College London, London, United Kingdom; Department of Computer Science, University of California, Irvine, USA; Department of Computer Science, University of California, Irvine, USA; Department of Computer Engineering, Hunan University, Changsha, China; Department of Computing, Imperial College London, London, United Kingdom;" Department of Computer Science, State University of New York, New Paltz, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Jan 2020"",""2020"",""31"",""6"",""1400"",""1418"",""The growth of Internet of Things (IoT) devices with multiple radio interfaces has resulted in a number of urban-scale deployments of IoT multinetworks, where heterogeneous wireless communication solutions coexist (e.g., WiFi, Bluetooth, Cellular). Managing the multinetworks for seamless IoT access and handover, especially in mobile environments, is a key challenge. Software-defined networking (SDN) is emerging as a promising paradigm for quick and easy configuration of network devices, but its application in urban-scale multinetworks requiring heterogeneous and frequent IoT access is not well studied. In this paper we present UbiFlow, the first software-defined IoT system for combined ubiquitous flow control and mobility management in urban heterogeneous networks. UbiFlow adopts multiple controllers to divide urban-scale SDN into different geographic partitions (assigning one controller per partition) and achieve distributed control of IoT flows. A distributed hashing based overlay structure is proposed to maintain network scalability and consistency. Based on this UbiFlow overlay structure, the relevant issues pertaining to mobility management such as scalable control, fault tolerance, and load balancing have been carefully examined and studied. The UbiFlow controller differentiates flow scheduling based on per-device requirements and whole-partition capabilities. Therefore, it can present a network status view and optimized selection of access points in multinetworks to satisfy IoT flow requests, while guaranteeing network performance for each partition. Simulation and realistic testbed experiments confirm that UbiFlow can successfully achieve scalable mobility management and robust flow scheduling in IoT multinetworks";" e.g., 67.21 percent throughput improvement, 72.99 percent reduced delay, and 69.59 percent jitter improvements, compared with alternative SDN systems."",""1558-2183"","""",""10.1109/TPDS.2018.2883438"",""National Natural Science Foundation of China(grant numbers:61602168,61972145)"; Intel Collaborative Research Institute for Sustainable Connected Cities; University of California Center on Economic Competitiveness in Transportation; Hu-Xiang Youth Talent Program(grant numbers:2018RS3040); research project OrganiCity(grant numbers:645198); European Unions Horizon 2020 research and innovation program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550695"",""Distributed control";flow scheduling;Internet of Things;mobility management;"software defined networking"",""Switches";Software;Internet of Things;Wireless communication;Wireless fidelity;"Decentralized control"","""",""38"","""",""63"",""IEEE"",""28 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Towards Fair and Privacy-Preserving Federated Deep Models,""L. Lyu"; J. Yu; K. Nandakumar; Y. Li; X. Ma; J. Jin; H. Yu;" K. S. Ng"",""Department of Computer Science, National University of Singapore, Singapore"; Faculty of Information Technology, Monash University, Clayton, Australia; IBM Singapore Lab, Singapore; School of Computing and Information Systems, The University of Melbourne, Melbourne, Australia; School of Computing and Information Systems, The University of Melbourne, Melbourne, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore;" Software Innovation Institute, Australian National University, Canberra, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""29 May 2020"",""2020"",""31"",""11"",""2524"",""2541"",""The current standalone deep learning framework tends to result in overfitting and low utility. This problem can be addressed by either a centralized framework that deploys a central server to train a global model on the joint data from all parties, or a distributed framework that leverages a parameter server to aggregate local model updates. Server-based solutions are prone to the problem of a single-point-of-failure. In this respect, collaborative learning frameworks, such as federated learning (FL), are more robust. Existing federated learning frameworks overlook an important aspect of participation: fairness. All parties are given the same final model without regard to their contributions. To address these issues, we propose a decentralized Fair and Privacy-Preserving Deep Learning (FPPDL) framework to incorporate fairness into federated deep learning models. In particular, we design a local credibility mutual evaluation mechanism to guarantee fairness, and a three-layer onion-style encryption scheme to guarantee both accuracy and privacy. Different from existing FL paradigm, under FPPDL, each participant receives a different version of the FL model with performance commensurate with his contributions. Experiments on benchmark datasets demonstrate that FPPDL balances fairness, privacy and accuracy. It enables federated learning ecosystems to detect and isolate low-contribution parties, thereby promoting responsible participation."",""1558-2183"","""",""10.1109/TPDS.2020.2996273"",""IBM PhD Fellowship"; ANU Translational Fellowship; Nanyang Assistant Professorship; NTU-WeBank JRI(grant numbers:NWJ-2019-007);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9098045"",""Federated learning";privacy-preserving;deep learning;fairness;"encryption"",""Machine learning";Biological system modeling;Data models;Collaboration;Servers;Privacy;"Computational modeling"","""",""104"","""",""44"",""IEEE"",""21 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Towards Higher Performance and Robust Compilation for CGRA Modulo Scheduling,""Z. Zhao"; W. Sheng; Q. Wang; W. Yin; P. Ye; J. Li;" Z. Mao"",""Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China"; Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Nvidia, China; Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China;" Department of Micro/Nano Electronics, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 May 2020"",""2020"",""31"",""9"",""2201"",""2219"",""Coarse-Grained Reconfigurable Architectures (CGRA) is a promising solution for accelerating computation intensive tasks due to its good trade-off in energy efficiency and flexibility. One of the challenging research topic is how to effectively deploy loops onto CGRAs within acceptable compilation time. Modulo scheduling (MS) has shown to be efficient on deploying loops onto CGRAs. Existing CGRA MS algorithms still suffer from the challenge of mapping loop with higher performance under acceptable compilation time, especially mapping large and irregular loops onto CGRAs with limited computational and routing resources. This is mainly due to the under utilization of the available buffer resources on CGRA, unawareness of critical mapping constraints and time consuming method of solving temporal and spatial mapping. This article focus on improving the performance and compilation robustness of the modulo scheduling mapping algorithm for CGRAs. We decomposes the CGRA MS problem into the temporal and spatial mapping problem and reorganize the processes inside these two problems. For the temporal mapping problem, we provide a comprehensive and systematic mapping flow that includes a powerful buffer allocation algorithm, and efficient interconnection & computational constraints solving algorithms. For the spatial mapping problem, we develop a fast and stable spatial mapping algorithm with backtracking and reordering mechanism. Our MS mapping algorithm is able to map loops onto CGRA with higher performance and faster compilation time. Experiment results show that given the same compilation time budget, our mapping algorithm generates higher compilation success rate. Among the successfully compiled loops, our approach can improve 5.4 to 14.2 percent performance and takes x24 to x1099 less compilation time in average comparing with state-of-the-art CGRA mapping algorithms."",""1558-2183"","""",""10.1109/TPDS.2020.2989149"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9075353"",""CGRA";modulo scheduling;temporal mapping;"spatial mapping"",""Routing";Registers;Signal processing algorithms;System-on-chip;Robustness;Processor scheduling;"Computer architecture"","""",""20"","""",""75"",""IEEE"",""21 Apr 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Towards Power Efficient High Performance Packet I/O,""X. Li"; W. Cheng; T. Zhang; F. Ren;" B. Yang"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Xi'an Research Institute of Hi-Tech, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""4"",""981"",""996"",""Recently, high performance packet I/O frameworks continue to flourish for their ability to process packets from high-speed links. To achieve high throughput and low latency, high performance packet I/O frameworks usually employ busy polling. As busy polling will burn all CPU cycles even if there's no packet to process, these frameworks are quite power inefficient. However, exploiting power management techniques such as DVFS and LPI in the frameworks is challenging, because neither the OS nor the frameworks can provide information (e.g., actual CPU utilization, available idle period, or the target frequency) required by these techniques. In this article, we establish a model that can formulate the packet processing flow of high performance packet I/O to help and address the above challenges. From the model, we can deduce the information needed for power management techniques, and gain the insights to balance the power and latency. After suggesting to use pause instruction to reduce CPU power within short idle period, we propose two approaches to conduct power conservation for high performance packet I/O: one with the aid of traffic information and the other without. Experiments with Intel DPDK show that both approaches can achieve significant power reduction with little latency increase."",""1558-2183"","""",""10.1109/TPDS.2019.2957746"",""National Basic Research Program of China (973 Program)(grant numbers:2018YFB1700103)"; National Natural Science Foundation of China(grant numbers:61872208);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8924615"",""High performance packet I/O";power consumption;latency;busy polling;DVFS;"pause instruction"",""Power system management";Power demand;Kernel;Batch production systems;Analytical models;Data centers;"Load modeling"","""",""4"","""",""35"",""IEEE"",""5 Dec 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Towards Unaligned Writes Optimization in Cloud Storage With High-Performance SSDs,""J. Shu"; F. Li; S. Li;" Y. Lu"",""Tsinghua University, Beijing, China"; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China;" Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Jul 2020"",""2020"",""31"",""12"",""2923"",""2937"",""NVMe SSDs provide extremely high performance and have been widely deployed in distributed object storage systems in data centers. However, we observe that there are still severe performance degradation and write amplification under the unaligned writes scenario with high-performance SSDs. In this article, we identify that the RMW sequence which is used to handle the unaligned writes incurs severe overhead in the data path. Besides, unaligned writes incur additional metadata management overhead in the block map table. To address these problems, we propose an object-based device system named NVStore to optimize the unaligned writes in cloud storage with NVMe SSDs. NVStore provides a Flexible Cache Management to reduce the RMW operations while supporting lazy page sync and ensuring data consistency. To optimize the metadata management, NVStore proposes a KV Affinity Metadata Management which co-designs the block map and key-value store to provides a flattened and decoupled metadata management. Evaluations show that NVStore provides at most 6.11× bandwidth of BlueStore in the cluster. Besides, NVStore can reduce at most 94.7 percent of the write traffic from metadata under unaligned writes compared to BlueStore and achieves smaller data write traffic which is about 50 percent of BlueStore and 65.7 percent of FileStore."",""1558-2183"","""",""10.1109/TPDS.2020.3006655"",""National Key Research & Development Program of China(grant numbers:2018YFB1003301)"; National Natural Science Foundation of China(grant numbers:61772300,61832011); Research and Development Plan in Key Field of Guangdong Province(grant numbers:2018B010109002); SenseTime Research Fund for Young Scholars;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9132670"",""Distributed system";object storage;unaligned writes;"solid state drives"",""Metadata";Bandwidth;Indexes;Performance evaluation;Cloud computing;Nonvolatile memory;"Optimization"","""",""2"","""",""74"",""IEEE"",""2 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Towards Usable Cloud Storage Auditing,""F. Chen"; F. Meng; T. Xiang; H. Dai; J. Li;" J. Qin"",""College of Computer Science and Engineering, Shenzhen University, Shenzhen, China"; College of Computer Science and Engineering, Shenzhen University, Shenzhen, China; College of Computer Science, Chongqing University, Chongqing, China; School of Computer Science and Technology, Nanjing University of Posts and Telecommunications, Nanjing, China; College of Computer Science and Engineering, Shenzhen University, Shenzhen, China;" Center for Smart Health, School of Nursing, The Hong Kong Polytechnic University, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Jun 2020"",""2020"",""31"",""11"",""2605"",""2617"",""Cloud storage security has gained considerable research efforts with the wide adoption of cloud computing. As a security mechanism, researchers have been investigating cloud storage auditing schemes that enable a user to verify whether the cloud keeps the user's outsourced data undamaged. However, existing schemes have usability issues in compatibility with existing real world cloud storage applications, error-tolerance, and efficiency. To mitigate this usability gap, this article proposes a new general cloud storage auditing scheme that is more usable. The proposed scheme uses the idea of integrating linear error correcting codes and linear homomorphic authentication schemes together. This integration uses only one additional block to achieve error tolerance and authentication simultaneously. To demonstrate the power of the general construction, we also propose one detailed scheme based on the proposed general construction using the Reed Solomon code and the universal hash based MAC authentication scheme, both of which are implemented over the computation-efficient Galois field $\mathrm {GF}{(2^8)}$ GF (28). We also show that the proposed scheme is secure under the standard definition. Moreover, we implemented and open-sourced the proposed scheme. Experimental results show that the proposed scheme is orders of magnitude more efficient than the state-of-the-art scheme."",""1558-2183"","""",""10.1109/TPDS.2020.2998462"",""National Natural Science Foundation of China(grant numbers:61872243,61872197,61672118,61672358,U1713212)"; Guangdong Basic and Applied Basic Research Foundation(grant numbers:2020A151501489); Science and Technology Plan Projects of Shenzhen(grant numbers:JCYJ20180305124126741); Jiangsu Key Laboratory of Big Data Security & Intelligent Processing NJUPT; Hong Kong Polytechnic University(grant numbers:YBZE);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9103614"",""Cloud storage";integrity checking;usability;homomorphic authentication;"error correction code"",""Cloud computing";Authentication;Error correction codes;Usability;Indexes;"Encoding"","""",""16"","""",""45"",""IEEE"",""29 May 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Traffic-Aware Erasure-Coded Archival Schemes for In-Memory Stores,""B. Xu"; J. Huang; X. Qin;" Q. Cao"",""Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Huazhong University of Science and Technology, Wuhan, China"; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Software Engineering, Shelby Center for Engineering Technology, Samuel Ginn College of Engineering, Auburn University, Auburn, USA;" Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Jul 2020"",""2020"",""31"",""12"",""2938"",""2953"",""Redundancy schemes are introduced to in-memory stores to provide fault tolerance. To achieve good trade-off between access performance and memory efficiency, it is appropriate to adopt replication and erasure coding to keep popular and unpopular data, respectively. Within such a hybrid-redundancy in-memory store, an issue of redundancy transition from replication to erasure coding (a.k.a., erasure-coded archival) should be addressed for unpopular in-memory datasets, since caching workloads exhibit long-tail distributions and most in-memory data are unpopular. If data replicas are distributed across nodes in randomly-selected racks, then subsequent data-block-replica retrieval for erasure-coded archival will create cross-rack traffic, and final parity-block relocation will cause extra cross-rack communications. In this article, we propose an encoding-oriented replica placement policy - ERP - by incorporating an interleaved declustering mechanism. We design two traffic-aware erasure-coded archival schemes -TEA-TL and TEA-SL - for ERP-powered in-memory stores by taking into account temporal locality and spatial locality, respectively. With ERP in place, both TEA-TL and TEA-SL schemes embrace the following three salient features: (i) they alleviate cross-rack traffic raised by retrieving required data-block replicas"; (ii) they improve rack-level load balancing by distributing replicas via load-aware primary-rack-selection approach; and (iii) they mitigate block-relocation operations launched to sustain rack-level and node-level fault-tolerance. We conduct quantitative performance evaluations using the YCSB benchmark. The empirical results show that both TEA-TL and TEA-SL schemes not only bring forth lower cross-rack traffic than the four candidate encoding schemes, but also exhibit superb archival-throughput and rack-level-balancing performance. In particular, within a group of comparative tests using the baseline configurations, TEA-TL and TEA-SL accelerate archival throughput by at least 36.3 and 70.8 percent, respectively;" both TEA-TL and TEA-SL schemes improve rack-level load-balancing by a factor of more than 1.45x relative to the four candidate encoding schemes."",""1558-2183"","""",""10.1109/TPDS.2020.3009092"",""National Natural Science Foundation of China(grant numbers:61821003)"; National Natural Science Foundation of China(grant numbers:61702004,61762075,61872413); NSF of Qinghai Province(grant numbers:2020-ZJ-926); National Science Foundation(grant numbers:CCF-0845257,CNS-0917137,CCF-0742187); Higher Education Discipline Innovation Project(grant numbers:B07038);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9140411"",""In-memory store";erasure encoding;replication;"encoding-oriented placement"",""Encoding";Fault tolerant systems;Redundancy;Ear;Distributed databases;"Facebook"","""",""3"","""",""38"",""IEEE"",""14 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Turbo: Dynamic and Decentralized Global Analytics via Machine Learning,""H. Wang"; D. Niu;" B. Li"",""Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada"; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, Canada;" Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Jan 2020"",""2020"",""31"",""6"",""1372"",""1386"",""Big data analytics are practiced in many fields to extract insights from massive amounts of data. With exponential growth in both the volume and variety of data, analytic queries have expanded from those executed in a single datacenter to those requiring inputs from multiple datacenters that are geographically separate or even globally distributed. Unfortunately, the software stack that supports data analytics is designed originally for a cluster environment and is not tailored to execute global analytic queries, where resources such as inter-datacenter networks may vary on the fly. Existing optimization strategies that determine the query execution plan before its execution are not able to adapt to resource variations at query runtime. In this article, we present Turbo, a lightweight and non-intrusive global analytics system that can dynamically adjust query execution plans for geo-distributed analytics in the presence of time-varying resources and network bandwidth across datacenters. Turbo uses machine learning to accurately predict the time cost of a query execution plan so that dynamic adjustments can be made to it when necessary. Turbo is non-intrusive in the sense that it does not require modifications to the existing software stack for data analytics. We have implemented a real-world prototype of Turbo, and evaluated it on a cluster of 33 instances across eight regions in the Google Cloud Platform. Our experimental results have shown that Turbo can achieve an accuracy of 95 percent for estimating time costs, and can reduce the query completion time by 41 percent."",""1558-2183"","""",""10.1109/TPDS.2020.2964667"",""Huawei Technologies"; NSERC Discovery Research Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8951093"",""Data analytics";distributed systems;"machine learning"",""Bandwidth";Optimization;Data analysis;Machine learning;Runtime;Task analysis;"Sparks"","""",""5"","""",""39"",""IEEE"",""7 Jan 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"WEED-MC: Wavelet Transform for Energy Efficient Data Gathering and Matrix Completion,""V. K. Singh"; B. Nathani;" M. Kumar"",""Department of Information Technology, Indian Institute of Information Technology, Lucknow, India"; CISCO Systems, Bengaluru, India;" Department of Information Technology, Indian Institute of Information Technology, Allahabad, India"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jan 2020"",""2020"",""31"",""5"",""1066"",""1073"",""Compressed sensing based data gathering is in-efficient in small scale wireless sensor networks because of uncorrelated observations at the sink. Failure to exploit the low rank and suitable transform domain to explore the correlation structure of sensor data, results in significantly low recovery accuracy in such matrix completion algorithms for small scale networks. Targeting the spatio temporal correlation structure of the data, a novel data gathering and matrix completion scheme, which exploits the low rank property and compactness of sensor data which exists in wavelet transform domain, is proposed in this work. The compactness of the sensor data in wavelet transform domain is used for recovering the missing entries of the matrix. Experiments and simulations, for single-node and multi-node scenarios, prove the efficacy of the proposed approach over existing schemes significantly in terms of recovery accuracy even at extremely low sampling rate."",""1558-2183"","""",""10.1109/TPDS.2019.2954902"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8908738"",""Compressed sensing";correlation;data gathering;discreet wavelet transform;energy efficiency;low rank;sparsity;"wireless sensor networks"",""Wireless sensor networks";Wavelet transforms;Distributed databases;Correlation;Information technology;"Sensors"","""",""2"","""",""17"",""IEEE"",""21 Nov 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Wiera: Policy-Driven Multi-Tiered Geo-Distributed Cloud Storage System,""K. Oh"; N. Qin; A. Chandra;" J. Weissman"",""Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, USA"; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, USA; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, USA;" Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jan 2020"",""2020"",""31"",""2"",""294"",""305"",""Multi-tiered geo-distributed cloud storage systems must tame complexity at many levels: uniform APIs for storage access, supporting flexible storage policies that meet a wide array of application metrics, determining an optimal data placement, handling uncertain network dynamics and access dynamism, and operating across many levels of heterogeneity both within and across data-centers (DCs). In this paper, we present an integrated solution called Wiera. Wiera enables the specification of data management policies both within a local DC and across DCs. Such policies enable the user to optimize for cost, performance, reliability, durability, and consistency, and to express their tradeoffs. In addition, Wiera determines an optimal data placement for the user to meet their desired tradeoffs easily in such an environment. A key aspect of Wiera is first-class support for dynamism due to network, workload, and access patterns changes. As far as we know, Wiera is the first geo-distributed cloud storage system which handles dynamism actively at run-time. Wiera allows unmodified applications to reap the benefits of flexible data/storage policies by externalizing the policy specification. We show how Wiera enables a rich specification of dynamic policies using a concise notation and describe the design and implementation of the system. We have implemented a Wiera prototype on multiple cloud environments, AWS and Azure, that illustrates potential benefits from managing dynamics and in using multiple cloud storage tiers both within and across DCs."",""1558-2183"","""",""10.1109/TPDS.2019.2935727"",""NSF(grant numbers:CSR-1162405)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805417"",""Data locality";multi-DCs;multi-tiered storage;wide area storage;"in memory storage"",""Cloud computing";Data models;Complexity theory;Data centers;Pricing;Fault tolerance;"Fault tolerant systems"","""",""10"","""",""38"",""IEEE"",""19 Aug 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"WPaxos: Wide Area Network Flexible Consensus,""A. Ailijiang"; A. Charapko; M. Demirbas;" T. Kosar"",""Microsoft, Redmond, USA"; Microsoft, Redmond, USA; State University of New York at Buffalo, Buffalo, USA;" State University of New York at Buffalo, Buffalo, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Dec 2019"",""2020"",""31"",""1"",""211"",""223"",""WPaxos is a multileader Paxos protocol that provides low-latency and high-throughput consensus across wide-area network (WAN) deployments. WPaxos uses multileaders, and partitions the object-space among these multileaders. Unlike statically partitioned multiple Paxos deployments, WPaxos is able to adapt to the changing access locality through object stealing. Multiple concurrent leaders coinciding in different zones steal ownership of objects from each other using phase-1 of Paxos, and then use phase-2 to commit update-requests on these objects locally until they are stolen by other leaders. To achieve fast phase-2 commits, WPaxos adopts the flexible quorums idea in a novel manner, and appoints phase-2 acceptors to be close to their respective leaders. We implemented WPaxos and evaluated it over WAN deployments across 5 AWS regions. The dynamic partitioning of the objectspace and emphasis on zone-local commits allow WPaxos to significantly outperform both partitioned Paxos deployments and leaderless Paxos approaches."",""1558-2183"","""",""10.1109/TPDS.2019.2929793"",""National Science Foundation(grant numbers:CNS-1527629,XPS-1533870)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8765834"",""Distributed systems";distributed applications;wide-area networks;"fault-tolerance"",""Protocols";Wide area networks;Safety;Throughput;Indexes;Fault tolerance;"Fault tolerant systems"","""",""12"","""",""34"",""IEEE"",""18 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;