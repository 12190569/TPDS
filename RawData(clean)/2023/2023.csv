"A Comprehensive Performance Model of Sparse Matrix-Vector Multiplication to Guide Kernel Optimization,""T. Xia"; G. Fu; C. Li; Z. Luo; L. Zhang; R. Chen; W. Zhao; N. Zheng;" P. Ren"",""College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China"; College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China; College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China; College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China; College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China; College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China; College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China; College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China;" College of Artificial Intelligence, Xi’an Jiaotong University, Xi’an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Dec 2022"",""2023"",""34"",""2"",""519"",""534"",""Sparse Matrix-Vector Multiplication (SpMV) is important in scientific and industrial applications and remains a well-known challenge for modern CPUs due to high sparsity and irregularity. Many researchers try to improve SpMV performance by designing dedicated data formats and computation patterns. However, out-of-order superscalar CPUs have complex micro-architectures where exist complicated interactions and restrictions among software and hardware factors. It is hard to systematically study the effectiveness of optimization methods on the overall performance, as its benefits may be undermined by other factors. In this paper, we thoroughly study the execution of SpMV on modern CPUs and propose a comprehensive performance model to reveal the critical factors and their relationships. Specifically, we first study the coding characteristics of SpMV kernels to identify key factors worthy of attention. Then we model the execution of SpMV as two overlapped parts: CPU pipeline and memory latency. Both are carefully modeled with related hardware and software factors. We also model SIMD performance with the usage of specific SIMD instructions and vector registers. Experiments show that our model matches the actual execution of real-world processors. Guided by the model, we propose SpV8, a novel SpMV kernel that optimizes critical factors to improve computation efficiency and memory bandwidth. Experiments on Intel/AMD x86 and ARM AArch64 platforms show that SpV8 outperforms several state-of-the-art approaches with large margins, achieving average $3.4\times$3.4× over Intel Math Kernel Library and $1.4\times$1.4× over the best existing approach. Such results indicate that the proposed model is capable of valuable guidance for efficient SpMV optimizations."",""1558-2183"","""",""10.1109/TPDS.2022.3225230"",""National Key Research and Development Program of China(grant numbers:2021YFB0300900)"; National Natural Science Foundation of China(grant numbers:62088102); Key Research and Development Projects of Shaanxi Province(grant numbers:2022ZDLGY01-08);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964419"",""Optimization";performance model;sparse-vector matrix multiplication;"SIMD"",""Pipelines";Kernel;Sparse matrices;Computational modeling;Memory management;Software;"Optimization"","""",""1"","""",""29"",""IEEE"",""28 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"A Distributed Network-Based Runtime Verification of Full Regular Temporal Properties,""B. Yu"; C. Tian; X. Lu; N. Zhang;" Z. Duan"",""School of Computer Science and Technology, Xidian University, Xi’an, Shaanxi, China"; School of Computer Science and Technology, Xidian University, Xi’an, Shaanxi, China; School of Computer Science and Technology, Xidian University, Xi’an, Shaanxi, China; School of Computer Science and Technology, Xidian University, Xi’an, Shaanxi, China;" School of Computer Science and Technology, Xidian University, Xi’an, Shaanxi, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2022"",""2023"",""34"",""1"",""76"",""91"",""As a lightweight method, runtime verification aims to check whether one program execution satisfies a desired property. For online runtime verification, the approach efficiency and property expressiveness are two key points restricting its wide application. In this paper, we propose a distributed network-based parallel runtime verification approach to verifying full regular temporal properties for a suitable subset of C (named by Xd-C) programs in an online manner. With this approach, an Xd-C program is translated into an equivalent Modeling, Simulation and Verification Language (MSVL) program, and a desired property is specified as a Propositional Projection Temporal Logic (PPTL) formula";" during the program execution, segments of the generated state sequence are verified in parallel by distributed multi-core machines. Experimental results show that, our approach has a speedup of 2.5X-5.0X over the state-of-art runtime verification approaches and supports full regular temporal properties, meaning that our approach can not only take full advantage of computing and storage resources in a distributed network, but also support more expressive properties."",""1558-2183"","""",""10.1109/TPDS.2022.3215854"",""National Key Research and Development Program of China(grant numbers:2018AAA0103202)"; National Natural Science Foundation of China(grant numbers:62202361,62192734,61732013,62172322); Natural Science Basic Research Program of Shaanxi(grant numbers:2021JQ-208);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925650"",""C program";full regular property;runtime verification;parallelism;"distributed network"",""Runtime";Task analysis;Semantics;Monitoring;Arithmetic;Testing;"Syntactics"","""","""","""",""39"",""IEEE"",""20 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"A Framework for Mapping DRL Algorithms With Prioritized Replay Buffer Onto Heterogeneous Platforms,""C. Zhang"; Y. Meng;" V. Prasanna"",""Department of Computer Science, University of Southern California, Los Angeles, CA, USA"; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA;" Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 May 2023"",""2023"",""34"",""6"",""1816"",""1829"",""Despite the recent success of Deep Reinforcement Learning (DRL) in self-driving cars, robotics and surveillance, training DRL agents takes tremendous amount of time and computation resources. In this article, we aim to accelerate DRL with Prioritized Replay Buffer due to its state-of-the-art performance on various benchmarks. The computation primitives of DRL with Prioritized Replay Buffer include environment emulation, neural network inference, sampling from Prioritized Replay Buffer, updating Prioritized Replay Buffer and neural network training. The speed of running these primitives varies for various DRL algorithms such as Deep Q Network and Deep Deterministic Policy Gradient. This makes a fixed mapping of DRL algorithms inefficient. In this work, we propose a framework for mapping DRL algorithms onto heterogeneous platforms consisting of a multi-core CPU, a GPU and a FPGA. First, we develop specific accelerators for each primitive on CPU, FPGA and GPU. Second, we relax the data dependency between priority update and sampling performed in the Prioritized Replay Buffer. By doing so, the latency caused by data transfer between GPU, FPGA and CPU can be completely hidden without sacrificing the rewards achieved by agents learned using the target DRL algorithms. Finally, given a DRL algorithm specification, our design space exploration automatically chooses the optimal mapping of various primitives based on an analytical performance model. On widely used benchmark environments, our experimental results demonstrate up to 997.3× improvement in training throughput compared with baseline mappings on the same heterogeneous platform. Compared with the state-of-the-art distributed Reinforcement Learning framework RLlib, we achieve 1.06$\times \sim$×∼ 1005× improvement in training throughput."",""1558-2183"","""",""10.1109/TPDS.2023.3264823"",""NSF(grant numbers:CNS-2009057,OAC-2209563)"; AMD Xilinx;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093111"",""Deep reinforcement learning";design space exploration;FPGA;GPU;heterogeneous platform;"prioritized replay buffer"",""Training";Field programmable gate arrays;Graphics processing units;Neural networks;Data collection;Throughput;"Memory"","""","""","""",""33"",""IEEE"",""5 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"A General Approach to Generate Test Packets With Network Configurations,""Y. Li"; H. Zhang; J. Wang; Z. Wang; X. Yin; X. Shi;" J. Wu"",""Institute for Network Sciences, Cyberspace, Tsinghua University, Beijing, China"; Institute for Network Sciences, Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences, Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences, Cyberspace, Tsinghua University, Beijing, China; Department of Computer Science, Technology, Tsinghua University, Beijing, China; Institute for Network Sciences, Cyberspace, Tsinghua University, Beijing, China;" Department of Computer Science, Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Mar 2023"",""2023"",""34"",""4"",""1362"",""1375"",""The correctness and reliability of modern networks are often the greatest concerns. A myriad network events like software update, device crash and resource exhaustion, inevitably lead to liveness errors on data plane. This paper focuses on fault detection of the network data plane using test packets. Existing test packet generation techniques are limited in two aspects: i) it is difficult to collect the input data plane snapshot through SNMP or terminals ii) it may rise false negatives due to inconsistent snapshot. In this paper, we propose a new framework, SWIFT, that automatically generates test packets with network configurations. SWIFT minimizes the number of test packets by allowing a packet to go through multiple links or interfaces. For network updates, SWIFT updates test packets in an incremental way to revalidate the network. We evaluate its performance using hundreds of benchmark network configurations. The results show that it takes few seconds to generate test packets to exercise all links and interfaces, and updates the test packets in few seconds for configuration changes. We also deployed a SWIFT prototype in a university network, and successfully detected many network outages."",""1558-2183"","""",""10.1109/TPDS.2023.3241433"",""National Science Foundation(grant numbers:62102020)"; National Key Research and Development Program of China(grant numbers:2020YFE0200500); National Science Foundation(grant numbers:62002009);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10035502"",""Network Configurations";Abstract Representation;Test Packets;Network Reliability;"Network Management"",""Routing protocols";Routing;Probes;Software;Hardware;Costs;"Computer bugs"","""","""","""",""40"",""IEEE"",""2 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"A Low-Memory Community Detection Algorithm With Hybrid Sparse Structure and Structural Information for Large-Scale Networks,""W. Zhu"; Y. Sun; R. Fang;" B. Xu"",""Key Laboratory of Big Data & Artificial Intelligence in Transportation, Ministry of Education, and School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China"; Key Laboratory of Big Data & Artificial Intelligence in Transportation, Ministry of Education, and School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Key Laboratory of Big Data & Artificial Intelligence in Transportation, Ministry of Education, and School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China;" Key Laboratory of Big Data & Artificial Intelligence in Transportation, Ministry of Education, and School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Aug 2023"",""2023"",""34"",""10"",""2671"",""2683"",""Community detection plays an essential role in the domains of social, bioinformatics, and e-commerce. The innovative structural information theory (SInfo, introduced by Li et al.) has achieved excellent performance for network analysis. Nevertheless, similar to traditional network algorithms, the SInfo algorithm will exhaust all system memory resources when processing large-scale networks based on adjacency data structures. Moreover, due to the irregular and sequential characteristics, the SInfo algorithm is challenging to parallelize. In this article, we propose a hybrid sparse data structure and design a low-memory community detection algorithm based on structural information theory (HSSInfo), which shrinks memory requirements and achieves high parallelism. Specifically, we first propose a general quantization method to quantify the information change in community transformation, with which community inner and outer connection complexity can be semantically interpreted. Second, we design a general hybrid sparse structure to store network data among CPU and GPU, which can reduce memory resource consumption for community detection on large-scale networks. Finally, we develop an HSSInfo algorithm based on the quantization method and hybrid sparse structure, in which parallelism intersection and community fusion algorithms are employed to improve parallel scalability. We execute the comparison experiments for HSSInfo and eleven baseline algorithms on nine real-world datasets. Empirically, HSSInfo can achieve orders of magnitude speedups and extend structural information theory to large-scale datasets of billions of edges. Meanwhile, compared with various community detection algorithms, HSSInfo can achieve higher or similar accuracy with up to 19x memory shrinkage ratio."",""1558-2183"","""",""10.1109/TPDS.2023.3277885"",""National Key R&D Program of China(grant numbers:2021ZD0113002)"; National Natural Science Foundation of China(grant numbers:62072292,61572005,61771058); Fundamental Research Funds for the Central Universities(grant numbers:2020YJS032);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129834"",""Community detection";hybrid sparse structure;structural information theory;social networks;"parallel algorithms"",""Graphics processing units";Information theory;Schedules;Quantization (signal);Memory management;Image edge detection;"Data mining"","""","""","""",""39"",""IEEE"",""19 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"A Memory-Constraint-Aware List Scheduling Algorithm for Memory-Constraint Heterogeneous Muti-Processor System,""Y. Yao"; Y. Song; Y. Huang; W. Ni;" D. Zhang"",""School of Microelectronics, Hefei University of Technology, Hefei, Anhui, China"; School of Microelectronics, Hefei University of Technology, Hefei, Anhui, China; School of Microelectronics, Hefei University of Technology, Hefei, Anhui, China; School of Microelectronics, Hefei University of Technology, Hefei, Anhui, China;" School of Microelectronics, Hefei University of Technology, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Mar 2023"",""2023"",""34"",""4"",""1082"",""1099"",""An effective scheduling algorithm is vital for the execution efficiency of applications on Heterogeneous Muti-Processor System (HMPS), especially Memory-Constraint Heterogeneous Muti-Processor System (MCHMPS). Stringent local and external memory constraints have significant impact on the execution performance of applications executed on MCHMPS, predictability is also a critical factor for task scheduling on MCHMPS. Therefore, a novel list scheduling algorithm termed Memory-constraint-aware Improved Predict Priority and Optimistic Processor Selection Scheduling (MIPPOSS), essentially a heuristic search optimization algorithm, is proposed in this paper. In MIPPOSS, a predictive approach is applied for task prioritization and processor selection, and a novel memory-constraint-aware approach is employed in the processor selection phase. MIPPOSS has polynomial complexity and produces better results for application scheduling on target architecture. Randomly generated DAGs and 3 real-world applications experiments, including Cybershake, LIGO, and Montage, show that MIPPOSS outperforms the other five competing algorithms by a large margin."",""1558-2183"","""",""10.1109/TPDS.2022.3229373"",""University Synergy Innovation Program of Anhui Province(grant numbers:GXXT-2019-030)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9987695"",""Heterogeneous systems";memory constraints;parallel computing;"list scheduling"",""Task analysis";Scheduling;Memory management;Costs;Computational modeling;Scheduling algorithms;"Time complexity"","""","""","""",""33"",""IEEE"",""15 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"A Multi-GPU Aggregation-Based AMG Preconditioner for Iterative Linear Solvers,""M. Bernaschi"; A. Celestini; F. Vella;" P. D'Ambra"",""Institute for Applied Computing (IAC) - CNR, Rome, Italy"; Institute for Applied Computing (IAC) - CNR, Rome, Italy; University of Trento, Trento, Italy;" Institute for Applied Computing (IAC) - CNR, Naples, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Jun 2023"",""2023"",""34"",""8"",""2365"",""2376"",""We present and release in open source format a sparse linear solver which efficiently exploits heterogeneous parallel computers. The solver can be easily integrated into scientific applications that need to solve large and sparse linear systems on modern parallel computers made of hybrid nodes hosting Nvidia Graphics Processing Unit (GPU) accelerators. The work extends previous efforts of some of the authors in the exploitation of a single GPU accelerator and proposes an implementation, based on the hybrid MPI-CUDA software environment, of a Krylov-type linear solver relying on an efficient Algebraic MultiGrid (AMG) preconditioner already available in the BootCMatchG library. Our design for the hybrid implementation has been driven by the best practices for minimizing data communication overhead when multiple GPUs are employed, yet preserving the efficiency of the GPU kernels. Strong and weak scalability results of the new version of the library on well-known benchmark test cases are discussed. Comparisons with the Nvidia AmgX solution show a speedup, in the solve phase, up to 2.0x."",""1558-2183"","""",""10.1109/TPDS.2023.3287238"",""EC"; Towards EXtreme scale Technologies and Accelerators for euROhpc hw/Sw Supercomputing Applications for exascale; Horizon 2020 Program for Research and Innovation(grant numbers:956831); Indam GNCS Project 2022(grant numbers:CUP_E55F22000270001); European Commission; NextGenerationEU;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155151"",""GPU accelerators";heterogeneous computing;iterative sparse linear solvers;parallel numerical algorithms;"scalability"",""Sparse matrices";Graphics processing units;Scalability;Linear systems;Iterative methods;Convergence;"Linear accelerators"","""","""","""",""36"",""IEEE"",""19 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"A Novel Compute-Efficient Tridiagonal Solver for Many-Core Architectures,""K. Liu";" W. Xue"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China";" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""195"",""206"",""The tridiagonal solver is an important kernel and is widely supported in mainstream numerical libraries. While parallel algorithms have been studied for many-core architectures, the performance of current algorithms and implementations is still hindered by input size sensitivity and cross-platform portability. In this paper, we propose a novel algorithm WM-pGE for the batched solution of diagonally dominant tridiagonal systems. The algorithm balances the key design objectives, including computation complexity, memory complexity, parallelism, and input size sensitivity, better than existing algorithms. Moreover, an elegant formulation is presented to show the implementation and cross-platform optimization without loss of efficiency and generality, by extracting the platform-dependent works into only four vector operators. The results from our batched tridiagonal experiments show that the proposed algorithm outperforms the prior work PCR-pThomas by 25% and 12% on NVIDIA Tesla V100 in single and double precision, respectively. On Intel KNL, our method achieves a 10% improvement in performance over PCR-pThomas in double precision."",""1558-2183"","""",""10.1109/TPDS.2022.3214762"",""Science and Technology Project of State Grid(grant numbers:5100-202055002A-0-0-00)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919397"",""Parallel numerical algorithm";Tridiagonal solver;batched Tridiagonal solver;"many-core architectures"",""Computer architecture";Layout;Sensitivity;Parallel algorithms;Heuristic algorithms;Graphics processing units;"Computational complexity"","""",""1"","""",""38"",""IEEE"",""14 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Novel Integrated Simulation Framework for Cyber-Physical Systems Modelling,""N. Tampouratzis"; P. Mousouliotis;" I. Papaefstathiou"",""Laboratory of Computer Systems Architecture, Aristotle University of Thessaloniki, Thessaloniki, Greece"; Laboratory of Computer Systems Architecture, Aristotle University of Thessaloniki, Thessaloniki, Greece;" Laboratory of Computer Systems Architecture, Aristotle University of Thessaloniki, Thessaloniki, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Aug 2023"",""2023"",""34"",""10"",""2684"",""2698"",""The growing use of Cyber-Physical Systems (CPS) in a plethora of domains (e.g. healthcare, industry, smart homes, transportation, etc.) triggers an urgent demand for simulation frameworks that can simulate in an integrated manner all the components (i.e. CPUs, Memories, Networks, Physical Environment) of a system-under-design(SuD). By utilizing such a simulator, software design can proceed in parallel with physical development which results in the reduction of the so important time-to-market. The main problem, however, is that currently there is a shortage of such simulation frameworks"; most simulators used for modelling the digital aspects of CPS applications (i.e. full-system CPU/Mem/Peripheral simulators) lack any support of the CPS physical aspects and vice versa. The presented fully-distributed simulation framework (APOLLON) is the first known open-source, high-performance simulator that can handle holistically complex CPSs including processors, peripherals, networks and physical aspects of them. APOLLON is an extension of the COSSIM simulation framework and it integrates, in a novel and efficient way, a combined processing and network simulator with the widely-used Ptolemy physical simulator, in a transparent way. Our highly integrated approach is further augmented with Machine Learning capabilities by implementing Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) recurrent neural networks in both the Cyber and Physical domains, enabling users to develop their complex recurrent neural networks significantly fast and accurately. APOLLON has been evaluated when executing a number of benchmarks and real-world use cases;" the end results demonstrate that the presented approach has up to 99% accuracy in the reported SuD aspects."",""1558-2183"","""",""10.1109/TPDS.2023.3300081"",""Greece and the European Union"; Operational Programme “Human Resources Development; Education and Lifelong Learning; Reinforcement of Postdoctoral Researchers - 2nd Cycle(grant numbers:MIS5033021); State Scholarships Foundation (IKY) for the author Nikolaos Tampouratzis; Hellenic Foundation for Research and Innovation; First Call for H.F.R.I. Research Projects to support Faculty members and Researchers(grant numbers:2198); Panagiotis Mousouliotis and Ioannis Papaefstathiou until 29/6/2023; Operational Program Competitiveness, Entrepreneurship and Innovation; RESEARCH-CREATE-INNOVATE(grant numbers:T2EDK-03595); Ioannis Papaefstathiou;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10197534"",""Cyber-physical system simulator";distributed systems simulator;integrated simulator;"machine learning"",""Protocols";Wireless sensor networks;Synchronization;Operating systems;Mathematical models;Estimation;"Recurrent neural networks"","""","""","""",""33"",""IEEE"",""31 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"A Novel Parallel Algorithm for Sparse Tensor Matrix Chain Multiplication via TCU-Acceleration,""H. Wang"; W. Yang; R. Hu; R. Ouyang; K. Li;" K. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China;" College of Computer Science and Electronic Engineering, Hunan University, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Jul 2023"",""2023"",""34"",""8"",""2419"",""2432"",""Analysis of multi-dimensional data, especially tensor decomposition, which extracts latent information, is becoming considerably popular. Although multi-dimensional sparse data is typically processed on multi-core processors, developing highly optimized GPU-based Sparse Tensor Matrix Chain Multiplication (SpTMCM) is challenging. The purpose of this paper is to investigate a novel approach named SpTMCM and to explore the discovery of SpTMCM coupled with the emerging computing core, Tensor Core Unit (TCU). In contrast to prior work, the proposed novel approach enables a uniform storage format and optimization approach for SpTMCM. We design a hybrid tensor format based on multi-dimensional tiling that divides the tensor depending on the tile threshold to address the inefficient memory accesses caused by the irregular nonzero distribution of the sparse tensor. Further, we develop a TCU-based tensor parallel algorithm with our novel approach to increase the memory bandwidth. Compared to state-of-the-art works, our method achieves $1.16\sim 24.12\times$1.16∼24.12× speedup for SpMTTKRP and $5.07\sim 7.15\times$5.07∼7.15× speedup for SpTTMChain across NVIDIA A100 GPU on a range of real-world sparse tensors."",""1558-2183"","""",""10.1109/TPDS.2023.3288520"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101190004)"; National Key R&D Program of China(grant numbers:2021YFB0300800); Key Program of the National Natural Science Foundation of China(grant numbers:U21A20461,92055213); Research Innovation Project for Postgraduate Students of Hunan Province(grant numbers:CX20220412); National Natural Science Foundation of China(grant numbers:61872127);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10159508"",""GPU";hybrid format;parallel performance;SpMTTKRP;SpTTMChain;sparse tensor;"tensor core"",""Tensors";Graphics processing units;Parallel algorithms;Sparse matrices;Symbols;Optimization;"Indexes"","""",""1"","""",""38"",""IEEE"",""22 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"A Parallel Algorithm to Accelerate DEVS Simulations in Shared Memory Architectures,""G. G. Trabes"; G. A. Wainer;" V. Gil-Costa"",""Department of Systems and Computing Engineering, Carleton University, Ottawa, ON, Canada"; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada;" National University of San Luis and CCT CONICET, San Luis, Argentina"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Mar 2023"",""2023"",""34"",""5"",""1609"",""1620"",""We propose a new algorithm for the execution of Discrete Event System Specification (DEVS) simulations on parallel shared memory architectures. Our approach executes parallel discrete-event simulations by executing all tasks in the PDEVS simulation protocol in parallel. The algorithm works by distributing the computations among different cores on shared memory architectures. To show the benefits of our algorithm, we present the results of a set of experiments using a synthetic benchmark and a real-world scenario using two independent computer architectures. The results obtained show how our algorithm accelerates simulations up to eight times, improving previous approaches. In addition, we show that our approach scales when we increase the number of CPU-cores used."",""1558-2183"","""",""10.1109/TPDS.2023.3256083"",""NSERC Canada";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10068737"",""Discrete-event";parallel algorithms;simulation;"shared memory"",""Computational modeling";Protocols;Synchronization;Cadmium;Task analysis;Approximation algorithms;"Software algorithms"","""","""","""",""34"",""IEEE"",""14 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Parallel Framework for Constraint-Based Bayesian Network Learning via Markov Blanket Discovery,""A. Srivastava"; S. P. Chockalingam;" S. Aluru"",""School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA"; Institute for Data Engineering and Science, Georgia Institute of Technology, Atlanta, GA, USA;" School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 May 2023"",""2023"",""34"",""6"",""1699"",""1715"",""Bayesian networks (BNs) are a widely used graphical model in machine learning. As learning the structure of BNs is NP-hard, high-performance computing methods are necessary for constructing large-scale networks. In this article, we present a parallel framework to scale BN structure learning algorithms to tens of thousands of variables. Our framework is applicable to learning algorithms that rely on the discovery of Markov blankets (MBs) as an intermediate step. We demonstrate the applicability of our framework by parallelizing three different algorithms: Grow-Shrink (GS), Incremental Association MB (IAMB), and Interleaved IAMB (Inter-IAMB). Our implementations are available as part of an open-source software called ramBLe, and are able to construct BNs from real data sets with tens of thousands of variables and thousands of observations in less than a minute on 1024 cores, with a speedup of up to 845X and 82.5% efficiency. Furthermore, we demonstrate using simulated data sets that our proposed parallel framework can scale to BNs of even higher dimensionality. Our implementations were selected for the reproducibility challenge component of the 2021 student cluster competition (SCC’21), which tasked undergraduate teams from around the world with reproducing the results that we obtained using the implementations. We discuss details of the challenge and the results of the experiments conducted by the top teams in the competition. The results of these experiments indicate that our key results are reproducible, despite the use of completely different data sets and experiment infrastructure, and validate the scalability of our implementations."",""1558-2183"","""",""10.1109/TPDS.2023.3244135"",""National Science Foundation(grant numbers:OAC-1828187,OAC-1854828)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10043026"",""Bayesian networks";constraint-based learning;parallel machine learning;gene networks;"reproducibility"",""Random variables";Scalability;Probability distribution;Markov processes;Machine learning algorithms;Bayes methods;"Software algorithms"","""",""2"","""",""63"",""IEEE"",""13 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Point Cloud Video Recognition Acceleration Framework Based on Tempo-Spatial Information,""Z. Song"; W. Liu; T. Yang; F. Liu; N. Jing;" X. Liang"",""School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China"; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Huawei Technology Company, Ltd., Shenzhen, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China;" School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Oct 2023"",""2023"",""34"",""12"",""3224"",""3237"",""In point cloud video recognition (PVR) tasks, deep neural networks (DNNs) have been widely adopted to enhance accuracy. However, real-time processing is hindered due to the increasing volume of points and frames that require processes. Point clouds represent 3D-shaped discrete objects using a multitude of points. Consequently, these points often exhibit an uneven distribution in the view space, resulting in strong spatial similarity within each point cloud frame. Taking advantage of this observation, this article introduces PRADA, a Point Cloud Recognition Acceleration algorithm via Dynamic Approximation. PRADA approximates and eliminates the similar local pairs’ computations and recovers their results by copying dissimilar local pairs’ features for speedup with negligible accuracy loss. Furthermore, considering the slow changes in point cloud frames that lead to the high temporal similarity among points across multiple frames, we design PointV, a Point Cloud Video Recognition Acceleration algorithm, to minimize unnecessary computations of similar points in the temporal domain. Moreover, we propose the PRADA and PointV architectures to accelerate the PRADA and PointV algorithms. These two architectures can be integrated to gain higher performance improvement. Our experiments on a wide variety of datasets show that PRADA averagely achieves about $7\times$7× speedup over 1080TI GPU. In addition, the experimental results show that the PointV architecture and the integrated architecture can respectively achieve $11.7\times$11.7× and $13.9\times$13.9× performance improvement with acceptable accuracy compared to the 1080TI GPU."",""1558-2183"","""",""10.1109/TPDS.2023.3323263"",""National Natural Science Foundation of China(grant numbers:62202288,92264108,61972242)"; National Key R&D Program of China(grant numbers:2018YFA0701500);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10285363"",""Point cloud video recognition task";deep neural network;acceleration;tempo-spatial information;"accelerator"",""Point cloud compression";Task analysis;Heuristic algorithms;Streaming media;Computer architecture;Approximation algorithms;"Artificial neural networks"","""","""","""",""26"",""IEEE"",""13 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"A Proactive On-Demand Content Placement Strategy in Edge Intelligent Gateways,""H. Sun"; Y. Chen; K. Sha; S. Huang; X. Wang;" W. Shi"",""School of Computer and Science, Anhui University, Hefei, China"; School of Computer and Science, Anhui University, Hefei, China; Department of Computer Science, University of Houston - Clear Lake (UHCL), Houston, TX, USA; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China;" Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""26 May 2023"",""2023"",""34"",""7"",""2072"",""2090"",""Bandwidth-intensive applications transmit large-scale video data in the network. It causes backhaul bottlenecks and affects user experience. Deploying edge cache on an access point (AP) is a popular method to bring content files closer to end-users, but it faces significant challenges, especially in efficiently predicting and satisfying different users’ future content requests with limited cache capacity. In this article, we propose an intelligent gateway assisted edge cache deployment strategy (GACD), which jointly considers traffic usage patterns in multiple APs and the impact of new content on the cache performance. In GACD, The cache content placement problem is formulated as a many-to-one bidirectional matching problem with a dynamic quota allocation, aiming to improve cache resource utilization and minimize the average delivery latency. To address this problem, we design a heterogeneous information networks based prediction algorithm to predict end-users’ potential preference of new content files. Then, we adapt the seasonal autoregressive integrated moving average model for traffic usage prediction, and propose a many-to-one matching algorithm to achieve dynamic matching quota adjustment and efficient cache content placement. We conduct extensive real-world trace-based experiments to validate the performance of GACD. Compared with six alternative cache strategies, GACD improves the hit rate by 23.9% on average, reduces the average content delivery delay by 19.02%, and increases the accuracy by 31.02% on average."",""1558-2183"","""",""10.1109/TPDS.2023.3249797"",""National Natural Science Foundation of China(grant numbers:61702001)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054497"",""Cache content placement";edge cache;intelligent gateway;"popularity prediction"",""Logic gates";Wireless fidelity;Delays;Prediction algorithms;Costs;5G mobile communication;"Backhaul networks"","""","""","""",""88"",""IEEE"",""28 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"A Space-Efficient Fair Cache Scheme Based on Machine Learning for NVMe SSDs,""W. Liu"; J. Cui; T. Li; J. Liu;" L. T. Yang"",""School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Nov 2022"",""2023"",""34"",""1"",""383"",""399"",""Non-volatile memory express (NVMe) solid-state drives (SSDs) have been widely adopted in multi-tenant cloud computing environments or multi-programming systems. The on-board DRAM cache inside NVMe SSDs can efficiently reduce the disk accesses and extend the lifetime of SSDs. Current SSD cache management research either improves cache hit ratio while ignoring fairness, or improves fairness while sacrificing overall performance. In this paper, we present MLCache, a space-efficient shared cache management scheme for NVMe SSDs. By learning the impact of reuse distance on cache allocation, a workload-generic neural network model is built. At runtime, MLCache continuously monitors the reuse distance distribution for the neural network module to obtain space-efficient allocation decisions. MLCache also proposes an efficient parallel writing back strategy based on hit ratio and response time, to improve fairness. Experimental results show MLCache improves the write hit ratio when compared to baseline, and MLCache strongly safeguards the fairness of SSDs with parallel write-back and maintains a low level of degradation."",""1558-2183"","""",""10.1109/TPDS.2022.3221410"",""National Natural Science Foundation of China(grant numbers:61902136,61932010)"; National Key Research and Development Program of China(grant numbers:2019YFB1705903); Hubei Provincial Natural Science Foundation of China(grant numbers:2019CFB119); Fundamental Research Funds for the Central Universities(grant numbers:2019kfyXJJS090);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946404"",""NVMe SSDs";cache partition;fairness;reuse distance;machine learning;"DFTL"",""Partitioning algorithms";Nonvolatile memory;Time factors;Throughput;Writing;Resource management;"Random access memory"","""",""2"","""",""54"",""IEEE"",""11 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"A Survey of Memory-Centric Energy Efficient Computer Architecture,""C. Zhang"; H. Sun; S. Li; Y. Wang; H. Chen;" H. Liu"",""College of Computer, National University of Defense Technology, Changsha, Hunan, China"; College of Computer, National University of Defense Technology, Changsha, Hunan, China; College of Computer, National University of Defense Technology, Changsha, Hunan, China; College of Computer, National University of Defense Technology, Changsha, Hunan, China; College of Computer, National University of Defense Technology, Changsha, Hunan, China;" College of Computer, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Aug 2023"",""2023"",""34"",""10"",""2657"",""2670"",""Energy efficient architecture is essential to improve both the performance and power consumption of a computer system. However, modern computers suffer from the severe “memory wall” problem due to the significant performance gap between the processor technology and the memory technology. Thus, the computer architecture community is evolving from compute-centric to memory-centric designs to reduce the data movement overhead. This paper presents a comprehensive survey of the main challenges and recent advances in memory-centric energy efficient computer architecture. We summarize two research directions: improving the memory technology and processing closer to memory. The former focuses on optimizing the conventional memory technology and exploiting emerging non-volatile memory (NVM) technology. The latter talks about currently popular processing in memory (PIM) technology, including near-memory processing (NMP) and in-memory processing (IMP). Moreover, some other topics like hardware for machine learning (ML), ML for hardware, security, privacy, and reliability are gaining increasing attention and should be considered seriously in the design phase of a computer system. The community is facing various challenges and opportunities simultaneously, requiring researchers to have a more comprehensive understanding of this field which is also the goal of this paper."",""1558-2183"","""",""10.1109/TPDS.2023.3297595"",""National Natural Science Foundation of China(grant numbers:62172430,62272477)"; Key Laboratory Fund, National University of Defense Technology(grant numbers:WDZC20215250109);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190135"",""Energy efficiency";computer architecture;non-volatile memory (NVM);processing in memory (PIM);"memory wall"",""Computer architecture";Random access memory;Nonvolatile memory;Three-dimensional displays;Surveys;Process control;"Bandwidth"","""","""","""",""169"",""IEEE"",""21 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"A Survey on Auto-Parallelism of Large-Scale Deep Learning Training,""P. Liang"; Y. Tang; X. Zhang; Y. Bai; T. Su; Z. Lai; L. Qiao;" D. Li"",""State Key Laboratory of Parallel and Distributed Processing, National University of Defense Technology, Changsha, China"; State Key Laboratory of Parallel and Distributed Processing, National University of Defense Technology, Changsha, China; Huawei Technologies Co. Ltd., Shenzhen, China; Huawei Technologies Co. Ltd., Shenzhen, China; Huawei Technologies Co. Ltd., Shenzhen, China; State Key Laboratory of Parallel and Distributed Processing, National University of Defense Technology, Changsha, China; State Key Laboratory of Parallel and Distributed Processing, National University of Defense Technology, Changsha, China;" State Key Laboratory of Parallel and Distributed Processing, National University of Defense Technology, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Jul 2023"",""2023"",""34"",""8"",""2377"",""2390"",""Deep learning (DL) has gained great success in recent years, leading to state-of-the-art performance in research community and industrial fields like computer vision and natural language processing. One of the reasons for this success is the huge amount parameters adopted in DL models. However, it is impractical to train a moderately large model with a large number of parameters on a typical single device. Thus, It is necessary to train DL models in clusters with distributed training algorithms. However, traditional distributed training algorithms are usually sub-optimal and highly customized, which owns the drawbacks to train large-scale DL models in varying computing clusters. To handle the above problem, researchers propose auto-parallelism, which is promising to train large-scale DL models efficiently and practically in various computing clusters. In this survey, we perform a broad and thorough investigation on challenges, basis, and strategy searching methods of auto-parallelism in DL training. First, we abstract basic parallelism schemes with their communication cost and memory consumption in DL training. Further, we analyze and compare a series of current auto-parallelism works and investigate strategies and searching methods which are commonly used in practice. At last, we discuss several trends in auto-parallelism which are promising in further research."",""1558-2183"","""",""10.1109/TPDS.2023.3281931"",""National Natural Science Foundation of China(grant numbers:62025208)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10163912"",""Auto-parallelism";large-scale deep learning model;training technique;"parallel and distributed training"",""Parallel processing";Training;Computational modeling;Costs;Surveys;Topology;"Deep learning"","""",""1"","""",""102"",""IEEE"",""26 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"A Utility-Based Distributed Pattern Mining Algorithm With Reduced Shuffle Overhead,""S. Kumar";" K. K. Mohbey"",""Department of Computer Science, Central University of Rajasthan, Ajmer, Rajasthan, India";" Department of Computer Science, Central University of Rajasthan, Ajmer, Rajasthan, India"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Nov 2022"",""2023"",""34"",""1"",""416"",""428"",""With the arrival of the current digital era and the advancement of information transmission technologies, there has been an unprecedented rise in data. Efficient extraction of useful information from the volumes of data has garnered growing interest from academics and the industry. Data mining research focuses on finding utility patterns in large datasets. But the inherent complications like frequent scans, creation of substantial candidate sets, etc. plague the mining process for large datasets. Distributive architecture-based approaches also prove inefficacious due to high communication overhead over iterations. High communication cost over data exchange both locally and remotely further aggravates the situation. We propose a Communication Cost Effective Utility-based Pattern Mining (CEUPM) algorithm based on the Spark framework to address this issue. Spark accelerates iterative scanning by storing scanned datasets in a memory abstraction called resilient distributed datasets (RDD). RDD operations need a redistribution of data among cluster nodes during processing. To minimize the communication cost incurred during the shuffle process, we adopt a search space division strategy based on data parallelism for a fair and effective task allocation across cluster nodes. Communication overhead is incurred during this redistribution or shuffle process while minimizing costs. Experimental results in four real datasets demonstrate that CEUPM considerably reduces shuffling overhead and outperforms other existing methods in terms of memory usage, communication cost, execution time, and scalability."",""1558-2183"","""",""10.1109/TPDS.2022.3221210"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944926"",""Communication cost";distributed computing;high utility pattern mining;scalability;"spark"",""Data mining";Costs;Sparks;Distributed databases;Task analysis;Cluster computing;"Scalability"","""",""1"","""",""61"",""IEEE"",""10 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerated Information Dissemination for Replica Selection in Distributed Key-Value Store Systems,""W. Jiang"; Y. Qiu; Y. Chen; F. Ji; H. Xie; X. Zhou; J. Chen; J. Huang; J. Wang;" Y. Li"",""Central South University, Changsha, China"; Central South University, Changsha, China; Central South University, Changsha, China; Central South University, Changsha, China; Central South University, Changsha, China; Central South University, Changsha, China; Central South University, Changsha, China; Central South University, Changsha, China; Central South University, Changsha, China;" Central South University, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Nov 2022"",""2023"",""34"",""1"",""358"",""371"",""In distributed key-value stores, multiple replica servers are always available for each key-value access operation when the eventual consistency model is employed. Accordingly, the completion times of the key-value access operations generated by an end-user request at different servers may be of great difference, especially when the replica servers are heterogeneous and have time-varying performance. Accordingly, the replica selection algorithm is crucial to cut the response time of end-user requests. The main challenge of making replica selection for each light-weighted key-value access operation is to timely know the status of replica serves. Recently, the adaptive replica selection algorithm C3 suggests guiding the replica selection with the piggybacked information of replica server in the returned “value”. Although C3 has good performance, the poor timeliness of feedback information makes a large performance gap between C3 and the ideal replica selection algorithm. To narrow this gap, the Accelerated Information Dissemination (AID) mechanism is proposed in this paper. Specifically, AID removes the bottleneck of information dissemination at the “slow” servers by letting both “client” and “replica server” store the records about the status of replica servers and both “key” and “value” piggyback multiple records. AID is implemented in Cassandra and evaluated by experiments and large scale simulations. The results show AID can significantly improve the timeliness of feedback information, especially when the number of nodes is large. Accordingly, AID helps C3 to greatly reduce the latency."",""1558-2183"","""",""10.1109/TPDS.2022.3221642"",""National Key Research and Development Project of China(grant numbers:2018YFB1702502)"; National Natural Science Foundation of China(grant numbers:61972421); National Natural Science Foundation of Hunan Province(grant numbers:2022JJ20078); Innovation-Driven Project of Central South University(grant numbers:2020CX033); Fundamental Research Funds for the Central Universities(grant numbers:2022ZZTS0643);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9946409"",""Distributed key-Value stores";information dissemination;replica selection;"tail latency"",""Servers";Tail;Time factors;Maintenance engineering;Heuristic algorithms;Load modeling;"Adaptation models"","""",""1"","""",""31"",""IEEE"",""11 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Accelerating Content-Defined Chunking for Data Deduplication Based on Speculative Jump,""X. Jin"; H. Liu; C. Ye; X. Liao; H. Jin;" Y. Zhang"",""National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, and Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Jul 2023"",""2023"",""34"",""9"",""2568"",""2579"",""In data deduplication systems, chunking has a significant impact on the deduplication ratio and throughput. Existing Content-Defined Chunking (CDC) approaches exploit a sliding window to calculate rolling hashes of the input data stream byte-by-byte, and then determine chunk cut-points if the rolling hash satisfies a given cut-condition. Since previous CDC approaches are extremely costly, it often significantly degrades the throughput of data deduplication systems. In this paper, we argue that calculating and checking the rolling hashes byte-by-byte is unnecessary. To reduce the CPU overhead of CDC, we propose a jump-based chunking (JC) approach. The key idea is to introduce a jump-condition, and the sliding window can jump over a specific length of the input data stream if the rolling hashes satisfy the jump-condition. Moreover, we also explore the impact of the cut-condition and the jump-condition on the chunk size. Our theoretic studies demonstrate the effectiveness and efficiency of JC, without compromising the deduplication ratio. Experimental results show that JC improves the throughput of chunking by about 2× on average compared with the state-of-the-art CDC approaches while still guaranteeing high deduplication ratio."",""1558-2183"","""",""10.1109/TPDS.2023.3290770"",""National Key Research and Development Program of China(grant numbers:2022YFB4500303)"; National Natural Science Foundation of China(grant numbers:62072198,61825202,61929103); Huawei Technologies Co., Ltd(grant numbers:YBN2021035018A7);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10168293"",""Boundary-Shift";content-defined chunking (CDC);data deduplication;"rolling hash"",""Fingerprint recognition";Throughput;Gears;Power capacitors;Redundancy;Costs;"Upper bound"","""","""","""",""32"",""CCBY"",""29 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Accelerating Convolutional Neural Networks by Exploiting the Sparsity of Output Activation,""Z. Fan"; W. Li; Z. Wang; T. Liu; H. Wu; Y. Liu; M. Wu; X. Wu; X. Ye; D. Fan; N. Sun;" X. An"",""State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China;" State Key Laboratory of Processor, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Nov 2023"",""2023"",""34"",""12"",""3253"",""3265"",""Deep Convolutional Neural Networks (CNNs) are the most widely used family of machine learning methods that have had a transformative effect on a wide range of applications. Previous studies have made great breakthroughs in accelerating CNNs, but they only target on the input sparsity of activation and weight, thus do not eliminate the unnecessary computations due to the fact that more zeros in the output results are not directly caused by the zero-valued positions of the input data. In this paper, we take advantage of the output activation sparsity to reduce the execution time and energy consumption of CNNs. First, we propose an effective prediction method that leverages the output activation sparsity. Our method first predicts the output activation polarity of convolutional layers based on the singular value decomposition (SVD) approach. Then, it uses the predicted negative value to skip invalid computations. Second, an effective accelerator is designed to take advantage of sparsity to achieve CNN inference acceleration. Each PE is equipped with a prediction unit and a non-zero value detection unit to remove invalid computation blocks. And an instruction bypass technique is proposed which further exploits the sparsity of the weights. The efficient dataflow graph mapping approach and pipeline execution ensure high computational resource utilization. Experiments show that our approach achieves up to 1.63× speedup and 55.30% energy reduction compared with dense networks with a slight loss of accuracy. Compared with Eyeriss, our accelerator achieves on average 1.31 × performance improvement and 54% energy reduction. Our accelerator also achieves a similar performance to SnaPEA, but with a better energy efficiency."",""1558-2183"","""",""10.1109/TPDS.2023.3324934"",""National Key R&D Program of China(grant numbers:2022YFB4501404)"; Beijing Nova Program(grant numbers:2022079); CAS Project for Young Scientists in Basic Research(grant numbers:YSBR- 029); CAS Project for Youth Innovation Promotion Association and Open Research Projects of Zhejiang Lab(grant numbers:2022PB0AB01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286398"",""Accelerator";output activation;prediction;"sparse convolutional neural network"",""Convolutional neural networks";Computational efficiency;Convolution;Fans;Matrix decomposition;Computational modeling;"Sun"","""","""","""",""40"",""IEEE"",""16 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Accelerating Data Delivery of Latency-Sensitive Applications in Container Overlay Network,""H. Liu"; W. Li; Y. Pang; R. Pei; Y. Hu; Y. Liu; L. Suo;" K. Li"",""Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China"; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China;" Tianjin Key Laboratory of Advanced Networking (TANK), College of Intelligence and Computing, Tianjin University, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Oct 2023"",""2023"",""34"",""12"",""3046"",""3058"",""Container overlay network, though being widely adopted to enable communication between containers on different hosts, is a key downside for latency-sensitive applications. The state-of-the-art solution seeks to shorten the data path in packet processing by replacing overlay connection file descriptors with host namespace ones. While promising, it must block each overlay connection until the relevant host connection is set up, thus heavily influencing the request latency. In this paper, we present ShuntFlow, a systematic data delivery framework that seamlessly integrates the host and overlay networks to reduce the application's request-response latency. ShuntFlow first lets all connections flow in the overlay network directly. Then, it adopts a simple-yet-effective syscall-threshold-based mechanism to pick appropriate connections and switches their data delivery to the host network in a blocking-free way using a multi-threading technique. As such, unnecessary connection switches are prevented";" yet, the pre-setup phase dilemma is eliminated. We have implemented a ShuntFlow prototype based on Linux and Docker and evaluated it extensively on a 40 Gbps testbed. The results show that ShuntFlow achieves 13%/72% and 19%/69% reductions, in average/tail request-response latency of a web server and an in-memory key-value store, respectively, while incurring less CPU overhead, compared to Slim."",""1558-2183"","""",""10.1109/TPDS.2023.3300745"",""National Natural Science Foundation of China(grant numbers:62202325)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198904"",""Container overlay network";distributed containerized application;locking-free multi-thread;"latency acceleration"",""Overlay networks";Containers;Web servers;IP networks;Switches;Hardware;"Virtualization"","""","""","""",""49"",""IEEE"",""1 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Accelerating Deep Learning Inference via Model Parallelism and Partial Computation Offloading,""H. Zhou"; M. Li; N. Wang; G. Min;" J. Wu"",""Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering, College of Computer and Information Technology, China Three Gorges University, Yichang, Hubei, China"; Hubei Key Laboratory of Intelligent Vision Based Monitoring for Hydroelectric Engineering, College of Computer and Information Technology, China Three Gorges University, Yichang, Hubei, China; Department of Computer Science, Rowan University, Glassboro, NJ, USA; Department of Computer Science, University of Exeter, Exeter, U.K.;" Center for Networked Computing, Temple University, Philadelphia, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""475"",""488"",""With the rapid development of Internet-of-Things (IoT) and the explosive advance of deep learning, there is an urgent need to enable deep learning inference on IoT devices in Mobile Edge Computing (MEC). To address the computation limitation of IoT devices in processing complex Deep Neural Networks (DNNs), computation offloading is proposed as a promising approach. Recently, partial computation offloading is developed to dynamically adjust task assignment strategy in different channel conditions for better performance. In this paper, we take advantage of intrinsic DNN computation characteristics and propose a novel Fused-Layer-based (FL-based) DNN model parallelism method to accelerate inference. The key idea is that a DNN layer can be converted to several smaller layers in order to increase partial computation offloading flexibility, and thus further create the better computation offloading solution. However, there is a trade-off between computation offloading flexibility as well as model parallelism overhead. Then, we investigate the optimal DNN model parallelism and the corresponding scheduling and offloading strategies in partial computation offloading. In particular, we propose a Particle Swarm Optimization with Minimizing Waiting (PSOMW) method, which explores and updates the FL strategy, path scheduling strategy, and path offloading strategy to reduce time complexity and avoid invalid solutions. Finally, we validate the effectiveness of the proposed method in commonly used DNNs. The results show that the proposed method can reduce the DNN inference time by an average of 12.75 times compared to the legacy No FL (NFL) algorithm, and is very close to the optimal solution achieved by the Brute Force (BF) algorithm with the difference of less than 0.04%."",""1558-2183"","""",""10.1109/TPDS.2022.3222509"",""National Natural Science Foundation of China(grant numbers:62172255,61872221)"; EU Horizon 2020 Research and Innovation Programme; Marie Sklodowska-Curie(grant numbers:101008297);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9953583"",""Mobile edge computing";fused-layer;DNN inference;partial offloading;"model parallelism"",""Computational modeling";Parallel processing;Adaptation models;Task analysis;Processor scheduling;Kernel;"Internet of Things"","""",""9"","""",""50"",""IEEE"",""16 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Accelerating Distributed DNN Training via Transport Layer Scheduling,""Q. Duan"; C. Peng; Z. Wang; Y. Xu; S. Liu; J. Wu;" J. C. S. Lui"",""Department of Electronic Engineering, Fudan University, Shanghai, China"; Department of Electronic Engineering, Fudan University, Shanghai, China; Department of Electronic Engineering, Fudan University, Shanghai, China; Department of Electronic Engineering, Fudan University, Shanghai, China; 2012 Lab, Huawei Technologies Co. Ltd., Shenzhen, Guangdong, China; School of Computer Science and Technology, Fudan University, Shanghai, China;" Computer Science & Engineering, The Chinese University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Apr 2023"",""2023"",""34"",""5"",""1650"",""1666"",""Communication scheduling is crucial to accelerate the training of large deep learning models, in which the transmission order of layer-wise deep neural network (DNN) tensors is determined for a better computation-communication overlap. Prior approaches adopt user-level tensor partitioning to enhance the priority scheduling with finer granularity. However, a startup time slot inserted before every tensor partition will neutralize this scheduling gain. Tuning hyper-parameters for tensor partitioning is difficult, especially when the network bandwidth is shared or time-varying in multi-tenant clusters. In this article, we propose Mercury, a simple transport layer scheduler that moves the priority scheduling to the transport layer at the packet granularity. The packets with the highest priority in the Mercury buffer will be transmitted first. Mercury achieves the near-optimal overlapping between communication and computation. It also leverages the immediate aggregation at the transport layer to enable the full overlapping of gradient push and pull. We implement Mercury in MXNet and conduct comprehensive experiments on five popular DNN models in various environments. Mercury can well adapt to dynamic communication and computation resources. Experiments show that Mercury accelerates the training by up to 130% compared to the classical PS architecture, and 104% compared to state-of-the-art tensor partitioning methods."",""1558-2183"","""",""10.1109/TPDS.2023.3250462"",""National Natural Science Foundation of China(grant numbers:62072117)"; Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010166003); Natural Science Foundation of Shanghai(grant numbers:22ZR1407000);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10056319"",""Computation-communication overlap";distri- buted machine learning;parameter server;"transport layer scheduling"",""Tensors";Training;Processor scheduling;Computer architecture;Computational modeling;Bandwidth;"Servers"","""","""","""",""57"",""IEEE"",""28 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Accelerating Distributed GNN Training by Codes,""Y. Wang"; T. Guan; D. Niu; Q. Zou; H. Zheng; C. . -J. R. Shi;" Y. Xie"",""Institute of Brain-inspired Circuits and Systems, Fudan University, Shanghai, China"; DAMO Academy, Alibaba Group, Hangzhou, China; DAMO Academy, Alibaba Group, Hangzhou, China; Graph Computing Center, Zhejiang Lab, Hangzhou, China; DAMO Academy, Alibaba Group, Hangzhou, China; Department of Electronic and Computer Engineering, University of Washington, Seattle, WA, USA;" DAMO Academy, Alibaba Group, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2023"",""2023"",""34"",""9"",""2598"",""2614"",""Emerging graph neural network (GNN) has recently attracted much attention and has been used extensively in many real-world applications thanks to its powerful expression ability of unstructured data. The real-world graph datasets are very large-scale, which can contain up to billions of nodes and tens of billions of edges. It usually requires distributed system to train GNN on such huge datasets. As a result, the data communication overheads between machines become the bottleneck of GNN computation. Our profiling results show that getting attributes from remote machines during sampling phase in GNN occupies $> $>75% of the time of the training process. To address this issue, in this article, we propose Coded Neighbor Sampling (CNS) framework, which introduces codes technique to reduce the communication overheads of GNN. In the proposed CNS framework, the codes technique is coupled with GNN sampling method to exploit the data excess among different machines caused by unstructured nature of graph data. An analytical performance model is built for the proposed CNS framework, whose results are corroborated by the simulation and validate the benefit of the proposed CNS framework over both conventional GNN training method and conventional codes technique. Performance metrics, such as communication overheads, runtime, and throughput, of the proposed CNS framework are evaluated on a distributed GNN training simulation system implemented on MPI4py platform. The results show that, on average, the proposed CNS framework can save communication overhead by 40.6%, 35.5%, and 16.5%, reduce the runtime by 12.1%, 17.0%, and 10.0%, and improve the throughput by 16.2%, 24.4%, and 11.2%, respectively, when training GNN models with Cora, PubMed, and Large Taobao."",""1558-2183"","""",""10.1109/TPDS.2023.3295184"",""Science and Technology Commission of Shanghai Municipality(grant numbers:2018SHZDZX01)"; National Key Research and Development Program of China(grant numbers:2021ZD0202200,2021ZD0202202); Science and Technology Commission of Shanghai Municipality(grant numbers:17JC1420200); Damo Academy through Damo Academy Research Intern Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10183372"",""GNN";neighbor sampling;codes;distributed machine learning;MPI;"communication"",""Training";Graph neural networks;Codes;Computational modeling;Runtime;Distributed databases;"Artificial neural networks"","""","""","""",""55"",""IEEE"",""13 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Accelerating k-Shape Time Series Clustering Algorithm Using GPU,""X. Wang"; R. Song; J. Xiao; T. Li;" X. Li"",""Department of Computer Science and Technology, China University of Petroleum (East China), Qingdao, Shangdong, China"; Department of Computer Science and Technology, China University of Petroleum (East China), Qingdao, Shangdong, China; High Performance Computer Research Center, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, China University of Petroleum (East China), Qingdao, Shangdong, China;" State Key Lab of Processors, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Aug 2023"",""2023"",""34"",""10"",""2718"",""2734"",""In the data space, time-series analysis has emerged in many fields, including biology, healthcare, and numerous large-scale scientific facilities like astronomy, climate science, particle physics, and genomics. Clustering is one of the most critical methods in time-series analysis. So far, the state-of-art time series clustering algorithm k-Shape has been widely used not only because of its high accuracy, but also because of its relatively low computation cost. However, due to the high heterogeneity of time series data, it can not be simply regarded as a high-dimensional vector. Two time series often need some alignment method in similarity comparison. The alignment between sequences is often a time-consuming process. For example, when using dynamic time warping as a sequence alignment algorithm and if the length of time series is greater than 1,000, a single iteration in the clustering process may take hundreds to tens of thousands of seconds, while the entire clustering cycle often requires dozens of iterations. In this article, we propose a set of novel parallel strategies suitable for GPU's computation model, called Times-C, which is an abbreviation for Time Series Clustering. We define three stages in the analysis process: aggregation, centroid, and class assignment. Times-C includes efficient parallel algorithms and corresponding implementations for these three stages. Overall, the experimental results show that the Times-C algorithm exhibits a performance improvement of one to two orders of magnitude compared to the multi-core CPU version of k-Shape. Furthermore, compared to the GPU version of the k-Shape algorithm, the Times-C algorithm achieves a maximum acceleration of up to 345 times."",""1558-2183"","""",""10.1109/TPDS.2023.3298148"",""National Key R&D Program of China(grant numbers:2022YFB4500403,2021YFA1000103,2021YFA1000100)"; National Natural Science Foundation of China(grant numbers:61972416,62202454,62272479,62202498); Taishan Scholarship(grant numbers:tsqn201812029); Foundation of Science and Technology Development of Jinan(grant numbers:201907116); Natural Science Foundation of Shandong Province(grant numbers:ZR2021QF023); Fundamental Research Funds for the Central Universities(grant numbers:21CX06018A); Spanish project(grant numbers:PID2019-106960GB-I00); Juan de la Cierva(grant numbers:IJC2018-038539-I); China National Postdoctoral Program for Innovative Talents(grant numbers:BX2021320); Chinese Academy of Engineering Strategic Research and Consulting Program(grant numbers:2023-XBZD-16);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10192069"",""Data space";time series analysis;time series clustering;GPU architecture;"k-shape algorithm"",""Time series analysis";Clustering algorithms;Time measurement;Graphics processing units;Heuristic algorithms;Meteorology;"Eigenvalues and eigenfunctions"","""",""1"","""",""48"",""IEEE"",""24 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"AdaptChain: Adaptive Scaling Blockchain With Transaction Deduplication,""J. Xu"; Q. Xie; S. Peng; C. Wang;" X. Jia"",""Department of Computer Science, City University of Hong Kong, Hong Kong"; City University of Hong Kong, Dongguan, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computer Science, City University of Hong Kong, Hong Kong;" Department of Computer Science, City University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1909"",""1922"",""Although existing schemes improve blockchain throughput by allowing concurrent blocks to be appended to the blockchain, little attention has been devoted to adjusting blockchain throughput dynamically and deduplicating transactions between concurrent blocks. In this article, we propose AdaptChain, an adaptive scaling blockchain with transaction deduplication. When the transaction demand of users in the network is high, the blockchain expands to meet the demand";" when the transaction demand is low, the blockchain shrinks to save communication and storage costs. Our transaction deduplication mechanism ensures that no duplicate transactions are added to the blockchain, thereby improving bandwidth utilization and achieving higher effective throughput. Besides, we randomly split the mining power of the system to achieve mining power load balancing and resist attacks. We formally analyze the blockchain security and implement the proposed prototype on Amazon EC2. Experimental results show that AdaptChain achieves dynamic and higher effective blockchain throughput."",""1558-2183"","""",""10.1109/TPDS.2023.3267071"",""Hong Kong RGC(grant numbers:CityU 11213920,11218521,R1012-21,C2004-21G)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102556"",""Distributed system";blockchain scalability;load balancing;transaction deduplication;"consensus protocol"",""Blockchains";Throughput;Peer-to-peer computing;Consensus protocol;Load management;Safety;"Bandwidth"","""","""","""",""27"",""IEEE"",""14 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Adaptive Data Placement in Multi-Cloud Storage: A Non-Stationary Combinatorial Bandit Approach,""L. Li"; J. Shen; B. Wu; Y. Zhou; X. Wang;" K. Li"",""School of Computer Science, Fudan University, Shanghai, China"; Informatization Office, Fudan University, Shanghai, China; Informatization Office, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China;" Department of Computer Science, State University of New York, New Paltz, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Sep 2023"",""2023"",""34"",""11"",""2843"",""2859"",""Multi-cloud storage is recently a viable approach to solve the vendor lock-in, reliability, and security issues in cloud storage systems. As a key concern, data placement influences the cost and performance of storage services. Yet, in practice it remains challenging to address the huge solution space. Previous studies typically focus on constructing efficient data placement schemes based on the predicted pattern of workloads or assuming fully a-priori known network conditions. They cannot be easily applied in multi-cloud storage scenarios, which typically involve dynamic network conditions and time-varying workloads. To this end, we formulate the data placement optimization in a combinatorial multi-arm bandit (CMAB) perspective and solve it by learning placement strategy online. In contrast to a stationary setting where reward distributions are unknown but identical over time, we consider a realistic multi-cloud environment with non-stationary conditions, i.e., reward distributions change over time. To swiftly accommodate this, we propose an adaptive window combinatorial upper confidence bound based data placement (AW-CUCB-DP) scheme to reduce latency and cost. In AW-CUCB-DP, a simple and efficient change detector, i.e., Page-Hinkley test with forgetting mechanism (FM-PHT), is employed to enable variable-size sliding windows to handle both gradual and abrupt variations in network conditions or workloads. We establish that AW-CUCB-DP is asymptotically optimal in the non-stationary multi-cloud environment. Trace-driven experiments further verify that our scheme outperforms alternatives, especially in highly dynamic environments."",""1558-2183"","""",""10.1109/TPDS.2023.3306150"",""Science and Technology Commission of Shanghai Municipality(grant numbers:22dz1204900)"; National Natural Science Foundation of China(grant numbers:61971145); Natural Science Foundation of Shanghai(grant numbers:22ZR1407900);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10223440"",""Combinatorial multi-armed bandit";data placement;erasure codes;multi-cloud storage;"non-stationary"",""Costs";Cloud computing;Heuristic algorithms;Uncertainty;Data centers;Security;"Operating systems"","""","""","""",""46"",""IEEE"",""17 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Adaptive Fragment-Based Parallel State Recovery for Stream Processing Systems,""H. Xu"; P. Liu; S. T. Ahmed; D. Da Silva;" L. Hu"",""Department of Computer Engineering and Computer Science, California State University, Long Beach, CA, USA"; School of Computing and Information Science, Florida International University, Miami, FL, USA; Texas A&M University, College Station, TX, USA; Texas A&M University, College Station, TX, USA;" Department of Computer Science and Engineering, University of California, Santa Cruz, Santa Cruz, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Jul 2023"",""2023"",""34"",""8"",""2464"",""2478"",""Today, large-scale cloud organizations are deploying datacenters and “edge” clusters globally to provide low-latency access to services. Running stream applications across geo-distributed sites are emerging as a daily requirement. However, existing efforts have dominantly centered around stateless stream processing, leaving another urgent trend-stateful stream processing-much less explored. A driving need is to store and update states during processing, and most importantly, successfully recover large distributed states when faults and failures happen. Existing studies exhibit major limitations including: (1) they mostly inherit MapReduce's “single master/many workers” architecture, where the central master can easily become ascalability bottleneck"; (2) they offer state recovery mainly through three approaches: replication recovery, checkpointing recovery, and DStream-based lineage recovery, which are either slow, resource-expensive or failing to handle multiple failures;" and (3) they are not adaptive to heterogeneous hardware settings. We present A-FP4S, a novel adaptive fragments-based parallel state recovery mechanism for stream processing systems. A-FP4S organizes stream operators into a distributed hash table based peer-to-peer overlay and divides each node's local state into many fragments. These fragments are periodically stored in node's multiple neighbors, ensuring different sets of available fragments can reconstruct failed states in parallel. This mechanism is extremely scalable to the lost state, significantly reduces failure recovery time, and can tolerate multiple node failures. A-FP4S is adaptive to heterogeneous hardware settings by automatic parameter tuning over phases. Compared to Apache Storm, A-FP4S achieves 31.8% to 50.5% reduction in recovery latency. Large-scale experiments using real-world datasets demonstrate A-FP4S's attractive scalability and adaptivity properties."",""1558-2183"","""",""10.1109/TPDS.2023.3251997"",""National Science Foundation(grant numbers:NSF-SPX-1919126,NSF-SPX-1919181,NSF-CAREER-1943071,NSF-CCF-1934904,NSF-OAC-2212256)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10077765"",""Distributed hash tables";state recovery;"stream processing"",""Checkpointing";Task analysis;Storms;Peer-to-peer computing;Hardware;Codes;"Adaptive systems"","""","""","""",""82"",""IEEE"",""21 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"AESM2 Attribute-Based Encrypted Search for Multi-Owner and Multi-User Distributed Systems,""M. Wang"; Y. Miao; Y. Guo; H. Huang; C. Wang;" X. Jia"",""School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China"; School of Cyber Engineering, Xidian University, Xi’an, China; School of Artificial Intelligence, Beijing Normal University, Beijing, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Department of Computer Science, City University of Hong Kong, Hong Kong;" School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2022"",""2023"",""34"",""1"",""92"",""107"",""With the rapid development of cloud computing, it is popular for data owners to outsource massive data to the cloud server for data sharing. To protect the privacy of sensitive data, many searchable encryption schemes are proposed. However, most of the existing studies focus on the single-owner model. In practice, users need to query data from distributed owners one by one, which inevitably brings great communication and computation overheads. Moreover, it lacks a secure scheme that realizes the access control requirements of individual owners. In this article, we propose AESM$^{2}$2, a new attribute-based encrypted search with ownership enhancement scheme for multi-owner and multi-user distributed systems. Our design enables users to search data from authorized owners with only one trapdoor. Owners can enforce owner level permission on users and encrypt their data individually with fine-grained attribute level permission. For practical consideration, we further devise an efficient revocation method of the owner level permission for users, where ciphertexts do not need to be updated. We formally define and prove the security of our design. Moreover, we implement a system prototype and analyze the performance from theoretical and experimental aspects. The evaluation results demonstrate that our scheme is effective and efficient."",""1558-2183"","""",""10.1109/TPDS.2022.3216320"",""National Natural Science Foundation of China(grant numbers:61732022,62072361,62102035)"; Shenzhen Science and Technology Program(grant numbers:GXWD20220817124827001,JCYJ20210324132406016); Fundamental Research Funds for the Central Universities(grant numbers:JB211505); Research Grants Council of Hong Kong(grant numbers:CityU 11213920,11217819,11217620,11218521,11202419,N_CityU139/21,RFS2122-1S04,C2004-21GF,R1012-21,R6021-20F); InnoHK initiative; Government of the HKSAR; Laboratory for AI-Powered Financial Technologies;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9926180"",""Searchable encryption";attribute-based encryption;access control;"multi-owner distributed systems"",""Cryptography";Encryption;Distributed databases;Data models;Computational modeling;Servers;"Cloud computing"","""",""5"","""",""48"",""IEEE"",""21 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"AGCM-3DLF: Accelerating Atmospheric General Circulation Model via 3-D Parallelization and Leap-Format,""H. Cao"; L. Yuan; H. Zhang; Y. Zhang; B. Wu; K. Li; S. Li; M. Zhang; P. Lu;" J. Xiao"",""Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Earth System Numerical Simulation Science Center, Institute of Atmospheric Physics, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Sensetime Research, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer Science, Beijing University of Posts and Telecommunications, Beijing, China; Institute of Atmospheric Physics, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China;" Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Jan 2023"",""2023"",""34"",""3"",""766"",""780"",""The atmospheric general circulation model (AGCM) has been an important research tool in the study of climate change for decades. As the demand for high-resolution simulation is becoming urgent, the scalability and simulation efficiency is faced with great challenges, especially for the latitude-longitude mesh-based models. In this paper, we propose a highly scalable 3-D atmospheric general circulation model based on leap-format, namely AGCM-3DLF. First, it utilizes a 3-D decomposition method allowing for parallelism release in all three physical dimensions. Then the leap-format difference computation scheme is adopted to maintain computational stability in grid updating and avoid additional filtering at the high latitudes. A novel shifting window communication algorithm is designed for parallelization of the unified model. Furthermore, a series of optimizations are conducted to improve the effectiveness of large-scale simulations. Experiment results in different platforms demonstrate good efficiency and scalability of the model. AGCM-3DLF scales up to the entire CAS-Xiandao1 supercomputer (196,608 CPU cores), attaining the speed of 11.1 simulation-year-per-day (SYPD) at a high resolution of 25KM. In addition, simulations conducted on the Sunway TaihuLight supercomputer exhibit a 1.06 million cores scalability with 36.1% parallel efficiency."",""1558-2183"","""",""10.1109/TPDS.2022.3231013"",""National Key R&D Program of China(grant numbers:2016YFB0200803)"; National Natural Science Foundation of China(grant numbers:61972376,62072431,62032023,62172391); National Key Scientific and Technological Infrastructure;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9996169"",""Atmospheric general circulation model";3-D decomposition;leap-format finite-difference;"heterogeneous acceleration"",""Climate change";Atmospheric modeling;Three-dimensional displays;Finite difference methods;Computational modeling;Supercomputers;Solid modeling;Power system stability;Parallel processing;"Scalability"","""","""","""",""31"",""IEEE"",""21 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"AIDTN: Towards a Real-Time AI Optimized DTN System With NVMeoF,""S. -Y. Yu"; Q. Zeng; J. Chen; Y. Chen;" J. Mambretti"",""International Center for Advanced Internet Research, Northwestern University, Evanston, IL, USA"; Zhejiang University, Hangzhou, China; International Center for Advanced Internet Research, Northwestern University, Evanston, IL, USA; Lab for Internet, Security Technology, Northwestern University, Evanston, IL, USA;" International Center for Advanced Internet Research, Northwestern University, Evanston, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1731"",""1742"",""Large-scale data transport for data-intensive sciences is a complex multidimensional challenge. The challenge includes optimizing the end-to-end Big Data movement performance in real-time, supporting direct remote data access using NVMe over Fabrics (NVMeoF) and deploying to existing research platforms. AIDTN is the first effort to provide a unique AI system designed to incorporate NVMe over Fabrics (NVMeoF) and optimize coordination among multiple components supporting large-scale, multi-domain Wide Area Network (WAN) data-intensive science. AIDTN's research objective is to integrate next-generation storage architecture using NVMeoF, specialized network design using high-performance network appliances, Data Transfer Nodes (DTNs), catalysts in driving data transport, and a unique AI system explicitly designed for high-performance data movement challenges. AIDTN is the first system that uses network and system features to predict the end-to-end performance of high-performance data movement and further extends the model with NVMe-specific features for NVMeoF remote data access. As a result, AIDTN improves data movement performance by up to 284% while minimizing packet loss compared to other heuristics approaches. It also has a prediction error rate as low as 0.16 compared to AI models with the only network (error rate = 0.29) or network and system features (error rate = 0.19)."",""1558-2183"","""",""10.1109/TPDS.2023.3260806"",""National Science Foundation(grant numbers:1450871)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10079144"",""Artificial intelligence";data transfer nodes;microservice;"NVMe over fabrics"",""Artificial intelligence";"Art"","""",""1"","""",""33"",""IEEE"",""23 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Alleviating the Impact of Abnormal Events Through Multi-Constrained VM Placement,""G. Zhao"; J. Liu; Y. Zhai; H. Xu;" H. He"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China;" School of Computer Science and Technology, Soochow University, Suzhou, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2023"",""2023"",""34"",""5"",""1508"",""1523"",""As a simple and low-cost way to obtain enough computing resources, more and more tenants migrate their tasks to the cloud. However, the frequent occurrence of abnormal events (e.g., malicious tenants and node failures) in the cloud will seriously affect the tenants’ QoS. Conventionally, the cloud vendors reduce the frequency of abnormal events by deploying auxiliary systems, which requires additional costs and increases network complexity. Considering that it is an unrealistic expectation to eliminate the occurrence of abnormal events in clouds, this paper proposes a complementary scheme to alleviate the negative impact scope when an abnormal event occurs through multi-constrained VM placement without consuming additional resources. Specifically, when deploying VMs, we limit the number of pods (or service nodes) each tenant can access and the number of tenants hosted by each pod (or service node). However, the multi-dimensional interaction among numerous system parameters and performance/resource considerations makes the problem of multi-constrained VM placement for alleviating the impact of abnormal events very challenging. To solve this problem, we formulate an integer linear programming and propose a rounding-based algorithm with a logarithmic approximation ratio. We implement our proposed algorithm on a physical testbed. The experimental and simulation results show the high efficiency of the proposed algorithm. For example, our algorithm reduces the impact scope of service node failure by 60%, the impact scope of malicious tenants by 40%, and the tenant task makespan by 25% compared with other alternatives."",""1558-2183"","""",""10.1109/TPDS.2023.3248681"",""National Natural Science Foundation of China(grant numbers:62102392)"; National Science Foundation of Jiangsu Province(grant numbers:BK20210121,Hefei Municipal Natural Science Foundation,2022013); Youth Innovation Promotion Association of CAS(grant numbers:2023481); IEEE/ACM International Symposium on Quality of Service;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10052762"",""Abnormal events";clouds;resource allocation;"VM placement"",""Cloud computing";Virtual private networks;Telecommunication traffic;Approximation algorithms;Quality of service;Task analysis;"Load management"","""","""","""",""61"",""IEEE"",""24 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"An Efficient Algorithm for Hamiltonian Path Embedding of $k$k-Ary $n$n-Cubes Under the Partitioned Edge Fault Model,""H. Zhuang"; X. -Y. Li; J. -M. Chang;" D. Wang"",""College of Computer and Data Science, Fuzhou University, Fuzhou, China"; College of Computer and Data Science, Fuzhou University, Fuzhou, China; Institute of Information and Decision Sciences, National Taipei University of Business, Taipei, Taiwan;" Department of Computer Science, Montclair State University, Montclair, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1802"",""1815"",""The $k$k-ary $n$n-cube $Q_{n}^{k}$Qnk is one of the most important interconnection networks for building network-on-chips, data center networks, and parallel computing systems owing to its desirable properties. Since edge faults grow rapidly and the path structure plays a vital role in large-scale networks for parallel computing, fault-tolerant path embedding and its related problems have attracted extensive attention in the literature. However, the existing path embedding approaches usually only focus on the theoretical proofs and produce an $n$n-related linear fault tolerance since they are based on the traditional fault model, which allows all faults to be adjacent to the same node. In this paper, we design an efficient fault-tolerant Hamiltonian path embedding algorithm for enhancing the fault-tolerant capacity of $k$k-ary $n$n-cubes. To facilitate the algorithm, we first introduce a new conditional fault model, named Partitioned Edge Fault model (PEF model). Based on this model, for the $k$k-ary $n$n-cube $Q_{n}^{k}$Qnk with $n\geq 2$n≥2 and odd $k\geq 3$k≥3, we explore the existence of a Hamiltonian path in $Q_{n}^{k}$Qnk with large-scale edge faults. Then we give an $O(N)$O(N) algorithm, named HP-PEF, to embed the Hamiltonian path into $Q_{n}^{k}$Qnk under the PEF model, where $N$N is the number of nodes in $Q_{n}^{k}$Qnk. The performance analysis of HP-PEF shows the average path length of adjacent node pairs in the Hamiltonian path constructed by HP-PEF. We also make comparisons to show that our result of edge fault tolerance has exponentially improved other known results. We further experimentally show that HP-PEF can support the dynamic degradation of average success rate of constructing Hamiltonian paths when increasing faulty edges exceed the fault tolerance."",""1558-2183"","""",""10.1109/TPDS.2023.3264698"",""National Natural Science Foundation of China(grant numbers:62002062)"; Ministry of Science and Technology of Taiwan(grant numbers:MOST-111-2221-E-141-006); Natural Science Foundation of Fujian Province(grant numbers:2022J05029);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093117"",""   $k$     k    -ary   $n$   n     -cubes";algorithm;fault-tolerant embedding;Hamiltonian path;"interconnection networks"",""Fault tolerant systems";Fault tolerance;Circuit faults;Routing;Three-dimensional displays;Through-silicon vias;"System recovery"","""",""1"","""",""42"",""IEEE"",""5 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"An Efficient Hierarchical-Reduction Architecture for Aggregation in Route Travel Time Estimation,""Z. Liu"; M. Li; M. Li; L. Liao;" K. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Semiconductor, Hunan University, Changsha, Hunan, China;" College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jul 2023"",""2023"",""34"",""9"",""2541"",""2552"",""Route travel time estimation (RTTE) is crucial in intelligent transportation systems. Performing aggregation is a fundamental operation in RTTE and is widely used in the traffic prediction and route calculation stages. Observations have revealed that aggregation operations in RTTE are influenced by the road network structure and aggregation requests, resulting in irregular data access, redundant processing, and workload imbalance. Existing architectures have not addressed these issues effectively. In this study, we begin by characterizing the execution pattern of performing aggregation operations on an Intel Core CPU. Guided by these characterizations, we propose an aggregation accelerator that utilizes a hierarchical reduction architecture (HRA) to perform aggregations in RTTE efficiently. Specifically, we construct an inverted table based on the road network and aggregation requests. Building upon the concept of hierarchical reduction, we design an HRA to accelerate aggregation operations which reduces irregular data access and eliminates redundant processing. Additionally, we introduce a reconfiguration mode for HRA to mitigate workload imbalance issues. Compared to a benchmark method executed on an Intel Core CPU, our design achieves an average $10\times$10× speedup."",""1558-2183"","""",""10.1109/TPDS.2023.3292841"",""National Key R&D Program of China(grant numbers:2020YFB2104000)"; Programs of NSFC(grant numbers:62172151,62172157,62206091,62202159,62172146); Natural Science Foundation of Hunan Province(grant numbers:2022JJ30009,2023JJ10016);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10174278"",""Aggregation";accelerator;hierarchical reduction;redundancy elimination;"route travel time estimation"",""Deep learning";Navigation;Roads;Buildings;Estimation;Benchmark testing;"Predictive models"","""","""","""",""36"",""IEEE"",""6 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"An Improved NSGA-III Algorithm Based on Deep Q-Networks for Cloud Storage Optimization of Blockchain,""Y. Zhou"; Y. Ren; M. Xu;" G. Feng"",""School of Communication and Information Engineering, Shanghai University, Shanghai, China"; School of Communication and Information Engineering, Shanghai University, Shanghai, China; School of Communication and Information Engineering, Shanghai University, Shanghai, China;" School of Communication and Information Engineering, Shanghai University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2023"",""2023"",""34"",""5"",""1406"",""1419"",""As the underlying technology of cryptocurrencies, blockchain has gained a lot of attention in recent years. However, the storage problem needs to be solved with the increasing number of blocks in the blockchain network. Cloud storage optimization is an effective way to solve the storage issue, which selects and stores parts of blocks to the cloud. Precisely, block selection can be described as a multiobjective optimization problem (MOP) and solved by evolutionary algorithms (EAs). To obtain well results of block selection, an improved NSGA-III algorithm based on deep Q-networks (DQN), termed NSGA-DQN, is proposed in this paper, which aims to maintain well convergence and diversity of the population. This way, a set of suitable solutions is obtained to determine the number of blocks stored to the cloud, and the storage problem can be solved effectively. To be specific, DQN creates a decision-making agent to maximize the expected reward by learning a policy that evaluates $Q$Q values of each action in each state. In the proposed selection mechanism, the reward values are set according to the convergence and diversity of the population, and the actions correspond to the individuals. This way, our method can determine a set of individuals that maximizes the convergence and diversity of the population. In addition, an adaptive maximum reward enhancement module (AMREM) is developed to further enhance the maximum expected reward by updating the new better reward and modifying the replay memory. We conduct the experimental study on block selection, and the results demonstrate that the proposed algorithm is superior to five state-of-the-art algorithms."",""1558-2183"","""",""10.1109/TPDS.2023.3243634"",""Science and Technology Planning Project of Zhejiang Province(grant numbers:2022C01090)"; National Natural Science Foundation of China(grant numbers:62072295); Natural Science Foundation of Shanghai(grant numbers:20ZR1419700,22ZR1481000);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10041737"",""Blockchain";cloud storage;deep Q-networks (DQN);genetic algorithm;"multiobjective optimization"",""Blockchains";Statistics;Sociology;Cloud computing;Optimization;Convergence;"Costs"","""",""1"","""",""75"",""IEEE"",""9 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"An Instability-Resilient Renewable Energy Allocation System for a Cloud Datacenter,""H. Shen"; H. Wang; J. Gao;" R. Buyya"",""Department of Computer Science, University of Virginia, Charlottesville, VA, USA"; Department of Computer Science, University of Virginia, Charlottesville, VA, USA; Department of Computer Science, University of Virginia, Charlottesville, VA, USA;" School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Jan 2023"",""2023"",""34"",""3"",""1020"",""1034"",""Renewable energy supply is a promising solution for datacenters' increasing electricity monetary cost, energy consumption and harmful gas emissions. However, due to the instability of renewable energy, insufficient renewable energy supply may lead to the use of stored energy or brown energy. To handle this problem, in this paper, we propose an instability-resilient renewable energy allocation system. We define a job's service-level-objective (SLO) as the successful running probability by only using supplied renewable energy. The system allocates jobs with the same SLO level to the same physical machine (PM) group, and powers each PM group with renewable energy generators that have probability no less than its SLO to produce the amount no less than its energy demand. We use a deep learning technique to predict the probability of producing the amount no less than each value of each renewable energy source, and predict the energy demands of each PM area. We formulate an optimization problem to match renewable energy resources with different instabilities to different PM groups for supply, and use reinforcement learning method and linear programming method to solve it. We further propose an energy-driven computing resource assignment method, which adjusts the amount of computing resource of each job based on job deadline and failure probability in each PM group, and a failure prediction based energy saving method. Real trace driven experiments show that our methods achieve much lower SLO violations, total energy monetary cost and total carbon emission compared to other methods and the effectiveness of individual methods."",""1558-2183"","""",""10.1109/TPDS.2023.3235957"",""U.S. NSF(grant numbers:NSF-2206522,NSF-1822965,NSF-1827674,NSF-1733596,NSF-1724845)"; Microsoft Research Faculty Fellowship(grant numbers:8300751); AWS Machine Learning Research Awards;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026238"",""Cloud datacenter";machine learning prediction;"renewable energy scheduling"",""Renewable energy sources";Costs;Carbon dioxide;Energy consumption;Generators;Servers;"Resource management"","""","""","""",""67"",""IEEE"",""25 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Unequal Caching Strategy for Shared-Memory Graph Analytics,""Y. Chen";" Y. -C. Chung"",""Chinese University of Hong Kong, Shenzhen, Guangdong, China";" Chinese University of Hong Kong, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 Jan 2023"",""2023"",""34"",""3"",""955"",""967"",""Recent advances in computer architecture significantly enhance the computational capacity of multicore systems. It allows large-scale graphs to be processed inside a single machine. Nevertheless, the irregular processing pattern of graph-structured data constrains the hardware resources from being productively utilized. In this paper, we investigate the constraints in two aspects: workload imbalance and parallel inefficiency. When a graph analytics algorithm is multithreaded, the thread time is highly diversified, indicating an uneven work distribution. Also, the intensive thread contention lowers the computing capacity of CPU cores, thereby hindering the effective utilization of CPU resources. To address these challenges, we present a proactive graph caching strategy that unequally segments graph components into cache-able subsets of varying sizes, namely Syze. First, the computational loads of cache-sized subgraphs are estimated. Then, the demanding subgraphs are further subdivided until certain threshold is met. Moreover, during the propagation of updates, a fraction of vertex ID (i.e., several bits) are encoded to facilitate the communication between subgraphs. As a result, Syze is able to balance the workloads amongst the logical cores by shortening the longest thread execution time. Meanwhile, it alleviates thread contention and thus elevates the parallel efficiency of multicores. Compared with well-optimized Ligra, Gemini and GPOP, Syze achieves accelerations by up to $17.76\times$17.76×, $11.67\times$11.67× and $2.81\times$2.81× respectively. Additionally, the side effects of Syze are evaluated, including raised cache misses and memory accesses. They play a trivial role in deciding the overall performance, as their costs are far outweighed by the gains from the even distribution of workloads and the improved utilization of multicores.Author: Please confirm or add details for any funding or financial support for the research of this article. ?>"",""1558-2183"","""",""10.1109/TPDS.2022.3218885"",""National Key Research & Development Program of China(grant numbers:2018YFB1003505)"; Large-scale Graph Pattern Query System and Its Optimization Strategy of Ali Damo Research Institute(grant numbers:2022E0018);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012415"",""Graph analytics";multicore system;"parallel computing"",""Multicore processing";Instruction sets;Message systems;Random access memory;Parallel processing;Optimization;"Synchronization"","""","""","""",""62"",""IEEE"",""9 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"APQ: Automated DNN Pruning and Quantization for ReRAM-Based Accelerators,""S. Yang"; S. He; H. Duan; W. Chen; X. Zhang; T. Wu;" Y. Yin"",""College of Computer Science and Technology, Zhejiang University, Hangzhou, China"; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Engineering and Computer Science, Washington State University Vancouver, Vancouver, WA, USA; College of Computer Science and Technology, Zhejiang University, Hangzhou, China;" College of Computer Science and Technology, Zhejiang University, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jul 2023"",""2023"",""34"",""9"",""2498"",""2511"",""Emerging ReRAM-based accelerators support in-memory computation to accelerate deep neural network (DNN) inference. Weight matrix pruning is a widely used technique to reduce the size of DNN models, thereby reducing the resource and energy consumption of ReRAM-based accelerators. However, existing pruning works for ReRAM-based accelerators have three major issues. First, they use heuristics or rules from domain experts to prune the weights, leading to sub-optimal pruning policies. Second, they use row or column-level coarse-granularity methods to prune weights, resulting in poor compression rates with model accuracy constraints. Third, they only apply the weight pruning technique individually, losing the compression opportunity of both pruning and quantization. In this article, we propose an Automated DNN Pruning and Quantization framework, named APQ, for ReRAM-based accelerators. First, APQ adopts reinforcement learning (RL) to automatically determine the pruning policy for DNN layers for a global optimum. Second, it prunes and maps weight matrices to a ReRAM-based accelerator in a finer granularity of column-vector, which improves the compression rates with the accuracy constraints. To address the dislocation problem, it uses a new data path in ReRAM-based accelerators to correctly index and feed input to matrix-vector computation. Third, to further reduce resource consumption, APQ also leverages reinforcement learning to automatically determine the quantization bitwidth of each layer of the pruned DNN model. Experimental results show that, APQ achieves up to 4.52X compression rate, 4.11X area efficiency, and 4.51X energy efficiency with similar or even higher model accuracy, compared to the state-of-the-art work."",""1558-2183"","""",""10.1109/TPDS.2023.3290010"",""National Key Research and Development Program of China(grant numbers:2021ZD0110700)"; Program of Zhejiang Province Science and Technology(grant numbers:2022C01044); National Natural Science Foundation of China(grant numbers:62172361); Zhejiang Lab Research Project(grant numbers:2020KC0AC01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10164176"",""ReRAM-based accelerator";pruning;quantization;"reinforcement learning"",""Quantization (signal)";Computational modeling;Convolution;Reinforcement learning;Arrays;Neural networks;"Matrix converters"","""","""","""",""39"",""IEEE"",""27 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Asynchronous Algorithms for Decentralized Resource Allocation Over Directed Networks,""Q. Lü"; X. Liao; S. Deng;" H. Li"",""College of Computer Science, Chongqing University, Chongqing, China"; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China;" College of Electronic and Information Engineering, Southwest University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""16"",""32"",""In this article, we consider a class of decentralized resource allocation problems over directed networks, where each node only communicates with its in-neighbors and attempts to minimize its own cost when network-wide resource constraints as well as local capacity limits are satisfied. Decentralized optimization to solve this problem has been a significant focus within engineering research due to its advantages in scalability, robustness, and flexibility. Most existing methods are synchronous while few works are devoted to asynchronously solving the problem. The problem becomes even more challenging when the networks are directed. To address the resource allocation problem when the above issues are considered, we propose a novel decentralized asynchronous algorithm based on the gossip-based communication protocol and epigraph strategy. An important feature of the algorithm is that it is implemented in a completely decentralized manner in the case of asynchronous communication and directed networks. We provide theoretical proof to guarantee the convergence of the proposed algorithm, which indicates that it can successfully allocate the optimal resource. When solving the resource allocation problem over time-varying directed networks, we further discuss a related decentralized asynchronous algorithm according to the random sleep protocol. Numerical examples are given to demonstrate the viability and performance of the algorithms."",""1558-2183"","""",""10.1109/TPDS.2022.3212424"",""China Postdoctoral Science Foundation(grant numbers:2021M700588)"; Key Laboratory of Industrial Internet of Things & Networked Control, Ministry of Education(grant numbers:2021FF09); Chongqing Postdoctoral Science Foundation(grant numbers:2021XM1006); National Key R&D Program of China(grant numbers:2018AAA0100101); National Natural Science Foundation of China(grant numbers:61932006,61772434,61806169);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9913307"",""Resource allocation";decentralized asynchronous algorithm;gossip-based communication protocol;epigraph strategy;"time-varying directed networks"",""Resource management";Protocols;Optimization;Switching circuits;Generators;Communication networks;"Robustness"","""",""3"","""",""46"",""IEEE"",""6 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Auction-Based Cluster Federated Learning in Mobile Edge Computing Systems,""R. Lu"; W. Zhang; Y. Wang; Q. Li; X. Zhong; H. Yang;" D. Wang"",""School of Cyberspace Science, Harbin Institute of Technology, Harbin, China"; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; Department of Computing, Macquarie University, Sydney, NSW, Australia; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; Department of New Networks, Peng Cheng Laboratory (PCL), Shenzhen, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China;" School of Cyberspace Science, Harbin Institute of Technology, Harbin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2023"",""2023"",""34"",""4"",""1145"",""1158"",""Federated Learning (FL), allowing data owners to conduct model training without sending their raw data to third-party servers, can enhance data privacy in Mobile Edge Computing (MEC) which brings data processing closer to the data sources. However, the heterogeneity of local data and constrained local resources in MEC bring new challenges hindering the development of FL. To this end, we propose an Auction-based Cluster Federated Learning scheme, called ACFL, comprising a clustered FL framework and an auction-based client selection strategy. Our clustered FL framework first introduces a mean-shift clustering algorithm to FL, which can intelligently cluster clients according to their local data distribution. Then, we select clients from each cluster using an auction mechanism to participate in FL training, which can mitigate the impact of data heterogeneity on model convergence and balance energy consumption. Moreover, we prove the proposed clustered FL framework converges at a sublinear rate. Extensive experiments conducted on real-world datasets demonstrate that the proposed FL scheme outperforms the conventional FL schemes in terms of convergence rate and energy balance."",""1558-2183"","""",""10.1109/TPDS.2023.3240767"",""National Key Research and Development Program of China(grant numbers:2021YFB3101102)"; Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101360001); Shenzhen Science and Technology Research and Development Foundation(grant numbers:JCYJ20190806143418198); Fundamental Research Funds for the Central Universities(grant numbers:HIT.OCEF.2021007); Peng Cheng Laboratory Project(grant numbers:PCL2021A02);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032710"",""Auction mechanism";clustering strategy;"federated learning"",""Data models";Training;Servers;Computational modeling;Convergence;Federated learning;"Energy consumption"","""",""5"","""",""54"",""IEEE"",""31 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Automatic Multi-Parameter Performance Modeling of HPC Applications on a New Sunway Supercomputer,""Y. Zhang"; Y. Liu; P. Jiao; Y. Zhou;" T. Wei"",""School of Data Science and Engineering, East China Normal University, Shanghai, China"; School of Data Science and Engineering, East China Normal University, Shanghai, China; Shanghai Artificial Intelligence Laboratory, Shanghai, China; School of Data Science and Engineering, East China Normal University, Shanghai, China;" School of Computer Science and Technology, East China Normal University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Oct 2023"",""2023"",""34"",""11"",""2965"",""2977"",""As the successor to Sunway TaihuLight, the new Sunway supercomputer has ultra-high computing capacity, but the unique heterogeneous architecture presents performance optimization challenges for High Performance Computing (HPC) applications. Performance modeling is an effective way to discover the performance bottlenecks and then improve the performance of HPC applications. Existing performance modeling techniques do not work well on large-scale HPC applications due to high overhead and low accuracy, and are not suitable for the heterogeneous architecture due to a lack of support for multi-resource parameters. To address the above challenges, we propose an automatic multi-parameter performance modeling method for HPC applications on the new Sunway supercomputer. First, a lightweight performance profiling method is proposed to achieve low overhead performance profiling. Then, performance models with multiple resource parameters based on the Fourier neural operator are built, achieving high prediction accuracy and generalization ability. Finally, the Fourier neural operator is extended on the new Sunway supercomputer to realize the performance modeling automatically. Experimental results show that the average prediction error is less than 10% and the average overhead is less than 4%, and the results are superior to the baselines."",""1558-2183"","""",""10.1109/TPDS.2023.3317296"",""National Key Research and Development Program of China(grant numbers:2020YFA0607900)"; National Natural Science Foundation of China(grant numbers:42375146,62272169); Laoshan Laboratory(grant numbers:LSKJ202202100); Ministry of Education's University-Industry Collaborative Education Program(grant numbers:202102511018);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255321"",""A new Sunway supercomputer";automatic performance modeling;fourier neural operator;HPC applications;lightweight profiling;"muti-parameter modeling"",""Computational modeling";Supercomputers;Analytical models;Predictive models;Computer architecture;Codes;"Load modeling"","""","""","""",""33"",""IEEE"",""19 Sep 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"AutoRS: Environment-Dependent Real-Time Scheduling for End-to-End Autonomous Driving,""J. Ma"; L. Li;" C. Xu"",""IOTSC, University of Macau, Taipa, China"; IOTSC, University of Macau, Taipa, China;" Faculty of Science and Technology, University of Macau, Taipa, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Oct 2023"",""2023"",""34"",""12"",""3238"",""3252"",""The rapid development of autonomous driving poses new research challenges for on-vehicle computing system. The execution time of autonomous driving tasks heavily depends on the driving environment. As the scene becomes complex, task execution time increases significantly, leading to end-to-end deadline misses and potential accidents. Hence, a framework that can effectively schedule tasks according to the driving environment in order to guarantee end-to-end deadlines is critical for autonomous driving. In this article, we propose AutoRS, an environment-dependent real-time scheduling framework for end-to-end autonomous driving. AutoRS consists of two nested control loops. The inner control loop schedules tasks based on the driving environment to help them meet end-to-end deadlines while prioritizing the responsiveness and throughput of control commands. The outer control loop tunes task rates based on schedulability to efficiently utilize system resources with an RL-based design. We conduct extensive experiments on both simulation and hardware testbeds using representative autonomous driving applications. The results demonstrate that AutoRS effectively improves the driving performance by $7.95\%-56.9\%$7.95%-56.9% in different driving environments. AutoRS can significantly enhance the safety and reliability of autonomous driving systems by providing timely control commands in complex and dynamic driving environments while guaranteeing task deadlines."",""1558-2183"","""",""10.1109/TPDS.2023.3323975"",""Science and Technology Development Fund of Macau SAR(grant numbers:0081/2022/A2,0123/2022/AFJ,0015/2019/AKP)"; Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020B515130004); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164003,SRG2022-00010-IOTSC);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10295999"",""Autonomous driving";"real-time scheduling"",""Task analysis";Autonomous vehicles;Automobiles;Throughput;Real-time systems;Schedules;"Runtime"","""","""","""",""32"",""IEEE"",""25 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Back to Homogeneous Computing: A Tightly-Coupled Neuromorphic Processor With Neuromorphic ISA,""Z. Yang"; L. Wang; W. Shi; Y. Wang; J. Tie; F. Wang; X. Yu; L. Peng; C. Xiao; X. Xiao; Y. Yao; G. Zhou; X. Yu; R. Gong; X. Zhao; Y. Tang;" W. Xu"",""College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China"; Defense Innovation Institute, Academy of Military Sciences, Beijing, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Computer Science, Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Computer Science, Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China; Defense Innovation Institute, Academy of Military Sciences, Beijing, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China;" College of Computer Science and Technology, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Sep 2023"",""2023"",""34"",""11"",""2910"",""2927"",""In recent years, neuromorphic processors are widely used in many scenarios, showing extreme energy efficiency over traditional architectures. However, almost all existing neuromorphic hardware are following the heterogeneous computing methodology without Instruction Set Architecture (ISA), leading to inflexibility in programming. In this paper, we first propose a RISC-V Neuromorphic Extension (RVNE) to enable fine-grained and flexible homogeneous programming for neuromorphic algorithms while utilizing SNN sparsity from different levels of granularity and computing flows. Based on RVNE, we next implement a neuromorphic micro-architecture that is tightly coupled to the CPU pipeline to accelerate neuromorphic computing. To demonstrate the proposed homogeneous neuromorphic architecture, we implement a prototype processor called NeuroRVcore based on RISC-V ISA and an open-source RISC-V core. The evaluation results show that RVNE achieves a 2.8 × −4.3 × reduction in code density compared with the general-purpose ISAs. Compared with the state-of-the-art neuromorphic processor, the proposed homogeneous computing reduces energy consumption by 3.4%−22.5% while enabling fine-grained and flexible homogeneous programming."",""1558-2183"","""",""10.1109/TPDS.2023.3307408"",""National Natural Science Foundation of China(grant numbers:62372461,62032001,62203457,62102438)"; Key Laboratory of Advanced Microprocessor Chips and Systems;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10226235"",""Instruction set architecture";neuromorphic computing;neuromorphic processor;RISC-V;"spiking neural network"",""Neurons";Computer architecture;Computational modeling;Biological system modeling;Neuromorphic engineering;Instruction sets;"Programming"","""","""","""",""66"",""IEEE"",""22 Aug 2023"","""","""",""IEEE"",""IEEE Journals"""
"BeauForT: Robust Byzantine Fault Tolerance for Client-Centric Mobile Web Applications,""K. Jannes"; E. H. Beni; B. Lagaisse;" W. Joosen"",""imec-DistriNet, KU Leuven, Leuven, Belgium"; imec-DistriNet and imec-COSIC, KU Leuven, Leuven, Belgium; imec-DistriNet, KU Leuven, Leuven, Belgium;" imec-DistriNet, KU Leuven, Leuven, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Feb 2023"",""2023"",""34"",""4"",""1241"",""1252"",""In recent years, part of the web is shifting to a client-centric, decentralized model where web clients become the leading execution environment for application logic and data storage. However, current solutions to build decentralized web applications with multiple distrusting parties often involve a decentralized backend of servers running a BFT protocol between them. Existing consensus protocols using either all-to-all communication, or leader-based gossip suffer from performance degradation in unstable network conditions. In this paper, we present BeauForT, a purely browser-based platform for decentralized BFT consensus in client-centric, community-driven applications. We propose a novel, optimistic, leaderless, gossip-based consensus protocol, tolerating Byzantine replicas, combined with a robust and efficient state-based synchronization protocol. This protocol makes BeauForT well suited for the decentralized client-centric web and its dynamic nature with many network disruptions or node failures."",""1558-2183"","""",""10.1109/TPDS.2023.3241963"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10036141"",""Byzantine fault tolerance";peer-to-peer systems;"web applications"",""Consensus protocol";Peer-to-peer computing;Fault tolerant systems;Fault tolerance;Synchronization;Browsers;"Throughput"","""",""2"","""",""73"",""IEEE"",""3 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Benzene: Scaling Blockchain With Cooperation-Based Sharding,""Z. Cai"; J. Liang; W. Chen; Z. Hong; H. -N. Dai; J. Zhang;" Z. Zheng"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Hong Kong Polytechnic University, Hong Kong SAR, China; Department of Computer Science, Hong Kong Baptist University, Hong Kong; Department of Computer Science, Purdue University, West Lafayette, IN, USA;" School of Software Engineering, Sun Yat-sen University, Zhuhai, Guangdong Province, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Jan 2023"",""2023"",""34"",""2"",""639"",""654"",""Sharding has been considered as a prominent approach to enhance the limited performance of blockchain. However, most sharding systems leverage a non-cooperative design, which lowers the fault tolerance resilience due to the decreased mining power as the consensus execution is limited to each separated shard. To this end, we present Benzene, a novel sharding system that enhances the performance by cooperation-based sharding while defending the per-shard security. First, we establish a double-chain architecture for function decoupling. This architecture separates transaction-recording functions from consensus-execution functions, thereby enabling the cross-shard cooperation during consensus execution while preserving the concurrency nature of sharding. Second, we design a cross-shard block verification mechanism leveraging Trusted Execution Environment (TEE), via which miners can verify blocks from other shards during the cooperation process with the minimized overheads. Finally, we design a voting-based consensus protocol for cross-shard cooperation. Transactions in each shard are confirmed by all shards that simultaneously cast votes, consequently achieving an enhanced fault tolerance and lowering the confirmation latency. We implement Benzene and conduct both prototype experiments and large-scale simulations to evaluate the performance of Benzene. Results show that Benzene achieves superior performance than existing sharding/non-sharding blockchain protocols. In particular, Benzene achieves a linearly-improved throughput with the increased number of shards (e.g., 32,370 transactions per second with 50 shards) and maintains a lower confirmation latency than Bitcoin (with more than 50 shards). Meanwhile, Benzene maintains a fixed fault tolerance at 1/3 even with the increased number of shards."",""1558-2183"","""",""10.1109/TPDS.2022.3227198"",""National Key Research and Development Plan(grant numbers:2021YFB2700302)"; National Natural Science Foundation of China(grant numbers:62172453); National Natural Science Foundation of Guangdong province(grant numbers:2022A1515010154,6142006200403,XM2021XT1084); Major Key Project of PCL(grant numbers:PCL2021A06); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Guangdong Provincial Pearl River Talents Program(grant numbers:2019QN01X130);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9983501"",""Blockchain";sharding;scalability;function decoupling;"consensus algorithm"",""Sharding";Fault tolerant systems;Fault tolerance;Security;Throughput;Protocols;"Bitcoin"","""",""6"","""",""52"",""IEEE"",""13 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Bio-ESMD: A Data Centric Implementation for Large-Scale Biological System Simulation on Sunway TaihuLight Supercomputer,""X. Duan"; Q. Shao; J. Weng; B. Schmidt; L. Gan; G. Li; H. Fu; W. Xue; W. Liu;" G. Yang"",""School of Software, Shandong University, Jinan, Shandong, China"; School of Software, Shandong University, Jinan, Shandong, China; Laboratory of Molecular Modeling and Design, State Key Laboratory of Molecular Reaction Dynamics, Dalian Institute of Chemical Physics, Chinese Academy of Sciences, Dalian, Liaoning, China; Institute for Computer Science, Johannes Gutenberg University, Mainz, Germany; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; Laboratory of Molecular Modeling and Design, State Key Laboratory of Molecular Reaction Dynamics, Dalian Institute of Chemical Physics, Chinese Academy of Sciences, Dalian, Liaoning, China; Ministry of Education Key Laboratory for Earth System Modeling, Department of Earth System Science, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; School of Software, Shandong University, Jinan, Shandong, China;" Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Jan 2023"",""2023"",""34"",""3"",""881"",""893"",""Molecular dynamics (MD) simulations of biological systems are playing an increasingly important role in the research of pathogens and drugs. Most MD methods for biological simulations rely on the listed bonds which interact among specific groups of atoms identified by atom tags (unique atom tags regardless the storage location). However, efficient mapping of tags to atom locations is often challenging on modern many-core processors because data locality can not always be guaranteed for large-scale systems. In this paper, we present Bio-ESMD, a new MD implementation supporting listed bonds. Bio-ESMD is designed and developed based on our previously designed ESMD framework for many-core processors. In Bio-ESMD, we have introduced a data-centric approach for refactoring MD algorithms by reorganizing the cell list data structure to adopt bond lists with guaranteed data locality. Our implementation achieves speedups of over two compared to SW_GROMACS on Sunway TaihuLight. Furthermore, Bio-ESMD can simulate a system of 308.8 million atoms at 1.33 ns/day or 14.44 million atoms at 17.28 ns/day with linear weak scaling efficiency."",""1558-2183"","""",""10.1109/TPDS.2022.3220559"",""National Key R&D Program of China(grant numbers:2019YFA0709400)"; National Natural Science Foundation of China(grant numbers:62102114,T2125006,U1839206,61972231); Jiangsu Innovation Capacity Building Program(grant numbers:BM2022028); Engineering Research Center of Digital Media Technology; Ministry of Education, China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10008088"",""Cell-list method";data-centric algorithm;molecular dynamics;"supercomputing"",""Biological system modeling";Computational modeling;Mathematical models;Force;Springs;Software;"Program processors"","""","""","""",""28"",""IEEE"",""6 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Blockchain-Based P2P Content Delivery With Monetary Incentivization and Fairness Guarantee,""S. He"; Y. Lu; Q. Tang; G. Wang;" C. Q. Wu"",""Southwest Jiaotong University, Chengdu, Sichuan, China"; Institute of Software Chinese Academy of Sciences, Beijing, China; School of Computer Science, The University of Sydney, Darlington, NSW, Australia; Department of Computer Science, Ying Wu College of Computing in New Jersey Institute of Technology, Newark, NJ, USA;" Department of Computer Science, Ying Wu College of Computing in New Jersey Institute of Technology, Newark, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Jan 2023"",""2023"",""34"",""2"",""746"",""765"",""Peer-to-peer (P2P) content delivery is up-and-coming to provide benefits comprising cost-saving and scalable peak-demand handling compared with centralized content delivery networks (CDNs), and also complementary to the popular decentralized storage networks such as Filecoin. However, reliable P2P delivery demands proper enforcement of delivery fairness, i.e., the deliverers should be rewarded in line with their in-time delivery. Unfortunately, most existing studies on delivery fairness are on the basis of non-cooperative game-theoretic assumptions that are arguably unrealistic in the ad-hoc P2P setting. We propose an expressive yet still minimalist security requirement for desired fair P2P content delivery, and give two efficient blockchain-enabled and monetary-incentivized solutions ${\mathsf {FairDownload}}$FairDownload and ${\mathsf {FairStream}}$FairStream for P2P downloading and P2P streaming scenarios, respectively. Our designs not only ensure delivery fairness where deliverers are paid (nearly) proportional to their in-time delivery, but also guarantee exchange fairness where content consumers and content providers are also fairly treated. The fairness of each party can be assured even when other two parties collude to arbitrarily misbehave. Our protocols provide a general design of fetching content chunk from any specific position so the delivery can be resumed in the presence of unexpected interruption. Further, our systems are efficient in the sense of achieving asymptotically optimal on-chain costs and optimal delivery communication. We implement the prototype and deploy on the Ethereum Ropsten network. Extensive experiments in both LAN and WAN settings are conducted to evaluate the on-chain costs as well as the efficiency of downloading and streaming. Experimental results show the practicality and efficiency of our protocols."",""1558-2183"","""",""10.1109/TPDS.2022.3217036"",""National Key R&D Project of China(grant numbers:2022YFB2701600)"; National Natural Science Foundation of China(grant numbers:62102404); Youth Innovation Promotion Association of the Chinese Academy of Sciences; Sichuan Science and Technology Program(grant numbers:2021YFG0040); Ethereum Foundation; Stellar Foundations and Protocol Labs; FHWA EAR(grant numbers:693JJ320C000021);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9929262"",""Blockchain application";content delivery;delivery fairness;monetary incentivization;"peer-to-peer"",""Protocols";Bandwidth;Costs;Cryptography;Indexes;Electronic mail;"Blockchains"","""","""","""",""54"",""IEEE"",""25 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Boosting Erasure-Coded Multi-Stripe Repair in Rack Architecture and Heterogeneous Clusters: Design and Analysis,""H. Zhou";" D. Feng"",""Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education of China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China";" Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education of China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jun 2023"",""2023"",""34"",""8"",""2251"",""2264"",""Large-scale storage systems have introduced erasure codes to guarantee high data reliability, yet inevitably at the expense of high repair costs. In practice, storage nodes are usually divided into different racks, and data blocks in nodes are organized into multiple stripes independently manipulated by erasure code. Due to the scarcity and heterogeneity of the cross-rack bandwidth, the cross-rack transmission dominates the entire repair costs. When erasure code is deployed in rack architectures, existing repair techniques are limited in different aspects: neglecting the heterogeneous cross-rack bandwidth, less consideration for multi-stripe failure, and no special treatment on repair-link scheduling. In this paper, we present CMRepair, a Cross-rack Multi-stripe Repair technique that aims to reduce the repair time for multi-stripes failure repair in heterogeneous erasure-coded clusters. CMRepair first carefully chooses the nodes for reading/repairing blocks and searches for the multi-stripe repair solution. It adopts different algorithms to adjust the solution, including the Computation Time Priority (CTP) algorithm based on the greedy idea and the Repair Time Priority (RTP) algorithm based on the meta-heuristics idea. Furthermore, CMRepair selectively schedules the execution orders of cross-rack links, with the primary objective of saturating the unused upload/download bandwidth resources and avoiding network congestion. The experiments show that CMRepair with the CTP algorithm can reduce 27.59%-58.12% of the repair time while only introducing negligible computation overhead, and CMRepair with the RTP algorithm can reduce 33.52%-97.75% of the repair time in an acceptable computation time, over existing repair techniques."",""1558-2183"","""",""10.1109/TPDS.2023.3282180"",""National Key R&D Program of China(grant numbers:2018YFB1003305)"; Key Laboratory of Information Storage System Ministry of Education of China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10143232"",""Erasure code";rack architecture;multiple stripes;heterogeneous network;"repair time"",""Maintenance engineering";Codes;Bandwidth;Computer architecture;Clustering algorithms;Costs;"Heterogeneous networks"","""","""","""",""42"",""IEEE"",""2 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Building Trust in Earth Science Findings through Data Traceability and Results Explainability,""P. Olaya"; D. Kennedy; R. Llamas; L. Valera; R. Vargas; J. Lofstead;" M. Taufer"",""University of Tennessee, Knoxville, TN, USA"; University of Tennessee, Knoxville, TN, USA; University of Delaware, Newark, DE, USA; University of Tennessee, Knoxville, TN, USA; University of Delaware, Newark, DE, USA; Sandia National Laboratories, Albuquerque, NM, USA;" University of Tennessee, Knoxville, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Dec 2022"",""2023"",""34"",""2"",""704"",""717"",""To trust findings in computational science, scientists need workflows that trace the data provenance and support results explainability. As workflows become more complex, tracing data provenance and explaining results become harder to achieve. In this paper, we propose a computational environment that automatically creates a workflow execution's record trail and invisibly attaches it to the workflow's output, enabling data traceability and results explainability. Our solution transforms existing container technology, includes tools for automatically annotating provenance metadata, and allows effective movement of data and metadata across the workflow execution. We demonstrate the capabilities of our environment with the study of SOMOSPIE, an earth science workflow. Through a suite of machine learning modeling techniques, this workflow predicts soil moisture values from the 27 km resolution satellite data down to higher resolutions necessary for policy making and precision agriculture. By running the workflow in our environment, we can identify the causes of different accuracy measurements for predicted soil moisture values in different resolutions of the input data and link different results to different machine learning methods used during the soil moisture downscaling, all without requiring scientists to know aspects of workflow design and implementation."",""1558-2183"","""",""10.1109/TPDS.2022.3220539"",""Sandia National Laboratories"; National Science Foundation(grant numbers:1841758,1941443,2028923,2103845,2103836,2138811); IBM; National Science Foundation(grant numbers:1548562);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9942337"",""Scientific workflows";scientific computing;provenance;reproducibility;replicability;"soil moisture predictions"",""Containers";Metadata;Soil moisture;Data transfer;Task analysis;Soft sensors;"Machine learning"","""",""5"","""",""44"",""CCBYNCND"",""8 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"CD-MSA: Cooperative and Deadline-Aware Scheduling for Efficient Multi-Tenancy on DNN Accelerators,""C. Wang"; Y. Bai;" D. Sun"",""Sino-German Joint Software Institute, School of Computer Science and Engineering, Beihang University, Beijing, China"; Sino-German Joint Software Institute, School of Computer Science and Engineering, Beihang University, Beijing, China;" Sino-German Joint Software Institute, School of Computer Science and Engineering, Beihang University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 May 2023"",""2023"",""34"",""7"",""2091"",""2106"",""With DNN turning into the backbone of AI cloud services and propelling the emergence of INFerence-as-a-Service (INFaaS), DNN-specific accelerators have become the indispensable components of cloud inference systems. Due to the conservative “one-task-at-a-time” working mode and deadline blindness of those accelerators, implementing multi-tenancy that aims to improve the cost-effectiveness and meet SLA requirements is intractable. Recent studies including the temporal and spatial approaches, employ manifold scheduling mechanisms and sophisticated architecture innovations to address the challenge. However, these researches either still neglect the deadline awareness or render inevitable and expensive hardware overheads such as switches and storage. In this paper, we present Cooperative and Deadline-aware Multi-Systolic-Array scheduling (CD-MSA), a low-cost solution for the cloud inference that utilizes the real time mechanism and task-level parallelism to enable efficient multi-tenancy. Based on our preemptive multi-systolic-array accelerator architecture supporting the simultaneous task co-location, we first construct a fine-grained DNN execution model to lay the groundwork for the lightweight preemption. Second, we design a cooperative, deadline- and laxity-aware scheduler in conjunction with an efficient schedulability test method for better QoS guarantee without introducing additional hardware cost. Finally, to further promote the overall throughput, we propose dynamic task fusion, a software approach that fuses different tasks into the logically “multi-threading” tasks at runtime. We compare CD-MSA with several state-of-the-art researches across three multi-DNN workloads. The evaluation results show CD-MSA improves the latency-bounded throughput, SLA satisfaction rate and weighted system throughput by up to 62%, 63% and 27%, respectively."",""1558-2183"","""",""10.1109/TPDS.2023.3276759"",""National Natural Science Foundation of China(grant numbers:51877004,61732002,61572062)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10125064"",""Domain-specific architectures";scheduling and task partitioning;accelerators;"deep neural networks"",""Task analysis";Throughput;Systolic arrays;Real-time systems;Quality of service;System-on-chip;"Processor scheduling"","""","""","""",""47"",""IEEE"",""16 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CERT-DF: A Computing-Efficient and Robust Distributed Deep Forest Framework With Low Communication Overhead,""L. Xie"; T. Wang; S. Du;" H. Cai"",""Engineering Research Center of Software/Hardware Co-design Technology and Application, Ministry of Education, The Shanghai Key Laboratory of Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, China"; Engineering Research Center of Software/Hardware Co-design Technology and Application, Ministry of Education, The Shanghai Key Laboratory of Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, China; Department of Houston International Institute, Dalian Maritime University, Dalian, Liaoning, China;" Engineering Research Center of Software/Hardware Co-design Technology and Application, Ministry of Education, The Shanghai Key Laboratory of Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Nov 2023"",""2023"",""34"",""12"",""3280"",""3293"",""As an alternative to the deep learning model, deep forest outperforms deep neural networks in many aspects with fewer hyperparameters and better robustness. To improve the computing performance of deep forest, ForestLayer proposes an efficient task-parallel algorithm S-FTA at a fine sub-forest granularity, but the granularity of the sub-forest cannot be adaptively adjusted. BLB-gcForest further proposes an adaptive sub-forest splitting algorithm to dynamically adjust the sub-forest granularity. However, with distributed storage, its BLB method needs to scan the whole dataset when sampling, which generates considerable communication overhead. Moreover, BLB-gcForest's tree-based vector aggregation produces extensive redundant transfers and significantly degrades the system's performance in vector aggregation stage. To deal with these existing issues and further improve the computing efficiency and scalability of the distributed deep forest, in this paper, we propose a novel Computing-Efficient and RobusT distributed Deep Forest framework, named CERT-DF. CERT-DF integrates three customized schemes, namely, block-level pre-sampling, two-stage pre-aggregation, and system-level backup. Specifically, CERT-DF adopts the block-level pre-sampling method to implement data blocks' local sampling eliminating frequent data remote access and maximizing parallel efficiency, applies the two-stage pre-aggregation method to adjust the class vector aggregation granularity to greatly decrease the communication overhead, and leverages the system-level backup method to enhance the system's disaster tolerance and immensely accelerate task recovery with minimal system resource overhead. Comprehensive experimental evaluations on multiple datasets show that our CERT-DF significantly outperforms the state-of-the-art approaches with higher computing efficiency, lower system resource overhead, and better system robustness while ensuring good accuracy."",""1558-2183"","""",""10.1109/TPDS.2023.3324911"",""National Key Research and Development Program of China(grant numbers:2021ZD0114600,2020AAA0107401)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10286399"",""Deep forest";distributed computing;Big Data bootstrap;"distributed AI"",""Forestry";Random forests;Regression analysis;Computational modeling;Training;Task analysis;"Robustness"","""","""","""",""34"",""IEEE"",""16 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CHEESE: Distributed Clustering-Based Hybrid Federated Split Learning Over Edge Networks,""Z. Cheng"; X. Xia; M. Liwang; X. Fan; Y. Sun; X. Wang;" L. Huang"",""School of Future Science and Engineering, Soochow University, Suzhou, China"; School of Computing Technologies, RMIT University, Melbourne, VIC, Australia; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Department of Information and Communication Engineering, Xiamen University, Xiamen, China; Navigation Institute, Jimei University, Xiamen, China; Department of Electrical and Computer Engineering, Western University, Ontario, ON, Canada;" Department of Information and Communication Engineering, Xiamen University, Xiamen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Oct 2023"",""2023"",""34"",""12"",""3174"",""3191"",""Implementing either Federated learning (FL) or split learning (SL) over clients with limited computation/communication resources faces challenges on achieving delay-efficient model training. To overcome such challenges, we investigate a novel distributed Clustering-based Hybrid fEdErated Split lEarning (CHEESE) framework, consolidating distributed resources among clients by device-to-device (D2D) communications, working in an intra-serial inter-parallel manner. In CHEESE, each learning client can form a cluster with its neighboring helping clients via D2D communications to train an FL model collaboratively. Inside each cluster, the model is split into multiple segments via a model splitting and allocation (MSA) strategy, while each cluster member trains one segment. After completing intra-cluster training, a transmission client (TC) is determined from each cluster to upload a complete model to the base station for global model aggregation under allocated bandwidth. Accordingly, an overall training delay cost minimization problem is formulated, involving the following subproblems: client clustering, MSA, TC selection, and bandwidth allocation. Due to its NP-Hardness, the problem is decoupled and solved iteratively. The client clustering problem is first transformed into a distributed clustering game based on potential game theory, where each cluster further investigates the remaining three subproblems to evaluate the utility of each clustering strategy. Specifically, a heuristic algorithm is proposed to solve the MSA problem under a given clustering strategy, while a greedy-based convex optimization approach is introduced to solve the joint TC selection and bandwidth allocation problem. Extensive experiments on practical models and datasets demonstrate that CHEESE can significantly reduce training delay costs."",""1558-2183"","""",""10.1109/TPDS.2023.3322755"",""National Natural Science Foundation of China(grant numbers:62271424)"; Natural Science Foundation of Xiamen Municipality(grant numbers:3502Z20227002,3502Z20227007); Fundamental Research Funds for the Central Universities(grant numbers:20720230035); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2022A1515110042);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10274134"",""Distributed clustering";federated learning;potential game;"split learning"",""Computational modeling";Costs;Training;Servers;Data models;Dairy products;"Device-to-device communication"","""",""1"","""",""52"",""IEEE"",""9 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"CIA: A Collaborative Integrity Auditing Scheme for Cloud Data With Multi-Replica on Multi-Cloud Storage Providers,""T. Li"; J. Chu;" L. Hu"",""College of Computer Science and Technology, Jilin University, Changchun, Jilin, China"; College of Computer Science and Technology, Jilin University, Changchun, Jilin, China;" College of Computer Science and Technology, Jilin University, Changchun, Jilin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2022"",""2023"",""34"",""1"",""154"",""162"",""The emergence of cloud storage has solved many pain points of the traditional storage model. However, the issue of cloud storage data integrity - whether the cloud storage provider has kept the data intact - has raised concerns about cloud storage. Integrity auditing of cloud storage data allows users to know the integrity of the outsourced data without downloading it in its entirety. However it is not enough to know its integrity. Multi-replicas, as a common means of redundancy, improves the reliability of cloud storage data. And storing multi-replicas on multi-cloud storage providers (CSPs) enhances this feature. In a multi-replica multi-CSPs scenario, if the independence of CSPs is given full play and auditing is performed among CSPs, not only the introduction of third party auditor (TPA) can be eliminated, but also the tag generation, which is a huge computational overhead, can be removed. Inspired by this, in this paper we propose a new model for remote data integrity auditing: CIA (Collaborative Integrity Auditing). In addition to the reduction in computational overhead, the proposed scheme provides unprecedented support for free data blocking. The proposed scheme employs only hash functions in the calculation, which has a negligible computational overhead compared to the traditional bilinear pairing-based schemes. Theoretical analysis and experimental results show that the proposed scheme provides high efficiency and flexibility with security assurance, and can be used as a lightweight alternative to traditional remote data integrity auditing schemes in multi-replica multi-CSP scenarios."",""1558-2183"","""",""10.1109/TPDS.2022.3216614"",""National Natural Science Foundation of China(grant numbers:61701190)"; National Key R&D Plan of China(grant numbers:2017YFA0604500); National Sci-Tech Support Plan of China(grant numbers:2014BAH02F00);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9927356"",""Cloud storage";data integrity auditing;multi-replica;multi-cloud providers;"collaborative auditing"",""Cloud computing";Data integrity;Data models;Collaboration;Reliability;Termination of employment;"Pain"","""",""5"","""",""43"",""IEEE"",""25 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cloud Configuration Optimization for Recurring Batch-Processing Applications,""Y. Liu"; H. Xu;" W. C. Lau"",""Shanghai University, Shanghai, China"; University of Macau, Taipa, Macau;" Chinese University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2023"",""2023"",""34"",""5"",""1495"",""1507"",""Recognizing the diversity of Big Data analytic jobs, cloud providers offer a wide range of VM instance types or even clusters to cater for different use cases. The choice of cloud configurations can have a significant impact on the response time and running cost of batch-processing applications, which may need to be re-run regularly with cloud-scale resources. However, identifying the best cloud configuration with a low search cost is quite challenging due to i) the large and high-dimensional configuration space, ii) the time-varying cloud service cost (e.g., AWS Spot instances), and iii) job response time variation even given the same configuration. To tackle these challenges, we design and implement Accordia, a system that enables Adaptive Cloud Configuration Optimization for Recurring Data-Intensive Applications. By leveraging recent algorithmic advances in Gaussian Process UCB techniques, Accordia can unearth the cost-optimal configuration with a deadline constraint (i.e., maximum tolerated running time) under the time-varying cloud service cost. More importantly, Accordia manages to achieve a theoretical performance guarantee, sub-linearly increasing dynamic regret of the job completion cost. Using extensive trace-driven simulations and empirical measurements of our Kubernetes-based implementation, we demonstrate that Accordia can identify a near-cost-optimal configuration (i.e., within 10% of the optimum) after fewer than 20 runs from over 7000 candidate choices, which translates to a 2X-speedup and up to 17.9% cost-savings, when comparing to the state-of-the-art approach, CherryPick."",""1558-2183"","""",""10.1109/TPDS.2023.3246086"",""National Natural Science Foundation of China(grant numbers:62202284b)"; Shanghai Pujiang Program(grant numbers:22PJ1404000);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10048581"",""Big data analytics";cloud configuration;Gaussian-process UCB;"Kubernetes"",""Cloud computing";Costs;Optimization;Engines;Sparks;Big Data;"Monitoring"","""","""","""",""34"",""IEEE"",""17 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"COFFEE: Cross-Layer Optimization for Fast and Efficient Executions of Sinkhorn-Knopp Algorithm on HPC Systems,""C. Sun"; H. Luo; H. Jiang; J. Zhang;" K. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, USA; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA;" College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Jun 2023"",""2023"",""34"",""7"",""2167"",""2179"",""In this paper, we present COFFEE, cross-layer optimization for fast and efficient executions of the Sinkhorn-Knopp (SK) algorithm on HPC systems with clusters of compute nodes by exploring some architectural features of the system. By analyzing the performance of a typical implementation of the SK algorithm on such a system, a huge performance gap is observed between the row rescaling and column rescaling of the algorithm, where the latter requires much more time than the former. We also found that the costly MPI communication of the column rescaling seriously hinders the exploitation of parallelism. By observing and leveraging unique architectural characteristics across different system optimizations, such as column rescaling redesign, data blocking, micro-kernel design, enhanced intra-node and inter-node communication in MPI, etc., COFFEE is able to explore cross-layer optimization opportunities that enable fast and efficient execution of the SK algorithm. Our experimental results show that COFFEE provides up to 7.5X with an average of 2.0X performance improvement over the typical implementation on a single node, and up to 2.9X with an average of 1.6X performance improvement over the state-of-the-art MPI Allreduce algorithms on Tianhe-1 supercomputer."",""1558-2183"","""",""10.1109/TPDS.2023.3277915"",""National Natural Science Foundation of China(grant numbers:62102141)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129913"",""Data blocking";HPC system;MPI allreduce;micro-kernel design;"sinkhorn-knopp algorithm"",""Optimization";Program processors;Clustering algorithms;Machine learning algorithms;Supercomputers;Cross layer design;"Standards"","""","""","""",""41"",""IEEE"",""19 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Collaborative Intrusion Detection System for SDVN: A Fairness Federated Deep Learning Approach,""J. Cui"; H. Sun; H. Zhong; J. Zhang; L. Wei; I. Bolodurina;" D. He"",""Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China"; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, China; Faculty of Mathematics and Information Technologies, Orenburg State University, Orenburg, Russia;" School of Cyber Science and Engineering, Wuhan University, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2023"",""2023"",""34"",""9"",""2512"",""2528"",""With the continuous innovations and development in communication technology and intelligent transportation systems, a new generation of vehicular ad hoc networks (VANETs) has become increasingly popular, making VANET communication security increasingly important. An intrusion detection system (IDS) is an important tool for detecting network attacks and is an effective means of improving network security. However, existing IDSs encounter several problems involving inaccurate detections, low detection efficiencies, and incomplete detections owing to extensive changes in vehicle locations in VANETs. This study explores federated learning in software-defined VANETs and designs an efficient and accurate collaborative intrusion detection system (CIDS) model. The model utilizes the collaboration among local software-defined networks (SDNs) to jointly train the CIDS model without directly exchanging local network data flows to improve the expansibility and globality of IDSs. To reduce the model difference between different SDN clients and improve the detection accuracy, this study regards the prediction loss for each SDN client as an objective from the perspective of constrained multi-objective optimization. By optimizing a surrogate maximum function containing all the objectives, the method adopts two-stage gradient optimization to achieve Pareto optimality for SDN clients with the worst fairness constraint maximization performance. In addition, this study evaluates the training model using two open-source datasets and compares it with the latest methods. Experimental results reveal that the proposed model ensures local data privacy and demonstrates high accuracy and efficiency in detecting attacks and is thus superior to the current schemes."",""1558-2183"","""",""10.1109/TPDS.2023.3290650"",""National Natural Science Foundation of China(grant numbers:62202008,62272002)"; Excellent Youth Foundation of Anhui Scientific Committee(grant numbers:2108085J31); Natural Science Foundation of Anhui Province, China(grant numbers:2008085QF297,2208085QF196); University Synergy Innovation Program of Anhui Province(grant numbers:GXXT-2022-049); Foundation of Anhui Educational Committee(grant numbers:KJ2020A0037); Open Fund of Key Laboratory of Embedded System and Service Computing; Ministry of Education(grant numbers:ESSCKF 2022-04);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10177377"",""Federated deep learning";collaborative intrusion detection system;intelligent transportation system;convolutional neural network;"gradient optimization"",""Training";Intrusion detection;Security;Vehicular ad hoc networks;Collaboration;Data models;"Computational modeling"","""",""2"","""",""49"",""IEEE"",""10 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Comments on “IPPTS: An Efficient Algorithm for Scientific Workflow Scheduling in Heterogeneous Computing Systems”,""R. Devaraj";" A. Sarkar"",""SW-TEGRA, Nvidia Graphics, Bangalore, Karnataka, India";" Advanced Technology Development Centre, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Jan 2023"",""2023"",""34"",""3"",""810"",""811"",""IPPTS (Improved Predict Priority Task Scheduling) is a list scheduling algorithm that schedules task graphs on fully connected heterogeneous distributed systems, with an objective of minimizing the overall makespan (i.e., schedule length). With respect to the literature on list scheduling techniques for task graphs, IPPTS improves the task prioritization by considering the “out-degree” of a task. However, we have observed that the IPPTS algorithm contains an ambiguity which introduces the possibility of assigning higher priority to a task compared to its predecessors in a task graph. This priority inversion may lead to the generation of an incorrect schedule due to the violation of precedence-constraints among tasks. In this note, we first highlight this issue using a counter example. Then, we discuss two possible ways to fix the ambiguity in the algorithm."",""1558-2183"","""",""10.1109/TPDS.2022.3232326"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999473"",""List scheduling";scientific workflows;"task graphs"",""Task analysis";Program processors;Phase change materials;Schedules;Costs;Resource management;"Optimal scheduling"","""","""","""",""6"",""IEEE"",""27 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Communication Optimization Algorithms for Distributed Deep Learning Systems: A Survey,""E. Yu"; D. Dong;" X. Liao"",""College of Computer, National University of Defense Technology, Changsha, China"; College of Computer, National University of Defense Technology, Changsha, China;" College of Computer, National University of Defense Technology, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Nov 2023"",""2023"",""34"",""12"",""3294"",""3308"",""Deep learning's widespread adoption in various fields has made distributed training across multiple computing nodes essential. However, frequent communication between nodes can significantly slow down training speed, creating a bottleneck in distributed training. To address this issue, researchers are focusing on communication optimization algorithms for distributed deep learning systems. In this paper, we propose a standard that systematically classifies all communication optimization algorithms based on mathematical modeling, which is not achieved by existing surveys in the field. We categorize existing works into four categories based on the optimization strategies of communication: communication masking, communication compression, communication frequency reduction, and hybrid optimization. Finally, we discuss potential future challenges and research directions in the field of communication optimization algorithms for distributed deep learning systems."",""1558-2183"","""",""10.1109/TPDS.2023.3323282"",""National Key Research and Development Program of China(grant numbers:2022YFB4501702)"; National Natural Science Foundation of China(grant numbers:62302512); Excellent Youth Foundation of Hunan Province(grant numbers:2021JJ10050);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275049"",""Communication optimization algorithms";distributed computing;distributed deep learning;"parallel algorithms"",""Training";Parallel processing;Optimization;Computational modeling;Deep learning;Computer architecture;"Costs"","""","""","""",""126"",""IEEE"",""10 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Congestion Control for Datacenter Networks: A Control-Theoretic Approach,""D. Menikkumbura"; P. Taheri; E. Vanini; S. Fahmy; P. Eugster;" T. Edsall"",""Department of Computer Science, Purdue University, West Lafayette, IN, USA"; Datacenter Networking, Cisco Systems Inc, San Jose, CA, USA; Datacenter Networking, Cisco Systems Inc, San Jose, CA, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Faculty of Informatics, Universita della Svizzera Italiana (USI), Lugano, Switzerland;" Datacenter Networking, Cisco Systems Inc, San Jose, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Apr 2023"",""2023"",""34"",""5"",""1682"",""1696"",""In this article, we present RoCC, a robust congestion control approach for datacenter networks based on RDMA. RoCC leverages switch queue size as an input to a PI controller, which computes the fair data rate of flows in the queue. The PI parameters are self-tuning to guarantee stability, rapid convergence, and fair and near-optimal throughput in a wide range of congestion scenarios. Our simulation and DPDK implementation results show that RoCC can achieve up to $7\times$7× reduction in PFC frames generated under high load levels, compared to DCQCN. At the same time, RoCC can achieve $1.7-4.5\times$1.7-4.5× and $1.4-3.9\times$1.4-3.9× lower tail latency for long flows and $2.1-7\times$2.1-7× and $3.5-8.2\times$3.5-8.2× lower tail latency for short flows, compared to DCQCN and HPCC, respectively. We also find that RoCC does not require PFC. The functional components of RoCC can be efficiently implemented in P4 and FPGA-based switch hardware."",""1558-2183"","""",""10.1109/TPDS.2023.3259799"",""National Science Foundation(grant numbers:CCR-1618923,CNS-1717493)"; Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung(grant numbers:200021_192121,200021_197353); DFG center(grant numbers:1053);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10082870"",""Congestion control";datacenter;network programmability;"RDMA"",""Switches";Switching circuits;Convergence;Bandwidth;PI control;Throughput;"Hardware"","""","""","""",""48"",""CCBY"",""27 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Consistent Low Latency Scheduler for Distributed Key-Value Stores,""W. Jiang"; H. Li; Y. Yan; F. Ji; J. Huang; J. Wang;" T. Zhang"",""School of Computer Science, Engineering, Central South University, Changsha, China"; School of Computer Science, Engineering, Central South University, Changsha, China; School of Computer Science, Engineering, Central South University, Changsha, China; School of Computer Science, Engineering, Central South University, Changsha, China; School of Computer Science, Engineering, Central South University, Changsha, China; School of Computer Science, Engineering, Central South University, Changsha, China;" College of Computer Science, Technology, Nanjing University of Aeronautics, Astronautics, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Oct 2023"",""2023"",""34"",""12"",""3012"",""3027"",""Nowadays, the distributed key-value stores have become the basic building block for large-scale cloud applications. In large-scale distributed key-value stores, many key-value access operations, which will be processed in parallel on different servers, are usually generated for a single end-user request. Accordingly, the completion time of an end-user request is determined by the last completed key-value access operation. Scheduling the order of serving key-value access operations can effectively reduce the completion times of end requests, thereby improving the user experience. However, existing scheduling algorithms hardly achieve consistent low latency due to the following challenges: the large overhead of cooperating clients and servers, the time-varying load and performance of servers, the traffic distribution can be either heavy-tailed or light-tailed and both the mean and the tail completion time are expected to be low. In this paper, we formalize the problem of scheduling key-value access operations and show it is NP-hard. Furthermore, we heuristically design the distributed adaptive scheduler (DAS), which distributively combines the largest remaining processing time last and the shortest remaining process time first algorithms. Theoretical analysis shows that DAS is adaptive to the time-varying traffic and server performance and can achieve consistent low mean and tail latency regardless of traffic distributions. Extensive simulations show that DAS reduces the mean request completion time by $17 \! \sim \! 50\%$17∼50% with heavy-tailed traffic and $2 \! \sim 26 \! \%$2∼26% with light-tailed traffic, while keeping the smallest tail completion time, compared to the default first come first served algorithm. Moreover, DAS outperforms the existing Rein-SBF algorithm under various scenarios."",""1558-2183"","""",""10.1109/TPDS.2023.3315777"",""National Key Research and Development Project of China(grant numbers:2018YFB1702502)"; National Natural Science Foundation of China(grant numbers:61972421,62132007,62002165); National Natural Science Foundation of Hunan Province(grant numbers:2022JJ20078); Key Research and Development Program of Hunan Province of China(grant numbers:2022SK2107); Fundamental Research Funds for the Central Universities of Central South University in China(grant numbers:2022ZZTS0643); Science and Technology Innovation Program of Hunan Province(grant numbers:2023RC3047); High Performance Computing Center of Central South University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261443"",""Adaptive";distributed key-value stores;request completion time;"scheduling"",""Servers";Tail;Lightly-tailed distribution;Heavily-tailed distribution;Scheduling algorithms;Low latency communication;"Job shop scheduling"","""","""","""",""42"",""IEEE"",""22 Sep 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"CoopEdge+: Enabling Decentralized, Secure and Cooperative Multi-Access Edge Computing Based on Blockchain,""L. Yuan"; Q. He; S. Tan; B. Li; J. Yu; F. Chen;" Y. Yang"",""Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia"; National Engineering Research Center for Big Data Technology and System, Service Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia;" Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jan 2023"",""2023"",""34"",""3"",""894"",""908"",""Multi-access Edge Computing (MEC) has emerged as a new distributed computing paradigm for its ability to offer low-latency services to users. Suffering from constrained computational resources because of their limited physical sizes, edge servers usually cannot handle all the incoming compute tasks on time when they operate independently. Thus, they need to cooperate by peer-offloading. Incentive and trust are the two major challenges towards to cooperative computing among edge servers operating in a distrusted environment. Another specific challenge in the MEC environment is to facilitate incentive and trust in a decentralized manner. This article proposes CoopEdge+, a novel blockchain-based decentralized platform, to drive and support cooperative multi-access edge computing to tackle these challenges in a systematic manner. On CoopEdge+, an edge server can publish a compute task for other edge servers to contend for. A winner is selected from candidate edge servers as the task executor based on their reputation to perform the compute task. After that, CoopEdge+ employs a random leader election scheme to elect a task recorder without revealing its leadership until its consensus epoch. The task recorder will coordinate a consensus among edge servers to record the task executor's performance on blockchain. We implement CoopEdge+ based on Hyperledger fabric and evaluate it experimentally against a baseline implementation and three state-of-the-art implementations in a simulated MEC environment. The results validate the usefulness of CoopEdge+ and demonstrate its performance."",""1558-2183"","""",""10.1109/TPDS.2022.3231296"",""Australian Research Council Discovery Projects(grant numbers:DP180100212,DP200102491)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9996564"",""Blockchain";cooperative multi-access edge computing;distributed consensus;multi-access edge computing;"peer offloading"",""Task analysis";Servers;Internet;Low latency communication;Voting;Reliability;"Multi-access edge computing"","""",""2"","""",""66"",""IEEE"",""22 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Dap-FL: Federated Learning Flourishes by Adaptive Tuning and Secure Aggregation,""Q. Chen"; Z. Wang; J. Chen; H. Yan;" X. Lin"",""State Key Laboratory of Integrated Service Networks, School of Cyber Engineering, Xidian University, Xi’an, China"; State Key Laboratory of Integrated Service Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Cyber Engineering, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks, School of Cyber Engineering, Xidian University, Xi’an, China;" School of Computer Science, University of Guelph, Guelph, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""11 May 2023"",""2023"",""34"",""6"",""1923"",""1941"",""Federated learning (FL), an attractive and promising distributed machine learning paradigm, has sparked extensive interest in exploiting tremendous data stored on ubiquitous mobile devices. However, conventional FL suffers severely from resource heterogeneity, as clients with weak computational and communication capabilities may be unable to complete local training using the same local training hyper-parameters. In this article, we propose Dap-FL, a deep deterministic policy gradient (DDPG)-assisted adaptive FL system, in which local learning rates and local training epochs are adaptively adjusted by all resource-heterogeneous clients through locally deployed DDPG-assisted adaptive hyper-parameter selection schemes. Particularly, the rationality of the proposed hyper-parameter selection scheme is confirmed through rigorous mathematical proof. Besides, due to the thoughtlessness of security consideration of adaptive FL systems in previous studies, we introduce the Paillier cryptosystem to aggregate local models in a secure and privacy-preserving manner. Rigorous analyses show that the proposed Dap-FL system could protect clients’ private local models against chosen-plaintext attacks and chosen-message attacks in a widely used honest-but-curious participants and active adversaries security model. More importantly, through ingenious and extensive experiments, the proposed Dap-FL achieves higher model prediction accuracy than two state-of-the-art RL-assisted FL methods, i.e., 6.03% higher than DDPG-based FL and 7.85% higher than DQN-based FL. In addition, experimental results also show that the proposed Dap-FL achieves higher global model prediction accuracy and faster convergence rates than conventional FL, and the comprehensiveness of the adjusted local training hyper-parameters is validated."",""1558-2183"","""",""10.1109/TPDS.2023.3267897"",""National Natural Science Foundation of China(grant numbers:62172319,U19B200073)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10103633"",""Federated learning";deep reinforcement learning;deep deterministic policy gradient;adaptive training;"privacy preservation"",""Adaptation models";Training;Predictive models;Security;Adaptive systems;Computational modeling;"Data models"","""","""","""",""54"",""IEEE"",""17 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Data-Centric Client Selection for Federated Learning Over Distributed Edge Networks,""R. Saha"; S. Misra; A. Chakraborty; C. Chatterjee;" P. K. Deb"",""Indian Institute of Technology, Kharagpur, West Bengal, India"; Indian Institute of Technology, Kharagpur, West Bengal, India; Indian Institute of Technology, Kharagpur, West Bengal, India; Department of Agriculture and Food Engineering, Indian Institute of Technology, Kharagpur, West Bengal, India;" Indian Institute of Technology, Kharagpur, West Bengal, India"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Dec 2022"",""2023"",""34"",""2"",""675"",""686"",""This work presents an efficient data-centric client selection approach, named DICE, to enable federated learning (FL) over distributed edge networks. Prior research focused on assessing the computation and communication ability of the client devices for selection in FL. On-device data quality, in terms of data volume and heterogeneity, across these distributed devices is largely overlooked. The obvious outcome is the selection of an improper subset of clients with poor-quallity data, which inevitably results in an inefficient trained model. With an aim to address this problem, in this work, we design DICE which prioritizes the data quality of the client devices in the selection phase, in addition to their computation and communication abilities, to improve the accuracy of FL. Additionally, in DICE, we introduce the assistance of vicinal edge devices to account for the lack of computation or communication abilities in certain devices without violating the privacy-preserving guarantees of FL. Towards this aim, we propose a scheme to decide the optimal edge device, in terms of latency and workload, to be selected as the helper device. The experimental results show that DICE improves convergence speed for a given level of model accuracy. Further, the simulation results show that DICE reduces delay by at least 16%, energy consumption by at least 17%, and packet loss by at least 55% compared to the existing benchmarks while prioritizing the on-device data quality across clients."",""1558-2183"","""",""10.1109/TPDS.2022.3217271"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930629"",""Client selection";distributed edge networks;federated learning;fog computing;"offloading"",""Computational modeling";Training;Servers;Data integrity;Data models;Performance evaluation;"Data privacy"","""",""8"","""",""41"",""IEEE"",""26 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"DeepBoot: Dynamic Scheduling System for Training and Inference Deep Learning Tasks in GPU Cluster,""Z. Chen"; X. Zhao; C. Zhi;" J. Yin"",""College of Computer Science and Technology, Zhejiang University, Hangzhou, China"; School of Software Technology, Zhejiang University, Ningbo, China; School of Software Technology, Zhejiang University, Ningbo, China;" College of Computer Science and Technology, Zhejiang University, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Jul 2023"",""2023"",""34"",""9"",""2553"",""2567"",""Deep learning tasks (DLT) include training and inference tasks, where training DLTs have requirements on minimizing average job completion time (JCT) and inference tasks need sufficient GPUs to meet real-time performance. Unfortunately, existing work separately deploys multi-tenant training and inference GPU cluster, leading to the high JCT of training DLTs with limited GPUs when the inference cluster is under insufficient GPU utilization due to the periodic inference workload. DeepBoot solves the challenges by utilizing idle GPUs in the inference cluster for the training DLTs. Specifically, 1) DeepBoot designs adaptive task scaling (ATS) algorithm to allocate GPUs in the training and inference clusters for training DLTs and minimize the performance loss when reclaiming inference GPUs. 2) DeepBoot implements auto-fast elastic (AFE) training based on Pollux to reduce the restart overhead by inference GPU reclaiming. Our implementation on the testbed and large-scale simulation in Microsoft deep learning workload shows that DeepBoot can achieve 32% and 38% average JCT reduction respectively compared with the scheduler without utilizing idle GPUs in the inference cluster."",""1558-2183"","""",""10.1109/TPDS.2023.3293835"",""Key Research and Development Program of China(grant numbers:2022YFF0902702)"; National Natural Science Foundation of China(grant numbers:61825205); Key Research and Development Program of Zhejiang Provence, China(grant numbers:2021C01017);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10179169"",""Deep learning system";distributed training;elastic deep learning;"GPU cluster scheduling"",""Training";Task analysis;Graphics processing units;Resource management;Deep learning;Costs;"Load modeling"","""","""","""",""54"",""IEEE"",""11 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"DELICIOUS: Deadline-Aware Approximate Computing in Cache-Conscious Multicore,""S. Saha"; S. Chakraborty; S. Agarwal; R. Gangopadhyay; M. Själander;" K. McDonald-Maier"",""Department of Computer Science, University of Huddersfield, Huddersfield, U.K."; Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway; School of Informatics, University of Edinburgh, Edinburgh, U.K.; Moscow Institute of Physics and Technology, Dolgoprudny, Russian Federation; Department of Computer Science, Norwegian University of Science and Technology, Trondheim, Norway;" School of Computer Science and Electronic Engineering, University of Essex, Colchester, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""9 Jan 2023"",""2023"",""34"",""2"",""718"",""733"",""Enhancing result-accuracy in approximate computing (AC) based real-time systems, without violating power constraints of the underlying hardware, is a challenging problem. Execution of such AC real-time applications can be split into two parts: (i) the mandatory part, execution of which provides a result of acceptable quality, followed by (ii) the optional part, that can be executed partially or fully to refine the initially obtained result in order to increase the result-accuracy, without violating the time-constraint. This article introduces DELICIOUS, a novel hybrid offline-online scheduling strategy for AC real-time dependent tasks. By employing an efficient heuristic algorithm, DELICIOUS first generates a schedule for a task-set with an objective to maximize the results-accuracy, while respecting system-wide constraints. During execution, DELICIOUS then introduces a prudential cache resizing that reduces temperature of the adjacent cores, by generating thermal buffers at the turned off cache ways. DELICIOUS further trades off this thermal benefits by enhancing the processing speed of the cores for a stipulated duration, called V/F Spiking, without violating the power budget of the core, to shorten the execution length of the tasks. This reduced runtime is exploited either to enhance result-accuracy by dynamically adjusting the optional part, or to reduce temperature by enabling sleep mode at the cores. While surpassing the prior art, DELICIOUS offers 80% result-accuracy with its scheduling strategy, which is further enhanced by 8.3% in online, while reducing runtime peak temperature by 5.8°C on average, as shown by benchmark based evaluation on a 4-core based multicore."",""1558-2183"","""",""10.1109/TPDS.2022.3228751"",""Engineering and Physical Sciences Research Council(grant numbers:EP/X015955/1,EP/V000462/1)"; Marie Curie Individual Fellowship(grant numbers:898296);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9982385"",""Real-time systems";approximate computing;thermal management;dead block;caches resizing;"TDP"",""Task analysis";Real-time systems;Quality of service;Processor scheduling;Runtime;Schedules;"Optimal scheduling"","""",""1"","""",""49"",""CCBY"",""12 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Design and Implementation of Deep Learning 2D Convolutions on Modern CPUs,""V. Kelefouras";" G. Keramidas"",""Department of Computing, Plymouth University, Plymouth, U.K.";" Department of Computer Engineering & Informatics, Aristoteleio Panepistemio Thessalonikes, Thessaloniki, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2023"",""2023"",""34"",""12"",""3104"",""3116"",""In this article, a new method is provided for accelerating the execution of convolution layers in Deep Neural Networks. This research work provides the theoretical background to efficiently design and implement the convolution layers on x86/x64 CPUs, based on the target layer parameters, quantization level and hardware architecture. The proposed work is general and can be applied to other processor families too, e.g., Arm. The proposed work achieves high speedup values over the state of the art, which is Intel oneDNN library, by applying compiler optimizations, such as vectorization, register blocking and loop tiling, in a more efficient way. This is achieved by developing an analytical modelling approach for finding the optimization parameters. A thorough experimental evaluation has been applied on two Intel CPU platforms, for DenseNet-121, ResNet-50 and SqueezeNet (including 112 different convolution layers), and for both FP32 and int8 input/output tensors (quantization). The experimental results show that the convolution layers of the aforementioned models are executed from $x1.1$x1.1 up to $x7.2$x7.2 times faster."",""1558-2183"","""",""10.1109/TPDS.2023.3322037"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271698"",""Deep neural networks";convolution;oneDNN;optimization;analytical model;vectorization;register blocking;"loop tiling"",""Optimization";Layout;Convolutional codes;Quantization (signal);Tensors;Training;"Registers"","""","""","""",""26"",""IEEE"",""4 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Design of a Quantization-Based DNN Delta Compression Framework for Model Snapshots and Federated Learning,""H. Jin"; D. Wu; S. Zhang; X. Zou; S. Jin; D. Tao; Q. Liao;" W. Xia"",""Harbin Institute of Technology, Shenzhen, Guangdong, China"; Harbin Institute of Technology, Shenzhen, Guangdong, China; Harbin Institute of Technology, Shenzhen, Guangdong, China; Harbin Institute of Technology, Shenzhen, Guangdong, China; Indiana University Bloomington, Pullman, WA, USA; Indiana University Bloomington, Pullman, WA, USA; Harbin Institute of Technology, Shenzhen, Guangdong, China;" Harbin Institute of Technology, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Jan 2023"",""2023"",""34"",""3"",""923"",""937"",""Deep neural networks (DNNs) have achieved remarkable success in many fields. However, large-scale DNNs also bring storage costs when storing snapshots for preventing clusters’ frequent failures or incur significant communication overheads when transmitting DNNs in the Federated Learning (FL). Recently, several approaches, such as Delta-DNN and LC-Checkpoint, aim to reduce the size of DNNs’ snapshot storage by compressing the difference between two neighboring versions of the DNNs (a.k.a., delta). However, we observe that existing approaches, applying traditional global lossy quantization techniques in DNN's delta compression, can not fully exploit the data similarity since the parameters’ value ranges vary among layers. To fully explore the similarity of the delta model and improve the compression ratio, we propose a quantization-based local-sensitive delta compression approach, named QD-Compressor, by developing a layer-based local-sensitive quantization scheme and error feedback mechanism. Specifically, the quantizers and number of quantization bits are adaptive among layers based on the value distribution and weighted entropy of the delta's parameters. To avoid quantization error degrading the performance of the restored model, an alternative error feedback mechanism is designed to dynamically correct the quantization error during the training process. Experiments on multiple popular DNNs and datasets show that QD-Compressor obtains a higher 7×-40× compression ratio in the model snapshot compression scenario than the state-of-the-art approaches. Additionally, QD-Compressor achieves an 11×-15× compression ratio to the residual model of the Federated Learning compression scenario."",""1558-2183"","""",""10.1109/TPDS.2022.3230840"",""National Key-Research and Development Program of China(grant numbers:2020YFB2104003)"; National Natural Science Foundation of China(grant numbers:61972441); Shenzhen Science and Technology Innovation Program(grant numbers:RCYX20210609104510007,JCYJ20200109113427092,GXWD20201230155427003-20200821172511002); Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies(grant numbers:2022B1212010005); National Science Foundation(grant numbers:OAC-2034169/2303820,OAC-2042084/2303064);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018182"",""Neural networks";quantization;delta compression;snapshot;"distribution learning"",""Quantization (signal)";Neural networks;Training;Computational modeling;Data models;Data compression;"Federated learning"","""",""2"","""",""57"",""IEEE"",""18 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Detailed Modeling of Heterogeneous and Contention-Constrained Point-to-Point MPI Communication,""A. Thune"; S. -A. Reinemo; T. Skeie;" X. Cai"",""Simula Research Laboratory, Oslo, Norway"; Simula Metropolitan Centre for Digital Engineering, Oslo, Norway; Simula Research Laboratory, Oslo, Norway;" Simula Research Laboratory, Oslo, Norway"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Mar 2023"",""2023"",""34"",""5"",""1580"",""1593"",""The network topology of modern parallel computing systems is inherently heterogeneous, with a variety of latency and bandwidth values. Moreover, contention for the bandwidth can exist on different levels when many processes communicate with each other. Many-pair, point-to-point MPI communication is thus characterized by heterogeneity and contention, even on a cluster of homogeneous multicore CPU nodes. To get a detailed understanding of the individual communication cost per MPI process, we propose a new modeling methodology that incorporates both heterogeneity and contention. First, we improve the standard max-rate model to better quantify the actually achievable bandwidth depending on the number of MPI processes in competition. Then, we make a further extension that more detailedly models the bandwidth contention when the competing MPI processes have different numbers of neighbors, with also non-uniform message sizes. Thereafter, we include more flexibility by considering interactions between intra-socket and inter-socket messaging. Through a series of experiments done on different processor architectures, we show that the new heterogeneous and contention-constrained performance models can adequately explain the individual communication cost associated with each MPI process. The largest test of realistic point-to-point MPI communication involves 8,192 processes and in total 2,744,632 simultaneous messages over 64 dual-socket AMD Epyc Rome compute nodes connected by InfiniBand, for which the overall prediction accuracy achieved is 84%."",""1558-2183"","""",""10.1109/TPDS.2023.3253881"",""Norges Forskningsråd(grant numbers:237898)"; European High-Performance Computing Joint Undertaking(grant numbers:956213); Norges Forskningsråd(grant numbers:329017); Experimental Infrastructure for the Exploration of Exascale Computing; Norges Forskningsråd(grant numbers:270053); Sigma2 - the National Infrastructure for High Performance Computing; Data Storage in Norway;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10064025"",""Intra-node communication";performance modeling;"point-to-point MPI communication"",""Bandwidth";Sockets;Benchmark testing;Size measurement;Protocols;Multicore processing;"Computational modeling"","""","""","""",""26"",""CCBY"",""8 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Differentiated Consistency for Worldwide Gossips,""D. Frey"; A. Mostefaoui; M. Perrin; P. -L. Roman;" F. Taïani"",""CNRS, Inria, IRISA, Univ Rennes, Rennes, France"; LS2N, Université de Nantes, Nantes, France; LS2N, Université de Nantes, Nantes, France; École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland;" CNRS, Inria, IRISA, Univ Rennes, Rennes, France"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""1"",""15"",""Eventual consistency is a consistency model that favors liveness over safety. It is often used in large-scale distributed systems where models ensuring a stronger safety incur performance that are too low to be deemed practical. Eventual consistency tends to be uniformly applied within a system, but we argue a demand exists for differentiated eventual consistency, e.g. in blockchain systems. We propose update-query consistency with primaries and secondaries (UPS) to address this demand. UPS is a novel consistency mechanism that works in pair with our novel two-phase epidemic broadcast protocol gossip primary-secondary (GPS) to offer differentiated eventual consistency and delivery speed. We propose two complementary analyses of the broadcast protocol: a continuous analysis and a discrete analysis based on compartmental models used in epidemiology. Additionally, we propose the formal definition of a scalable consistency metric to measure the consistency trade-off at runtime. We evaluate UPS in two simulated worldwide settings: a one-million-node network and a network emulating that of the Ethereum blockchain. In both settings, UPS reduces inconsistencies experienced by a majority of the nodes and reduces the average message latency for the remaining nodes."",""1558-2183"","""",""10.1109/TPDS.2022.3209150"",""French ANR(grant numbers:O'Browser ANR-16-CE25-0005-03,ByBloS ANR-20-CE25-0002-01)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9900441"",""Distributed system";epidemic protocol;eventual consistency;"blockchain"",""Uninterruptible power systems";Clocks;Blockchains;Protocols;Data structures;Delays;"Convergence"","""","""","""",""91"",""IEEE"",""23 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Dissecting Tensor Cores via Microbenchmarks: Latency, Throughput and Numeric Behaviors,""W. Sun"; A. Li; T. Geng; S. Stuijk;" H. Corporaal"",""Electronic System Group, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands"; Physical and Computational Sciences Directorate, Pacific Northwest National Laboratory, Richland, WA, USA; Physical and Computational Sciences Directorate, Pacific Northwest National Laboratory, Richland, WA, USA; Electronic System Group, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands;" Electronic System Group, Eindhoven University of Technology, Eindhoven, AZ, The Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""246"",""261"",""Tensor Cores have been an important unit to accelerate Fused Matrix Multiplication Accumulation (MMA) in all NVIDIA GPUs since Volta Architecture. To program Tensor Cores, users have to use either legacy wmma APIs or current mma APIs. Legacy wmma APIs are more easy-to-use but can only exploit limited features and power of Tensor Cores. Specifically, wmma APIs support fewer operand shapes and can not leverage the new sparse matrix multiplication feature of the newest Ampere Tensor Cores. However, the performance of current programming interface has not been well explored. Furthermore, the computation numeric behaviors of low-precision floating points (TF32, BF16, and FP16) supported by the newest Ampere Tensor Cores are also mysterious. In this paper, we explore the throughput and latency of current programming APIs. We also intuitively study the numeric behaviors of Tensor Cores MMA and profile the intermediate operations including multiplication, addition of inner product, and accumulation. All codes used in this work can be found in https://github.com/sunlex0717/DissectingTensorCores."",""1558-2183"","""",""10.1109/TPDS.2022.3217824"",""Nederlandse Organisatie voor Wetenschappelijk Onderzoek(grant numbers:ZERO-ARM P3)"; U.S. Department of Energy(grant numbers:DE-AC05-76RL01830);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931992"",""GPU";tensor cores;numeric profiling;ampere;turing;"microbenchmark"",""Tensors";Graphics processing units;Programming;Hidden Markov models;Computer architecture;Behavioral sciences;"Registers"","""",""3"","""",""48"",""IEEE"",""28 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Distributed Approaches to Butterfly Analysis on Large Dynamic Bipartite Graphs,""T. Weng"; X. Zhou; K. Li; K. -L. Tan;" K. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; School of Computing, National University of Singapore, Singapore;" College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""431"",""445"",""Tip decomposition has a pivotal role in mining cohesive subgraphs in bipartite graphs by computing the tip number of each vertex in accordance with the non-trivial motif butterfly ((2,2)-biclique). It has been a popular research topic with applications in document clustering, spam group detection, and analysis of affiliation networks. In such applications, the graphs are not only large, but they evolve quickly with new edges being continuously added/deleted. While existing centralized techniques could solve the tip decomposition problem for static bipartite graphs, they are not efficient for maintaining the tip numbers of vertices on large-scale graphs. In this paper, we study butterfly analysis problems on bipartite graphs in a distributed environment with the vertex-centric model. We first extend a centralized butterfly counting algorithm to a distributed version, called DBCA. An ingenious message aggregation strategy is designed to reduce massive redundant messaging and avoid the memory overflow problem while processing large-scale graphs. Based on the results of DBCA, we develop a distributed tip decomposition algorithm (DTDA) to get the tip number of each vertex in parallel. Finally, to maintain the tip numbers of vertices efficiently while graphs evolve over time, we explore a distributed tip maintenance algorithm (DTMA) along with a novel task-split strategy. Specifically, for an updated edge (insertion/deletion), several sub-tasks will be generated in line with the topology structure of the original bipartite graph. To our best knowledge, this is the first study to process the butterfly analysis problems in a distributed environment. In addition, comprehensive experiments have been conducted on real-world bipartite graphs. The experiment results demonstrate that our proposed algorithms are efficient and scalable."",""1558-2183"","""",""10.1109/TPDS.2022.3221821"",""National Key R&D Program of China(grant numbers:2020YFB2104000)"; National Natural Science Foundation of China(grant numbers:62172146,62172157,62102143); Natural Science Foundation of Hunan Province(grant numbers:2022JJ30009); Open Research Projects of Zhejiang Lab(grant numbers:2021KD0AB02); Postgraduate Scientific Resarch Innovation Project of Hunan Province(grant numbers:QL20210096);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9947316"",""Bipartite graph";butterfly counting;distributed algorithm;tip decomposition;"tip maintenance"",""Bipartite graph";Task analysis;Maintenance engineering;Computer science;Approximation algorithms;Toy manufacturing industry;"Topology"","""",""8"","""",""27"",""IEEE"",""14 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Distributed Encoding and Updating for SAZD Coded Distributed Training,""M. Dai"; J. Yuan; Q. Huang; X. Lin;" H. Wang"",""Guangdong Provincial Engineering Center for Ubiquitous Computing and Intelligent Networking, College of Electronic and Information Engineering, Shenzhen University, Shenzhen, Guangdong, China"; Guangdong Provincial Engineering Center for Ubiquitous Computing and Intelligent Networking, College of Electronic and Information Engineering, Shenzhen University, Shenzhen, Guangdong, China; Guangdong Provincial Engineering Center for Ubiquitous Computing and Intelligent Networking, College of Electronic and Information Engineering, Shenzhen University, Shenzhen, Guangdong, China; Guangdong Provincial Engineering Center for Ubiquitous Computing and Intelligent Networking, College of Electronic and Information Engineering, Shenzhen University, Shenzhen, Guangdong, China;" Shenzhen Institute of Information Technology, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Jun 2023"",""2023"",""34"",""7"",""2124"",""2137"",""Linear combination (LC) based coded distributed computing (CDC) suffers from the problem of poor numerical stability. Therefore, LC-CDC based model parallel (MP) training for a deep nueral network (DNN) may have poor accuracy. To enhance accuracy, we propose to replace LC by shift-and-addition (SA) and replace matrix inversion by zigzag decoding (ZD) in the encoding and decoding process of each layer, respectively, and call the scheme Naive SAZD-CDC based MP training (N-SAZD-CDC-MP-T). However, N-SAZD-CDC-MP-T encounters the problem of bottleneck at the master node, which is caused by frequent encoding/decoding at the master node and frequent huge volume of data delivery between master and worker node. This bottleneck problem may pull down the training speed significantly. To alleviate this bottleneck problem, we further design an enhanced version, by offloading certain processing from master node to distributed encoding and updating (DEU) at the worker nodes and call it DEU-SAZD-CDC-MP-T. A proof that DEU-SAZD-CDC-MP-T automatically maitains the code structure during each iteration is provided. Extensive numerical studies show that the prediction accuracy of SAZD-CDC-MP-T improves significantly over that of Poly (which is representative of LC) based scheme. In addition, the training speed of DEU-SAZD-CDC-MP-T over N-SAZD-CDC-MP-T is improved significantly."",""1558-2183"","""",""10.1109/TPDS.2023.3276888"",""National Natural Science Foundation of China(grant numbers:62071304)"; Natural Science Foundation of Guangdong Province(grant numbers:2020A1515010381,2022A1515011219); Basic Research foundation of Shenzhen City(grant numbers:20200826152915001,20220809155455002,20190808120415286); Natural Science Foundation of Shenzhen University(grant numbers:00002501); Xinjiang Uygur Autonomous Region Natural Science Foundation General Project(grant numbers:2022D01A237); Kashi Regional Science and Technology Plan Project(grant numbers:KS2022084);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10130495"",""Coded distributed computing (CDC)";distributed encoding and updating (DEU);distributed training;shift-and-addition;"zigzag decoding"",""Training";Encoding;Decoding;Data models;Numerical stability;Computational modeling;"Numerical models"","""","""","""",""48"",""IEEE"",""22 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Distributed Multicast Traffic Engineering for Multi-Domain Software-Defined Networks,""S. -H. Chiang"; C. -H. Wang; D. -N. Yang; W. Liao;" W. -T. Chen"",""Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan"; Institute of Information Science, Academia Sinica, Taipei, Taiwan; Institute of Information Science and the Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan;" Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""446"",""462"",""Previous research on SDN multicast traffic engineering mainly focused on intra-domain optimization. However, the main traffic on the Internet is inter-domain, and the selection of border nodes and sharing of network information between domains are usually distributed but ignored in previous works. In this article, we explore multi-domain online distributed multicast traffic engineering (MODMTE). To effectively solve MODMTE, we first prove that MODMTE is inapproximable within $|D_{\max }|$|Dmax|, which indicates that it is impossible to find any algorithm with a ratio better than $|D_{\max }|$|Dmax| for MODMTE, and $|D_{\max }|$|Dmax| is the maximum number of destinations for a multicast tree. Then, we design a $|D_{\max }|$|Dmax|-competitive distributed algorithm with the ideas of Domain Tree, Dual Candidate Forest Construction, and Forest Rerouting to achieve the tightest performance bound for MODMTE. Experiments on a real SDN with YouTube traffic manifest that the proposed distributed algorithm can reduce more than 30% of the total cost of bandwidth consumption and rule updates for multicast tree rerouting compared with the state-of-the-art algorithms."",""1558-2183"","""",""10.1109/TPDS.2022.3205219"",""Ministry of Science and Technology, Taiwan(grant numbers:MOST 109-2221-E-007-077-MY3,110-2223-E002-004)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9903079"",""Competitive ratio";distributed algorithm;"multi-domain SDN"",""Forestry";Costs;Bandwidth;Unicast;Routing;Distributed algorithms;"Network topology"","""",""1"","""",""66"",""IEEE"",""26 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Divide&Content: A Fair OS-Level Resource Manager for Contention Balancing on NUMA Multicores,""C. Bilbao"; J. C. Saez;" M. Prieto-Matias"",""Facultad de Informática, Complutense University of Madrid, Madrid, Spain"; Facultad de Informática, Complutense University of Madrid, Madrid, Spain;" Facultad de Informática, Complutense University of Madrid, Madrid, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Sep 2023"",""2023"",""34"",""11"",""2928"",""2945"",""Chip multicore processors (CMPs) constitute the cherry-picked architecture for high-performance servers employed in supercomputers and cloud datacenters. In the last few years, Non-Uniform Memory Access (NUMA) multicore systems have become the dominant choice in these domains. Regardless of the technology advances enabling to pack an increasing number of cores and bigger caches on the same chip, contention for shared resources still represents an important challenge for the system software. Cores in CMPs typically share multiple resources, such as the last-level cache (LLC) or a DRAM controller. The competition for the usage of these resources leads to uneven performance degradation across co-running applications. Previous research has demonstrated that contention effects on CMPs can be mitigated via smart partitioning of the LLC or by distributing threads across groups of cores so as to even out the degree of competition on multiple LLCs or memory nodes. However, most existing resource-management strategies fail to effectively combine both contention-mitigating techniques, thus providing suboptimal results on NUMA multicores. In this paper, we analyze how to best combine these techniques to improve system-wide fairness, and, based on the conclusions of our analysis, propose a fair OS-level NUMA-aware resource manager that leverages dynamic contention-aware thread-to-socket mappings and cache-partitioning. We implemented our resource manager in the Linux kernel and assessed its effectiveness on a real dual-socket system featuring Intel Skylake processors. Our results show that it reduces unfairness by more than 17% on average compared to Linux and a state-of-the-art NUMA-aware resource manager."",""1558-2183"","""",""10.1109/TPDS.2023.3309999"",""Spanish MCIN(grant numbers:PID2021-126576NB-I00,MCIN/AEI/10.13039/501100011033)"; ERDF A way of making Europe; Comunidad de Madrid(grant numbers:S2018/TCS-4423);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234605"",""Multicore processors";NUMA;cache-partitioning;fairness;linux kernel;resource management;"operating system"",""Multicore processing";Linux;Degradation;Message systems;Throughput;Resource management;"Measurement"","""","""","""",""47"",""CCBYNCND"",""30 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"DOPpler: Parallel Measurement Infrastructure for Auto-Tuning Deep Learning Tensor Programs,""D. Borowiec"; G. Yeung; A. Friday; R. Harper;" P. Garraghan"",""Lancaster University, Lancaster, U.K."; Lancaster University, Lancaster, U.K.; Lancaster University, Lancaster, U.K.; Lancaster University, Lancaster, U.K.;" Lancaster University, Lancaster, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""8 Jun 2023"",""2023"",""34"",""7"",""2208"",""2220"",""The heterogeneity of Deep Learning models, libraries, and hardware poses an important challenge for improving model inference performance. Auto-tuners address this challenge via automatic tensor program optimization towards a target-device. However, auto-tuners incur a substantial time cost to complete given their design necessitates performing tensor program candidate measurements serially within an isolated target-device to minimize latency measurement inaccuracy. In this article we propose DOPpler, a parallel auto-tuning measurement infrastructure. DOPpler allows for considerable auto-tuning speedup over conventional approaches whilst maintaining high-quality tensor program optimization. DOPpler accelerates the auto-tuning process by proposing a parallel execution engine to efficiently execute candidate tensor programs in parallel across the CPU-host and GPU target-device, and overcomes measurement inaccuracy by introducing a high-precision on-device measurement technique when measuring tensor program kernel latency. DOPpler is designed to automatically calculate the optimal degree of parallelism to provision fast and accurate auto-tuning for different tensor programs, auto-tuners and target-devices. Experiment results show that DOPpler reduces total auto-tuning time by 50.5% on average whilst achieving optimization gains equivalent to conventional auto-tuning infrastructure."",""1558-2183"","""",""10.1109/TPDS.2023.3279233"",""Engineering and Physical Sciences Research Council(grant numbers:EP/V007092/1)"; Leverhulme Trust Doctoral Scholarships programme in Material Social Futures(grant numbers:DS-2017-036);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10132048"",""Deep learning";adaptive systems;performance evaluation;parallel processing;"program auto-tuning"",""Tensors";Graphics processing units;Kernel;Doppler effect;Optimization;Computational modeling;"Parallel processing"","""","""","""",""45"",""IEEE"",""23 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"DRFL: Federated Learning in Diabetic Retinopathy Grading Using Fundus Images,""N. J. Mohan"; R. Murugan; T. Goel;" P. Roy"",""Department of Electronics and Communication Engineering, Bio-Medical Imaging Laboratory, National Institute of Technology Silchar, Silchar, Assam, India"; Department of Electronics and Communication Engineering, Bio-Medical Imaging Laboratory, National Institute of Technology Silchar, Silchar, Assam, India; Department of Electronics and Communication Engineering, Bio-Medical Imaging Laboratory, National Institute of Technology Silchar, Silchar, Assam, India;" Department of Ophthalmology, Silchar Medical College and Hospital, Silchar, Assam, India"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1789"",""1801"",""Diabetic retinopathy (DR) is a complication of diabetic Mellitus, developing retinal lesions that impair vision. The DR detection in the early stages avoids permanent vision loss. The treatments provide relief, and the vision loss due to DR is irreversible. The manual grading of DR is time-consuming and prone to human errors. The other real-time problem is exchanging patient fundus image information with hospitals worldwide while upholding the organisations’ privacy concerns. When training a deep learning (DL) network, two critical factors to keep in mind are creating a collaborative platform and protecting patient data privacy. Therefore, an automated DR detection technique is required while protecting patient data and privacy. In this work, we propose a novel DR severity grading technique based on Federated Learning (FL), a recent advancement in DL called DRFL. FL is a new research paradigm that allows DL models to be trained collectively without disclosing clinical information. In DRFL, we combined the Federated averaging (FedAvg) technique and the median of the categorical cross-entropy loss. Since in comparison to FedAvg, the median cross-entropy is better suited for either under-fitted or over-fitted clients. Also, we propose a novel central server that extracts multi-scale features from the fundus images to identify small lesions present in the fundus image. In this work, we consider five clients holding different preprocessed fundus images collected from publicly available databases such as MESSIDOR-2, IDRiD, Kaggle, and a local database collected from Silchar Medical College and Hospital. The proposed model obtained an accuracy of 98.6%, specificity of 99.3%, precision of 97.25%, and an F1 score of 97.5%, which are better results than other techniques."",""1558-2183"","""",""10.1109/TPDS.2023.3264473"",""Ministry of Education, India"; National Institute of Technology, Silchar;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093080"",""Diabetic retinopathy";deep learning;fundus image;federated learning;preprocessing;"retina"",""Feature extraction";Databases;Lesions;Biomedical imaging;Computational modeling;Diabetes;"Retina"","""",""2"","""",""40"",""IEEE"",""5 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"DRONE: An Efficient Distributed Subgraph-Centric Framework for Processing Large-Scale Power-law Graphs,""S. Zhang"; Z. Jiang; X. Hou; M. Li; M. Yuan;" H. You"",""State Key Lab of Processors, Institute of Computing Technology, CAS, Beijing, China"; State Key Lab of Processors, Institute of Computing Technology, CAS, Beijing, China; State Key Lab of Processors, Institute of Computing Technology, CAS, Beijing, China; State Key Lab of Processors, Institute of Computing Technology, CAS, Beijing, China; School of Computer Science, Wuhan University, Wuhan, Hubei, China;" State Key Lab of Processors, Institute of Computing Technology, CAS, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""463"",""474"",""Nowadays, the ever-increasing volume of graph-structured data such as social networks, graph databases and knowledge graphs requires to be processed efficiently and scalably. These natural graphs commonly found in the real world have highly skewed power-law degree distribution and are called power-law graphs. The subgraph-centric programming model is a promising approach applied in many state-of-the-art distributed graph computing frameworks. However, the performance of subgraph-centric frameworks is limited when processing large-scale power-law graphs. When deployed to the subgraph-centric framework, existing graph partitioning algorithms are not suitable for power-law graphs. In this paper, we present a novel distributed graph computing framework, DRONE (Distributed gRaph cOmputiNg Engine), which leverages the subgraph-centric model and the vertex-cut graph partitioning strategy. DRONE also supports the fault tolerance mechanism to accommodate the increasing scale of machines with negligible overhead (6.48% on average). We further study the execution workflow of DRONE and propose an efficient and balanced graph partition algorithm (EBV) for DRONE. Experiments show that DRONE reduces the running time on real-world graphs by 25.6%, on average, compared to the state-of-the-art distributed graph computing frameworks. In addition, the EBV graph partition algorithm reduces the replication factor by at least 21.8% than other self-based partition algorithms. Our results indicate that DRONE has excellent potential in processing large-scale power-law graphs."",""1558-2183"","""",""10.1109/TPDS.2022.3223068"",""National Natural Science Foundation of China(grant numbers:41930110,61872272)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954601"",""Fault tolerance";graph partition;large-scale power-law graph;parallel graph computation;"subgraph-centric model"",""Drones";Partitioning algorithms;Computational modeling;Fault tolerant systems;Fault tolerance;Synchronization;"Engines"","""",""2"","""",""46"",""IEEE"",""18 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"EESaver: Saving Energy Dynamically for Green Multi-Access Edge Computing,""G. Cui"; Q. He; X. Xia; F. Chen;" Y. Yang"",""Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia"; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computing Technologies, RMIT University, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia;" Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Jun 2023"",""2023"",""34"",""7"",""2155"",""2166"",""With the rollout of the 5G network around the globe, a massive number of edge servers have been deployed to host online applications demanding low service latency for users. These edge servers constitute multi-access edge computing (MEC) systems. Running 24/7, edge servers consume tremendous energy and take up a great part of global carbon emissions. The edge energy-saving (EES) problem is needed to facilitate energy-efficient edge resource provisions. Unfortunately, existing energy-saving approaches designed for data centers are becoming impractical. First, edge servers are used to provide services to a specific geographical area. Its energy utilization is impacted by the temporal distribution of users within its coverage. Second, a user could be accommodated by any of its neighbor edge servers. Third, it is possible to activate and deactivate individual physical machines that facilitate an edge server as needed. Thus, EES is designed to save the system energy of physical machines in a long term by serving the users over time. EES problem has been formulated systematically and its problem hardness has been analyzed theoretically, then we propose EESaver (Edge Energy Saver) for formulating EES strategies dynamically over time to facilitate green MEC. EESaver's superior performance is tested comprehensively."",""1558-2183"","""",""10.1109/TPDS.2023.3277619"",""Australian Research Council Discovery Projects(grant numbers:DP200102491,DP230101790)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129089"",""Energy-saving";online approach;"green multi-access edge computing"",""Servers";Energy consumption;Data centers;Cloud computing;Multi-access edge computing;Demand response;"5G mobile communication"","""",""1"","""",""33"",""IEEE"",""18 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Blockchain-Based Data Integrity Auditing for Multi-Copy in Decentralized Storage,""Q. Zhang"; Z. Zhang; J. Cui; H. Zhong; Y. Li; C. Gu;" D. He"",""Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui Engineering Laboratory of IoT Security Technologies, Anhui University, Hefei, China"; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui Engineering Laboratory of IoT Security Technologies, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui Engineering Laboratory of IoT Security Technologies, Anhui University, Hefei, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui Engineering Laboratory of IoT Security Technologies, Anhui University, Hefei, China; Anhui Province Key Laboratory of Cyberspace Security Situation Awareness and Evaluation, Anhui, China; School of Public Security and Emergency Management, Anhui University of Science and Technology, Hefei, China;" School of Cyber Science and Engineering, Wuhan University, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Oct 2023"",""2023"",""34"",""12"",""3162"",""3173"",""As the disruptor of cloud storage, decentralized storage could lead to a major shift in how organizations store data in the future. To ensure data availability, users generally encrypt the data and distribute it to multiple storage service providers. It is necessary to study data integrity verification in decentralized storage. Although some recent studies have proposed the using blockchain technology to assist auditing work in decentralized storage networks, the on-chain overhead still increases linearly with an increase in audit requests. Blockchain networks will inevitably be overloaded. In this study, we propose an efficient data integrity auditing scheme for multiple copies in decentralized storage. Particularly, using different polynomial commitment schemes, we first propose a basic scheme for verifying multiple copies of a single file, and then we propose an efficient batch auditing scheme for multiple copies of multiple files. Our scheme can significantly reduce the computation overhead of storage service providers while keeping the on-chain storage overhead constant. Security analysis and performance analysis show that our scheme is efficient and practical."",""1558-2183"","""",""10.1109/TPDS.2023.3323155"",""National Natural Science Foundation of China(grant numbers:62272002,62202005)"; National Key R&D Program of China(grant numbers:2021YFB3100500); Excellent Youth Foundation of Anhui Scientific Committee(grant numbers:2108085J31); Natural Science Foundation ofAnhui Province, China(grant numbers:2008085QF297,2208085QF198); University Synergy Innovation Program of Anhui Province(grant numbers:GXXT-2022-049);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10274890"",""Multi-copy";data auditing;efficiency;blockchain;decentralized storage;"polynomial commitment"",""Blockchains";Cloud computing;Data integrity;Indexes;Smart contracts;Public key;"Metadata"","""","""","""",""30"",""IEEE"",""9 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Efficient Edge Data Management Framework for IIoT via Prediction-Based Data Reduction,""L. Yang"; Y. Liao; X. Cheng; M. Xia;" G. Xie"",""Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China;" Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Nov 2023"",""2023"",""34"",""12"",""3309"",""3322"",""Large amounts of time series data are required to support data analysis at the edge in the end-edge-cloud Industrial Internet of Things (IIoT) architecture. Reducing the storage cost is one of the main challenges in edge data management due to the limited storage resource of edge nodes. The state-of-the-art data reduction method has a high time overhead and poor reduction efficiency for unstable data sets. To solve this problem, this study proposes a time-series data management framework that combines data partition and data compression techniques. For the data partition technique, we propose an adaptive selection strategy to integrate the access pattern of the application and the characteristics of the time series data, thereby improving the partition accuracy. For the data compression technique, we propose a compression scheme based on time series data segmentation by using the idea of divide and conquer";" we further introduce a change point detection technique to improve the compression efficiency for unstable data sets. Experimental results obtained with three types of real industrial data sets show that our framework is significantly better than the state-of-the-art method in terms of compression ratio and time overhead."",""1558-2183"","""",""10.1109/TPDS.2023.3327750"",""National Natural Science Foundation of China(grant numbers:62372167,61972139,62141212,62133014)"; Natural Science Foundation of Hunan Province(grant numbers:2021JJ30153,2022JJ10021); Natural Science Foundation of Chongqing(grant numbers:cstc2021jcyj-msxmX0817,cstc2021jcyj-msxmX0461,CSTB2022NSCQ-MSX1393); Chongqing Science and Technology Innovation Project(grant numbers:CQYC20220511500);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10297424"",""Data reduction";edge data management framework;"industrial internet of things (IIoT)"",""Industrial Internet of Things";Image edge detection;Data compression;Computer architecture;Time series analysis;Real-time systems;"Streams"","""","""","""",""36"",""IEEE"",""26 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Efficient GPU Implementations of Post-Quantum Signature XMSS,""Z. Wang"; X. Dong; H. Chen;" Y. Kang"",""School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China"; School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China; School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China;" School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jan 2023"",""2023"",""34"",""3"",""938"",""954"",""The National Institute of Standards and Technology (NIST) approved XMSS as part of the post-quantum cryptography (PQC) development effort in 2018. XMSS is currently one of only two standardized PQC algorithms, but its performance limits its use. For example, the fastest record for some standardized parameters still takes more than a minute to generate a keypair. In this article, we present the first GPU implementation for XMSS and its variant XMSS$^{\mathsf {MT}}$MT. The high parallelism of GPUs is especially effective for reducing latency in key generation and improving throughput for signing and verifying. In order to meet various application scenarios, we provide three parallel XMSS schemes: algorithmic parallelism, multi-keypair data parallelism, and single-keypair data parallelism. For these schemes, we design custom parallel strategies that use more than 10,000 cores for all parameters provided by NIST. In addition, we analyze the availability of most previous serial optimizations and explore numerous techniques to fully exploit GPU performance. Our evaluations are made with the XMSSMT-SHA2_20/2_256 parameter set on a GeForce RTX 3090. The result shows the key generation latency is 3.20 ms, a speedup of 21,899× compared to the GPU ported version, which is also 54× speedup faster than the fastest work (174 ms). When 16384 tasks are executed, the throughput (task/s) for signing/verifying in the single-key and multi-key cases is 311,424/415,100 and 145,100/419,887, respectively. Compared to the throughput for signing/verifying (1695/4000) of the fastest work, we obtain a speedup of 184×/104× and 86×/105× in single-key and multi-key cases, respectively."",""1558-2183"","""",""10.1109/TPDS.2022.3233348"",""China National Key R&D Program(grant numbers:2018YFB1700405)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004747"",""Post-quantum cryptography";stateful hash-based signatures;XMSS;XMSS   $^{\mathsf {MT}}$       MT     ;parallel computing;"GPU"",""Graphics processing units";Parallel processing;Throughput;Digital signatures;NIST;Hash functions;"Public key"","""",""1"","""",""46"",""IEEE"",""2 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"eHotSnap: An Efficient and Hot Distributed Snapshots System for Virtual Machine Cluster,""B. Li"; L. Cui; Z. Hao; L. Li; Y. Liu;" Y. Li"",""School of Computer Science and Engineering, Beihang University, Beijing, China"; Zhongguancun Laboratory, Beijing, China; Zhongguancun Laboratory, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China;" School of National Security, People's Public Security University of China, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Jul 2023"",""2023"",""34"",""8"",""2433"",""2447"",""With the popularity of IaaS clouds, many distributed and networked applications are running in virtual machine cluster (VMC). The distributed snapshots of VMC are a practical approach to guarantee system reliability. It rewinds the system to an intermediate state from failures so that the applications can continue execution from a point near the failure. However, the applications running in the VMC suffer from long disruption and significant performance degradation due to the heavy cost distributed snapshots, especially when designed to guarantee global consistency of VMC snapshots. This article presents eHotSnap, which takes distributed snapshots of a VMC efficiently. eHotSnap divides the native snapshot into light cost transient snapshot and heavy cost memory snapshot and then coordinates the VM snapshots immediately after transient snapshots. In this way, it decouples coordination from heavy cost snapshots so that the distributed snapshots are taken (completed in logic) within a second. Then, it performs memory snapshot and optimizes it with a two-layer optimization, which first employs de-duplication to reduce the amount of snapshot data and then leverages priority queue to serve guest write operations preferentially. In addition to presenting eHotSnap, we have implemented a prototype on QEMU/KVM. The experimental results demonstrate the effectiveness and efficiency of the proposed approach."",""1558-2183"","""",""10.1109/TPDS.2023.3272014"",""National Natural Science Foundation of China(grant numbers:61972392,6207245)"; Open Research Fund of the Public Security Behavioral Science Laboratory; People's Public Security University of China(grant numbers:2020SYS07); Fundamental Research Funds for the Central Universities(grant numbers:2021JKF106);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113689"",""Virtual machine cluster";distributed snapshots;TCP backoff;"reliability"",""Transient analysis";Costs;Cloud computing;Virtual machining;Memory management;Random access memory;"Distributed databases"","""","""","""",""47"",""IEEE"",""1 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Enabling Balanced Data Deduplication in Mobile Edge Computing,""R. Luo"; H. Jin; Q. He; S. Wu;" X. Xia"",""National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" School of Computing Technologies, RMIT University, Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2023"",""2023"",""34"",""5"",""1420"",""1431"",""In the mobile edge computing (MEC) environment, edge servers with storage and computing resources are deployed at base stations within users’ geographic proximity to extend the capabilities of cloud computing to the network edge. Edge storage system (ESS), is comprised by connected edge servers in a specific area, which ensures low-latency services for users. However, high data storage overheads incurred by edge servers’ limited storage capacities is a key challenge in ensuring the performance of applications deployed on an ESS. Data deduplication, as a classic data reduction technology, has been widely applied in cloud storage systems. It also offers a promising solution to reducing data redundancy in ESSs. However, the unique characteristics of MEC, such as edge servers’ geographic distribution and coverage, render cloud data deduplication mechanisms obsolete. In addition, data distribution must be balanced over edge storage systems to accommodate future data demands, which cannot be undermined by data deduplication. Thus, balanced edge data deduplication (BEDD) must consider deduplication ratio, data storage benefits, and resource balance systematically under the latency constraint. In this article, we model the novel BEDD problem formally and prove its $\mathcal {NP}$NP-hardness. Then, we propose an optimal approach for solving the BEDD problem exactly in small-scale scenarios and a sub-optimal approach to solve large-scale BEDD problems with a theoretical performance guarantee. Extensive and comprehensive experiments conducted on a real-world dataset demonstrate the significant performance improvements of our approaches against four representative approaches."",""1558-2183"","""",""10.1109/TPDS.2023.3247061"",""National Key Research and Development Program of China(grant numbers:2022YFB4500704)"; National Natural Science Foundation of China(grant numbers:62032008);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049532"",""Data deduplication";edge storage system;mobile edge computing;optimization problem;"storage resource balance"",""Servers";Memory;Cloud computing;Redundancy;Indexes;Multi-access edge computing;"Low latency communication"","""",""2"","""",""34"",""CCBYNCND"",""22 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Enabling Efficient Random Access to Hierarchically Compressed Text Data on Diverse GPU Platforms,""Y. Hu"; F. Zhang; Y. Xia; Z. Yao; L. Zeng; H. Ding; Z. Wei; X. Zhang; J. Zhai; X. Du;" S. Ma"",""Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and School of Information, Renmin University of China, Beijing, China"; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and School of Information, Renmin University of China, Beijing, China;" School of Engineering and Information Technology, University of New South Wales, ADFA, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Aug 2023"",""2023"",""34"",""10"",""2699"",""2717"",""The tremendous computing capacity of GPU offers significant potential in processing hierarchically compressed text data without decompression. However, current GPU techniques offer only traversal-based text data analytics";" random access is exceedingly inefficient, limiting their utility significantly. To address this issue, we develop a novel and widely applicable solution that prompts random access to hierarchically compressed text data without decompression in GPU memory. We address three main challenges for enabling efficient random access to compressed text data on GPUs. The first challenge is designing GPU data structures that facilitate random access. The second challenge is efficiently generating data structures on GPU. The CPU is inefficient when generating data structures for random access, and this inefficiency increases considerably when PCIe transmission is incorporated. The third challenge is query processing on compressed text data in GPU memory. Random accesses, such as data updates, cause massive conflicts among countless threads. In order to address the first challenge, we develop several compressed GPU data structures, including indexing within the intricate GPU memory hierarchy. To handle the second challenge, we propose a two-phase process for producing these data structures on GPU. For the third challenge, a double-parsing design is proposed as a solution to avoid conflicts. We evaluate our solution on three platforms, two server-grade GPU platforms and one edge-grade GPU platform, using five real-world datasets. Experimental results show that random access operations on GPU achieve an average speedup of 52.98× compared to the state-of-the-art solution."",""1558-2183"","""",""10.1109/TPDS.2023.3294341"",""National Natural Science Foundation of China(grant numbers:62172419,62322213)"; Beijing Nova Program; Public Computing Cloud, Renmin University of China; Renmin University of China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10178044"",""Big data applications";data compression;parallel architectures;query processing;"text analysis"",""Graphics processing units";Data structures;Query processing;Electronic mail;Task analysis;Parallel processing;"Instruction sets"","""","""","""",""105"",""IEEE"",""11 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Energy-Aware, Device-to-Device Assisted Federated Learning in Edge Computing,""Y. Li"; W. Liang; J. Li; X. Cheng; D. Yu; A. Y. Zomaya;" S. Guo"",""School of Computing, Australian National University, Canberra, ACT, Australia"; Department of Computer Science, City University of Hong Kong, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hong Kong; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science and Technology, Shandong University, Qingdao, China; School of Computer Science, University of Sydney, Sydney, NSW, Australia;" Department of Computing, Hong Kong Polytechnic University, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Jun 2023"",""2023"",""34"",""7"",""2138"",""2154"",""The surging of deep learning brings new vigor and vitality to shape the prospect of intelligent Internet of Things (IoT), and the rise of edge intelligence enables provisioning real-time deep neural network (DNN) inference services for mobile users. To perform efficient and effective DNN model training in edge computing environments while preserving training data security and privacy of IoT devices, federated learning has been envisioned as an ideal learning paradigm for this purpose. In this article, we study energy-aware DNN model training in edge computing. We first formulate a novel energy-aware, Device-to-Device (D2D) assisted federated learning problem with the aim to minimize the global loss of a training DNN model, subject to bandwidth capacity on an edge server and energy capacity on each IoT device. We then devise a near-optimal learning algorithm for the problem when the training data follows the i.i.d. data distribution. The crux of the proposed algorithm is to explore using the energy of neighboring devices of each device for its local model uploading, by reducing the problem to a series of weighted maximum matching problems in corresponding auxiliary graphs. We also consider the problem without the assumption of the i.i.d. data distribution, for which we propose an efficient heuristic algorithm. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results show that the proposed algorithms are promising."",""1558-2183"","""",""10.1109/TPDS.2023.3277423"",""Australian Research Council"; Discovery Project Scheme(grant numbers:DP200101985); City University of Hong Kong(grant numbers:9380137/CS); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,15220320/20E); National Natural Science Foundation of China(grant numbers:61872310); Shenzhen Science and Technology Innovation Commission(grant numbers:R2020A045);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10129058"",""Edge computing";energy-aware federated learning;D2D-assisted learning;weighted maximum matching;budgeted-energy DNN model training;constrained optimization;"decentralized machine learning"",""Servers";Federated learning;Training;Computational modeling;Data models;Performance evaluation;"Convergence"","""",""1"","""",""38"",""IEEE"",""18 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Energy-Minimized Scheduling of Intermittent Real-Time Tasks in a CPU-GPU Cloud Computing Platform,""B. Hu"; X. Yang;" M. Zhao"",""College of Engineering, China Agricultural University, Beijing, China"; College of Engineering, China Agricultural University, Beijing, China;" Department of Automation, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Jul 2023"",""2023"",""34"",""8"",""2391"",""2402"",""Due to the flexibility, availability, and scalability of cloud computing services, more and more users seek solutions via cloud computing techniques. A cloud computing platform often consists of a large number of infrastructures, and its energy consumption is a big problem. In this article, we study how to minimize the energy consumption of a cloud computing platform when handling some intermittent real-time tasks. Unlike previous works that abstract users’ submitted tasks as single computation jobs and process them using CPU, this work proposes using CPU and GPU to process intermittent real-time tasks that occur at irregular intervals and their released computation jobs must be completed within required time limits. The energy consumption minimization problem is formulated as an integer nonlinear programming problem that needs to decide on a task assignment plan and a specific resource allocation plan. To effectively solve this problem, we define a state that represents the optimal solution for a given set of tasks with a given amount of resources, as well as a value function that represents the value of a state. In this way, we derive a state-transition equation and develop a dynamic programming method to solve the problem. This method is also extended to handle tasks whose arrival time is dynamic and unpredictable. Experiments show that the proposed algorithm can effectively reduce energy consumption, while its computation time is quite low compared to some other greedy methods."",""1558-2183"","""",""10.1109/TPDS.2023.3288702"",""STI2030-Major Projects(grant numbers:2021ZD0201401,2021ZD0201402)"; National Natural Science Foundation of China(grant numbers:61802013);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10163932"",""Energy consumption minimization";cloud computing;CPU-GPU;"intermittent real-time tasks"",""Task analysis";Cloud computing;Real-time systems;Servers;Energy consumption;Costs;"Virtual machining"","""",""1"","""",""47"",""IEEE"",""26 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"ENLARGE: An Efficient SNN Simulation Framework on GPU Clusters,""P. Qu"; H. Lin; M. Pang; X. Liu; W. Zheng;" Y. Zhang"",""Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China"; Department of Precision Instrument, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China; School of Information Science and Engineering, Yunnan University, Kunming, Yunnan, China; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2023"",""2023"",""34"",""9"",""2529"",""2540"",""Spiking Neural Networks (SNNs) are currently the most widely used computing model for neuroscience communities. There is also an increasing research interest in exploring the potential of SNN in brain-inspired computing, artificial intelligence, and other areas. As SNNs possess distinguished characteristics that originate from biological authenticity, they require dedicated simulation frameworks to achieve usability and efficiency. However, there is no widely-used, easily accessible, high performance SNN simulation framework for GPU clusters. In this paper, we propose ENLARGE, an efficient SNN simulation framework on GPU clusters. ENLARGE provides a multi-level architecture that deals with computation, communication, and synchronization hierarchically. We also propose an efficient communication method with an all-to-all communication pattern. To deal with the delay of spike delivery, which is the most distinguished SNN characteristic, several delay-aware optimization methods are also proposed. We further propose a multilevel workload management method. Various experiments are carried out to demonstrate the performance and scalability of the framework, as well as the effects of the optimization methods. Test results show that ENLARGE can achieve $3.17\times \sim 28.12\times$3.17×∼28.12× speedup compared with the most widely used NEST simulator and $3.26\times \sim 13.57\times$3.26×∼13.57× speedup compared with the widely used NEST GPU simulator for GPU clusters."",""1558-2183"","""",""10.1109/TPDS.2023.3291825"",""National Natural Science Foundation of China(grant numbers:62202254,62250006,62072266,61836004)"; Beijing National Research Center for Information Science and Technology(grant numbers:BNR2022RC01003); Tsinghua University Initiative Scientific Research Program; Suzhou-Tsinghua Innovation Leadership Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172038"",""Spiking neural network";computing framework;GPU cluster;brain-inspired computing;"high performance computing"",""Graphics processing units";Computational modeling;Biological system modeling;Neurons;Synchronization;Optimization methods;"Synapses"","""","""","""",""49"",""IEEE"",""3 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"ESMO: Joint Frame Scheduling and Model Caching for Edge Video Analytics,""T. Li"; J. Sun; Y. Liu; X. Zhang; D. Zhu; Z. Guo;" L. Geng"",""Institute of Information Engineering, Chinese Academy of Sciences and School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China"; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences and School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; School of Electronic Science and Engineering, Nanjing University, China; Institute of Information Engineering, Chinese Academy of Sciences and School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences and School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China;" Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Jun 2023"",""2023"",""34"",""8"",""2295"",""2310"",""With the advancements in Machine Learning (ML) and edge computing, increasing efforts have been devoted to edge video analytics. However, most of the existing works fail to consider the cooperation of edge nodes for ML model caching and video frame scheduling, thus less efficient in practical scenarios with diverse requirements. In this article, we propose a novel approach named ESMO (joint framE Scheduling and MOdel caching) to jointly optimize Frame Scheduling and Model Caching (FSMC), aiming at enhancing the performance of edge video analytics. In detail, we decompose the FSMC as three sub-problems, where the first two sub-problems (i.e., user's transmit power and edge computing resources allocation problems) are proven to be quasi-convex and strictly convex, respectively";" while the third main sub-problem (i.e., trade-off among the video analytics (VA) accuracy, service delay and energy consumption) is NP-hard. Therefore, an efficient Two-layers Genetic Algorithm based algorithm (i.e., TGA-FSMC) is designed to find the close-to-optimal frame scheduling and the model caching decisions in an iterative manner. Finally, we deploy a target recognition prototype to comprehensively evaluate the practical performance in diverse edge nodes and CNN models. Extensive experiments demonstrate the empirical superiority of the ESMO over alternatives on real-world edge video analytics platforms, and it achieves 37.5%$\sim$∼87.2% performance improvement."",""1558-2183"","""",""10.1109/TPDS.2023.3281598"",""Climbing Program of Institute of Information Engineering"; Chinese Academy of Sciences(grant numbers:E3Z0031);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138921"",""Edge computing";video analytics;frame scheduling;"ML model caching"",""Visual analytics";Delays;Computational modeling;Analytical models;Processor scheduling;Streaming media;"Energy consumption"","""",""1"","""",""56"",""IEEE"",""31 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Expediting Distributed DNN Training With Device Topology-Aware Graph Deployment,""S. Zhang"; X. Yi; L. Diao; C. Wu; S. Wang;" W. Lin"",""Department of Computer Science, University of Hong Kong, Pokfulam, Hong Kong"; Department of Computer Science, University of Hong Kong, Pokfulam, Hong Kong; Alibaba Group, Hangzhou, Zhejiang, China; Department of Computer Science, University of Hong Kong, Pokfulam, Hong Kong; Alibaba Group, Hangzhou, Zhejiang, China;" Alibaba Group, Hangzhou, Zhejiang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Feb 2023"",""2023"",""34"",""4"",""1281"",""1293"",""This paper presents TAG, an automatic system to derive optimized DNN training graph and its deployment onto any device topology, for expedited training in device- and topology- heterogeneous ML clusters. We novelly combine both the DNN computation graph and the device topology graph as input to a graph neural network (GNN), and join the GNN with a search-based method to quickly identify optimized distributed training strategies. To reduce communication in a heterogeneous cluster, we further explore a lossless gradient compression technique and solve a combinatorial optimization problem to automatically apply the technique for training time minimization. We evaluate TAG with various representative DNN models and device topologies, showing that it can achieve up to 4.56x training speed-up as compared to existing schemes. TAG can produce efficient deployment strategies for both unseen DNN models and unseen device topologies, without heavy fine-tuning."",""1558-2183"","""",""10.1109/TPDS.2023.3243261"",""Alibaba Group"; Hong Kong RGC(grant numbers:17204619,17208920);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040900"",""Distributed systems";"machine learning"",""Training";Topology;Computational modeling;Tensors;Graphics processing units;Synchronization;"Parallel processing"","""","""","""",""55"",""IEEE"",""8 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Experimental Survey of FPGA-Based Monolithic Switches and a Novel Queue Balancer,""P. Papaphilippou"; K. Sano; B. A. Adhi;" W. Luk"",""Department of Computing, Imperial College London, London, U.K."; RIKEN Center for Computational Science, Kobe, Hyogo, Japan; RIKEN Center for Computational Science, Kobe, Hyogo, Japan;" Department of Computing, Imperial College London, London, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""31 Mar 2023"",""2023"",""34"",""5"",""1621"",""1634"",""This article studies small to medium-sized monolithic switches for FPGA implementation and presents a novel switch design that achieves high algorithmic performance and FPGA implementation efficiency. Crossbar switches based on virtual output queues (VOQs) and variations have been rather popular for implementing switches on FPGAs, with applications in network switches, memory interconnects, network-on-chip (NoC) routers etc. The implementation efficiency of crossbar-based switches is well-documented on ASICs, though we show that their disadvantages can outweigh their advantages on FPGAs. One of the most important challenges in such input-queued switches is the requirement for iterative scheduling algorithms. In contrast to ASICs, this is more harmful on FPGAs, as the reduced operating frequency and narrower packets cannot “hide” multiple iterations of scheduling that are required to achieve a modest scheduling performance. Our proposed design uses an output-queued switch internally for simplifying scheduling, and a queue balancing technique to avoid queue fragmentation and reduce the need for memory-sharing VOQs. Its implementation approaches the scheduling performance of a state-of-the-art FPGA-based switch, while requiring considerably fewer resources."",""1558-2183"","""",""10.1109/TPDS.2023.3244589"",""Engineering and Physical Sciences Research Council(grant numbers:EP/L016796/1,EP/L00058X/1,EP/N031768/1)"; Japan Society for the Promotion of Science(grant numbers:20H00593,21H04869);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10043741"",""Crossbar";FPGA;NoCs;output-queued;queue balancing;scheduling algorithms;switch;"virtual output queues"",""Field programmable gate arrays";Scheduling;Complexity theory;Scheduling algorithms;Integrated circuit interconnections;Switches;"Hardware"","""","""","""",""55"",""IEEE"",""13 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Exploring Fine-Grained In-Memory Database Performance for Modern CPUs,""Z. Liu"; R. Han; Y. Zhang; Y. Zhang; X. Tang; G. Deng; T. Zhong; R. Dementiev; Y. Lu;" M. Que"",""Intel Corp, Beijing, China"; School of Information in Renmin University of China, Beijing, China; School of Information in Renmin University of China, Beijing, China; National Satellite Meteorological Center of China, Beijing, China; Intel Corp, Beijing, China; Intel Corp, Beijing, China; Intel Corp, Beijing, China; Intel Corp, Beijing, China; Huawei Technologies Co., Ltd, Hangzhou, Zhejiang, China;" Huawei Technologies Co., Ltd, Hangzhou, Zhejiang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1757"",""1772"",""Modern CPUs keep integrating more cores and large size cache, which is beneficial for in-memory databases to improve parallel processing power and cache locality. While state-of-the-art CPUs have diverse architectures and roadmaps such as large core count and large cache size (AMD x86), moderate core count and cache size (intel x86), large core count and moderate cache size (ARM), exploring in-memory databases performance characteristics for different CPU architectures is important for in-memory database designs and optimizations. In this article, we develop a fine-grained in-memory database benchmark to evaluate the performance of each operator on different CPUs to explore how CPU hardware architectures influence performance. Different from well known conclusions that more cores and larger cache size can achieve higher performance, we find out that the micro cache architectures play an important role opposite to core count and cache size, the shared monolithic L3 cache with moderate size beats large disaggregated L3 cache. The experiments also show that predicting operator performance on different CPUs is difficult according to diverse CPU architectures and micro cache architectures, and different implementations of each operator are not always high or low with interleaved strong and weak performance regions influenced by CPU hardware architectures. Intel x86 CPUs represent cache-centric processor design, while AMD x86 and ARM CPUs represent computing-centric processor design, the OLAP benchmark experiments of SSB discover that OmniSciDB and OLAP Accelerator with vector-wise processing model performs well on intel x86 CPUs compared to AMD x86 CPUs and the JIT compliant based Hyper prefers to AMD x86 CPUs rather than intel x86 CPUs. The CPU roadmaps of increasing cores or improving cache locality should be considered for in-memory database algorithm design and platform selection."",""1558-2183"","""",""10.1109/TPDS.2023.3262782"",""National Natural Science Foundation of China(grant numbers:61732014,61772533)"; Natural Science Foundation of Beijing(grant numbers:4192066); Kunpeng Application Lab of Huawei;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086665"",""Cache architecture";in-memory database;multi-core;"OLAP benchmark"",""Databases";Computer architecture;Benchmark testing;Program processors;Hardware;Sockets;"Micromechanical devices"","""","""","""",""24"",""IEEE"",""29 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Exploring Memory Access Similarity to Improve Irregular Application Performance for Distributed Hybrid Memory Systems,""W. Liu"; X. He;" Q. Liu"",""Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA"; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA;" Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Jan 2023"",""2023"",""34"",""3"",""797"",""809"",""With the increasing problem complexity, more irregular applications are deployed on high-performance clusters due to the parallel working paradigm, and yield irregular memory access behaviors across nodes. However, the irregularity of memory access behaviors is not comprehensively studied, which results in low utilization of the integrated hybrid memory system compositing of stacked DRAM and off-chip DRAM. To address this problem, we devise a novel method called Similarity-Managed Hybrid Memory System (SM-HMS) to improve the hybrid memory system performance by leveraging the memory access similarity among nodes in a cluster. Within SM-HMS, two techniques are proposed, Memory Access Similarity Measuring and Similarity-based Memory Access Behavior Sharing. To quantify the memory access similarity, memory access behaviors of each node are vectorized, and the distance between two vectors is used as the memory access similarity. The calculated memory access similarity is used to share memory access behaviors precisely across nodes. With the shared memory access behaviors, SM-HMS divides the stacked DRAM into two sections, the sliding window section and the outlier section. The shared memory access behaviors guide the replacement of the sliding window section while the outlier section is managed in the LRU manner. Our evaluation results with a set of irregular applications on various clusters consisting of up to 256 nodes have shown that SM-HMS outperforms the state-of-the-art approaches, Cameo, Chameleon, and Hyrbid2, on job finish time reduction by up to $58.6\%$58.6%, $56.7\%$56.7%, and $31.3\%$31.3%, with $46.1\%$46.1%, $41.6\%$41.6%, and $19.3\%$19.3% on average, respectively. SM-HMS can also achieve up to $98.6\%$98.6% ($91.9\%$91.9% on average) of the ideal hybrid memory system performance."",""1558-2183"","""",""10.1109/TPDS.2022.3227544"",""National Science Foundation(grant numbers:2134203,1828363,1812861,2134202,2144403)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9976297"",""Cluster";irregular application;memory system;DRAM;"hybrid memory system"",""Behavioral sciences";Random access memory;Monitoring;Task analysis;Indium tin oxide;Operating systems;"Data structures"","""","""","""",""38"",""IEEE"",""8 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Faber: A Hardware/SoftWare Toolchain for Image Registration,""E. D'Arnese"; D. Conficconi; E. D. Sozzo; L. Fusco; D. Sciuto;" M. D. Santambrogio"",""Department of Electronic, Information and Bioengineering, Politecnico di Milano, Milano, IT, Italy"; Department of Electronic, Information and Bioengineering, Politecnico di Milano, Milano, IT, Italy; Department of Electronic, Information and Bioengineering, Politecnico di Milano, Milano, IT, Italy; Department of Electronic, Information and Bioengineering, Politecnico di Milano, Milano, IT, Italy; Department of Electronic, Information and Bioengineering, Politecnico di Milano, Milano, IT, Italy;" Department of Electronic, Information and Bioengineering, Politecnico di Milano, Milano, IT, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Nov 2022"",""2023"",""34"",""1"",""291"",""303"",""Image registration is a well-defined computation paradigm widely applied to align one or more images to a target image. This paradigm, which builds upon three main components, is particularly compute-intensive and represents many image processing pipelines’ bottlenecks. State-of-the-art solutions leverage hardware acceleration to speed up image registration, but they are usually limited to implementing a single component. We present Faber, an open-source HW/SW CAD toolchain tailored to image registration. The Faber toolchain comprises HW/SW highly-tunable registration components, supports users with different expertise in building custom pipelines, and automates the design process. In this direction, Faber provides both default settings for entry-level users and latency and resource models to guide HW experts in customizing the different components. Finally, Faber achieves from 1.5× to 54× in speedup and from 2× to 177× in energy efficiency against state-of-the-art tools on a Xeon Gold."",""1558-2183"","""",""10.1109/TPDS.2022.3218898"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935260"",""FPGAs";HW/SW design automation;"image registration"",""Measurement";Pipelines;Image registration;Field programmable gate arrays;Optimization;Graphics processing units;"Registers"","""",""3"","""",""33"",""IEEE"",""2 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Falcon: Fair and Efficient Online File Transfer Optimization,""M. Arifuzzaman"; B. Bockelman; J. Basney;" E. Arslan"",""University of Nevada, Reno, NV, USA"; Morgridge Institute for Research, Madison, WI, USA; National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Reno, NV, USA;" University of Nevada, Reno, NV, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jun 2023"",""2023"",""34"",""8"",""2265"",""2278"",""Research networks provide high-speed wide-area network connectivity between research and education institutions to facilitate large-scale data transfers. However, scalability issues of legacy transfer applications such as scp and FTP hinder the effective utilization of these networks. Although researchers extended the legacy transfer applications to increase their performance by exploiting I/O and network parallelism, these solutions necessitate users to fine-tune parallelism level, a task that is challenging even for experienced users due to the dynamic nature of networks. In this article, we propose an online optimization algorithm, Falcon, to tune the degree of parallelism for file transfers to maximize transfer throughput while keeping system overhead at a minimum. As research networks are shared infrastructures, we introduce a game theory-inspired novel utility function to evaluate the performance of various parallelism levels such that competing transfers are guaranteed to converge to a fair and stable solution. We assessed the performance of Falcon in isolated and production high-speed networks and found that it can discover optimal transfer parallelism in as little as 20 seconds and outperform the state-of-the-art solutions by more than $2\times$2×. Moreover, Falcon is guaranteed to converge to Nash Equilibrium when multiple transfers compete for the same resources with the help of its game theory-inspired utility function. Finally, we demonstrate that Falcon can also be used as a central transfer scheduler to speed up convergence time, increase stability, and enforce system/user-level resource limitations in shared networks."",""1558-2183"","""",""10.1109/TPDS.2023.3282872"",""NSF(grant numbers:2145742,2007789,2209955,2114989)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10143760"",""Online transfer optimization";throughput optimization in research networks;file transfer tuning;"high-speed networks"",""Throughput";Concurrent computing;Optimization;High-speed networks;Data transfer;Bandwidth;"Resource management"","""",""1"","""",""36"",""IEEE"",""5 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Faster Federated Learning With Decaying Number of Local SGD Steps,""J. Mills"; J. Hu;" G. Min"",""Department of Computer Science, University of Exeter, Exeter, U.K."; Department of Computer Science, University of Exeter, Exeter, U.K.;" Department of Computer Science, University of Exeter, Exeter, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""8 Jun 2023"",""2023"",""34"",""7"",""2198"",""2207"",""In Federated Learning (FL) client devices connected over the internet collaboratively train a machine learning model without sharing their private data with a central server or with other clients. The seminal Federated Averaging (FedAvg) algorithm trains a single global model by performing rounds of local training on clients followed by model averaging. FedAvg can improve the communication-efficiency of training by performing more steps of Stochastic Gradient Descent (SGD) on clients in each round. However, client data in real-world FL is highly heterogeneous, which has been extensively shown to slow model convergence and harm final performance when $K > 1$K>1 steps of SGD are performed on clients per round. In this article we propose decaying $K$K as training progresses, which can jointly improve the final performance of the FL model whilst reducing the wall-clock time and the total computational cost of training compared to using a fixed $K$K. We analyse the convergence of FedAvg with decaying $K$K for strongly-convex objectives, providing novel insights into the convergence properties, and derive three theoretically-motivated decay schedules for $K$K. We then perform thorough experiments on four benchmark FL datasets (FEMNIST, CIFAR100, Sentiment140, Shakespeare) to show the real-world benefit of our approaches in terms of real-world convergence time, computational cost, and generalisation performance."",""1558-2183"","""",""10.1109/TPDS.2023.3277367"",""EPSRC New Horizons(grant numbers:EP/X019160/1)"; UK Research and Innovation(grant numbers:EP/X038866/1); EPSRC DTP studentship; Horizon EU(grant numbers:101086159);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10128796"",""Computational efficiency";deep learning;edge computing;"federated learning"",""Training";Convergence;Computational modeling;Servers;Data models;Costs;"Benchmark testing"","""","""","""",""29"",""IEEE"",""17 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"FCDedup: A Two-Level Deduplication System for Encrypted Data in Fog Computing,""M. Song"; Z. Hua; Y. Zheng; T. Xiang;" X. Jia"",""School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China"; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China; College of Computer Science, Chongqing University, Chongqing, China;" Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Aug 2023"",""2023"",""34"",""10"",""2642"",""2656"",""Distributed fog computing has received increasing attention recently and fog-assisted cloud storage can provide a real-time service to collect and manage large-scale data for the applications of Internet of Things. Encrypted data deduplication over cloud storage can significantly save storage space of the cloud server while protecting the confidentiality of the outsourced data. Previous encrypted data deduplication schemes are mostly designed for traditional cloud storage with a two-layer architecture and cannot be applied to the emerging fog-assisted cloud storage that has a more complex three-layer architecture (i.e., cloud server, fog node and endpoint device). In this paper, we design, analyze and implement FCDedup, a new encrypted data deduplication scheme for fog-assisted cloud storage. FCDedup is a two-level deduplication system that enables each fog node to detect duplicated encrypted data uploaded by different endpoint devices, as well as enables cloud server to detect duplicated encrypted data from different fog nodes. By doing so, FCDedup can achieve both intra-deduplication within a single data owner and inter-deduplication across different data owners. FCDedup is also designed to prevent cloud server and fog nodes launching the brute-force attacks, and to guarantee the reliability of files downloaded from the cloud. Formal analysis is provided to justify its deduplication correctness and security. Besides, we implement a prototype of FCDedup using Alibaba Cloud as backend storage. Our evaluations demonstrate that FCDedup is completely compatible with existing cloud storage systems and achieves modest performance overhead."",""1558-2183"","""",""10.1109/TPDS.2023.3298684"",""National Key R&D Program of China(grant numbers:2022YFB3103500)"; National Natural Science Foundation of China(grant numbers:62071142); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515110027,2021A1515011406,2023A1515010714); Shenzhen Science and Technology Program(grant numbers:RCBS20210609103056041,JCYJ20220531095416037); Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies(grant numbers:2022B1212010005); Shenzhen Science and Technology Program(grant numbers:ZDSYS20210623091809029);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10193829"",""Brute-force attacks";data reliability;encrypted data deduplication;"fog-assisted cloud storage"",""Cloud computing";Servers;Encryption;Reliability;Edge computing;Costs;"Fingerprint recognition"","""","""","""",""46"",""IEEE"",""25 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"FCPP+Miosix: Scaling Aggregate Programming to Embedded Systems,""G. Audrito"; F. Terraneo;" W. Fornaciari"",""Dipartimento di Informatica, Università degli Studi di Torino, Torino, Italy"; Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy;" Dipartimento di Elettronica, Informazione e Bioingegneria, Politecnico di Milano, Milano, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Jan 2023"",""2023"",""34"",""3"",""869"",""880"",""As the density of nodes capable of sensing, computing and actuation increases, it becomes increasingly useful to model an entire network of physical devices as a single, continuous space-time computing machine. The emergent behaviour of the whole software system is then induced by local computations deployed within each node and by the dynamics of the information diffusion. A relevant example of this distribution model is given by aggregate programming and its minimal set of functional constructs used to manipulate distributed data structures evolving over space and time, and resulting in robustness to changes. In this paper, we propose the first implementation of the aggregate computing paradigm targeting microcontrollers, by integrating FCPP, a C++ implementation of the paradigm, with Miosix, a modern operating system for microcontrollers with full C++ support. To the best of the author's knowledge, we are the first to present results on the effectiveness of FCPP in an embedded operating system setting as opposed to a simulation environment, thus considering tight memory and computational constraints and accounting for packet losses due to nonidealities of the radio channel. We implemented and tested on a network of WandStem nodes two benchmark applications: a network connectivity checker for network planning and preventive maintenance, and a decentralised contact tracing application. Additionally, we show that common problems in sensor networks such as neighbour discovery, construction of a graph of the network topology, coarse grain clock synchronisation as well as network monitoring and the collection of statistics (such as memory occupation data) can be easily performed thanks to the expressive semantics of aggregate programming."",""1558-2183"","""",""10.1109/TPDS.2022.3232633"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002307"",""Aggregate programming";distributed systems;"embedded systems"",""Aggregates";Operating systems;Programming;Microcontrollers;Embedded systems;Libraries;"Java"","""",""2"","""",""32"",""IEEE"",""28 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Federated Ensemble Model-Based Reinforcement Learning in Edge Computing,""J. Wang"; J. Hu; J. Mills; G. Min; M. Xia;" N. Georgalas"",""Department of Computer Science, University of Exeter, Exeter, U.K."; Department of Computer Science, University of Exeter, Exeter, U.K.; Department of Computer Science, University of Exeter, Exeter, U.K.; Department of Computer Science, University of Exeter, Exeter, U.K.; Google, Mountain View, CA, USA;" Applied Research Department, British Telecom, London, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""11 May 2023"",""2023"",""34"",""6"",""1848"",""1859"",""Federated learning (FL) is a privacy-preserving distributed machine learning paradigm that enables collaborative training among geographically distributed and heterogeneous devices without gathering their data. Extending FL beyond the supervised learning models, federated reinforcement learning (FRL) was proposed to handle sequential decision-making problems in edge computing systems. However, the existing FRL algorithms directly combine model-free RL with FL, thus often leading to high sample complexity and lacking theoretical guarantees. To address the challenges, we propose a novel FRL algorithm that effectively incorporates model-based RL and ensemble knowledge distillation into FL for the first time. Specifically, we utilise FL and knowledge distillation to create an ensemble of dynamics models for clients, and then train the policy by solely using the ensemble model without interacting with the environment. Furthermore, we theoretically prove that the monotonic improvement of the proposed algorithm is guaranteed. The extensive experimental results demonstrate that our algorithm obtains much higher sample efficiency compared to classic model-free FRL algorithms in the challenging continuous control benchmark environments under edge computing settings. The results also highlight the significant impact of heterogeneous client data and local model update steps on the performance of FRL, validating the insights obtained from our theoretical analysis."",""1558-2183"","""",""10.1109/TPDS.2023.3264480"",""EU Horizon 2020 INITIATE Project(grant numbers:101008297)"; Royal Society International Exchanges Project(grant numbers:IEC/NSFC/ 211460); EPSRC New Horizons(grant numbers:EP/X019160/1); UKRI Project(grant numbers:EP/X038866/1);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093139"",""Deep reinforcement learning";distributed machine learning;edge computing;"federated learning"",""Computational modeling";Data models;Heuristic algorithms;Training;Edge computing;Reinforcement learning;"Analytical models"","""",""2"","""",""49"",""IEEE"",""5 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Federated Learning Over Coupled Graphs,""R. Lei"; P. Wang; J. Zhao; L. Lan; J. Tao; C. Deng; J. Feng; X. Wang;" X. Guan"",""MOE Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, Shaanxi, China"; MOE Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, Shaanxi, China; MOE Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, Shaanxi, China; MOE Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, Shaanxi, China; MOE Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, Shaanxi, China; China Mobile Research Institute, Beijing, China; China Mobile Research Institute, Beijing, China; China Mobile Group Design Institute, Beijing, China;" MOE Key Laboratory for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, Shaanxi, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2023"",""2023"",""34"",""4"",""1159"",""1172"",""Graphs are widely used to represent the relations among entities. When one owns the complete data, an entire graph can be easily built, therefore performing analysis on the graph is straightforward. However, in many scenarios, it is impractical to centralize the data due to data privacy concerns. An organization or party only keeps a part of the whole graph data, i.e., graph data is isolated from different parties. Recently, Federated Learning (FL) has been proposed to solve the data isolation issue, mainly for Euclidean data. It is still a challenge to apply FL on graph data because graphs contain topological information which is notorious for its non-IID nature and is hard to partition. In this work, we propose a novel FL framework for graph data, FedCog, to efficiently handle coupled graphs that are a kind of distributed graph data, but widely exist in a variety of real-world applications such as mobile carriers’ communication networks and banks’ transaction networks. We theoretically prove the correctness and security of FedCog. Experimental results demonstrate that our method FedCog significantly outperforms traditional FL methods on graphs. Remarkably, our FedCog improves the accuracy of node classification tasks by up to 14.7%."",""1558-2183"","""",""10.1109/TPDS.2023.3240527"",""National Key R&D Program of China(grant numbers:2021YFB1715600)"; National Natural Science Foundation of China(grant numbers:U22B2019,62272372,61902305); MoE-CMCC “Artificial Intelligence” Project(grant numbers:MCM20190701);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10029878"",""Distributed graph processing";federated learning;graph neural networks;"privacy preservation"",""Task analysis";Distributed databases;Federated learning;Data privacy;Data models;Training;"Optimization"","""","""","""",""55"",""IEEE"",""30 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"FedMDS: An Efficient Model Discrepancy-Aware Semi-Asynchronous Clustered Federated Learning Framework,""Y. Zhang"; D. Liu; M. Duan; L. Li; X. Chen; A. Ren; Y. Tan;" C. Wang"",""College of Computer Science, Chongqing University, Chongqing, China"; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China;" College of Computer Science, Chongqing University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Jan 2023"",""2023"",""34"",""3"",""1007"",""1019"",""Federated learning (FL) is an emerging distributed machine learning paradigm that protects privacy and tackles the problem of isolated data islands. At present, there are two main communication strategies of FL: synchronous FL and asynchronous FL. The advantages of synchronous FL are the high precision and easy convergence of the model. However, this synchronous communication strategy has the risk of the straggler effect. Asynchronous FL has a natural advantage in mitigating the straggler effect, but there are threats of model quality degradation and server crash. In this paper, we propose a model discrepancy-aware semi-asynchronous clustered FL framework, FedMDS, which alleviates the straggler effect by 1) a clustered strategy based on the delay and direction of the model update and 2) a synchronous trigger mechanism that limits the model staleness. FedMDS leverages the clustered algorithm to reschedule the clients. Each group of clients performs asynchronous updates until the synchronous update mechanism based on the model discrepancy is triggered. We evaluate FedMDS based on four typical federated datasets in a non-IID setting and compare FedMDS to the baselines. The experimental results show that FedMDS significantly improves average test accuracy by more than $+9.2\%$+9.2% on the four datasets compared to TA-FedAvg. In particular, FedMDS improves absolute Top-1 test accuracy by $+37.6\%$+37.6% on FEMNIST compared to TA-FedAvg. The frequency of the average synchronization waiting time of FedMDS is significantly lower than that of TA-FedAvg on all datasets. Moreover, FedMDS can improve the accuracy and alleviate the straggler effect."",""1558-2183"","""",""10.1109/TPDS.2023.3237752"",""National Natural Science Foundation of China(grant numbers:62102051,62072059)"; Chongqing Postdoctoral Science Foundation(grant numbers:2021LY75); Funds for Chongqing Distinguished Young Scholars(grant numbers:cstc2020jcyj-jqX0012);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018536"",""Distributed machine learning";federated learning;"neural network"",""Servers";Training;Federated learning;Task analysis;Training data;Optimization;"Data models"","""",""4"","""",""41"",""IEEE"",""17 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"FedProf: Selective Federated Learning Based on Distributional Representation Profiling,""W. Wu"; L. He; W. Lin;" C. Maple"",""University of Warwick, Coventry, England"; Department of Computer Science, University of Warwick, Coventry, U.K.; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China;" WMG, University of Warwick, Coventry, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1942"",""1953"",""Federated Learning (FL) has shown great potential as a privacy-preserving solution to learning from decentralized data that are only accessible to end devices (i.e., clients). The data locality constraint offers strong privacy protection but also makes FL sensitive to the condition of local data. Apart from statistical heterogeneity, a large proportion of the clients, in many scenarios, are probably in possession of low-quality data that are biased, noisy or even irrelevant. As a result, they could significantly slow down the convergence of the global model we aim to build and also compromise its quality. In light of this, we first present a new view of local data by looking into the representation space and observing that they converge in distribution to Normal distributions before activation. We provide theoretical analysis to support our finding. Further, we propose FedProf, a novel algorithm for optimizing FL over non-IID data of mixed quality. The key of our approach is a distributional representation profiling and matching scheme that uses the global model to dynamically profile data representations and allows for low-cost, lightweight representation matching. Using the scheme we sample clients adaptively in FL to mitigate the impact of low-quality data on the training process. We evaluated our solution with extensive experiments on different tasks and data conditions under various FL settings. The results demonstrate that the selective behavior of our algorithm leads to a significant reduction in the number of communication rounds and the amount of time (up to 2.4× speedup) for the global model to converge and also provides accuracy gain."",""1558-2183"","""",""10.1109/TPDS.2023.3265588"",""National Natural Science Foundation of China(grant numbers:62072187)"; Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101420002); Guangdong Major Project of Basic and Applied Basic Research(grant numbers:2019B030302002); Guangdong Marine Economic Development Special Fund Project(grant numbers:GDNRC[2022]17); Guangzhou Development Zone Science and Technology Project(grant numbers:2021GH10,2020GH10);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10097520"",""Distributed systems";federated learning;machine learning;neural networks;"representation learning"",""Training";Distributed databases;Data models;Computational modeling;Servers;Task analysis;"Convergence"","""",""1"","""",""65"",""IEEE"",""7 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Fine-Grained Performance and Cost Modeling and Optimization for FaaS Applications,""C. Lin"; N. Mahmoudi; C. Fan;" H. Khazaei"",""Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada"; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada;" Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""180"",""194"",""Function-as-a-Service (FaaS) has become a mainstream cloud computing paradigm for developers to build cloud-native applications in recent years. By taking advantage of serverless architecture, FaaS applications bring many desirable benefits, including built-in scalability, high availability, and improved cost-effectiveness. However, predictability and trade-off of performance and cost are still key pitfalls for FaaS applications due to poor infrastructure transparency and lack of performance and cost models that fit the new paradigm. In this study, we therefore fill this gap by proposing formal performance and cost modeling and optimization algorithms, which enable accurate prediction and fine-grained control over the performance and cost of FaaS applications. The proposed model and algorithms provide better predictability and trade-off of performance and cost for FaaS applications, which help developers to make informed decisions on cost reduction, performance improvement, and configuration optimization. We validate the proposed model and algorithms via extensive experiments on AWS. We show that the modeling algorithms can accurately estimate critical metrics, including response time, cost, exit status, and their distributions, regardless of the complexity and scale of the application workflow. Also, the depth-first bottleneck alleviation algorithm for trade-off analysis can effectively solve two optimization problems with fine-grained constraints."",""1558-2183"","""",""10.1109/TPDS.2022.3214783"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9919386"",""Cloud serverless computing";performance modeling;performance optimization;cost modeling;"cost optimization"",""Costs";Optimization;Time factors;Firing;Analytical models;Computational modeling;"Cloud computing"","""",""7"","""",""51"",""IEEE"",""14 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Floating Point Calculation of the Cube Function on FPGAs,""R. R. Osorio"",""CITIC, Computer Architecture Group, Universidade da Coruña, Coruña, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Nov 2022"",""2023"",""34"",""1"",""372"",""382"",""Specialized arithmetic units allow fast and efficient computation of lesser used mathematical functions. The overall impact of those units would be negligible in a general purpose processor, as added circuitry makes chips more complex despite most software would seldom make use of it. On the opposite side, custom computing machines are built for a specific task, and they can always benefit from specialized units if they are available. In this work, floating point architectures are proposed for computing the cube on Intel and Xilinx FPGAs. Those implementations reduce the cost and latency compared to using simple floating point multiplications and squarers."",""1558-2183"","""",""10.1109/TPDS.2022.3220039"",""Ministry of Science and Innovation of Spain(grant numbers:PID2019-104184RB-I00 / AEI / 10.13039/501100011033)"; Xunta de Galicia and FEDER funds;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9940568"",""Floating-point arithmetic";numerical analysis;field programmable gate arrays;"digital integrated circuits"",""Field programmable gate arrays";Computer architecture;Complexity theory;Mathematical models;Arithmetic;Pipeline processing;"Costs"","""","""","""",""21"",""IEEE"",""7 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FLUPS - A Flexible and Performant Massively Parallel Fourier Transform Library,""P. Balty"; P. Chatelain;" T. Gillis"",""Institute of Mechanics, Materials, and Civil Engineering, UCLouvain, Ottignies-Louvain-la-Neuve, Belgium"; Institute of Mechanics, Materials, and Civil Engineering, UCLouvain, Ottignies-Louvain-la-Neuve, Belgium;" Institute of Mechanics, Materials, and Civil Engineering, UCLouvain, Ottignies-Louvain-la-Neuve, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""23 May 2023"",""2023"",""34"",""7"",""2011"",""2024"",""Massively parallel Fourier transforms are widely used in computational sciences, and specifically in computational fluid dynamics which involves unbounded Poisson problems. In practice the latter is usually the most time-consuming operation due to its inescapable all-to-all communication pattern. The original flups library tackles that issue with an implementation of the distributed Fourier transform tailor-made for successive resolutions of unbounded Poisson problems. However the proposed implementation lacks of flexibility as it only supports cell-centered data layout and features a plain communication strategy. This work extends the library along two directions. First, flups’ implementation is generalized to support a node-centered data layout. Second, three distinct approaches are provided to handle the communications: one all-to-all, and two non-blocking implementations relying on manual packing and MPI_Datatype to communicate over the network. The proposed software is validated against analytical solutions for unbounded, semi-unbounded, and periodic domains. The performance of the approaches is then compared against accFFT, another distributed FFT implementation, using a periodic case. Finally the performance metrics of each implementation are analyzed and detailed on various top-tier European facilities up to 49,152 cores. This work brings flups up to a fully production-ready and performant distributed FFT library, featuring all the possible types of FFTs and with flexibility in the data-layout."",""1558-2183"","""",""10.1109/TPDS.2023.3254302"",""Wallonie-Bruxelles International (WBI) excellence fellowship (TG)"; F.R.S.-FNRS postdoctoral fellowship (TG); Université catholique de Louvain; Fonds de la Recherche Scientifique de Belgique (F.R.S.-FNRS)(grant numbers:2.5020.11); Waalse Gewest(grant numbers:1117545); MeluXina and Vega(grant numbers:EHPC-BEN-2022B01,EHPC-BEN-2022B06); LUMI-C(grant numbers:EHPC-DEV-2022D01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10064099"",""Distributed applications";"fast fourier transforms"",""Layout";Boundary conditions;Libraries;Fourier transforms;Discrete Fourier transforms;Three-dimensional displays;"Discrete cosine transforms"","""","""","""",""20"",""IEEE"",""8 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Fold3D: Rethinking and Parallelizing Computational and Communicational Tasks in the Training of Large DNN Models,""F. Li"; S. Zhao; Y. Qing; X. Chen; X. Guan; S. Wang; G. Zhang;" H. Cui"",""Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, China"; Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, China; Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, China; Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, China; Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, China; Theory Lab, 2012 Labs, Huawei Technoloies, Co. Ltd, Hong Kong, SAR, China; Theory Lab, 2012 Labs, Huawei Technoloies, Co. Ltd, Hong Kong, SAR, China;" Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Mar 2023"",""2023"",""34"",""5"",""1432"",""1449"",""Training a large DNN (e.g., GPT3) efficiently on commodity clouds is challenging even with the latest 3D parallel training systems (e.g., Megatron v3.0). In particular, along the pipeline parallelism dimension, computational tasks that produce a whole DNN's gradients with multiple input batches should be concurrently activated"; along the data parallelism dimension, a set of heavy-weight communications (for aggregating the accumulated outputs of computational tasks) is inevitably serialized after the pipelined tasks, undermining the training performance (e.g., in Megatron, data parallelism caused all GPUs idle for over 44% of the training time) over commodity cloud networks. To deserialize these communicational and computational tasks, we propose the AIAO scheduling (for 3D parallelism) which slices a DNN into multiple segments, so that the computational tasks processing the same DNN segment can be scheduled together, and the communicational tasks that synchronize this segment can be launched and overlapped (deserialized) with other segments’ computational tasks. We realized this idea in our Fold3D training system. Extensive evaluation shows Fold3D eliminated most of the all-GPU 44% idle time in Megatron (caused by data parallelism), leading to 25.2%–42.1% training throughput improvement compared to four notable baselines over various settings;" Fold3D's high performance scaled to many GPUs."",""1558-2183"","""",""10.1109/TPDS.2023.3247883"",""Huawei Flagship Research"; HKU-SCF FinTech Academy R&D Funding Scheme in 2021 and 2022; National Key R&D Program of China(grant numbers:2022ZD0160200); HK RIF(grant numbers:R7030-22); HK ITF(grant numbers:GHP/169/20SZ); Shanghai AI Lab;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10050126"",""3D parallelism";DNN;deep learning;distributed training;GPU;machine learning;"pipeline parallelism"",""Task analysis";Training;Three-dimensional displays;Processor scheduling;Graphics processing units;Pipeline processing;"Computational modeling"","""","""","""",""79"",""CCBY"",""22 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Frequency-Domain Inference Acceleration for Convolutional Neural Networks Using ReRAMs,""B. Liu"; Z. Jiang; Y. Wu; J. Wu; X. Chen; P. Liu; Q. Zhou;" Y. Han"",""School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, Guangdong, China"; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, Guangdong, China; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, Guangdong, China; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, Guangdong, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, Guangdong University of Technology, Guangzhou, Guangdong, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, Gansu, China;" Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2023"",""2023"",""34"",""12"",""3133"",""3146"",""Convolutional neural networks (CNNs) (including 2D and 3D convolutions) are popular in video analysis tasks such as action recognition and activity understanding. Fast algorithms such as fast Fourier transforms (FFTs) are promising in significantly reducing computation complexity by transforming convolution into frequency domain. In frequency space, conventional spatial convolutions are replaced with simpler element-wise complex multiplications. Conventional application-specific-integrated-circuit (ASIC) based frequency-domain accelerators can achieve effective performance boost but come at the cost of significant energy consumption, owing to the hierarchical memory organization. We propose a frequency-domain resistive random access memory (ReRAM) based inference accelerator called FDA that can process element-wise complex multiplication in memory for both 2D and 3D CNNs. Each ReRAM-based frequency-domain process element (PE) with two ReRAM cells can perform an element-wise complex multiplication in two continuous execution cycles. We then provide a flexible dataflow to alleviate the redundant data movements by frequency-domain data reuse and inherent symmetrical characteristic for both 2D and 3D convolutions. Evaluation results based on representative both 2D and 3D CNN benchmarks demonstrate that FDA outperforms state-of-the-art baselines with better performance and energy efficiency."",""1558-2183"","""",""10.1109/TPDS.2023.3322907"",""National Natural Science Foundation of China(grant numbers:62302102,62122076,62174038)"; State Key Laboratory of Computer Architecture(grant numbers:CARCHB202119); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515012844,2022A1515110599,202201010347);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10274825"",""Frequency-domain accelerator";energy efficiency;resistive random access memory;"frequency-domain convolutions"",""Frequency-domain analysis";Three-dimensional displays;Energy efficiency;Memory management;Energy consumption;Organizations;"Convolutional neural networks"","""","""","""",""41"",""IEEE"",""9 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"From Deterioration to Acceleration: A Calibration Approach to Rehabilitating Step Asynchronism in Federated Optimization,""F. Wu"; S. Guo; H. Wang; H. Zhang; Z. Qu; J. Zhang;" Z. Liu"",""Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong"; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Computer Science and Engineering department, Michigan State University, East Lansing, MI, USA; School of Computer and Information, Hohai University, Nanjing, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong;" Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Mar 2023"",""2023"",""34"",""5"",""1548"",""1559"",""In the setting of federated optimization, where a global model is aggregated periodically, step asynchronism occurs when participants conduct model training by efficiently utilizing their computational resources. It is well acknowledged that step asynchronism leads to objective inconsistency under non-i.i.d. data, which degrades the model’s accuracy. To address this issue, we propose a new algorithm FedaGrac, which calibrates the local direction to a predictive global orientation. Taking advantage of the estimated orientation, we guarantee that the aggregated model does not excessively deviate from the global optimum while fully utilizing the local updates of faster nodes. We theoretically prove that FedaGrac holds an improved order of convergence rate than the state-of-the-art approaches and eliminates the negative effect of step asynchronism. Empirical results show that our algorithm accelerates the training and enhances the final accuracy."",""1558-2183"","""",""10.1109/TPDS.2023.3250513"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101400003)"; Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); Areas of Excellence Scheme(grant numbers:AoE/E-601/22-R); General Research Fund(grant numbers:152203/20E,152244/21E,152169/22E); Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ20200109142008673); National Natural Science Foundation of China(grant numbers:62102131); Natural Science Foundation of Jiangsu Province(grant numbers:BK20210361);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10070571"",""Computational heterogeneity";data heterogeneity;"federated learning"",""Convergence";Training;Computational modeling;Data models;Servers;Distributed databases;"Prediction algorithms"","""",""1"","""",""57"",""IEEE"",""14 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"FSP: Towards Flexible Synchronous Parallel Frameworks for Distributed Machine Learning,""Z. Wang"; Y. Tu; N. Wang; L. Gao; J. Nie; Z. Wei; Y. Gu;" G. Yu"",""Faculty of Information Science and Engineering, Ocean University of China, Qingdao, Shandong, China"; Faculty of Information Science and Engineering, Ocean University of China, Qingdao, Shandong, China; Faculty of Information Science and Engineering, Ocean University of China, Qingdao, Shandong, China; Department of Electrical and Computer Engineering, University of Massachusetts Amherst, Amherst, MA, USA; Faculty of Information Science and Engineering, Ocean University of China, Qingdao, Shandong, China; Faculty of Information Science and Engineering, Ocean University of China, Qingdao, Shandong, China; School of Computer Science and Engineering, Northeastern University, Shenyang, Liaoning, China;" School of Computer Science and Engineering, Northeastern University, Shenyang, Liaoning, China"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Dec 2022"",""2023"",""34"",""2"",""687"",""703"",""Myriad of machine learning (ML) algorithms refine model parameters iteratively. Existing synchronous data-parallel frameworks can accelerate training with convergence guarantees. However, the pre-assigned workload-based synchronous design still poses great challenges, since fast workers must wait for slow, straggling ones, especially in a heterogeneous computing cluster. Asynchronous alternatives can bypass this performance bottleneck, but at expense of potentially losing convergence guarantees. This article proposes a new time-based flexible synchronous parallel framework (FSP). It provides strict convergence analysis by consistently updating parameters, as well as significant cost reduction by completely unleashing the power of fast workers. It identifies the optimal synchronization frequency, by online balancing costs of parameters’ update and benefits brought by their freshness. Besides the basic goal of keeping all workers fully CPU-utilized, FSP also aims to keep data spread over the cluster fully utilized, so that they can contribute to convergence with equal opportunities. These proposals are all implemented in a prototype system Flegel, with additional engineering optimizations for further performance enhancement and programming facilitation. Experiments demonstrate that Flegel significantly outperforms recent studies."",""1558-2183"","""",""10.1109/TPDS.2022.3228733"",""National Natural Science Foundation of China(grant numbers:61902366,U22A2068)"; Fundamental Research Funds for the Central Universities(grant numbers:202042008); National Key Research and Development Program of China(grant numbers:2021YFF0704000); National Natural Science Foundation of China(grant numbers:61902365,62072083); National Science Foundation(grant numbers:CNS-1815412,CNS-1908536); Computer Department of Ocean University of China(grant numbers:CSZS2022004);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9983509"",""Machine learning";distributed computation;synchronous parallel model;straggler;"workload balance"",""Training";Convergence;Computational modeling;Runtime;Costs;Synchronization;"Heuristic algorithms"","""","""","""",""57"",""IEEE"",""13 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"FT-BLAS: A Fault Tolerant High Performance BLAS Implementation on x86 CPUs,""Y. Zhai"; E. Giem; K. Zhao; J. Liu; J. Huang; B. M. Wong; C. R. Shelton;" Z. Chen"",""University of California, Riverside, Riverside, CA, USA"; University of California, Riverside, Riverside, CA, USA; University of Alabama at Birmingham, Birmingham, AL, USA; University of California, Riverside, Riverside, CA, USA; University of California, Riverside, Riverside, CA, USA; University of California, Riverside, Riverside, CA, USA; University of California, Riverside, Riverside, CA, USA;" University of California, Riverside, Riverside, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Oct 2023"",""2023"",""34"",""12"",""3207"",""3223"",""Basic Linear Algebra Subprograms (BLAS) serve as a foundational library for scientific computing and machine learning. In this article, we present a new BLAS implementation, FT-BLAS, that provides performance comparable to or faster than state-of-the-art BLAS libraries, while being capable of tolerating soft errors on-the-fly. At the algorithmic level, we propose a hybrid strategy to incorporate fault-tolerant functionality. For memory-bound Level-1 and Level-2 BLAS routines, we duplicate computing instructions and re-use data at the register level to avoid memory overhead when validating the runtime correctness. Here we novelly propose to utilize mask registers on AVX512-enabled processors and SIMD registers on AVX2-enabled processors to store intermediate comparison results. For compute-bound Level-3 BLAS routines, we fuse memory-intensive operations such as checksum encoding and verification into the GEMM assembly kernels to optimize the memory footprint. We also design cache-friendly parallel algorithms for our fault-tolerant library. Through a series of architectural-aware optimizations, we manage to maintain the fault-tolerant overhead at a negligible order ($< $<3%). Experimental results obtained on widely-used processors such as Intel Skylake, Intel Cascade Lake, and AMD Zen2 demonstrate that FT-BLAS offers high reliability and high performance – faster than Intel MKL, OpenBLAS, and BLIS by up to 3.50%, 22.14%, and 21.70%, respectively, for both serial and parallel routines spanning all three levels of BLAS we benchmarked, even under hundreds of errors injected per minute."",""1558-2183"","""",""10.1109/TPDS.2023.3316011"",""U.S. Department of Energy"; Office of Science; Office of Advanced Scientific Computing Research, Scientific Discovery; Advanced Computing(grant numbers:DE-SC0022209);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10262212"",""BLAS";SIMD;assembly optimization;dual modular redundancy;algorithm-based fault tolerance;AVX-512;AVX2;OpenMP;"parallel algorithm"",""Program processors";Libraries;Fault tolerant systems;Transient analysis;Circuit faults;Error correction codes;"Registers"","""","""","""",""80"",""IEEE"",""25 Sep 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Full-Stack Optimizing Transformer Inference on ARM Many-Core CPU,""J. Jiang"; J. Du; D. Huang; Z. Chen; Y. Lu;" X. Liao"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China;" School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Jun 2023"",""2023"",""34"",""7"",""2221"",""2235"",""The past several years have witnessed tremendous success of transformer models in natural language processing (NLP), and their current landscape is increasingly diverse. Although GPU gradually becomes the dominating workhorse and de facto standard for deep learning, there are still many scenarios where using CPU remains a prevalent choice.Recently, ARM many-core processor starts emigrating to cloud computing and high-performance computing, which is promising to deploy transformer inference. In this paper, we identify several performance bottlenecks of existing inference runtime on many-core CPU including low-core usage, isolated thread configuration, inappropriate implementation of general matrix multiply (GEMM), and redundant computations for variable-length inputs. To tackle these problems, full-stack optimizations are conducted for these challenges from service level to operator level. We explore multi-instance parallelization at the service level to improve CPU core usage. To improve parallel efficiency of the inference runtime, we design NUMA-aware thread scheduling and a look-up table for optimal parallel configurations. The GEMM implementation is tailored for some critical modules to exploit the characteristics of transformer workload. To eliminate redundant computations, a novel storage format is designed and implemented to pack sparse data and a load balancing strategy is proposed for tasks with different sparsity. Experiments show that our implementation can outperform existing solutions by 1.1x to 6x with fixed-length inputs. For variable-length inputs, it achieves 1.9x to 8x speedups on different ARM many-core processors."",""1558-2183"","""",""10.1109/TPDS.2023.3280805"",""National Key R&D Program of China(grant numbers:2021YFB0301300)"; Zhejiang Lab(grant numbers:2021KC0AB04); Major Program of Guangdong Basic and Applied Research(grant numbers:2019B030302002); Program for Guangdong Natural Science Foundation(grant numbers:2018B030312002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10138027"",""ARM";inference;many-core CPU;NUMA;"transformer"",""Transformers";Optimization;Transformer cores;Graphics processing units;Computational modeling;Deep learning;"Runtime"","""",""1"","""",""31"",""IEEE"",""29 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"GEM: Ultra-Efficient Near-Memory Reconfigurable Acceleration for Read Mapping by Dividing and Predictive Scattering,""L. Chen"; J. Zhu; G. Peng; M. Liu; S. Wei;" L. Liu"",""School of Integrated Circuits, Tsinghua University, Beijing, China"; School of Integrated Circuits, Tsinghua University, Beijing, China; School of Integrated Circuits, Tsinghua University, Beijing, China; Department of DRAM, Beijing Superstring Academy of Memory Technology, Beijing, China; School of Integrated Circuits, Tsinghua University, Beijing, China;" School of Integrated Circuits, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Oct 2023"",""2023"",""34"",""12"",""3059"",""3072"",""Read mapping, which maps billions of reads to a reference DNA, poses a significant performance bottleneck in genomic analysis. Current accelerators for read mapping are primarily bounded by the intensive and random memory access to huge datasets. Near-data processing (NDP) infrastructures are promising to provide extremely high bandwidth. However, existing frameworks failed to reach this potential due to poor locality and high redundancy. Our idea is to introduce prediction under the insight that candidate mapping positions become predictable when the reference is organized in coarse-grain slices. We present GEM (Genomic Memory), an ultra-efficient near-memory accelerator for read mapping. GEM adopts a novel data-centric framework, named dividing-and-predictive-scattering (DPS), which synthesizes information of seed existence to predict the target mapping locations to reduce memory access redundancy. During preparation, DPS divides the reference into coarse-grained slices and creates predictive filters to assess the likelihood of reads belonging to each slice. During mapping, DPS predicts and scatters reads to considerably fewer slices compared than without prediction. By employing small on-chip SRAM-based predictors with high accuracy, DPS minimizes unnecessary DRAM access and data movement from remote memory. In essence, DPS trades pre-seeding predictors for localized access patterns and low redundancy, hence achieving high throughput for data-intensive applications. We implement GEM by integrating coarse-grain reconfigurable architectures (CGRAs) in the logic layer of a 3D-stacked DRAM infrastructure, utilizing the massive banks as slices. GEM leverages CGRAs for their flexibility in supporting various algorithms tailored to different datasets. Bloom filters are leveraged for slice prediction, providing an error rate below 1%. Evaluation results demonstrate that GEM reduces memory requests by 95% and alignments by 87%, achieving a throughput improvement of 15.3× and 11.0× compared to compute-centric and broadcast-based baselines on the same NDP platform. Overall, GEM achieves a $3.5\times$3.5× throughput improvement and $2.1\times$2.1× energy efficiency compared to state-of-the-art ASIC accelerators."",""1558-2183"","""",""10.1109/TPDS.2023.3309462"",""National Natural Science Foundation of China(grant numbers:61834002,62204139)"; National Key R&D Program of China(grant numbers:2021YFB2701201);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10233117"",""Data-centric computing";dividing and scattering;genome assembly;near memory computing;read mapping;"reconfigurable computing"",""Genomics";Bioinformatics;Indexes;Random access memory;Computational modeling;Throughput;"Termination of employment"","""","""","""",""83"",""IEEE"",""28 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"GossipFL: A Decentralized Federated Learning Framework With Sparsified and Adaptive Communication,""Z. Tang"; S. Shi; B. Li;" X. Chu"",""Hong Kong Baptist University, Hong Kong"; Harbin Institute of Technology, Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong;" The Hong Kong University of Science and Technology (Guangzhou), Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Jan 2023"",""2023"",""34"",""3"",""909"",""922"",""Recently, federated learning (FL) techniques have enabled multiple users to train machine learning models collaboratively without data sharing. However, existing FL algorithms suffer from the communication bottleneck due to network bandwidth pressure and/or low bandwidth utilization of the participating clients in both centralized and decentralized architectures. To deal with the communication problem while preserving the convergence performance, we introduce a communication-efficient decentralized FL framework GossipFL. In GossipFL, we 1) design a novel sparsification algorithm to enable that each client only needs to communicate with one peer with a highly sparsified model, and 2) propose a new and novel gossip matrix generation algorithm that can better utilize the bandwidth resources while preserving the convergence property. We also theoretically prove that GossipFL has convergence guarantees. We conduct experiments with three convolutional neural networks on two datasets (IID and non-IID) under two distributed environments (14 clients and 100 clients) to verify the effectiveness of GossipFL. Experimental results show that GossipFL takes less communication traffic for 38.5% and less communication time for $49.8$49.8% than state-of-the-art solutions while achieving comparative model accuracy."",""1558-2183"","""",""10.1109/TPDS.2022.3230938"",""RGC RIF(grant numbers:R6021-20)"; RGC GRF(grant numbers:16209120,16200221);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9996127"",""Deep learning";federated learning;"communication efficiency"",""Bandwidth";Training;Servers;Convergence;Topology;Data models;"Federated learning"","""",""7"","""",""53"",""IEEE"",""21 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GraphAGILE: An FPGA-Based Overlay Accelerator for Low-Latency GNN Inference,""B. Zhang"; H. Zeng;" V. K. Prasanna"",""Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA"; Meta Platforms, Inc., Menlo Park, CA, USA;" Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Jul 2023"",""2023"",""34"",""9"",""2580"",""2597"",""This article presents GraphAGILE, a domain-specific FPGA-based overlay accelerator for graph neural network (GNN) inference. GraphAGILE consists of (1) a novel unified architecture design with an instruction set, and (2) a compiler built upon the instruction set that can quickly generate optimized code. Due to the proposed instruction set architecture (ISA) and the compiler, GraphAGILE does not require any FPGA reconfiguration when performing inference on various GNN models and input graphs. For the architecture design, we propose a novel hardware module named Adaptive Computation Kernel (ACK), that can execute various computation kernels of GNNs, including general matrix multiplication (GEMM), sparse-dense matrix multiplication (SpDMM), and sampled dense-dense matrix multiplication (SDDMM). The compiler takes the specifications of a GNN model and the graph meta data (e.g., the number of vertices and edges) as input, and generates a sequence of instructions for inference execution. We develop the following compiler optimizations to reduce inference latency: 1) computation order optimization that automatically reorders the computation graph to reduce the total computation complexity, 2) layer fusion that merges adjacent layers to reduce data communication volume, 3) data partitioning with a partition-centric execution scheme that partitions the input graph to fit the available on-chip memory of FPGA, 4) kernel mapping that automatically selects execution mode for ACK, and performs task scheduling to overlap computation with data communication and achieves dynamic load balance. We implement GraphAGILE on a state-of-the-art FPGA platform, Xilinx Alveo U250. GraphAGILE can execute widely used GNN models, including GCN, GAT, GIN, GraphSAGE, SGC and other GNN models supported by GraphGym. Experimental results show that GraphAGILE achieves up to $47.1\times$47.1× ($3.9\times$3.9×) reduction in end-to-end latency, including the latency of compilation and hardware execution, compared with the state-of-the-art implementations on CPU (GPU), and achieves up to $2.9\times$2.9× reduction in hardware execution latency compared with the state-of-the-art FPGA accelerators."",""1558-2183"","""",""10.1109/TPDS.2023.3287883"",""National Science Foundation(grant numbers:CCF-1919289,OAC-2209563)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158054"",""Graph neural network";FPGA overlay accele- rator;hardware architecture;"low-latency inference"",""Graph neural networks";Field programmable gate arrays;Hardware;Computational modeling;Kernel;Optimization;"Instruction sets"","""",""2"","""",""33"",""IEEE"",""20 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Guest Editorial Reproducibility Initiative at the SC Conference Series: A Preface to the Special Section,""L. M. Weakley";" T. Robinson"",""Indiana University, Bloomington, IN, USA";" Swiss National Computing Centre, Lugano, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""11 May 2023"",""2023"",""34"",""6"",""1697"",""1698"",""Welcome to our special section on reproducibility in large-scale computational science. Reproducibility is one of the foundations of science, ensuring not only the transparency and rigor of the methods utilized for discovery but also that the research involved can be built upon. In order to enable research and to support open science, it is crucial that the computational work performed on high performance computing (HPC) systems is also reproducible."",""1558-2183"","""",""10.1109/TPDS.2023.3262899"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10123123"","""",""Special issues and sections";Reproducibility of results;Scientific computing;High performance computing;"Large scale systems"","""","""","""",""1"",""IEEE"",""11 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"HashCache: Accelerating Serverless Computing by Skipping Duplicated Function Execution,""Z. Wu"; Y. Deng; Y. Zhou; L. Cui;" X. Qin"",""Department of Computer Science, Jinan University, Guangzhou, China"; Department of Computer Science, Jinan University, Guangzhou, China; TSYS School of Computer Science, Columbus State University, Columbus, GA, USA; Department of Computer Science, Jinan University, Guangzhou, China;" Department of Computer Science and Software Engineering, Auburn University, Auburn, AL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Oct 2023"",""2023"",""34"",""12"",""3192"",""3206"",""Serverless computing is a leading force behind deploying and managing software in cloud computing. One inherent challenge in serverless computing is the increased overall latency due to duplicate computations. Our initial investigation into the function invocations of serverless applications reveals an abundance of duplicate invocations. Inspired by this critical observation, we introduce HashCache, a system designed to cache duplicate function invocations, thereby mitigating duplicate computations. In HashCache, serverless functions are classified into three categories, namely, computational functions, stateful functions, and environment-related functions. On the grounds of such a function classification, HashCache associates the stateful functions and their states to build an adaptive synchronization mechanism. With this support, HashCache exploits the cached results of computational and stateful functions to serve upcoming invocation requests to the same functions, thereby reducing duplicate computations. Moreover, HashCache stores remote files probed by stateful functions into a local cache layer, which further curtails invocation latency. We implement HashCache within the Apache OpenWhisk to forge a cache-enabled serverless computing platform. We conduct extensive experiments to quantitatively evaluate the performance of HashCache in terms of invocation latency and resource utilization. We compare HashCache against two state-of-the-art approaches - FaaSCache and OpenWhisk. The experimental results unveil that our HashCache remarkably reduces invocation latency and resource overhead. More specifically, HashCache curbs the 99-tail latency of FaaSCache and OpenWhisk by up to 91.37% and 95.96% in real-world serverless applications. HashCache also slashes the resource utilization of FaaSCache and OpenWhisk by up to 31.62% and 35.51%, respectively."",""1558-2183"","""",""10.1109/TPDS.2023.3323330"",""National Natural Science Foundation of China(grant numbers:62072214)"; Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021B1515120048); Open Project Program of Wuhan National Laboratory for Optoelectronics(grant numbers:2020WNLOKF006); National Science Foundation(grant numbers:IIS-1618669,OAC-1642133);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10275106"",""Cloud computing";function-as-a-service;performance optimization;"serverless computing"",""Containers";Runtime;Serverless computing;Python;Optimization;Monitoring;"Computer science"","""","""","""",""46"",""IEEE"",""10 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"HE-Booster: An Efficient Polynomial Arithmetic Acceleration on GPUs for Fully Homomorphic Encryption,""Z. Wang"; P. Li; R. Hou; Z. Li; J. Cao; X. Wang;" D. Meng"",""State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China"; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China; Indiana University at Bloomington, Bloomington, IN, USA;" State Key Laboratory of Information Security, Institute of Information Engineering, CAS, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Mar 2023"",""2023"",""34"",""4"",""1067"",""1081"",""Fully Homomorphic Encryption (FHE) enables secure offloading of computations to untrusted cloud servers as it allows computing on encrypted data. However, existing well-known FHE schemes suffer from heavy performance overheads. Thus numerous accelerations based on FPGAs, ASICs, and GPUs have been proposed. Compared to FPGAs and ASICs, GPUs have obvious advantages in productivity and development costs. And also, GPUs have already been widely deployed in commercial cloud or supercomputing centers. Therefore, we present HE-Booster, an efficient GPU-based FHE acceleration design. For single-GPU acceleration, a thorough systematic design is exploited to map five common phases in typical FHE schemes to the GPU parallel architecture. In particular, inspired by the regular architecture of NTT/INTT, a novel inter-thread local synchronization is proposed to exploit thread-level parallelism. For multi-GPU acceleration, we propose a scalable parallelization design that exploits data-level parallelism through fine-grained data partition under different representations. Finally, experiments on 1 NVIDIA GPU demonstrate that our work outperforms 251.7×, 78.5× and 164.9× than three mainstream CPU-based libraries HElib, SEAL, and PALISADE, and up to 170.5× speedup is obtained compared to the GPU-accelerated library cuHE. What's more, performing 8 homomorphic multiplications on 8 GPUs can deliver up to a 7.66× performance boost compared to a single-GPU implementation."",""1558-2183"","""",""10.1109/TPDS.2022.3228628"",""National Natural Science Fund of China for Distinguished Young Scholars(grant numbers:62125208)"; National Natural Science Foundation of China(grant numbers:62202467);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012383"",""Fully homomorphic encryption";GPU acceleration;"number-theoretic transform"",""Graphics processing units";Arithmetic;Cathode ray tubes;Synchronization;Libraries;Instruction sets;"Field programmable gate arrays"","""",""4"","""",""81"",""CCBYNCND"",""9 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Hierarchical Federated Learning With Momentum Acceleration in Multi-Tier Networks,""Z. Yang"; S. Fu; W. Bao; D. Yuan;" A. Y. Zomaya"",""School of Computer Science, University of Sydney, Sydney, NSW, Australia"; School of Computer Science, University of Sydney, Sydney, NSW, Australia; School of Computer Science, University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, University of Sydney, Sydney, NSW, Australia;" School of Information Technologies, University of Sydney, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Aug 2023"",""2023"",""34"",""10"",""2629"",""2641"",""In this article, we propose Hierarchical Federated Learning with Momentum Acceleration (HierMo), a three-tier worker-edge-cloud federated learning algorithm that applies momentum for training acceleration. Momentum is calculated and aggregated in the three tiers. We provide convergence analysis for HierMo, showing a convergence rate of $\mathcal {O}(\frac{1}{T})$O(1T). In the analysis, we develop a new approach to characterize model aggregation, momentum aggregation, and their interactions. Based on this result, we prove that HierMo achieves a tighter convergence upper bound compared with HierFAVG without momentum. We also propose HierOPT, which optimizes the aggregation periods (worker-edge and edge-cloud aggregation periods) to minimize the loss given a limited training time. By conducting the experiment, we verify that HierMo outperforms existing mainstream benchmarks under a wide range of settings. In addition, HierOPT can achieve a near-optimal performance when we test HierMo under different aggregation periods."",""1558-2183"","""",""10.1109/TPDS.2023.3294688"",""Australian Research Council Discovery Project(grant numbers:DP200103718)"; Australian Research Council Discovery Project(grant numbers:DP200103494);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10180030"",""Federated learning";momentum;convergence analysis;"edge computing"",""Convergence";Cloud computing;Training;Computer architecture;Internet;Federated learning;"Servers"","""","""","""",""45"",""IEEE"",""12 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"HierFedML: Aggregator Placement and UE Assignment for Hierarchical Federated Learning in Mobile Edge Computing,""Z. Xu"; D. Zhao; W. Liang; O. F. Rana; P. Zhou; M. Li; W. Xu; H. Li;" Q. Xia"",""Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, Liaoning, China"; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, Liaoning, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Physical Sciences and Engineering College, Cardiff University, Cardiff, U.K.; Hubei Key Laboratory of Distributed System Security, Hubei Engineering Research Center on Big Data Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China; Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, Liaoning, China; Department of Computer Science, Sichuan University, Chengdu, Sichuan, China; Ningxia Research Institute, China Coal Research Institute, Beijing, China;" Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, the International School of Information Science & Engineering, Dalian University of Technology, Dalian, Liaoning, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Dec 2022"",""2023"",""34"",""1"",""328"",""345"",""Federated learning (FL) is a distributed machine learning technique that enables model development on user equipments (UEs) locally, without violating their data privacy requirements. Conventional FL adopts a single parameter server to aggregate local models from UEs, and can suffer from efficiency and reliability issues – especially when multiple users issue concurrent FL requests. Hierarchical FL consisting of a master aggregator and multiple worker aggregators to collectively combine trained local models from UEs is emerging as a solution to efficient and reliable FL. The placement of worker aggregators and assignment of UEs to worker aggregators plays a vital role in minimizing the cost of implementing FL requests in a Mobile Edge Computing (MEC) network. Cost minimization associated with joint worker aggregator placement and UE assignment problem in an MEC network is investigated in this work. An optimization framework for FL and an approximation algorithm with an approximation ratio for a single FL request is proposed. Online worker aggregator placements and UE assignments for dynamic FL request admissions with uncertain neural network models, where FL requests arrive one by one without the knowledge of future arrivals, is also investigated by proposing an online learning algorithm with a bounded regret. The performance of the proposed algorithms is evaluated using both simulations and experiments in a real testbed with its hardware consisting of server edge servers and devices and software built upon an open source hierarchical FedML (HierFedML) environment. Simulation results show that the performance of the proposed algorithms outperform their benchmark counterparts, by reducing the implementation cost by at least 15% per FL request. Experimental results in the testbed demonstrate the performance gain using the proposed algorithms using real datasets for image identification and text recognition applications."",""1558-2183"","""",""10.1109/TPDS.2022.3218807"",""National Natural Science Foundation of China(grant numbers:62172068,62172071)"; Xinghai scholar; City University of Hong Kong(grant numbers:9380137/CS); National Natural Science Foundation of China(grant numbers:61972448);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9935309"",""Mobile edge computing";federated learning;aggregator placement and UE assignment;hierarchical federated learning framework;approximation algorithms;"online learning algorithms"",""Computational modeling";Costs;Training;Task analysis;Uncertainty;Servers;"Optimization"","""",""2"","""",""63"",""IEEE"",""2 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"HiFlash: Communication-Efficient Hierarchical Federated Learning With Adaptive Staleness Control and Heterogeneity-Aware Client-Edge Association,""Q. Wu"; X. Chen; T. Ouyang; Z. Zhou; X. Zhang; S. Yang;" J. Zhang"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China"; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, Shaanxi, China;" ECE Department, University of California, Davis, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Mar 2023"",""2023"",""34"",""5"",""1560"",""1579"",""Federated learning (FL) is a promising paradigm that enables collaboratively learning a shared model across massive clients while keeping the training data locally. However, for many existing FL systems, clients need to frequently exchange model parameters of large data size with the remote cloud server directly via wide-area networks (WAN), leading to significant communication overhead and long transmission time. To mitigate the communication bottleneck, we resort to the hierarchical federated learning paradigm of HiFL, which reaps the benefits of mobile edge computing and combines synchronous client-edge model aggregation and asynchronous edge-cloud model aggregation together to greatly reduce the traffic volumes of WAN transmissions. Specifically, we first analyze the convergence bound of HiFL theoretically and identify the key controllable factors for model performance improvement. We then advocate an enhanced design of HiFlash by innovatively integrating deep reinforcement learning based adaptive staleness control and heterogeneity-aware client-edge association strategy to boost the system efficiency and mitigate the staleness effect without compromising model accuracy. Extensive experiments corroborate the superior performance of HiFlash in model accuracy, communication reduction, and system efficiency."",""1558-2183"","""",""10.1109/TPDS.2023.3238049"",""National Natural Science Foundation of China(grant numbers:U20A20159,61972432)"; Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021B151520008); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021868"",""Client-edge association";federated learning;hierarchical mechanism;"staleness control"",""Adaptation models";Data models;Servers;Computational modeling;Wide area networks;Convergence;"Local area networks"","""",""7"","""",""51"",""IEEE"",""19 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"High-Level Data Abstraction and Elastic Data Caching for Data-Intensive AI Applications on Cloud-Native Platforms,""R. Gu"; Z. Xu; Y. Che; X. Wang; H. Dai; K. Zhang; B. Fan; H. Hou; L. Yi; Y. Ding; Y. Huang;" G. Chen"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Alibaba Group, Hangzhou, Zhejiang, China; Alluxio. Inc., San Mateo, CA, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Sep 2023"",""2023"",""34"",""11"",""2946"",""2964"",""Nowdays, it is prevalent to train deep learning models in cloud-native platforms that actively leverage containerization and orchestration technologies for high elasticity, low and flexible operation cost, and many other benefits. However, it also faces new challenges and our work is focusing on those related to I/O throughput for training, including complex data access, lack of matching dynamic I/O requirement, and inefficient I/O resource scheduling across different jobs. We propose Fluid, a cloud-native platform that provides DL training jobs with high-level data abstraction called Fluid Dataset to access training data from heterogeneous sources with elastic data acceleration. In addition, it comes with an on-the-fly cache system autoscaler that can match the online training speed and increase the number of cache replicas adaptively to alleviate I/O bottlenecks. To improve the overall performance of multiple DL jobs, Fluid co-orchestrate the data cache and DL jobs by arranging job scheduling in an appropriate order and can also schedule data cache and DL jobs on the same node to realize cache affinity. Experimental results show significant performance improvement of each individual DL job which uses dynamic computing resources with Fluid. For scheduling multiple DL jobs with same datasets, Fluid achieves around 2x performance speedup when integrated with existing widely-used and cutting-edge scheduling solutions through the appropriate job scheduling order. Besides, the cache affinity scheduling policy also improves job execution performance significantly. Fluid is now an open source project hosted by Cloud Native Computing Foundation (CNCF) with many production adopters."",""1558-2183"","""",""10.1109/TPDS.2023.3314659"",""National Natural Science Foundation of China(grant numbers:62072230,62272223)"; Jiangsu Province Science and Technology Key Program(grant numbers:BE2021729); Fundamental Research Funds for the Central Universities(grant numbers:020214380089,020214380098,020214912216); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10249214"",""Cloud native";dataset abstraction;elastic data cache;"job scheduling"",""Training";Processor scheduling;Fluids;Data models;Graphics processing units;Containers;"Job shop scheduling"","""","""","""",""41"",""IEEE"",""12 Sep 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"HPC Hardware Design Reliability Benchmarking With HDFIT,""P. Omland"; A. Netti; Y. Peng; A. Baldovin; M. Paulitsch; G. Espinosa; J. Parra; G. Hinz;" A. Knoll"",""Intel Corporation, Santa Clara, CA, USA"; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Intel Corporation, Santa Clara, CA, USA; Department of Informatics, Technical University Munich, Munich, Germany;" Department of Informatics, Technical University Munich, Munich, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Jan 2023"",""2023"",""34"",""3"",""995"",""1006"",""Chips pack ever more, ever smaller transistors. Fault rates increase in turn and become more concerning, particularly at the scale of High-Performance Computing (HPC) systems: on one hand, hardware fault protection is costly - more than 10% silicon area for floating-point units";" on the other, HPC users expect correct application output after the anticipated time of computation, but workloads are seldom bit-reproducible and tolerances in output are allowed for. Benign hardware faults causing errors within these tolerances are therefore acceptable: however, with abstract reliability targets such as ’undetected failures per time,’ current HPC system design does not allow for pursuing trade-offs between reliability and performance with respect to faults. To address the above, we propose a user-centric reliability benchmark to specify HPC system reliability targets, allowing for better performance optimizations in hardware design, while meeting HPC user expectations. Our open-source Hardware Design Fault Injection Toolkit (HDFIT) enables - for the first time - end-to-end hardware design reliability experiments: from netlist-level fault injection to application output error. In a proof of concept we present an HPC general matrix multiply (GEMM) reliability study, targeting a series of popular applications, and using HDFIT to benchmark an open-source GEMM accelerator."",""1558-2183"","""",""10.1109/TPDS.2023.3237777"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018868"",""Fault injection";fault model;fault tolerance;HPC reliability;hardware faults;hardware reliability;"program vulnerability"",""Hardware";Reliability;Circuit faults;Reliability engineering;Benchmark testing;Integrated circuit reliability;"Computational modeling"","""",""2"","""",""40"",""IEEE"",""17 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Hypergraph-Based Numerical Neural-Like P Systems for Medical Image Segmentation,""J. Xue"; L. Ren; B. Song; Y. Guo; J. Lu; X. Liu; G. Gong;" D. Li"",""Business School, Shandong Key Laboratory of Medical Physics and Image Processing, Shandong Normal University, Jinan, Shandong, China"; Business School, Shandong Key Laboratory of Medical Physics and Image Processing, Shandong Normal University, Jinan, Shandong, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences, Jinan, Shandong, China; Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences, Jinan, Shandong, China; Business School, Shandong Key Laboratory of Medical Physics and Image Processing, Shandong Normal University, Jinan, Shandong, China; Shandong Cancer Hospital and Institute, Shandong First Medical University and Shandong Academy of Medical Sciences, Jinan, Shandong, China;" School of Physics and Electronics, Shandong Key Laboratory of Medical Physics and Image Processing, Shandong Institute of Industrial Technology for Health Sciences and Precision Medicine, Shandong Normal University, Jinan, Shandong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2023"",""2023"",""34"",""4"",""1202"",""1214"",""Neural-like P systems are membrane computing models inspired by natural computing and are viewed as third-generation neural network models. Although real neurons have complex structures, classical neural-like P systems simplify the structures and corresponding mechanisms to two-dimensional graphs or tree-based firing and forgetting communications, which limit the real applications of these models. In this paper, we propose a hypergraph-based numerical neural-like (HNN) P system containing five types of neurons to describe the high-order correlations among neuron structures. Three new kinds of communication mechanisms among neurons are also proposed to address numerical variables and functions. Based on the new neural-like P system, a tumor/organ segmentation model for medical images is developed. The experimental results indicate that the proposed models outperform the state-of-the-art methods based on two hippocampal datasets and a multiple brain metastases dataset, thus verifying the effectiveness of the HNN P system in correctly segmenting tumors/organs."",""1558-2183"","""",""10.1109/TPDS.2023.3240174"",""National Natural Science Foundation of China(grant numbers:62172262,61876101,61971271,62272151,61972138,62271294)"; Natural Science Foundation of Shandong Province(grant numbers:ZR2019QF007); Jinan City-School Integration Development Strategy Project(grant numbers:JNSX2021023); Shandong Province Major Technological Innovation Project(grant numbers:2022CXGC020505); Natural Science Foundation of Hunan Province(grant numbers:2022JJ20016); Science and Technology Innovation Program of Hunan Province(grant numbers:2022RC1099); Open Research Projects of Zhejiang Lab(grant numbers:2021RD0AB02);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026615"",""Hypergraph";neural-like P systems;"medical image segmentation"",""Neurons";Image segmentation;Computational modeling;Medical diagnostic imaging;Numerical models;Hippocampus;"Biological neural networks"","""","""","""",""72"",""IEEE"",""26 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"iGniter: Interference-Aware GPU Resource Provisioning for Predictable DNN Inference in the Cloud,""F. Xu"; J. Xu; J. Chen; L. Chen; R. Shang; Z. Zhou;" F. Liu"",""Shanghai Key Laboratory of Multidimensional Information Processing, School of Computer Science and Technology, East China Normal University, Shanghai, China"; Shanghai Key Laboratory of Multidimensional Information Processing, School of Computer Science and Technology, East China Normal University, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, School of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; Shanghai Key Laboratory of Multidimensional Information Processing, School of Computer Science and Technology, East China Normal University, Shanghai, China; Guangdong Key Laboratory of Big Data Analysis and Processing, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong Province, China;" Peng Cheng Laboratory, Shenzhen, Guangdong Province, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Jan 2023"",""2023"",""34"",""3"",""812"",""827"",""GPUs are essential to accelerating the latency-sensitive deep neural network (DNN) inference workloads in cloud datacenters. To fully utilize GPU resources, spatial sharing of GPUs among co-located DNN inference workloads becomes increasingly compelling. However, GPU sharing inevitably brings severe performance interference among co-located inference workloads, as motivated by an empirical measurement study of DNN inference on EC2 GPU instances. While existing works on guaranteeing inference performance service level objectives (SLOs) focus on either temporal sharing of GPUs or reactive GPU resource scaling and inference migration techniques, how to proactively mitigate such severe performance interference has received comparatively little attention. In this paper, we propose iGniter, an interference-aware GPU resource provisioning framework for cost-efficiently achieving predictable DNN inference in the cloud. iGniter is comprised of two key components: (1) a lightweight DNN inference performance model, which leverages the system and workload metrics that are practically accessible to capture the performance interference";" (2) A cost-efficient GPU resource provisioning strategy that jointly optimizes the GPU resource allocation and adaptive batching based on our inference performance model, with the aim of achieving predictable performance of DNN inference workloads. We implement a prototype of iGniter based on the NVIDIA Triton inference server hosted on EC2 GPU instances. Extensive prototype experiments on four representative DNN models and datasets demonstrate that iGniter can guarantee the performance SLOs of DNN inference workloads with practically acceptable runtime overhead, while saving the monetary cost by up to $25\%$25% in comparison to the state-of-the-art GPU resource provisioning strategies."",""1558-2183"","""",""10.1109/TPDS.2022.3232715"",""NSFC(grant numbers:61972158,62172454)"; Science and Technology Commission of Shanghai Municipality(grant numbers:20511102802,18DZ2270800); BoRSF(grant numbers:LEQSF(2019-22)-RD-A-21,LEQSF(2021-22)-RD-D-07); NSF(grant numbers:OIA-2019511); National Key Research & Development (R&D) Plan(grant numbers:2022YFB4500704); The Major Key Project of PCL(grant numbers:PCL2022A05);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10002315"",""Cloud-based DNN inference";predictable performance;GPU resource provisioning;"performance interference"",""Graphics processing units";Interference;Resource management;Kernel;Performance evaluation;Delays;"Adaptation models"","""",""1"","""",""50"",""IEEE"",""28 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Improving Effectiveness of Simulation-Based Inference in the Massively Parallel Regime,""S. Kulkarni";" C. A. Moritz"",""Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA, USA";" Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2023"",""2023"",""34"",""4"",""1100"",""1114"",""Simulation-based Inference (SBI) is a widely used set of algorithms to learn the parameters of complex scientific simulation models. While primarily run on CPUs in High-Performance Compute clusters, these algorithms have been shown to scale in performance when developed to be run on massively parallel architectures such as GPUs. While parallelizing existing SBI algorithms provides us with performance gains, this might not be the most efficient way to utilize the achieved parallelism. This work proposes a new parallelism-aware adaptation of an existing SBI method, namely approximate Bayesian computation with Sequential Monte Carlo(ABC-SMC). This new adaptation is designed to utilize the parallelism not only for performance gain, but also toward qualitative benefits in the learnt parameters. The key idea is to replace the notion of a single ‘step-size’ hyperparameter, which governs how the state space of parameters is explored during learning, with step-sizes sampled from a tuned Beta distribution. This allows this new ABC-SMC algorithm to more efficiently explore the state-space of the parameters being learned. We test the effectiveness of the proposed algorithm to learn parameters for an epidemiology model running on a Tesla T4 GPU. Compared to the parallelized state-of-the-art SBI algorithm, we get similar quality results in $\sim 100$∼100x fewer simulations and observe $\sim 80$∼80x lower run-to-run variance across 10 independent trials."",""1558-2183"","""",""10.1109/TPDS.2023.3238045"",""Meta Research for ‘Probability and Programming’";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021877"",""Approximate Bayesian computation";Bayesian inference;COVID-19;compartmental models;epidemiology;GPU;high performance computing;likelihood-free inference;parallel algorithms;simulation-based inference;statistical machine learning;"stochastic optimization"",""Inference algorithms";Computational modeling;Biological system modeling;Approximation algorithms;Perturbation methods;Optimization;"Bayes methods"","""","""","""",""29"",""IEEE"",""19 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Improving the Efficiency of Deadlock Detection in MPI Programs Through Trace Compression,""Y. Huang"; T. Wang; Z. Yin; E. Mercer;" B. Ogles"",""School of Information and Engineering, Southwestern University of Finance and Economics, Chengdu, China"; School of Information and Engineering, Southwestern University of Finance and Economics, Chengdu, China; School of Information and Engineering, Southwestern University of Finance and Economics, Chengdu, China; Computer Science Department, Brigham Young University, Provo, UT, USA;" Computer Science Department, Brigham Young University, Provo, UT, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Nov 2022"",""2023"",""34"",""1"",""400"",""415"",""This article presents a static deadlock analysis for single-path MPI programs. Deadlock is when processes are blocked indefinitely by a circular communication dependency. A single path program is one that does not decode messages for control flow. The analysis records a program execution in the form of a trace and then determines from that trace whether there exists any feasible deadlocking schedules. The primary contribution is the combining of identical consecutive sends or receives into single macro actions. This simplified trace is analyzed for potential deadlock cycles. An abstract machine identifies infeasible cycles, and those not identified by the machine are encoded as satisfiability problems for an SMT solver to resolve. The action combination reduces the complexity of identifying and filtering cycles before needing the costly SMT solver. This article shows the effectiveness of the action combination in experiments on a benchmark suite comparing to traces without action combination and other state-of-the-art deadlock analyses."",""1558-2183"","""",""10.1109/TPDS.2022.3218346"",""National Natural Science Foundation of China(grant numbers:62102325)"; Ministry of Education of Humanities and Social Science Project(grant numbers:20YJC630146);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933641"",""Static analysis";MPI;"SMT"",""System recovery";Encoding;Benchmark testing;Runtime;Schedules;Syntactics;"Semantics"","""","""","""",""35"",""IEEE"",""31 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving the Scalability of GPU Synchronization Primitives,""P. Dalmia"; R. Mahapatra; J. Intan; D. Negrut;" M. D. Sinclair"",""University of Wisconsin-Madison, Madison, WI, USA"; University of California, San Diego, La Jolla, CA, USA; University of Illinois at Urbana-Champaign, Champaign, IL, USA; University of Wisconsin-Madison, Madison, WI, USA;" University of Wisconsin-Madison, Madison, WI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2022"",""2023"",""34"",""1"",""275"",""290"",""General-purpose GPU applications increasingly use synchronization to enforce ordering between many threads accessing shared data. Accordingly, recently there has been a push to establish a common set of GPU synchronization primitives. However, the expressiveness of existing GPU synchronization primitives is limited. In particular the expensive GPU atomics often used to implement fine-grained synchronization make it challenging to implement efficient algorithms. Consequently, as GPU algorithms scale to millions or billions of threads, existing GPU synchronization primitives either scale poorly or suffer from livelock or deadlock issues because of heavy contention between threads accessing shared synchronization objects. We seek to overcome these inefficiencies by designing more efficient, scalable GPU barriers and semaphores. In particular, we show how multi-level sense reversing barriers and priority mechanisms for semaphores can be designed with the GPUs unique processing model in mind to improve performance and scalability of GPU synchronization primitives. Our results show that the proposed designs significantly improve performance compared to state-of-the-art solutions like CUDA Cooperative Groups and optimized CPU-style synchronization algorithms at medium and high contention levels, scale to an order of magnitude more threads, and avoid livelock in these situations unlike prior open source algorithms. Overall, across three modern GPUs the proposed barrier algorithm improves performance by an average of 33% over a GPU tree barrier algorithm and improves performance by an average of 34% over CUDA Cooperative Groups for five full-sized benchmarks at high contention levels";" the new semaphore algorithm improves performance by an average of 83% compared to prior GPU semaphores."",""1558-2183"","""",""10.1109/TPDS.2022.3218508"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9933620"","""",""Synchronization";Graphics processing units;Instruction sets;Scalability;Kernel;Coherence;"Message systems"","""","""","""",""109"",""IEEE"",""31 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Incentive Mechanism Design for Joint Resource Allocation in Blockchain-Based Federated Learning,""Z. Wang"; Q. Hu; R. Li; M. Xu;" Z. Xiong"",""Department of Computer and Information Science, Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA"; Department of Computer and Information Science, Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA; Department of Computer Science, Bowling Green State University, Bowling Green, OH, USA; School of Computer Science and Technology, Shandong University, Jinan, China;" Pillar of Information Systems Technology and Design, Singapore University of Technology Design, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Mar 2023"",""2023"",""34"",""5"",""1536"",""1547"",""Blockchain-based federated learning (BCFL) has recently gained tremendous attention because of its advantages, such as decentralization and privacy protection of raw data. However, there has been few studies focusing on the allocation of resources for the participated devices (i.e., clients) in the BCFL system. Especially, in the BCFL framework where the FL clients are also the blockchain miners, clients have to train the local models, broadcast the trained model updates to the blockchain network, and then perform mining to generate new blocks. Since each client has a limited amount of computing resources, the problem of allocating computing resources to training and mining needs to be carefully addressed. In this paper, we design an incentive mechanism to help the model owner (MO) (i.e., the BCFL task publisher) assign each client appropriate rewards for training and mining, and then the client will determine the amount of computing power to allocate for each subtask based on these rewards using the two-stage Stackelberg game. After analyzing the utilities of the MO and clients, we transform the game model into two optimization problems, which are sequentially solved to derive the optimal strategies for both the MO and clients. Further, considering the fact that local training related information of each client may not be known by others, we extend the game model with analytical solutions to the incomplete information scenario. Extensive experimental results demonstrate the validity of our proposed schemes."",""1558-2183"","""",""10.1109/TPDS.2023.3253604"",""National Science Foundation(grant numbers:CNS-2105004)"; National Natural Science Foundation of China(grant numbers:62232010); National Research Foundation; Infocomm Media Development Authority through the Future Communications Research Development Programme; Singapore University of Technology and Design(grant numbers:SRG-ISTD-2021-165); SUTD-ZJU IDEA(grant numbers:202102); Ministry of Education - Singapore; SUTD Kickstarter Initiative(grant numbers:SKI 20210204);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10061576"",""Blockchain";federated learning;game theory;incentive mechanism;"resource allocation"",""Training";Blockchains;Task analysis;Resource management;Computational modeling;Data models;"Games"","""",""5"","""",""41"",""IEEE"",""7 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Incremental Multilayer Resource Partitioning for Application Placement in Dynamic Fog,""Z. N. Samani"; N. Mehran; D. Kimovski; S. Benedict; N. Saurabh;" R. Prodan"",""Institute of Information Technology, University of Klagenfurt, Klagenfurt, Austria"; Institute of Information Technology, University of Klagenfurt, Klagenfurt, Austria; Institute of Information Technology, University of Klagenfurt, Klagenfurt, Austria; Computer Science and Engineering, Indian Institute of Information Technology Kottayam, Valavoor, Kerala, India; Department of Information and Computing Sciences, Utrecht University, Utrecht, The Netherlands;" Institute of Information Technology, University of Klagenfurt, Klagenfurt, Austria"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1877"",""1896"",""Fog computing platforms became essential for deploying low-latency applications at the network's edge. However, placing and managing time-critical applications over a Fog infrastructure with many heterogeneous and resource-constrained devices over a dynamic network is challenging. This paper proposes an incremental multilayer resource-aware partitioning (M-RAP) method that minimizes resource wastage and maximizes service placement and deadline satisfaction in a dynamic Fog with many application requests. M-RAP represents the heterogeneous Fog resources as a multilayer graph, partitions it based on the network structure and resource types, and constantly updates it upon dynamic changes in the underlying Fog infrastructure. Finally, it identifies the device partitions for placing the application services according to their resource requirements, which must overlap in the same low-latency network partition. We evaluated M-RAP through extensive simulation and two applications executed on a real testbed. The results show that M-RAP can place 1.6 times as many services, satisfy deadlines for 43% more applications, lower their response time by up to 58%, and reduce resource wastage by up to 54% compared to three state-of-the-art methods."",""1558-2183"","""",""10.1109/TPDS.2023.3262695"",""European Horizon 2020 project DataCloud(grant numbers:101016835)"; Horizon Europe project Graph-Massivizer(grant numbers:101093202); Austrian Research Promotion Agency; Kärntner Fog(grant numbers:888098);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10083229"",""Application placement";deadline satisfaction;Fog computing;resource partitioning;"resource wastage"",""Time factors";Dynamic scheduling;Nonhomogeneous media;Low latency communication;Resource management;Monitoring;"Memory management"","""",""3"","""",""37"",""CCBY"",""28 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Intermittent Fault Diagnosis of Split-Star Networks and its Applications,""J. Song"; L. Lin; Y. Huang;" S. -Y. Hsieh"",""College of Computer and Cyber Security, Key Laboratory of Network Security and Cryptology, Fujian Normal University, Fuzhou, Fujian, China"; College of Computer and Cyber Security, Key Laboratory of Network Security and Cryptology, Fujian Normal University, Fuzhou, Fujian, China; School of Computer Science and Mathematics, Fujian University of Technology, Fuzhou, Fujian, China;" Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Feb 2023"",""2023"",""34"",""4"",""1253"",""1264"",""With the rapid increase of the number of processors in multiprocessor systems and the fast expansion of interconnection networks, the reliability of interconnection network is facing severe challenges, where the fast recognition of fault processors is crucial. In practice, most of the processor failures are intermittent faults. In this article, we first determine the intermittent fault diagnosability $t_{I}^{PMC}(S_{n}^{2})$tIPMC(Sn2) of $n$n-dimensional split-star network $S_{n}^{2}$Sn2 under the PMC model. In addition, we propose a fast intermittent fault probabilistic diagnosis algorithm FIFPDPMC to identify the nodes with intermittent fault in the $n$n-dimensional split-star network $S_{n}^{2}$Sn2 under the PMC model, and we calculated the time complexity of the algorithm FIFPDPMC. Then we implement the algorithm FIFPDPMC in the IoT-based wireless sensor network (IoTWSN) and a randomly generated network (RGN) under different number of nodes with intermittent fault, and we evaluate the performance and efficiency of the algorithm FIFPDPMC in terms of accuracy, precision, recall (TPR), F1, G-mean, FPR, TNR and FNR. Experimental results show that, as the number of stages of executing the algorithm FIFPDPMC increases, the number of nodes with intermittent fault being diagnosed by the algorithm FIFPDPMC increases, which implies that the algorithm FIFPDPMC has good performance and efficiency in both IoTWSN and RGN."",""1558-2183"","""",""10.1109/TPDS.2023.3242089"",""National Natural Science Foundation of China(grant numbers:62171132,62102088,U1905211)"; Fok Ying Tung Education Foundation(grant numbers:171061); Natural Science Foundation of Fujian Province(grant numbers:2021J05228);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10036103"",""Adaptive diagnosis";intermittent fault diagnosis;probabilistic model;reliability;"wireless sensor network"",""Circuit faults";Fault diagnosis;Program processors;Multiprocessing systems;Multiprocessor interconnection;Wireless sensor networks;"Maintenance engineering"","""",""4"","""",""38"",""IEEE"",""3 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"IO-Sets: Simple and Efficient Approaches for I/O Bandwidth Management,""F. Boito"; G. Pallez; L. Teylo;" N. Vidal"",""CNRS, Bordeaux INP, INRIA, LaBRI, University of Bordeaux, Talence, France"; Inria, Rennes, France; CNRS, Bordeaux INP, INRIA, LaBRI, University of Bordeaux, Talence, France;" Oak Ridge National Laboratory, Oak Ridge, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Aug 2023"",""2023"",""34"",""10"",""2783"",""2796"",""One of the main performance issues faced by high-performance computing platforms is the congestion caused by concurrent I/O from applications. When this happens, the platform's overall performance and utilization are harmed. From the extensive work in this field, I/O scheduling is the essential solution to this problem. The main drawback of current techniques is the amount of information needed about applications, which compromises their applicability. In this paper, we propose a novel method for I/O management, IO-Sets. We present its potential through a scheduling heuristic called Set-10, which is simple and requires only minimal information. Our extensive experimental campaign shows the importance of IO-Sets and the robustness of Set-10 under various workloads. In particular in most of the simulated scenarios we improve the I/O slowdown over fairshare by 50%, which corresponds in our scenarios to a platform utilization gain of 2.5%. In the practical scenarios that we did, the utilization gain varies between 10 and 30%. We also provide insights on using our proposal in practice."",""1558-2183"","""",""10.1109/TPDS.2023.3305028"",""French National Research Agency"; DASH(grant numbers:ANR-17-CE25-0004); Project Région Nouvelle Aquitaine(grant numbers:2018-1R50119); Adaptive multitier intelligent data manager for Exascale; European Union's Horizon 2020 JTI-EuroHPC Research and Innovation Programme(grant numbers:956748);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10219066"",""Bandwidth management";i/o scheduling;"i/o performance"",""Bandwidth";File systems;Task analysis;Supercomputers;Stress;Processor scheduling;"High performance computing"","""","""","""",""48"",""IEEE"",""15 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Joint Caching and Routing in Cache Networks With Arbitrary Topology,""T. Xie"; S. Thakkar; T. He; P. McDaniel;" Q. Burke"",""Pennsylvania State University, University Park, PA, USA"; Pennsylvania State University, University Park, PA, USA; Pennsylvania State University, University Park, PA, USA; University of Wisconsin-Madison, Madison, WI, USA;" University of Wisconsin-Madison, Madison, WI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jun 2023"",""2023"",""34"",""8"",""2237"",""2250"",""In-network caching and flexible routing are two of the most celebrated advantages of next generation network infrastructures. Yet few solutions are available for jointly optimizing caching and routing that provide performance guarantees for networks with arbitrary topology. We take a holistic approach towards this fundamental problem by analyzing its complexity in all the cases and developing polynomial-time algorithms with approximation guarantees in important special cases. We also reveal the fundamental challenge in achieving guaranteed approximation in the general case and propose an alternating optimization algorithm with good empirical performance and fast convergence. Our algorithms have demonstrated superior performance in both routing cost and congestion compared to the state-of-the-art solutions in evaluations based on real topology and request traces."",""1558-2183"","""",""10.1109/TPDS.2023.3276724"",""National Science Foundation(grant numbers:CNS-1946022,CNS-2106294)"; Combat Capabilities Development Command Army Research Laboratory(grant numbers:W911NF-13-2-0045);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10135157"",""Approximation algorithm";cache network;joint caching and routing;"unsplittable flow problem"",""Routing";Costs;Optimization;Topology;Network topology;Approximation algorithms;"Complexity theory"","""",""1"","""",""50"",""IEEE"",""25 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Joint Deployment and Request Routing for Microservice Call Graphs in Data Centers,""Y. Hu"; H. Wang; L. Wang; M. Hu; K. Peng;" B. Veeravalli"",""Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science, Technology, Wuhan, China"; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science, Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science, Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science, Technology, Wuhan, China; Hubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communications, Huazhong University of Science, Technology, Wuhan, China;" Department of Electrical and Computer Engineering, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Oct 2023"",""2023"",""34"",""11"",""2994"",""3011"",""Microservices are an architectural and organizational paradigm for Internet application development. In cloud data centers, delay-sensitive applications receive massive user requests, which are fed into multiple queues and subsequently served by multiple microservice instances. Accordingly, effective deployment of multiple queues and containers can significantly reduce queuing delay, processing delay, and communication delay. Due to the increased complexity of call dependencies and probabilistic routing paths, the deployment of service instances fully interacts with request routing, bringing great difficulties to service orchestration. In this case, it is valuable to simultaneously consider service deployment and request routing in a fine-grained manner. However, most existing studies considered them as two independent components with local optimization, while data dependencies and the instance-level deployment are ignored. Therefore, this paper proposes to jointly optimize the deployment and request routing of microservice call graphs based on fine-grained queuing network analysis and container orchestration. We first formulate the problem as a mixed-integer nonlinear program and exploit open Jackson queuing networks to model intrinsic data dependencies and analyze response latency. To optimize the overall cost and latency, this paper presents an efficient two-stage heuristic algorithm, which consists of a resource-splitting-based deployment approach and a partition-mapping-based routing method. Further, this paper also provides mathematical analysis on the performance and complexity of the proposed algorithm. Finally, comprehensive trace-driven experiments demonstrate that the overall performance of our approach is better than existing microservice benchmarks. The average deployment cost is reduced by 27.4% and end-to-end response latency is reduced by 15.1% on average."",""1558-2183"","""",""10.1109/TPDS.2023.3311767"",""National Key Research and Development Program of China(grant numbers:2022ZD0117104)"; National Natural Science Foundation of China(grant numbers:62171189,62272183); Key Research and Development Program of Hubei Province, China(grant numbers:2022BAA038,2023BAB074,2021BAA026);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247200"",""Cloud data centers";microservice call graphs;queuing theory;request routing;service deployment;"services computing"",""Microservice architectures";Routing;Delays;Containers;Optimization;Queueing analysis;"Data centers"","""","""","""",""41"",""IEEE"",""11 Sep 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Joint Model Pruning and Topology Construction for Accelerating Decentralized Machine Learning,""Z. Jiang"; Y. Xu; H. Xu; L. Wang; C. Qiao;" L. Huang"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science and Engineering, University at Buffalo, State University of New York, Buffalo, NY, USA;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Sep 2023"",""2023"",""34"",""10"",""2827"",""2842"",""Recently, mobile and embedded devices worldwide generate a massive amount of data at the network edge. To efficiently exploit the data from distributed devices, we concentrate on decentralized machine learning (DML), where the workers collaboratively train models under the peer-to-peer (P2P) setting. DML avoids the bottleneck of the parameter server (PS) by enabling the workers to exchange local models with their neighbors rather than the PS. However, DML still faces some key challenges, i.e., resource limitation, system heterogeneity, network dynamics and non-IID data. In this article, we design and implement MOTOR, an efficient DML mechanism that simultaneously addresses these challenges by applying model pruning and topology construction, thus accelerating DML. Specifically, MOTOR assigns different pruning ratios to heterogeneous workers. After model pruning, each worker will train and transmit a sub-model that fits its capabilities, reducing both computation and communication overhead. Besides, MOTOR dynamically constructs the network topology considering the time-varying network conditions and non-IID data distributions. We theoretically analyze the impact of pruning ratio and network topology on model training performance. Guided by the theoretical analysis, we develop a joint optimization algorithm for pruning ratio decision and topology construction to achieve the trade-off between resource overhead and training performance. We implement MOTOR on commercial devices and evaluate the performance with different DML tasks. Extensive experiments show that MOTOR achieves up to 4.2× speedup compared to the existing DML approaches."",""1558-2183"","""",""10.1109/TPDS.2023.3303967"",""National Key Research and Development Program of China(grant numbers:2021YFB3301500)"; National Natural Science Foundation of China(grant numbers:62102391,62132019,61936015); Jiangsu Province Science Foundation for Youths(grant numbers:BK20210122);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10214335"",""Model pruning";peer-to-peer;topology construction;"decentralized machine learning"",""Computational modeling";Training;Network topology;Data models;Topology;Machine learning;"Distributed databases"","""","""","""",""90"",""IEEE"",""10 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Landlord: Coordinating Dynamic Software Environments to Reduce Container Sprawl,""T. Shaffer"; T. S. Phung; K. Chard;" D. Thain"",""University of Notre Dame, Notre Dame, IN, USA"; University of Notre Dame, Notre Dame, IN, USA; University of Chicago, Chicago, IL, USA;" University of Notre Dame, Notre Dame, IN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Mar 2023"",""2023"",""34"",""5"",""1376"",""1389"",""Containers provide customizable software environments that are independent from the system on which they are deployed. Online services for task execution must often generate containers on the fly to meet user-generated requests. However, as the number of users grows and container environments are changed and updated over time, there is an explosion in the number of containers that must be managed, despite the fact that there is significant overlap among many of the containers in use. We analyze a trace of container launches on the public Binder service and demonstrate the performance and resource usage issues associated with container sprawl. We present Landlord, an algorithm that coalesces related container environments, and show that it can improve container reuse and reduce the number of container builds required in the Binder trace by 40%. We perform a sensitivity analysis of Landlord using randomized synthetic workloads on a high-energy physics (HEP) software repository and demonstrate that Landlord shows benefits for container management across a wide range of usage patterns. Finally, we compare Landlord to offline clustering, and observe that the continuous churn in software necessitates an online approach."",""1558-2183"","""",""10.1109/TPDS.2023.3241598"",""NSF(grant numbers:OAC-1931348,DOE Graduate Computer Science Fellowship)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10038448"",""Cluster computing";containers;"file systems"",""Containers";Task analysis;Software;Python;Software algorithms;Sensitivity;"High performance computing"","""","""","""",""42"",""IEEE"",""6 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Large-Scale Measurements and Prediction of DC-WAN Traffic,""Z. Wang"; Z. Li; H. Pan; G. Liu; Y. Chen; Q. Wu; G. Tyson;" G. Cheng"",""Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Baidu Inc., Beijing, China; Baidu Inc., Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Hong Kong University of Science and Technology (GZ), Hong Kong;" Baidu Inc., Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2023"",""2023"",""34"",""5"",""1390"",""1405"",""Large cloud service providers have built an increasing number of geo-distributed data centers (DCs) connected by Wide Area Networks (WANs). These DC-WANs carry both high-priority traffic from interactive services and low-priority traffic from bulk transfers. Given that a DC-WAN is an expensive resource, providers often manage it via traffic engineering algorithms that rely on accurate predictions of inter-DC high-priority (delay-sensitive) traffic. In this article, we perform a large-scale measurement study of high-priority inter-DC traffic from Baidu. We measure how inter-DC traffic varies across their global DC-WAN and show that most existing traffic prediction methods either cannot capture the complex traffic dynamics or overlook traffic interrelations among DCs. Building on our measurements, we propose the Interrelated-Temporal Graph Convolutional Network (IntegNet) model for inter-DC traffic prediction. In contrast to prior efforts, our model exploits both temporal traffic patterns and inferred co-dependencies between DC pairs. IntegNet forecasts the capacity needed for high-priority traffic demands by accounting for the balance between resource provisioning (i.e., allocating resources exceeding actual demand) and QoS losses (i.e., allocating fewer resources than actual demand). Our experiments show that IntegNet can keep a very limited QoS loss, while also reducing overprovisioning by up to 42.1% compared to the state-of-the-art and up to 66.2% compared to the traditional method used in DC-WAN traffic engineering."",""1558-2183"","""",""10.1109/TPDS.2023.3245092"",""National Natural Science Foundation of China(grant numbers:U20A20180,62072437,62002344)"; Beijing Natural Science Foundation(grant numbers:JQ20024); CAS-Austria Joint Project(grant numbers:171111KYSB20200001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045033"",""Data center networks";graph convolutional network;traffic measurement;"traffic prediction"",""Quality of service";Traffic control;Predictive models;Wide area networks;Web and internet services;Data models;"Data centers"","""","""","""",""48"",""IEEE"",""15 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"LAS: Locality-Aware Scheduling for GEMM-Accelerated Convolutions in GPUs,""H. Kim";" W. J. Song"",""School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea";" School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2023"",""2023"",""34"",""5"",""1479"",""1494"",""This article presents a graphics processing unit (GPU) scheduling scheme that maximizes the exploitation of data locality in deep neural networks (DNNs). Convolution is one of the fundamental operations used in DNNs and accounts for more than 90% of the total execution time. To leverage massive thread-level parallelism (TLP) in a GPU, deeply nested convolution loops are lowered (or unrolled) into large matrix multiplication, which trades memory capacity and bandwidth for TLP augmentation. A large workspace matrix is split into tiles of general matrix multiplication (GEMM) and concurrently executed by many thread blocks. Notably, the workspace is filled with a number of duplicate data that originate from the same sources in the input feature map during the lowering process. However, conventional GPU scheduling is oblivious to data duplication patterns in the workspace, and thread blocks are assigned to streaming multiprocessors (SMs) irrespective of data similarity between GEMM tiles. Such scheduling misses a significant opportunity to exploit data locality manifested in the DNN convolution. This article proposes a GPU scheduling technique called Locality-Aware Scheduling (LAS) that i) identifies which thread blocks share the largest amount of identical data based on the lowered patterns of a DNN convolution and ii) allocates such thread blocks showing the greatest data similarity to the same SM. In this way, small caches in SMs can efficiently utilize the data locality of the DNN convolution. Experimental results show that LAS with tensor cores achieves 20.1% performance improvements on average with 14.8% increases in L1 cache hit rates."",""1558-2183"","""",""10.1109/TPDS.2023.3247808"",""National Research Foundation(grant numbers:2021R1A2C1095162)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10050129"",""Deep neural network";GEMM;GPU;cache;convolution;"scheduling"",""Graphics processing units";Instruction sets;Memory management;Registers;System-on-chip;Parallel processing;"Information filters"","""",""1"","""",""42"",""IEEE"",""22 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"LB-Chain: Load-Balanced and Low-Latency Blockchain Sharding via Account Migration,""M. Li"; W. Wang;" J. Zhang"",""Research Institute of Trustworthy Autonomous Systems and Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, China"; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong;" Research Institute of Trustworthy Autonomous Systems and Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Aug 2023"",""2023"",""34"",""10"",""2797"",""2810"",""Blockchain sharding has been increasingly used to improve blockchain systems’ performance, in which a blockchain is split into multiple smaller, disjoint shards. In practice, however, sharding can only achieve limited throughput and latency improvement, especially for the user-perceived transaction confirmation delay. The performance degradation is believed to be caused by the cross-shard transactions. However, we show, through comprehensive system deployment and measurement studies, that the main culprit is the imbalanced transaction load on different blockchain shards. To address this problem, we propose a novel sharding system, called LB-Chain, which dynamically balances the transaction load on different shards by periodically migrating active accounts from heavily-loaded shards to less-loaded ones. We have implemented a prototype of LB-Chain, and evaluated its performance through large-scale blockchain deployment using real-world transaction traces. Extensive experiments confirm that LB-Chain significantly boosts sharding performance, reducing the transaction confirmation delays by up to 90% while increasing the transaction throughput by more than 10%. The delay difference between different accounts is also reduced dramatically, leading to improved fairness in the system."",""1558-2183"","""",""10.1109/TPDS.2023.3238343"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026494"",""Account migration";blockchain;blockchain sharding;"load balance"",""Blockchains";Sharding;Throughput;Resource management;Delays;Protocols;"Prediction algorithms"","""","""","""",""43"",""IEEE"",""26 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Learning to Schedule Multi-Server Jobs With Fluctuated Processing Speeds,""H. Zhao"; S. Deng; F. Chen; J. Yin; S. Dustdar;" A. Y. Zomaya"",""College of Computer Science and Technology, Zhejiang University, Hangzhou, China"; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Distributed Systems Group, Technische Universität Wien, Vienna, Austria;" School of Computer Science, University of Sydney, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""234"",""245"",""Multi-server jobs are imperative in modern cloud computing systems. A noteworthy feature of multi-server jobs is that, they usually request multiple computing devices simultaneously for their execution. How to schedule multi-server jobs online with a high system efficiency is a topic of great concern. First, the scheduling decisions have to satisfy the service locality constraints. Second, the scheduling decisions needs to be made online without the knowledge of future job arrivals. Third, and most importantly, the actual service rate experienced by a job is usually in fluctuation because of the dynamic voltage and frequency scaling (DVFS) and power oversubscription techniques when multiple types of jobs co-locate. A majority of online algorithms with theoretical performance guarantees are proposed. However, most of them require the processing speeds to be knowable, thereby the job completion times can be exactly calculated. To present a theoretically guaranteed online scheduling algorithm for multi-server jobs without knowing actual processing speeds apriori, in this article, we propose Esdp (Efficient Sampling-based Dynamic Programming), which learns the distribution of the fluctuated processing speeds over time and simultaneously seeks to maximize the cumulative overall utility. The cumulative overall utility is formulated as the sum of the utilities of successfully serving each multi-server job minus the penalty on the operating, maintaining, and energy cost. Esdp is proved to have a polynomial complexity and a logarithmic regret, which is a State-of-the-Art result. We also validate it with extensive simulations and the results show that the proposed algorithm outperforms several benchmark policies with improvements by up to 73%, 36%, and 28%, respectively."",""1558-2183"","""",""10.1109/TPDS.2022.3215947"",""National Natural Science Foundation of China(grant numbers:U20A20173,62125206)"; Key Research Project of Zhejiang Province(grant numbers:2022C01145); Zhejiang University Deqing Institute of Advanced technology and Industrilization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925644"",""Bipartite graph";dynamic programming;multi-server job;online learning;"regret analysis"",""Servers";Bipartite graph;Costs;Stochastic processes;Computational modeling;Heuristic algorithms;"Dynamic programming"","""","""","""",""40"",""IEEE"",""20 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Level-Based Blocking for Sparse Matrices: Sparse Matrix-Power-Vector Multiplication,""C. Alappat"; G. Hager; O. Schenk;" G. Wellein"",""Erlangen National High Performance Computing Center, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany"; Erlangen National High Performance Computing Center, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany; Institute of Computing at Faculty of Informatics, Università della Svizzera italiana, Lugano, Switzerland;" Erlangen National High Performance Computing Center, Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""581"",""597"",""The multiplication of a sparse matrix with a dense vector (SpMV) is a key component in many numerical schemes and its performance is known to be severely limited by main memory access. Several numerical schemes require the multiplication of a sparse matrix polynomial with a dense vector which is typically implemented as a sequence of SpMVs. This results in low performance and ignores the potential to increase the arithmetic intensity by reusing the matrix data from cache. In this work we use the recursive algebraic coloring engine (RACE) to enable blocking of sparse matrix data across the polynomial computations. In the graph representing the sparse matrix we form levels using a breadth-first search. Locality relations of these levels are then used to improve spatial and temporal locality when accessing the matrix data and to implement an efficient multithreaded parallelization. Our approach is independent of the matrix structure and avoids shortcomings of existing “blocking” strategies in terms of hardware efficiency and parallelization overhead. We quantify the quality of our implementation using performance modelling and demonstrate speedups of up to 3× and 5× compared to an optimal SpMV-based baseline on a single multicore chip of recent Intel and AMD architectures. Various numerical schemes like $s$s-step Krylov solvers, polynomial preconditioners and power clustering algorithms will benefit from our development."",""1558-2183"","""",""10.1109/TPDS.2022.3223512"",""NHR@FAU"; State of Bavaria; German Federal Ministry of Education and Research;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956740"",""Algorithm design and analysis";computer architecture;graph algorithms;kernel optimization;memory hierarchies;performance evaluation;"sparse matrices"",""Sparse matrices";Kernel;Multicore processing;Bandwidth;Hardware;Optimization;"Computer architecture"","""",""2"","""",""51"",""CCBY"",""21 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Leveraging Deep Reinforcement Learning With Attention Mechanism for Virtual Network Function Placement and Routing,""N. He"; S. Yang; F. Li; S. Trajanovski; L. Zhu; Y. Wang;" X. Fu"",""School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China"; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Microsoft, London, U.K.; School of Cyberspace Security, Beijing Institute of Technology, Beijing, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA;" Institute of Computer Science, University of Göttingen, Göttingen, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Feb 2023"",""2023"",""34"",""4"",""1186"",""1201"",""The efficacy of Network Function Virtualization (NFV) depends critically on (1) where the virtual network functions (VNFs) are placed and (2) how the traffic is routed. Unfortunately, these aspects are not easily optimized, especially under time-varying network states with different QoS requirements. Given the importance of NFV, many approaches have been proposed to solve the VNF placement and Service Function Chaining (SFC) routing problem. However, those prior approaches mainly assume that the network state is static and known, disregarding dynamic network variations. To bridge that gap, we leverage Markov Decision Process (MDP) to model the dynamic network state transitions. To jointly minimize the delay and cost of NFV providers and maximize the revenue, we first devise a customized Deep Reinforcement Learning (DRL) algorithm for the VNF placement problem. The algorithm uses the attention mechanism to ascertain smooth network behavior within the general framework of network utility maximization (NUM). We then propose attention mechanism-based DRL algorithm for the SFC routing problem, which is to find the path to deliver traffic for the VNFs placed on different nodes. The simulation results show that our proposed algorithms outperform the state-of-the-art algorithms in terms of network utility, delay, cost, and acceptance ratio."",""1558-2183"","""",""10.1109/TPDS.2023.3240404"",""National Natural Science Foundation of China(grant numbers:62172038)"; National Natural Science Foundation of China(grant numbers:62072040); National Natural Science Foundation of China(grant numbers:62232002); EU H2020 COSAFE project(grant numbers:824019); EU Horizon CODECO project(grant numbers:101092696);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10029903"",""Deep reinforcement learning";network function virtualization;placement;"routing"",""Routing";Costs;Delays;Approximation algorithms;Heuristic algorithms;Reinforcement learning;"Optimization"","""",""1"","""",""50"",""IEEE"",""30 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Liberator: A Data Reuse Framework for Out-of-Memory Graph Computing on GPUs,""S. Li"; R. Tang; J. Zhu; Z. Zhao; X. Gong; W. Wang; J. Zhang;" P. -C. Yew"",""Institute of Systems and Networks, College of Computer Science, Nankai University, Tianjin, China"; Institute of Systems and Networks, College of Computer Science, Nankai University, Tianjin, China; Institute of Systems and Networks, College of Computer Science, Nankai University, Tianjin, China; Department of Computer Science, Rice University, Houston, TX, USA; Institute of Systems and Networks, College of Computer Science, Nankai University, Tianjin, China; Department of Computer Science, University of Georgia, Athens, GA, USA; Institute of Systems and Networks, College of Computer Science, Nankai University, Tianjin, China;" Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1954"",""1967"",""Graph analytics are widely used including recommender systems, scientific computing, and data mining. Meanwhile, GPU has become the major accelerator for such applications. However, the graph size increases rapidly and often exceeds the GPU memory, incurring severe performance degradation due to frequent data transfers between the main memory and GPUs. To relieve this problem, we focus on the utilization of data in GPUs by taking advantage of the data reuse across iterations. In our studies, we deeply analyze the memory access patterns of graph applications at different granularities. We have found that the memory footprint is accessed with a roughly sequential scan without a hotspot, which infers an extremely long reuse distance. Based on our observation, we propose a novel framework, called Liberator, to exploit the data reuse within GPU memory. In Liberator, GPU memory is reserved for the data potentially accessed across iterations to avoid excessive data transfer between the main memory and GPUs. For the data not existing in GPU memory, a Merged and Aligned memory access manner is employed to improve the transmission efficiency. We also further optimize the framework by parallel processing of data in GPU memory and data in the main memory. We have implemented a prototype of the Liberator framework and conducted a series of experiments on performance evaluation. The experimental results show that Liberator can significantly reduce the data transfer overhead, which achieves an average of 2.7x speedup over a state-of-the-art approach."",""1558-2183"","""",""10.1109/TPDS.2023.3268662"",""Key Technologies R&D Program of Guangdong Province, China(grant numbers:2021B0101310002)"; National Natural Science Foundation of China(grant numbers:62172239); Shandong Provincial Natural Science Foundation, China(grant numbers:ZR2022LZH009);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10107428"",""Data reuse";GPU memory oversubscription;graph computing;partition-based method;"zero-copy"",""Graphics processing units";Memory management;Data transfer;Parallel processing;Lead;Computer science;"Sparse matrices"","""","""","""",""44"",""IEEE"",""24 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Loop-the-Loops: Fragmented Learning Over Networks for Constrained IoT Devices,""P. K. Deb"; A. Mukherjee; D. Singh;" S. Misra"",""Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India"; Institute for Manufacturing (IfM), University of Cambridge, Cambridge, U.K.; Department of Electrical and Electronics Engineering, BITS Pilani Hyderabad Campus, Secunderabad, Telangana, India;" Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Dec 2022"",""2023"",""34"",""1"",""316"",""327"",""In this work, we propose Timed Loop Gears (TLG), as a distributed method for enabling fragmented learning in Resource-Constrained networked IoT edge devices. TLG identifies atomic operations (gears), such as feed-forward and back-propagation, necessary for training Machine Learning (ML) models. Each of these gears executes on a Fog Node (FN) exclusively for each data point at a time rather than the whole dataset in its entirety. Additionally, the networked Edge Devices (EDs) offload the training data to the fog layer using the Message Queuing Telemetry Transport (MQTT) protocol such that the participating FNs subscribe to incoming training data and store them based on topics, simplifying data sharing. TLG enables the FN to then transfer the partially learned weights to the next suitable FN for further training. This looping of weights is repeated across FNs until the training is complete. Through extensive analysis, we observe that, compared to existing distributed ML training approaches, for $n$n devices, TLG reduces the probability of disruption due to device failure by $n^{2}$n2 times. Implementation results of our fragmented learning method demonstrate that, although TLG negligibly increases the memory consumption of the IoT devices by $0.8\%$0.8%, it reduces CPU usage by almost $90\%$90%. The proposed method proves beneficial for developing and hosting ML models, even on constrained IoT devices, in contrast to existing lightweight ML methods."",""1558-2183"","""",""10.1109/TPDS.2022.3220221"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9941367"",""Fog computing";distributed machine learning;computation offloading;"model parallelism"",""Training";Computational modeling;Parallel processing;Machine learning;Data models;Task analysis;"Gears"","""","""","""",""30"",""IEEE"",""8 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Merak: An Efficient Distributed DNN Training Framework With Automated 3D Parallelism for Giant Foundation Models,""Z. Lai"; S. Li; X. Tang; K. Ge; W. Liu; Y. Duan; L. Qiao;" D. Li"",""National Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, China"; National Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, China; National Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, China; National Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, China; National Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, China; National Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, China; National Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, China;" National Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2023"",""2023"",""34"",""5"",""1466"",""1478"",""Foundation models are in the process of becoming the dominant deep learning technology. Pretraining a foundation model is always time-consuming due to the large scale of both the model parameter and training dataset. Besides being computing-intensive, the pretraining process is extremely memory- and communication-intensive. These challenges make it necessary to apply 3D parallelism, which integrates data parallelism, pipeline model parallelism, and tensor model parallelism, to achieve high training efficiency. However, current 3D parallelism frameworks still encounter two issues: i) they are not transparent to model developers, requiring manual model modification to parallelize training, and ii) their utilization of computation resources, GPU memory, and network bandwidth is insufficient. We propose Merak, an automated 3D parallelism deep learning training framework with high resource utilization. Merak automatically deploys 3D parallelism with an automatic model partitioner, which includes a graph-sharding algorithm and proxy node-based model graph. Merak also offers a non-intrusive API to scale out foundation model training with minimal code modification. In addition, we design a high-performance 3D parallel runtime engine that employs several techniques to exploit available training resources, including a shifted critical path pipeline schedule that increases computation utilization, stage-aware recomputation that makes use of idle worker memory, and sub-pipelined tensor model parallelism that overlaps communication and computation. Experiments on 64 GPUs demonstrate Merak's capability to speed up training performance over state-of-the-art 3D parallelism frameworks of models with 1.5, 2.5, 8.3, and 20 billion parameters by up to 1.42, 1.39, 1.43, and 1.61×, respectively."",""1558-2183"","""",""10.1109/TPDS.2023.3247001"",""National Key R&D Program of China(grant numbers:2021YFB0301200)"; National Natural Science Foundation of China(grant numbers:62025208);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049507"",""Deep learning";distributed systems;"foundation model training"",""Training";Parallel processing;Three-dimensional displays;Solid modeling;Computational modeling;Pipelines;"Data models"","""",""5"","""",""52"",""IEEE"",""22 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Minimizing the Average Packet Access Time of the Application Layer for Buffered Instantly Decodable Network Coding,""Z. Mei"",""College of Telecommunication and Information Engineering, Nanjing University of Posts and Telecommunications, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Jan 2023"",""2023"",""34"",""3"",""1035"",""1046"",""In B-IDNC (buffered instantly decodable network coding), each receiver can cache the non-instantly decodable network coded packets (NIDPs) which contain two wanted packets of the network layer for subsequent network decoding, so that more packets can be recovered at the network layer than the traditional instantly decodable network coding (IDNC). By employing B-IDNC, we consider a radio access network wherein a base station (BS) is required to broadcast a block of packets to a set of receivers. After completing network decoding at the network layer, each receiver can deliver its recovered packets from the network layer to the application layer in order. We consider minimizing the average packet access time of the application layer for B-IDNC. For the optimization problem is intractable, we approximate it to reduce the sum minimum access delay of the application layer across all receivers. The approximate problem is shown to be equivalent to a maximum weight encoding clique problem over the B-IDNC graph. We propose a simple heuristic algorithm based on greedy maximum weight vertex search to solve the approximate problem. Simulation results verify the effectiveness of our proposed algorithm as compared with the existing techniques."",""1558-2183"","""",""10.1109/TPDS.2023.3237989"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10021830"",""Average packet access time of the application layer";average packet decoding delay of the network layer;instantly decodable network coding;"maximum weight clique"",""Receivers";Decoding;Network coding;Encoding;Delays;Radio access networks;"Performance gain"","""",""1"","""",""23"",""IEEE"",""19 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"mpi4py.futures: MPI-Based Asynchronous Task Execution for Python,""M. Rogowski"; S. Aseeri; D. Keyes;" L. Dalcin"",""Extreme Computing Research Center (ECRC) and the Computer Science program, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"; Extreme Computing Research Center (ECRC), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center (ECRC) and the Applied Mathematics and Computational Science program, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia;" Extreme Computing Research Center (ECRC), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Dec 2022"",""2023"",""34"",""2"",""611"",""622"",""We present mpi4py.futures, a lightweight, asynchronous task execution framework targeting the Python programming language and using the Message Passing Interface (MPI) for interprocess communication. mpi4py.futures follows the interface of the concurrent.futures package from the Python standard library and can be used as its drop-in replacement, while allowing applications to scale over multiple compute nodes. We discuss the design, implementation, and feature set of mpi4py.futures and compare its performance to other solutions on both shared and distributed memory architectures. On a shared-memory system, we show mpi4py.futures to consistently outperform Python's concurrent.futures with speedup ratios between 1.4X and 3.7X in throughput (tasks per second) and between 1.9X and 2.9X in bandwidth. On a Cray XC40 system, we compare mpi4py.futures to Dask – a well-known Python parallel computing package. Although we note more varied results, we show mpi4py.futures to outperform Dask in most scenarios."",""1558-2183"","""",""10.1109/TPDS.2022.3225481"",""King Abdullah University of Science and Technology";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965751"",""MPI";Python;parallelism;master-worker;parallel programming models;distributed computing;high performance computing;task execution;"multiprocessing"",""Python";Task analysis;Message systems;Libraries;Standards;Codes;"Sockets"","""",""4"","""",""48"",""IEEE"",""29 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"MRCN: Throughput-Oriented Multicast Routing for Customized Network-on-Chips,""Y. S. Lee"; Y. W. Kim;" T. H. Han"",""Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Republic of Korea"; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Republic of Korea;" Department of Semiconductor System Engineering, Sungkyunkwan University, Suwon, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""163"",""179"",""The relentless proliferation of Big Data and artificial intelligence has compelled computing platform architectures to evolve into heterogeneous multicores for greater energy efficiency. A customized network-on-chip (NoC) supporting interconnection diversity is pivotal for the asymmetric data-access traffic requirements of modern heterogeneous multicore system-on-chip (SoC). A significant portion of on-chip data access comprises single-source multi-destination (SSMD) traffic, which supports barrier synchronization, multi-threading, cache coherency protocols, and deep neural network (DNN) acceleration. By amortizing SSMD traffic, multicast routing is essential for effectively utilizing communication bandwidth. One of the primary concerns in supporting multicast routing in NoCs is to circumvent the additional deadlock conditions caused by branch operations among the active routers. However, it is challenging to implement the throughput-optimized multicast routing in irregular topology-based NoCs because the deadlock conditions become highly complicated, and the Hamiltonian path required to apply the labeling rule may not exist. Two important observations were identified regarding multicast routing in customized NoCs: 1) Even if the NoC lacks a Hamiltonian path, deadlock-freedom can be guaranteed by restricting branch operations to a specific destination. 2) A variable path diversity in a custom topology can be leveraged in routing path allocation and branch. Based on these properties, this study proposes a deadlock-free and throughput-enhanced multicast routing for customized NoC (MRCN). MRCN ensures deadlock freedom by utilizing extended routing and router labeling rules. Furthermore, destination router partitioning and traffic-aware adaptive branching are incorporated to reduce packet routing hops and disperse channel traffic. The effectiveness of MRCN was verified using Noxim, a well-known cycle-accurate NoC simulator, under various topologies and traffic patterns. The simulation revealed that MRCN improved the average latency by 13.98 % and the throughput by 12.16 % under the saturated traffic conditions over the previous multicast routings in customized NoCs."",""1558-2183"","""",""10.1109/TPDS.2022.3217296"",""National Research Foundation of Korea"; Ministry of Science and ICT(grant numbers:2020M3H2A1076786); Ministry of Trade, Industry and Energy(grant numbers:20011074); Institute of Information & communications Technology Planning & Evaluation(grant numbers:2019-0-00421);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930627"",""Customized network-on-chip";multicast routing;irregular topology;path-based routing;"packet branching"",""Routing";System recovery;Topology;Labeling;Multicore processing;Partitioning algorithms;"Network topology"","""",""4"","""",""39"",""CCBY"",""26 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Multi-Job Intelligent Scheduling With Cross-Device Federated Learning,""J. Liu"; J. Jia; B. Ma; C. Zhou; J. Zhou; Y. Zhou; H. Dai;" D. Dou"",""Baidu Inc., Beijing, China"; Soochow University, Suzhou, China; Baidu Inc., Beijing, China; Soochow University, Suzhou, China; Baidu Inc., Beijing, China; Auburn University, Auburn, AL, USA; North Carolina State University, Raleigh, NC, USA;" Baidu Inc., Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""535"",""551"",""Recent years have witnessed a large amount of decentralized data in various (edge) devices of end-users, while the decentralized data aggregation remains complicated for machine learning jobs because of regulations and laws. As a practical approach to handling decentralized data, Federated Learning (FL) enables collaborative global machine learning model training without sharing sensitive raw data. The servers schedule devices to jobs within the training process of FL. In contrast, device scheduling with multiple jobs in FL remains a critical and open problem. In this article, we propose a novel multi-job FL framework, which enables the training process of multiple jobs in parallel. The multi-job FL framework is composed of a system model and a scheduling method. The system model enables a parallel training process of multiple jobs, with a cost model based on the data fairness and the training time of diverse devices during the parallel training process. We propose a novel intelligent scheduling approach based on multiple scheduling methods, including an original reinforcement learning-based scheduling method and an original Bayesian optimization-based scheduling method, which corresponds to a small cost while scheduling devices to multiple jobs. We conduct extensive experimentation with diverse jobs and datasets. The experimental results reveal that our proposed approaches significantly outperform baseline approaches in terms of training time (up to 12.73 times faster) and accuracy (up to 46.4% higher)."",""1558-2183"","""",""10.1109/TPDS.2022.3224941"",""Collaborative Innovation Center of Novel Software Technology and Industrialization";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964248"",""Federated learning";scheduling;multi-job;parallel execution;"distributed learning"",""Training";Data models;Scheduling;Costs;Servers;Adaptation models;"Machine learning"","""",""3"","""",""73"",""IEEE"",""28 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Multi-SP Network Slicing Parallel Relieving Edge Network Conflict,""R. Han"; D. Chen; S. Guo; J. Wang; Q. Qi; L. Lu;" J. Liao"",""State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China"; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; Department of Network and IT Technology, China Mobile Research Institute, Bejing, China;" State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Sep 2023"",""2023"",""34"",""11"",""2860"",""2875"",""Network slicing is rapidly prevailing in the edge network, which provides computing, network, and storage resources for various services. When the multiple service providers (SPs) respond to their tenants in parallel, individual decisions on the dynamic and shared edge network may lead to resource conflicts, which affects the delivery of network slicing services. Existing works ignore resource interaction and coordination in the multi-SP scenario, which is not in line with the actual situation. Indeed, the complexity of resource interaction caused by the coexistence of multiple SP policies increases the difficulty to solve the formulated optimization model. In this article, we focus on the multi-SP network slicing deployment in parallel. The coordination of network resources between SPs is designed as an effective multi-agent communication mechanism that is merged into multi-agent deep reinforcement learning (MADRL). To deal with dynamic edge networks, we design the neurons hotplugging learning which realizes scalability without a high cost of model retraining. Experiments on real and random networks demonstrate that the proposed multi-SP network slicing mechanism can successfully learn coordination policies and easily adapt to various network scales. It improves the accepted requests by 7.4%, reduces resource conflicts by 14.5%, and shortens the model convergence time by 83.3%."",""1558-2183"","""",""10.1109/TPDS.2023.3310013"",""National Natural Science Foundation of China(grant numbers:62171057,62201072,62071067,62001054)"; Beijing University of Posts and Telecommunications-China Mobile Research Institute Joint Innovation Center; Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101400003); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19,R5034-18); Areas of Excellence Scheme(grant numbers:AoE/E-601/22-R); General Research Fund(grant numbers:152203/20E,152244/21E,152169/22E,152228/23E); Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ20200109142008673); Supported by BUPT Excellent Ph.D. Students Foundation(grant numbers:CX2023132);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234621"",""Network slicing";resource conflict;MADRL;multi-agent communication;"neurons hotplugging learning"",""Network slicing";Resource management;Neurons;Scalability;Privacy;Network topology;"Adaptation models"","""","""","""",""60"",""IEEE"",""30 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Multi-Tier GPU Virtualization for Deep Learning in Cloud-Edge Systems,""J. Kennedy"; V. Sharma; B. Varghese;" C. Reaño"",""Queen's University Belfast, Belfast, United Kingdom"; Queen's University Belfast, Belfast, United Kingdom; University of St Andrews, St Andrews, United Kingdom;" Universitat de València, València, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""26 May 2023"",""2023"",""34"",""7"",""2107"",""2123"",""Accelerator virtualization offers several advantages in the context of cloud-edge computing. Relatively weak user devices can enhance performance when running workloads by accessing virtualized accelerators available on other resources in the cloud-edge continuum. However, cloud-edge systems are heterogeneous, often leading to compatibility issues arising from various hardware and software stacks present in the system. One mechanism to alleviate this issue is using containers for deploying workloads. Containers isolate applications and their dependencies and store them as images that can run on any device. In addition, user devices may move during the course of application execution, and thus mechanisms such as container migration are required to move running workloads from one resource to another in the network. Furthermore, an optimal destination will need to be determined when migrating between virtual accelerators. Scheduling and placement strategies are incorporated to choose the best possible location depending on the workload requirements. This paper presents AVEC, a framework for accelerator virtualization in cloud-edge computing. The AVEC framework enables the offloading of deep learning workloads for inference from weak user devices to computationally more powerful devices in a cloud-edge network. AVEC incorporates a mechanism that efficiently manages and schedules the virtualization of accelerators. It also supports migration between accelerators to enable stateless container migration. The experimental analysis highlights that AVEC can achieve up to 7x speedup by offloading applications to remote resources. Furthermore, AVEC features a low migration downtime that is less than 5 seconds."",""1558-2183"","""",""10.1109/TPDS.2023.3274957"",""ETRI-Korea"; NRF-Korea; IITP-Korea; MSIT-Korea; AFOSR-USA; Innovate U.K., PwC, VIAVI, UKRI- Horizon, DCMS; Defence Science and Technology Laboratory;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10122723"",""Accelerators";containers;edge computing;migration;"virtualization"",""Cloud computing";Containers;Virtualization;Graphics processing units;Deep learning;Data centers;"Virtual machine monitors"","""","""","""",""46"",""IEEE"",""10 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Near-Lossless MPI Tracing and Proxy Application Autogeneration,""C. Wang"; Y. Guo; P. Balaji;" M. Snir"",""Department of Computer Science, University of Illinois Urbana-Champaign, Urbana, IL, USA"; Argonne National Laboratory, Lemont, IL, USA; Meta Inc., Menlo Park, CA, USA;" Department of Computer Science, University of Illinois Urbana-Champaign, Champaign, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""123"",""140"",""Traces of MPI communications are used by many performance analysis and visualization tools. Storing exhaustive traces of large-scale MPI applications is infeasible, however, because of their large volume. Aggregated or lossy MPI traces are smaller but provide much less information. In this paper we present Pilgrim, a near-lossless MPI tracing tool that, by using sophisticated compression techniques, generates small trace files at large scales and incurs only moderate overheads. We perform comprehensive studies of various compression techniques used for storing timestamps associated with each call. This timing information is essential for analysis purposes such as skews study. To demonstrate the usefulness of the detailed information stored by Pilgrim, we present a proxy application generator that can generate proxy apps that preserve original communication patterns from the Pilgrim traces."",""1558-2183"","""",""10.1109/TPDS.2022.3215942"",""National Science Foundation(grant numbers:19-09144)"; Exascale Computing Project(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925630"",""Communication tracing";MPI tracing;"proxy application generation"",""Production";Grammar;Codes;Symbols;Generators;Synchronization;"Standards"","""","""","""",""53"",""IEEE"",""20 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Neighbor Graph Based Tensor Recovery For Accurate Internet Anomaly Detection,""X. Li"; K. Xie; X. Wang; G. Xie; K. Li; J. Cao; D. Zhang; H. Jiang;" J. Wen"",""College of Computer Science and Electronics Engineering, Hunan University, Changsha, Hunan, China"; College of Computer Science and Electronics Engineering, Hunan University, Changsha, Hunan, China; Department of Electrical and Computer Engineering, State University of New York at Stony Brook, Stony Brook, NY, USA; Computer Network Information Center(CNIC), Chinese Academy of Sciences, Beijing, China; College of Computer Science and Electronics Engineering, Hunan University, Changsha, Hunan, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; College of Computer Science and Electronics Engineering, Hunan University, Changsha, Hunan, China; College of Computer Science and Electronics Engineering, Hunan University, Changsha, Hunan, China;" College of Computer Science and Electronics Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Dec 2022"",""2023"",""34"",""2"",""655"",""674"",""Detecting anomalous traffic is a crucial task for network management. Although many anomaly detection algorithms have been proposed recently, constrained by their matrix-based traffic data model, existing algorithms often suffer from low detection accuracy. To fully utilize the multi-dimensional information hidden in the traffic data, this paper uses the tensor model for more accurate Internet anomaly detection. Only considering the low-rank linearity features hidden in the data, current tensor factorization techniques would result in low anomaly detection accuracy. We propose a novel Graph-based Tensor Recovery model (Graph-TR) to well explore both low-rank linearity features as well as the non-linear proximity information hidden in the traffic data for better anomaly detection. We encode the non-linear proximity information of the traffic data by constructing nearest neighbor graphs and incorporate this information into the tensor factorization using the graph Laplacian. Moreover, to facilitate the quick building of neighbor graph, we propose a nearest neighbor searching algorithm with the simple locality-sensitive hashing (LSH). Besides only detecting random anomalies, our algorithm can also effectively detect structured anomalies that appear as bursts. We have conducted extensive experiments using Internet traffic trace data Abilene and GÈANT. Compared with the state of art algorithms on matrix-based anomaly detection and tensor recovery approach, our Graph-TR can achieve higher Accuracy and Recall."",""1558-2183"","""",""10.1109/TPDS.2022.3227570"",""National Science Foundation(grant numbers:62025201)"; National Natural Science Foundation of China(grant numbers:62102138,61972144,61976087); China National Postdoctoral Program for Innovative Talents(grant numbers:BX20200120); China Postdoctoral Science Foundation(grant numbers:2020M682556); Natural Science Foundation of Hunan Province(grant numbers:2021JJ40115);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9976292"",""Traffic anomaly detection";tensor recovery;"neighbor graph"",""Tensors";Anomaly detection;Feature extraction;Data models;Matrix decomposition;Manifolds;"Telecommunication traffic"","""",""1"","""",""61"",""IEEE"",""8 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"NeuroSpector: Systematic Optimization of Dataflow Scheduling in DNN Accelerators,""C. Park"; B. Kim; S. Ryu;" W. J. Song"",""School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea"; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea; School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea;" School of Electrical and Electronic Engineering, Yonsei University, Seoul, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jun 2023"",""2023"",""34"",""8"",""2279"",""2294"",""This paper presents an optimization framework named NeuroSpector that systematically analyzes the dataflow of deep neural network (DNN) accelerators and rapidly identifies optimal execution methods. The proposed methodology is demonstrated to work effectively with a variety of accelerator architectures and DNN workloads. It has been a baffling challenge to devise scheduling schemes for neural accelerators to maximize energy efficiency and performance. The challenge lies in that hardware specifications associated with multi-dimensional DNN data create an enormous number of possible scheduling options that can be exerted on accelerators. Related work suggested various techniques to solve the challenge encompassing brute-force search of massive solution spaces pruned by user constraints, solving the objective functions of system models, learning-based optimization, etc. However, each suggested technique was devised only for a specific accelerator model. Therefore, we find that they are not adaptively applicable to different accelerators and DNN workloads in that they produce hit-or-miss results with 100.1% greater energy and cycles on average compared to optimal scheduling schemes obtained from fully comprehensive brute-force searches. In contrast, NeuroSpector identifies efficient execution methods for various accelerators and workloads with only 1.5% differences on average to the optimal scheduling solutions. The optimization strategy of NeuroSpector is based on an observation that optimal executions are strongly correlated with minimizing data movements to the lower-level memory hierarchy of accelerators rather than maximizing the utilization of processing elements. Thus, NeuroSpector prioritizes optimizing lower-level components in the accelerator hierarchy, which is proven highly effective for various accelerators and DNN workloads."",""1558-2183"","""",""10.1109/TPDS.2023.3283491"",""National Research Foundation of Korea(grant numbers:#2021R1A2C1095162)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10145602"",""Deep neural network";accelerator;dataflow;mapping;scheduling;"optimization"",""Optimal scheduling";System-on-chip;Neural networks;Scheduling;Random access memory;Processor scheduling;"Correlation"","""","""","""",""42"",""IEEE"",""7 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"NIOT: A Novel Inference Optimization of Transformers on Modern CPUs,""Z. Zhang"; Y. Chen; B. He;" Z. Zhang"",""School of Computing, National University of Singapore, Singapore"; School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore;" Neuron Mobility Pte. Ltd., Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1982"",""1995"",""In the machine learning era, model inference efficiency is one of the most important issues for machine learning systems. It is a major challenge to find the optimal configuration in a huge search space as the combinations of kernel fusion, memory tiling, and thread allocation strategies result in highly variable and unpredictable inference performance. The problem is particularly pronounced in models with large parameter matrices such as Transformers. In this article, we aim to develop a general and powerful framework for inference optimization, called NIOT, to achieve desirable efficiency for the prevailing Transformer-like models on CPUs. To take full advantage of modern CPU features such as SIMD and cache hierarchy, NIOT employs various methods to provide promising strategies tailored to the target Transformer model. Our C++ implementation of NIOT shows significant performance improvements over popular well-optimized model-serving runtimes such as PyTorch and ONNXRuntime."",""1558-2183"","""",""10.1109/TPDS.2023.3269530"",""National Research Foundation Singapore"; AI Singapore Programme(grant numbers:AISG2-TC-2021-002); Ministry of Education AcRF Tier 2(grant numbers:MOE-000242-00,MOE-000242-01); Centre for Trusted Internet and Community; National University of Singapore;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10107474"",""Acceleration";CPU;neural network;"transformers"",""Transformers";Optimization;Layout;Instruction sets;Computational modeling;Bit error rate;"Resource management"","""","""","""",""58"",""IEEE"",""24 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Node Essentiality Assessment and Distributed Collaborative Virtual Network Embedding in Datacenters,""W. Fan"; F. Xiao; M. Lv; L. Han; J. Wang;" X. He"",""College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China"; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China;" College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Feb 2023"",""2023"",""34"",""4"",""1265"",""1280"",""Network virtualization (NV) has extensive and significant applications in cloud computing and parallel and distributed systems. Virtual network embedding (VNE) is a key issue in NV, which is an effective means to advance systems’ performance. While existing VNE research lacks resource allocation coordination between mappings of different virtual network requests, resulting in insufficient resource utilization and high overhead. In this article, we propose a novel node essentiality evaluation model for data center networks (DCNs), and design an efficient distributed collaborative virtual network embedding. Firstly, we propose a node essentiality evaluation scheme based on dynamic model, which combines the characteristics of network topology and nodes to make the evaluation results more comprehensive. Secondly, we establish the two-stage node importance evaluation criteria for the deviation mean of the data center dynamic model and the variance based on the deviation mean. Furthermore, we investigate a nodal importance assessment method based on the data center dynamic model for perturbation testing. Finally, we design a distributed coordinated VNE algorithm (CNI-VNE) which calculates the importance index of physical nodes through topology awareness. The proposed algorithm can increase the coordination between different request mappings, thereby reducing the mapping cost of physical node resources and minimizing the cost of VNE. We use the real Fat-tree DCN of 128 servers and 80 switches as testbed, and evaluate them from indicators such as average reliability, average bandwidth consumption, average energy consumption, and average mapping time. Massive simulation results in different scenarios show that our algorithm achieves the best performance on most indicators compared with the existing state-of-the-art proposals, mapping acceptance and average revenue increased by 19.4% and 21.3%, respectively, and DCN reduced bandwidth consumption by about 30%."",""1558-2183"","""",""10.1109/TPDS.2023.3242952"",""National Science Fund for Distinguished Young Scholars(grant numbers:62125203)"; National Natural Science Foundation of China(grant numbers:62102196,62102195,62172291,61932013); Research Foundation of Jiangsu(grant numbers:BRA2020065); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200753); Jiangsu Postdoctoral Science Foundation(grant numbers:2021K096A);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10040224"",""Data center network";dynamics model;node essentiality;"virtual network embedding"",""Collaboration";Heuristic algorithms;Network topology;Dynamic scheduling;Data models;Data centers;"Costs"","""",""2"","""",""43"",""IEEE"",""7 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"ObjDedup: High-Throughput Object Storage Layer for Backup Systems With Block-Level Deduplication,""A. Jackowski"; Ł. Ślusarczyk; K. Lichota; M. Wełnicki; R. Wijata; M. Kielar; T. Kopeć; C. Dubnicki;" K. Iwanicki"",""LLC, 9LivesData, Warsaw, Poland"; LLC, 9LivesData, Warsaw, Poland; LLC, 9LivesData, Warsaw, Poland; LLC, 9LivesData, Warsaw, Poland; LLC, 9LivesData, Warsaw, Poland; LLC, 9LivesData, Warsaw, Poland; LLC, 9LivesData, Warsaw, Poland; LLC, 9LivesData, Warsaw, Poland;" Faculty of Mathematics, Informatics, and Mechanics, University of Warsaw, Warsaw, Poland"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Jun 2023"",""2023"",""34"",""7"",""2180"",""2197"",""The immense popularity of object storage is also affecting the market of backup. Not only have novel backup solutions emerged that utilize cloud-based object storage as backends, but also support for object storage interfaces is increasingly expected from traditional dedicated backup appliances. This latter trend especially concerns systems with data deduplication, as they can offer compelling gains in storage capacity and throughput. However, such systems have been designed for interfaces and workloads that are markedly different from those encountered in object storage. Notably, they expect data to be written in portions that are orders of magnitude longer than those in the novel object-storage-oriented backup applications. In this light, we contribute twofold. First, contrasting the properties of object storage interfaces with usage patterns from 686 commercial deployments of backup appliances, we identify specific issues an implementation of such an interface has to address to offer adequate performance in a backup system with block-level deduplication. In particular, we show that a major challenge is efficient metadata management. Second, we present distributed data structures and algorithms to handle object metadata in backup systems with block-level deduplication. Subsequently, we implement them as an object storage layer for our HYDRAstor backup system. In comparison to object storage without in-line deduplication, our solution achieves 1.8–3.93x higher write throughput. Compared to object storage on top of a state-of-the-art file-based backup system, it processes 5.26–11.34x more object put operations per time unit."",""1558-2183"","""",""10.1109/TPDS.2023.3250501"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10057482"",""Backup storage";deduplication;object storage;"secondary storage"",""Metadata";Engines;Throughput;Object recognition;Cloud computing;Aerospace electronics;"Quality of service"","""","""","""",""75"",""IEEE"",""2 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"μBench: An Open-Source Factory of Benchmark Microservice Applications,""A. Detti"; L. Funari;" L. Petrucci"",""Electronic Engineering Department, University of Rome Tor Vergata, Roma, Italy"; Electronic Engineering Department, University of Rome Tor Vergata, Roma, Italy;" Electronic Engineering Department, University of Rome Tor Vergata, Roma, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jan 2023"",""2023"",""34"",""3"",""968"",""980"",""$\mu$μBench is an open-source tool for benchmarking cloud/edge computing platforms that run microservice applications. The tool creates dummy microservice applications that can be customized and executed on a Kubernetes cluster. $\mu$μBench allows users to control fundamental properties of the microservice applications it creates, such as service mesh topology, microservices’ behaviors using a portfolio of stress functions (e.g., for CPU, memory, I/O, network) or implementing new ones, microservice-to-microservice API (HTTP or gRPC), etc. Application performance can be evaluated by stochastic or trace-driven workloads. μBench is aimed at researchers and cloud platform developers who lack real microservice applications to benchmark their findings (e.g., new resource control mechanisms, artificial intelligence-driven orchestration, etc.) or wish to thoroughly evaluate their proposals versus a broad set of heterogeneous applications that μBench can create. In addition to the description of μBench, in this article, we show one possible use of it. We compared advantages and disadvantages of microservice architectures versus monolithic ones, and analyzed the performance impact of key architectural choices, such as service mesh topology and the use of replication. For this analysis, we generated several microservice applications with different properties, and two of them are derived from a real cloud dataset."",""1558-2183"","""",""10.1109/TPDS.2023.3236447"",""Italian MIUR PRIN Liquid_Edge project"; PNRR RESTART program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10015693"",""Cloud computing";decentralized applications;"benchmark testing"",""Microservice architectures";Benchmark testing;Measurement;Java;Containers;Servers;"Python"","""",""2"","""",""47"",""IEEE"",""12 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Offloading Algorithms for Maximizing Inference Accuracy on Edge Device in an Edge Intelligence System,""A. Fresa";" J. P. Champati"",""Edge Networks Group, IMDEA Networks Institute, Leganés, Madrid, Spain";" Edge Networks Group, IMDEA Networks Institute, Leganés, Madrid, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""24 May 2023"",""2023"",""34"",""7"",""2025"",""2039"",""With the emergence of edge computing, the problem of offloading jobs between an Edge Device (ED) and an Edge Server (ES) received significant attention in the past. Motivated by the fact that an increasing number of applications are using Machine Learning (ML) inference from the data samples collected at the EDs, we study the problem of offloading inference jobs by considering the following novel aspects: 1) in contrast to a typical computational job, the processing time of an inference job depends on the size of the ML model, and 2) recently proposed Deep Neural Networks (DNNs) for resource-constrained devices provide the choice of scaling down the model size by trading off the inference accuracy. Considering that multiple ML models are available at the ED, and a powerful ML model is available at the ES, we formulate an Integer Linear Programming (ILP) problem with the objective of maximizing the total inference accuracy of $n$n data samples at the ED subject to a time constraint $T$T on the makespan. Noting that the problem is NP-hard, we propose an approximation algorithm Accuracy Maximization using LP-Relaxation and Rounding (AMR$^{2}$2) and prove that it results in a makespan at most $\text{2}T$2T and achieves a total accuracy that is lower by a small constant from the optimal total accuracy implying that AMR$^{2}$2 is asymptotically optimal. Further, if the data samples are identical we propose Accuracy Maximization using Dynamic Programming (AMDP), an optimal pseudo-polynomial time algorithm. Furthermore, we extend AMR$^{2}$2 for the case of multiple ESs, where each ES is equipped with a powerful ML model. As proof of concept, we implemented AMR$^{2}$2 on a Raspberry Pi, equipped with MobileNets, that is connected to a server equipped with ResNet, and studied the total accuracy and makespan performance of AMR$^{2}$2 for image classification."",""1558-2183"","""",""10.1109/TPDS.2023.3267458"",""European Commission(grant numbers:101062011)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10105465"",""Approximation algorithms";deep learning;edge computing;linear programming;"machine learning"",""Data models";Computational modeling;Inference algorithms;Costs;Servers;Approximation algorithms;"Scheduling"","""","""","""",""39"",""IEEE"",""19 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"OfpCNN: On-Demand Fine-Grained Partitioning for CNN Inference Acceleration in Heterogeneous Devices,""L. Yang"; C. Zheng; X. Shen;" G. Xie"",""Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China;" Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2023"",""2023"",""34"",""12"",""3090"",""3103"",""Collaborative inference is a promising method for balancing the limited computational power of Internet of Things (IoT) devices with the huge computational demands of convolutional neural networks (CNNs). In this approach, a CNN is divided into multiple partitions and placed on multiple devices to run simultaneously. However, two major challenges are raised. (1) Computational latencies vary when the central processing unit (CPU) loads of devices are different. However, no suitable methods are available for accurately determining computation latencies on the basis of CPU utilization. (2) Existing methods partition a CNN model either vertically or horizontally. The granularity of these methods is extremely coarse and their accuracy is low. To address the aforementioned issues, this study proposes a distributed collaborative inference framework that supports a fine-grained partitioning scheme for CNN in heterogeneous devices (hereafter referred to as OfpCNN). First, the framework uses the layer latency prediction model based on floating-point operations and CPU load (FCPM) to accurately predict the computation latency of each layer of CNN in different devices. Subsequently, OfpCNN uses horizontal and vertical partitioning methods (HVPM) to partition the input feature maps and the structure of CNN respectively in accordance with network conditions and computing capacity, then assigns them to multiple devices for execution. The HVPM solution overall considers the execution position of the layer, parallelism, and location of devices responsible for data aggregation and distribution, which can consequently obtain more fine-grained partition schemes. Experimental results show that FCPM can achieve a minimum accuracy of 88% and HVPM can improve the inference speed by 1–2.54 times compared with other state-of-the-art methods."",""1558-2183"","""",""10.1109/TPDS.2023.3321755"",""National Natural Science Foundation of China(grant numbers:62372167,61972139,62141212,62133014)"; Natural Science Foundation of Hunan Province(grant numbers:2021JJ30153,2022JJ10021); Natural Science Foundation of Chongqing(grant numbers:cstc2021jcyj-msxmX0817,cstc2021jcyj-msxmX0461,CSTB2022NSCQ-MSX1393);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10269729"",""Edge computing";deep neural networks;edge intelligence;collaborative inference;"model partitioning"",""Computational modeling";Load modeling;Collaboration;Predictive models;Internet of Things;Convolutional neural networks;"Partitioning algorithms"","""","""","""",""34"",""IEEE"",""3 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"On Jointly Optimizing Partial Offloading and SFC Mapping: A Cooperative Dual-Agent Deep Reinforcement Learning Approach,""X. Wang"; H. Xing; F. Song; S. Luo; P. Dai;" B. Zhao"",""School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China"; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China;" School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Jul 2023"",""2023"",""34"",""8"",""2479"",""2497"",""Multi-access edge computing (MEC) and network function virtualization (NFV) are promising technologies to support emerging IoT applications, especially those computation-intensive. In NFV-enabled MEC environment, service function chain (SFC), i.e., a set of ordered virtual network functions (VNFs), can be mapped on MEC servers. Mobile devices (MDs) can offload computation-intensive applications, which can be represented by SFCs, fully or partially to MEC servers for remote execution. This article studies the partial offloading and SFC mapping joint optimization (POSMJO) problem in an NFV-enabled MEC system, where the data from an incoming task is partitioned into two parts, with one part executed locally and the other offloaded to the edge infrastructure for execution. These two parts are independent of each other, but both need to be processed by the same SFC. The objective is to minimize the average cost in the long term which is a combination of execution delay, MD's energy consumption, and usage charge for edge computing. This problem consists of two closely related decision-making steps, namely task partition and VNF placement, which is highly complex and quite challenging. To address this, we propose a cooperative dual-agent deep reinforcement learning (CDADRL) algorithm, where two agents interact with each other. Simulation results show that the proposed algorithm outperforms three combinations of deep reinforcement learning algorithms with respect to cumulative reward and it overweighs a number of baseline algorithms in terms of execution delay, energy consumption, and usage charge."",""1558-2183"","""",""10.1109/TPDS.2023.3287633"",""National Natural Science Foundation of China(grant numbers:62172342)"; Natural Science Foundation of Hebei Province(grant numbers:F2022105027); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0568,2022NSFSC0944); Fundamental Research Funds for the Central Universities, P. R. China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10158417"",""Deep reinforcement learning";multi-access edge computing;network function virtualization;partial offloading;"SFC mapping"",""Task analysis";Optimization;Partitioning algorithms;Internet of Things;Servers;Reinforcement learning;"Network function virtualization"","""","""","""",""55"",""IEEE"",""20 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"On Model Transmission Strategies in Federated Learning With Lossy Communications,""X. Su"; Y. Zhou; L. Cui;" J. Liu"",""College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China"; School of Computing, FSE, Macquarie University, Macquarie Park, NSW, Australia; College of Computer Science and Software Engineering, Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ), Shenzhen University, Shenzhen, China;" School of Computing Science, Simon Fraser University, Burnaby, BC, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2023"",""2023"",""34"",""4"",""1173"",""1185"",""Recently, federated learning (FL) has received tremendous attention in both academia and industry, in which decentralized clients collaboratively complete model training by exchanging model updates with a parameter server through the Internet. Its distributed nature well utilizes the localized data and preserves clients’ privacy, but also incurs heavy communication overhead. Existing studies on model update have mostly focused on the bandwidth constraint of the communication channels. Today's Internet however is highly unreliable. Simply using Transmission Control Protocol (TCP) would lead to low network utilization under frequent losses. In this paper, we closely examine the optimal transmission strategies in FL over the realistic lossy Internet. We systematically integrate model compression, forward error correction (FEC) and retransmission towards Federated Learning with Lossy Communications (FedLC). We derive the convergence rate of FedLC under non-convex loss with the optimal transmission. We then decompose this non-convex problem and present effective practical solutions. Public datasets are exploited for performance evaluation by varying the packet loss rate from 10% to 50%. In a fixed training time budget, FedLC can improve model accuracy by 3.91% on average or reduce the communication traffic by 34.27%-47.57% in comparison with state-of-the-art baselines."",""1558-2183"","""",""10.1109/TPDS.2023.3240883"",""National Key Research and Development Plan of China(grant numbers:2022YFB3102302)"; Shenzhen Science and Technology Program(grant numbers:RCYX20200714114645048); Shenzhen Fundamental Research Program(grant numbers:20200814105901001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032555"",""Compression";federated learning;forward error correction;lossy communication;"retransmission"",""Computational modeling";Training;Federated learning;Quantization (signal);Propagation losses;Packet loss;"Convergence"","""",""2"","""",""57"",""IEEE"",""31 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"On the Design and Evaluation of an Optimal Security-and-Time Cognizant Data Placement for Dynamic Fog Environments,""X. Wang"; B. Veeravalli; J. Song;" H. Liu"",""School of Computer Science and Technology, Xidian University, Xi’an, China"; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; School of Computer Science and Technology, Xidian University, Xi’an, China;" School of Computer Science and Technology, Xidian University, Xi’an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""489"",""500"",""Fog Computing usefully extends Cloud to the edge of the network for the sake of meeting users’ expanding demand for low latency. However, due to its scattered distribution and open architecture, fog nodes are highly vulnerable to security threats, resulting in an inevitable sharp conflict between quick response time and high data security. This conflict motivates the need for effective data placement among fog nodes towards a trade-off between security and time. Existing studies merely offer independent solutions by considering either security or response time. By contrast, we establish a dynamic multi-objective optimization model in this article by optimizing security and response time simultaneously. With this model, we propose an efficient evolutionary algorithm, referred to as Dynamic Interactive Security-and-Time cognizant algorithm (DIST), to obtain optimal data placement strategies under Fog environments. To improve efficiency, DIST allows users to gradually incorporate their preference information into the search process so as to find their most preferred solutions without exploring the whole search space. We demonstrate the superiority of DIST by rigorous comparison with the most state-of-art data placement strategy and other well-applied strategies. Experimental results manifest that DIST outperforms other strategies in obtaining solutions with higher data security and shorter response time. Furthermore, DIST is capable of efficiently and continuously tracking the Pareto optimal solution under dynamically changing Fog environments while other existing strategies cannot."",""1558-2183"","""",""10.1109/TPDS.2022.3223796"",""National Natural Science Foundation of China(grant numbers:U22A2098,62172457,62272367)"; Key Research and Development Program of Shaanxi Province(grant numbers:2022ZDLGY01-06,2022ZDLGY01-01); Agency for Science, Technology and Research; CISCO Systems (USA) Pte. Ltd; National University of Singapore(grant numbers:I21001E0002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9956843"",""Fog computing";dynamic environment;interactive multi-objective optimization;response time;"security"",""Security";Time factors;Data security;Peer-to-peer computing;Encryption;Cloud computing;"Protocols"","""","""","""",""23"",""IEEE"",""21 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"On the Message Complexity of Fault-Tolerant Computation: Leader Election and Agreement,""M. Kumar";" A. R. Molla"",""Indian Statistical Institute, Kolkata, West Bengal, India";" Indian Statistical Institute, Kolkata, West Bengal, India"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2023"",""2023"",""34"",""4"",""1115"",""1127"",""This article investigates the message complexity of two fundamental problems, leader election and agreement in the crash-fault synchronous and fully-connected distributed network. We present randomized (Monte Carlo) algorithms for both the problems and also show non-trivial lower bounds on the message complexity. Our algorithms achieve sublinear message complexity in the so-called implicit version of the two problems when tolerating more than a constant fraction of the faulty nodes. In comparison to the state-of-art, our results improved and extended the works of [Gilbert-Kowalski, SODA’10] (which studied only the agreement problem) in several directions. Specifically, our algorithms tolerate any number of faulty nodes up to $(n -\operatorname{polylog}n)$(n-polylogn). The message complexity (and also the time complexity) of our algorithms is optimal (up to a $\operatorname{polylog}n$polylogn factor). Further, our algorithm works in anonymous networks, where nodes do not know each other. To the best of our knowledge, these are the first sub-linear results for both the leader election and the agreement problem in the crash-fault distributed networks."",""1558-2183"","""",""10.1109/TPDS.2023.3239993"",""ISI DCSW/TAC(grant numbers:F5484)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10026332"",""Distributed algorithm";randomized algorithm;fault-tolerant algorithm;crash-fault;leader election;agreement;"message complexity"",""Complexity theory";Voting;Fault tolerant systems;Fault tolerance;Computer crashes;Peer-to-peer computing;"Protocols"","""","""","""",""53"",""IEEE"",""26 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"On-Line Network Traffic Anomaly Detection Based on Tensor Sketch,""S. Pei"; J. Wen; K. Xie; G. Xie;" K. Li"",""College of Computer Science and Electronics Engineering, Hunan University, Changsha, China"; School of Computer Science and Engineering, Hunan University of Science and Technology, Xiangtan, China; College of Computer Science and Electronics Engineering, Hunan University, Changsha, China; Computer Network Information Center, Chinese Academy of Sciences, Beijing, China;" College of Computer Science and Electronics Engineering, Hunan University, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Oct 2023"",""2023"",""34"",""12"",""3028"",""3045"",""Network traffic anomaly detection is critical for advanced network applications. However, network traffic monitoring data arrive in a streaming fashion and could be infinite, which makes the offline algorithms that attempt to store the entire stream monitoring data for analysis not scalable. To well utilize the strong ability of tensor model, we use a tensor to represent the prior non-anomalous traffic matrices and propose a novel unsupervised anomaly detection framework that can be used to detect anomalies in a streaming fashion by making only one pass over the data while utilizing limited storage. In the framework, we propose a succinct tensor sketch to maintain, in a streaming model, the subspace that can well represent all prior non-anomalous data detected. Using the subspace, anomalies in each new incoming traffic monitoring data can be quickly detected based on a simple outlier score calculation. Further, we prove that the tensor sketch is mergeable. Exploiting this property, we propose a distributed anomaly detection framework in which the distributed node only needs to upload its succinct tensor sketch instead of the raw monitoring data to the central node to calculate the global subspace of the whole network, which greatly saves the transmission cost. We theoretically prove that our tensor sketch based anomaly detection algorithm compares favorably with the offline approach which calculates the subspace based on expensive global Singular Value Decomposition (SVD). The experimental results demonstrate the effectiveness and efficiency of our approach over other popular online anomaly detection algorithms."",""1558-2183"","""",""10.1109/TPDS.2023.3316717"",""National Science Fund for Distinguished Young Scholars(grant numbers:62025201)"; National Natural Science Foundation of China(grant numbers:61972144); Key Research and Development Program of Hunan Province(grant numbers:2023GK2001); Huawei Innovation Project(grant numbers:TC20201201003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10255295"",""Stream monitoring data";network anomaly detection;"tensor sketch"",""Tensors";Anomaly detection;Monitoring;Data models;Telecommunication traffic;Matrix converters;"Feature extraction"","""","""","""",""43"",""IEEE"",""19 Sep 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"P4SGD: Programmable Switch Enhanced Model-Parallel Training on Generalized Linear Models on Distributed FPGAs,""H. Huang"; Y. Li; J. Sun; X. Zhu; J. Zhang; L. Luo; J. Li;" Z. Wang"",""Collaborative Innovation Center of Artificial Intelligence, Zhejiang University, Hangzhou, Zhejiang, China"; Collaborative Innovation Center of Artificial Intelligence, Zhejiang University, Hangzhou, Zhejiang, China; Collaborative Innovation Center of Artificial Intelligence, Zhejiang University, Hangzhou, Zhejiang, China; Collaborative Innovation Center of Artificial Intelligence, Zhejiang University, Hangzhou, Zhejiang, China; Collaborative Innovation Center of Artificial Intelligence, Zhejiang University, Hangzhou, Zhejiang, China; University of Washington, Seattle, WA, USA; National University of Singapore, Singapore;" Collaborative Innovation Center of Artificial Intelligence, Zhejiang University, Hangzhou, Zhejiang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Jun 2023"",""2023"",""34"",""8"",""2311"",""2324"",""Generalized linear models (GLMs) are a widely utilized family of machine learning models in real-world applications. As data size increases, it is essential to perform efficient distributed training for these models. However, existing systems for distributed training have a high cost for communication and often use large batch sizes to balance computation and communication, which negatively affects convergence. Therefore, we argue for an efficient distributed GLM training system that strives to achieve linear scalability, while keeping batch size reasonably low. As a start, we propose P4SGD, a distributed heterogeneous training system that efficiently trains GLMs through model parallelism between distributed FPGAs and through forward-communication-backward pipeline parallelism within an FPGA. Moreover, we propose a light-weight, latency-centric in-switch aggregation protocol to minimize the latency of the AllReduce operation between distributed FPGAs, powered by a programmable switch. As such, to our knowledge, P4SGD is the first solution that achieves almost linear scalability between distributed accelerators through model parallelism. We implement P4SGD on eight Xilinx U280 FPGAs and a Tofino P4 switch. Our experiments show P4SGD converges up to 6.5X faster than the state-of-the-art GPU counterpart."",""1558-2183"","""",""10.1109/TPDS.2023.3279255"",""National Key R&D Program of China(grant numbers:2022ZD0119301)"; Program of Zhejiang Province Science and Technology(grant numbers:2022C01044); Fundamental Research Funds for the Central Universities(grant numbers:226-2022-00151); Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study(grant numbers:SN-ZJU-SIAS-0010); Key Laboratory for Corneal Diseases Research of Zhejiang Province; Alibaba Group through Alibaba Innovative Research;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10147044"",""Distributed training system";FPGA;GLMs;"P4"",""Parallel processing";Training;Computational modeling;Data models;Field programmable gate arrays;Backpropagation;"Switches"","""","""","""",""56"",""IEEE"",""8 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Parallel Training of Pre-Trained Models via Chunk-Based Dynamic Memory Management,""J. Fang"; Z. Zhu; S. Li; H. Su; Y. Yu; J. Zhou;" Y. You"",""Tencent Inc., Shenzhen, Guangdong, China"; Tencent Inc., Shenzhen, Guangdong, China; National Singapore University, Singapore; Tencent Inc., Shenzhen, Guangdong, China; Tencent Inc., Shenzhen, Guangdong, China; Tencent Inc., Shenzhen, Guangdong, China;" National Singapore University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Nov 2022"",""2023"",""34"",""1"",""304"",""315"",""The pre-trained model (PTM) is revolutionizing Artificial Intelligence (AI) technology. However, the hardware requirement of PTM training is prohibitively high, making it a game for a small proportion of people. Therefore, we proposed PatrickStar system to lower the hardware requirements of PTMs and make them accessible to everyone. PatrickStar uses the CPU-GPU heterogeneous memory space to store the model data. Different from existing works, we organize the model data in memory chunks and dynamically distribute them in the heterogeneous memory. Guided by the runtime memory statistics collected in a warm-up iteration, chunks are orchestrated efficiently in heterogeneous memory and generate lower CPU-GPU data transmission volume and higher bandwidth utilization. Symbiosis with the Zero Redundancy Optimizer, PatrickStar scales to multiple GPUs on multiple nodes. The system can train tasks on bigger models and larger batch sizes, which cannot be accomplished by existing works. Experimental results show that PatrickStar extends model scales 2.27 and 2.5 times of DeepSpeed, and exhibits significantly higher execution speed. PatricStar also successfully runs the 175B GPT3 training task on a 32 GPU cluster. Our code is available at https://github.com/Tencent/PatrickStar."",""1558-2183"","""",""10.1109/TPDS.2022.3219819"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9940581"",""Deep learning";parallel computing;distributed system;"NLP"",""Training";Data models;Graphics processing units;Memory management;Computational modeling;Tensors;"Hardware"","""",""6"","""",""36"",""IEEE"",""7 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Performance Analysis of Machine Learning Centered Workload Prediction Models for Cloud,""D. Saxena"; J. Kumar; A. K. Singh;" S. Schmid"",""Department of Computer Science, Goethe University Frankfurt, Frankfurt, Germany"; Department of Computer Applications, NIT Tiruchirappalli, Tamilnadu, India; Department of Computer Applications, NIT Kurukshetra, Thanesar, HR, India;" TU Berlin, Berlin, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Mar 2023"",""2023"",""34"",""4"",""1313"",""1330"",""The precise estimation of resource usage is a complex and challenging issue due to the high variability and dimensionality of heterogeneous service types and dynamic workloads. Over the last few years, the prediction of resource usage and traffic has received ample attention from the research community. Many machine learning-based workload forecasting models have been developed by exploiting their computational power and learning capabilities. This paper presents the first systematic survey cum performance analysis-based comparative study of diversified machine learning-driven cloud workload prediction models. The discussion initiates with the significance of predictive resource management followed by a schematic description, operational design, motivation, and challenges concerning these workload prediction models. Classification and taxonomy of different prediction approaches into five distinct categories are presented focusing on the theoretical concepts and mathematical functioning of the existing state-of-the-art workload prediction methods. The most prominent prediction approaches belonging to a distinct class of machine learning models are thoroughly surveyed and compared. All five classified machine learning-based workload prediction models are implemented on a common platform for systematic investigation and comparison using three distinct benchmark cloud workload traces via experimental analysis. The essential key performance indicators of state-of-the-art approaches are evaluated for comparison and the paper is concluded by discussing the trade-offs and notable remarks."",""1558-2183"","""",""10.1109/TPDS.2023.3240567"",""National Institute of Technology, Kurukshetra"; Goethe University, Frankfurt; Austrian Science Fund; Deutsche Forschungsgemeinschaft(grant numbers:I4800-N,2020-2023);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10029931"",""Cloud computing";deep learning;ensemble lear- ning;evolutionary neural network;forecasting;hybrid learning;"quantum neural network"",""Predictive models";Cloud computing;Servers;Neural networks;Resource management;Data models;"Dynamic scheduling"","""",""3"","""",""64"",""IEEE"",""30 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Performance Portable Batched Sparse Linear Solvers,""K. Liegeois"; S. Rajamanickam;" L. Berger-Vergiat"",""Center for Computing Research, Sandia National Laboratories, Albuquerque, NM, USA"; Center for Computing Research, Sandia National Laboratories, Albuquerque, NM, USA;" Center for Computing Research, Sandia National Laboratories, Albuquerque, NM, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2023"",""2023"",""34"",""5"",""1524"",""1535"",""Solving large number of small linear systems is increasingly becoming a bottleneck in computational science applications. While dense linear solvers for such systems have been studied before, batched sparse linear solvers are just starting to emerge. In this paper, we discuss algorithms for solving batched sparse linear systems and their implementation in the Kokkos Kernels library. The new algorithms are performance portable and map well to the hierarchical parallelism available in modern accelerator architectures. The sparse matrix vector product (SPMV) kernel is the main performance bottleneck of the Krylov solvers we implement in this work. The implementation of the batched SPMV and its performance are therefore discussed thoroughly in this paper. The implemented kernels are tested on different Central Processing Unit (CPU) and Graphic Processing Unit (GPU) architectures. We also develop batched Conjugate Gradient (CG) and batched Generalized Minimum Residual (GMRES) solvers using the batched SPMV. Our proposed solver was able to solve 20,000 sparse linear systems on V100 GPUs with a mean speedup of 76x and 924x compared to using a parallel sparse solver with a block diagonal system with all the small linear systems, and compared to solving the small systems one at a time, respectively. We see mean speedup of 0.51 compared to dense batched solver of cuSOLVER on V100, while using lot less memory. Thorough performance evaluation on three different architectures and analysis of the performance are presented."",""1558-2183"","""",""10.1109/TPDS.2023.3249110"",""Exascale Computing Project(grant numbers:17-SC-20-SC)"; Office of Science; National Nuclear Security Administration;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10054414"",""Batch sparse solvers";batch BLAS;kokkos kernels;"performance portable"",""Linear systems";Kernel;Graphics processing units;Tensors;Sparse matrices;Libraries;"Instruction sets"","""","""","""",""19"",""CCBY"",""27 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Personalized Edge Intelligence via Federated Self-Knowledge Distillation,""H. Jin"; D. Bai; D. Yao; Y. Dai; L. Gu; C. Yu;" L. Sun"",""National Engineering Research Center for Big Data Technology and System, Service Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center for Big Data Technology and System, Service Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Service Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Lehigh University, Bethlehem, PA, USA; National Engineering Research Center for Big Data Technology and System, Service Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Service Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" Lehigh University, Bethlehem, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""567"",""580"",""Federated Learning (FL) is an emerging approach in edge computing for collaboratively training machine learning models among multiple devices, which aims to address limited bandwidth, system heterogeneity, and privacy issues in traditional centralized training. However, the existing federated learning methods focus on learning a shared global model for all devices, which may not always be ideal for different devices. Such situations become even worse when each edge device has its own data distribution or task. In this paper, we study personalized federated learning in which our goal is to train models to perform well for individual clients. We observe that the initialization in each communication round causes the forgetting of historical personalized knowledge. Based on this observation, we propose a novel Personalized Federated Learning (PFL) framework via self-knowledge distillation, named pFedSD. By allowing clients to distill the knowledge of previous personalized models to current local models, pFedSD accelerates the process of recalling the personalized knowledge for the latest initialized clients. Moreover, self-knowledge distillation provides different views of data in feature space to realize an implicit ensemble of local models. Extensive experiments on various datasets and settings demonstrate the effectiveness and robustness of pFedSD."",""1558-2183"","""",""10.1109/TPDS.2022.3225185"",""National Natural Science Foundation of China(grant numbers:62072204)"; Fundamental Research Funds for the Central Universities(grant numbers:HUST:2020kfyXJJS019);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964434"",""Edge computing";personalized federated learning;"knowledge distillation"",""Data models";Computational modeling;Training;Federated learning;Servers;Task analysis;"Sun"","""",""1"","""",""52"",""CCBYNCND"",""28 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"PetaKV: Building Efficient Key-Value Store for File System Metadata on Persistent Memory,""Y. Zhang"; J. Zhou; X. Min; S. Ge; J. Wan; T. Yao;" D. Wang"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of data storage systems and Technology, Ministry of Education of China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of data storage systems and Technology, Ministry of Education of China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Cloud Storage Service Product Dept, Huawei Technologies Co. Ltd, Shenzhen, Guangdong Province, China;" Cloud Storage Service Product Dept, Huawei Technologies Co. Ltd, Shenzhen, Guangdong Province, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Jan 2023"",""2023"",""34"",""3"",""843"",""855"",""Previous works proposed building file systems and organizing the metadata with KV stores because KV stores handle entries of various sizes efficiently and have excellent scalability. The emergence of the byte-addressable persistent memory (PM) enables metadata service to be faster than before by tailoring the KV store for the PM. However, existing PM-based KV stores cannot handle the workloads of file systems’ metadata well because simply depending on hash tables or trees cannot simultaneously provide fast file accessing and efficient directory traversing. In this paper, we exploit the insight of the metadata operations and propose the PetaKV, a KV store tailored for the metadata management of file systems on PM. PetaKV leverages dual hash indexing to achieve fast file put and get operations. Moreover, it cooperates with PM-tailored peta logs to collocate KV entries for each directory, thus supporting efficient directory scans. Our evaluation indicates PetaKV outperforms state-of-art tree-based KV stores on put, get and scan $2.5\times$2.5×, $3.2\times$3.2×, and $2.8\times$2.8× on average, respectively. Moreover, the file system built with PetaKV achieves $1.2\times$1.2× to $6.4\times$6.4× speedup compared to those built with tree-based KV stores on the metadata operations."",""1558-2183"","""",""10.1109/TPDS.2022.3232382"",""Creative Research Group Project of NSFC(grant numbers:61821003)"; National Natural Science Foundation of China(grant numbers:62072196,62102155); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2021B0101400003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999527"",""Key-Value Store";file system metadata;persistent memory;hash index;"log-structure"",""Metadata";File systems;Throughput;Indexing;Complexity theory;Buildings;"Three-dimensional displays"","""","""","""",""35"",""IEEE"",""27 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Practical Cloud-Edge Scheduling for Large-Scale Crowdsourced Live Streaming,""R. Zhang"; C. Yang; X. Wang; T. Huang; C. Wu; J. Liu;" L. Sun"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Innovation Algorithm, Huawei Cloud, Shenzhen, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China; Department of Computer Science and Technology and Key Laboratory of Pervasive Computing, Ministry of Education, Tsinghua University, Beijing, China; Department of Computer Science and Technology and Key Laboratory of Pervasive Computing, Ministry of Education, Tsinghua University, Beijing, China; School of Computer Science, Simon Fraser University, Vancouver, BC, Canada;" Department of Computer Science and Technology and Key Laboratory of Pervasive Computing, Ministry of Education, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 May 2023"",""2023"",""34"",""7"",""2055"",""2071"",""Even though conventional wisdom claims that in order to improve viewer engagement, the cloud-edge providers should serve the viewers with the nearest edge nodes, however, we show that doing this for crowdsourced live streaming (CLS) services can introduce significant costs inefficiency. In this paper, we first carry out large-scale measurement analysis by using the real-world service data from Huawei Cloud, a representative cloud-edge provider in China. We observe that the massive number of channels has proposed great burdens to the operating expenditure of the cloud-edge providers, and most importantly, unbalanced viewer distribution makes the edge nodes suffer significant costs inefficiency. To tackle the above concerns, we propose AggCast, a novel CLS scheduling framework to optimize the edge node utilization for the cloud-edge provider. The core idea of AggCast is to aggregate some viewers that are initially scattered on different regions, and assign them to fewer pre-selected nodes, thereby reducing bandwidth costs. In particular, by integrating the useful insights obtained from our large-scale measurement, AggCast can not only ensure that quality of experience (QoS) does not suffer degradation, but also satisfy the systematic requirements of CLS services. AggCast has been A/B tested and fully deployed. The online and trace-driven experiments show that, compared to the most prevalent method, AggCast saves over 16.3% back-to-source (BTS) bandwidth costs while significantly improving QoS (startup latency, stall frequency and stall time are reduced over 12.3%, 4.57% and 3.91%, respectively)."",""1558-2183"","""",""10.1109/TPDS.2023.3267731"",""National Natural Science Foundation of China(grant numbers:61936011)"; Beijing Key Lab of Networked Multimedia;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10104139"",""Cloud edge computing";content delivery;live streaming;resource scheduling;"traffic engineering"",""Costs";Quality of service;Bandwidth;Servers;Optimization;Computer science;"Systematics"","""","""","""",""62"",""IEEE"",""18 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Practice of Streaming Processing of Dynamic Graphs: Concepts, Models, and Systems,""M. Besta"; M. Fischer; V. Kalavri; M. Kapralov;" T. Hoefler"",""ETH Zurich, Zurich, Switzerland"; PRODYNA (Schweiz) AG, Basel, Switzerland; Boston University, Boston, MA, USA; School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland;" ETH Zurich, Zurich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1860"",""1876"",""Graph processing has become an important part of various areas of computing, including machine learning, medical applications, social network analysis, computational sciences, and others. A growing amount of the associated graph processing workloads are dynamic, with millions of edges added or removed per second. Graph streaming frameworks are specifically crafted to enable the processing of such highly dynamic workloads. Recent years have seen the development of many such frameworks. However, they differ in their general architectures (with key details such as the support for the concurrent execution of graph updates and queries, or the incorporated graph data organization), the types of updates and workloads allowed, and many others. To facilitate the understanding of this growing field, we provide the first analysis and taxonomy of dynamic and streaming graph processing. We focus on identifying the fundamental system designs and on understanding their support for concurrency, and for different graph updates as well as analytics workloads. We also crystallize the meaning of different concepts associated with streaming graph processing, such as dynamic, temporal, online, and time-evolving graphs, edge-centric processing, models for the maintenance of updates, and graph databases. Moreover, we provide a bridge with the very rich landscape of graph streaming theory by giving a broad overview of recent theoretical related advances, and by discussing which graph streaming models and settings could be helpful in developing more powerful streaming frameworks and designs. We also outline graph streaming workloads and research challenges.Author: Please confirm or add details for any funding or financial support for the research of this article. ?>"",""1558-2183"","""",""10.1109/TPDS.2021.3131677"",""Google European Doctoral Fellowship"; European Research Council; European Union's Horizon 2020(grant numbers:678880);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629281"",""Streaming graphs";dynamic graphs;evolving graphs;streaming graph processing;dynamic graph processing;evolving graph processing;online graph processing;graph streaming frameworks;"graph databases"",""Heuristic algorithms";Taxonomy;Analytical models;Data models;Computational modeling;Distributed databases;"Social networking (online)"","""",""5"","""",""166"",""IEEE"",""30 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Precise Event Sampling on AMD Versus Intel: Quantitative and Qualitative Comparison,""M. A. Sasongko"; M. Chabbi; P. H. J. Kelly;" D. Unat"",""Department of Computer Engineering, Koç University, Istanbul, Turkey"; Scalable Machines Research, San Jose, CA, USA; Department of Computing, Imperial College London, London, U.K.;" Department of Computer Engineering, Koç University, Istanbul, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Mar 2023"",""2023"",""34"",""5"",""1594"",""1608"",""Precise event sampling is a profiling feature in commodity processors that can sample hardware events and accurately locate the instructions that trigger the events. This feature has been used in a large number of tools to detect application performance issues. Although precise event sampling is readily supported in modern multicore architectures, vendor supports exhibit great differences that affect their accuracy, stability, overhead, and functionality. This work presents the most comprehensive study to date on benchmarking the event sampling features of Intel PEBS and AMD IBS and performs in-depth analysis on key differences through series of microbenchmarks. Our qualitative and quantitative analysis shows that PEBS allows finer-grained and more accurate sampling of hardware events, while IBS offers richer set of information at each sample though it suffers from lower accuracy and stability. Moreover, OS signal delivery, which is a common method used by the profiling software, introduces significant time overhead to the original overhead incurred by the hardware mechanisms in both PEBS and IBS. We also found that both PEBS and IBS have bias in sampling events across multiple different locations in a code. Lastly, we demonstrate how our findings on microbenchmarks under different thread counts hold for a full-fledged profiling tool that runs on the state-of-the-art Intel and AMD machines. Overall our detailed comparisons serve as a great reference and provide invaluable information for hardware designers and profiling tool developers."",""1558-2183"","""",""10.1109/TPDS.2023.3257105"",""European Research Council"; European Union's Horizon 2020(grant numbers:949587); Scientific and Technological Research Council of Turkey(grant numbers:120E492); Royal Society-Newton Advanced Fellowship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10068807"",""Precise event sampling";PMUs;"profiling"",""Registers";Hardware;Monitoring;Phasor measurement units;Instruction sets;Benchmark testing;"Software"","""","""","""",""56"",""CCBY"",""14 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Preventive Priority Setting Against Multiple Controller Failures in Software Defined Networks,""F. He";" E. Oki"",""Graduate School of Informatics, Kyoto University, Kyoto, Japan";" Graduate School of Informatics, Kyoto University, Kyoto, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Jun 2023"",""2023"",""34"",""8"",""2352"",""2364"",""This paper proposes a preventive priority setting model to minimize the worst-case maximum utilization ratio against multiple controller failures in software defined networks. For a set of controllers able to manage a switch, we introduce a priority for each controller to become the main controller that controls the switch. Once the existing main controller fails, the survived controller which has the highest priority works as the main controller. This reassignment is automatically obtained according to the priority setting decided at the network operation start time. In this way, the proposed model provides a prompt recovery with reducing the network instability due to unnecessary controller reassignment. We formulate the proposed model in two different forms, which are an integer linear programming formulation and a min-max formulation. We prove that the considered problem is NP-hard. A basic heuristic is introduced based on the min-max formulation. Its two extensions are further developed considering the accuracy and the computation time in practical computation. Numerical results reveal that, compared to two baselines that sacrifice certain network stability to achieve a more flexible reassignment, the proposed model reduces the network instability by 27% and 52% in average, respectively, with obtaining the comparable maximum utilization ratio."",""1558-2183"","""",""10.1109/TPDS.2023.3285898"",""JSPS KAKENHI, Japan(grant numbers:21H03426,21K17733)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10149508"",""Controller failures";load balancing;priority setting;"software defined networks"",""Control systems";Switches;Computational modeling;Load management;Load modeling;Software;"Numerical stability"","""",""1"","""",""39"",""IEEE"",""13 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Privacy Preserving $n$n-Party Scalar Product Protocol,""F. van Daalen"; L. Ippel; A. Dekker;" I. Bermejo"",""Department of Radiation Oncology (MAASTRO), GROW School for Oncology and Reproduction, Maastricht University Medical Centre+, Maastricht, HX, The Netherlands"; Statistics Netherlands, Heerlen, The Netherlands; Department of Radiation Oncology (MAASTRO), GROW School for Oncology and Reproduction, Maastricht University Medical Centre+, Maastricht, HX, The Netherlands;" Department of Radiation Oncology (MAASTRO), GROW School for Oncology and Reproduction, Maastricht University Medical Centre+, Maastricht, HX, The Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2023"",""2023"",""34"",""4"",""1060"",""1066"",""Privacy-preserving machine learning enables the training of models on decentralized datasets without the need to reveal the information, both on horizontally and vertically partitioned data. However, it requires specialized techniques and algorithms to perform the necessary computations. The privacy preserving scalar product protocol, which enables the dot product of vectors without revealing them, is one popular example for its versatility. For example it can be used to perform analyses that require counting the number of samples which fulfill certain criteria defined across various sites, such as calculating the information gain at a node in a decision tree. Unfortunately, the solutions currently proposed in the literature focus on two-party scenarios, even though scenarios with a higher number of data parties are becoming more relevant. In this article, we propose a generalization of the protocol for an arbitrary number of parties, based on an existing two-party method. Our proposed solution relies on a recursive resolution of smaller scalar products. After describing our proposed method, we discuss potential scalability issues. Finally, we describe the privacy guarantees and identify any concerns, as well as comparing the proposed method to the original solution in this aspect. Additionally we provide an online repository containing the code."",""1558-2183"","""",""10.1109/TPDS.2023.3238768"",""Netherlands Organisation for Scientific Research";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024342"",""Federated learning";  $n$   n     -party scalar product protocol;"privacy preserving"",""Protocols";Privacy;Machine learning algorithms;Decision trees;Data privacy;Training;"Servers"","""",""1"","""",""19"",""CCBY"",""23 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Privacy vs. Efficiency: Achieving Both Through Adaptive Hierarchical Federated Learning,""Y. Guo"; F. Liu; T. Zhou; Z. Cai;" N. Xiao"",""College of Computer, National University of Defense Technology, Hunan, China"; School of Design, Hunan University, Hunan, China; College of Computer, National University of Defense Technology, Hunan, China; College of Computer, National University of Defense Technology, Hunan, China;" College of Computer, National University of Defense Technology, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Mar 2023"",""2023"",""34"",""4"",""1331"",""1342"",""As a decentralized training paradigm, Federated learning (FL) promises data privacy by exchanging model parameters instead of raw local data. However, it is still impeded by the resource limitations of end devices and privacy risks from the ‘curious’ cloud. Yet, existing work predominately ignores that these two issues are non-orthogonal in nature. In this article, we propose a joint design (i.e., AHFL) that accommodates both the efficiency expectation and privacy protection of clients towards high inference accuracy. Based on a cloud-edge-end hierarchical FL framework, we carefully offload the training burden of devices to one proximate edge for enhanced efficiency and apply a two-level differential privacy mechanism for privacy protection. To resolve the conflicts of dynamical resource consumption and privacy risk accumulation, we formulate an optimization problem for choosing configurations under correlated learning parameters (e.g., iterations) and privacy control factors (e.g., noise intensity). An adaptive algorithmic solution is presented based on performance-oriented resource scheduling, budget-aware device selection, and adaptive local noise injection. Extensive evaluations are performed on three different data distribution cases of two real-world datasets, using both a networked prototype and large-scale simulations. Experimental results show that AHFL relieves the end's resource burden (w.r.t. computation time 8.58% $\downarrow$↓, communication time 59.35%$\downarrow$↓ and memory consumption 43.61%$\downarrow$↓) and has better accuracy (6.34%$\uparrow$↑) than 3 typical baselines under the limited resource and privacy budgets. The code for our implementation is available at https://github.com/Guoyeting/AHFL."",""1558-2183"","""",""10.1109/TPDS.2023.3244198"",""National Key Research and Development Program of China(grant numbers:2020YFC2003404)"; National Natural Science Foundation of China(grant numbers:62172155,61832020,62072465,62102425); Natural Science Foundation of Guangdong Province(grant numbers:2018B030312002); Science and Technology Innovation Program of Hunan Province(grant numbers:2021RC2071,2022RC3061); National Natural Science Foundation of China-Guangdong Joint Fund(grant numbers:NSFCU1811461);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10043027"",""Distributed machine learning";edge computing;federated learning;"privacy protection"",""Privacy";Training;Computational modeling;Task analysis;Performance evaluation;Data models;"Adaptation models"","""",""1"","""",""39"",""IEEE"",""13 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"ProScale: Proactive Autoscaling for Microservice With Time-Varying Workload at the Edge,""K. Cheng"; S. Zhang; C. Tu; X. Shi; Z. Yin; S. Lu; Y. Liang;" Q. Gu"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Electronic and Computer Engineering, The Hong Kong University of Science and Technology, Hong Kong, SAR, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Computer and Electronic Information/School of Artificial Intelligence, Nanjing Normal University, Nanjing, Jiangsu, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Mar 2023"",""2023"",""34"",""4"",""1294"",""1312"",""Deploying microservice instances on the edge device close to end users can provide on-site processing thus reducing request response time. Each microservice has multiple instances that can process requests in parallel. To achieve high processing efficiency, the number of these instances is scaled according to the workload, which is also known as autoscaling. Previous studies of microservice autoscaling in the edge computing environment lack in-depth consideration of time-varying workload, they assume that the workload of each microservice always depends on that of its upstream. However, through an analysis of Alibaba's microservice trace with hundreds of millions of records, we find that the assumption is impractical thus hurting autoscaling effectiveness. To solve this problem, we propose ProScale, a prediction-driven proactive autoscaling framework for microservices at the edge. ProScale proactively forecasts the workload for each individual microservice per timeslot. Then it utilizes an efficient online algorithm to leverage the predicting results to determine the instance number for each microservice jointly with making placement decisions. For each microservice instance deployed on the edge device, ProScale handles burst requests using a designed offloading strategy. In addition, ProScale can also balance the load for multiple instances of each microservice. Extensive trace-driven experiments show that ProScale has great scalability. It can reduce average response time by 96.7% and resource usage by 96.5% compared with existing strategies and designed baselines."",""1558-2183"","""",""10.1109/TPDS.2023.3238429"",""National Natural Science Foundation of China(grant numbers:61872175,61832008)"; Fundamental Research Funds for the Central Universities(grant numbers:2022300297); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023993"",""Edge computing";microservice;proactive autoscaling;"workload prediction"",""Microservice architectures";Time factors;Edge computing;License plate recognition;Queueing analysis;Data centers;"Computer architecture"","""",""4"","""",""60"",""IEEE"",""20 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Redesign and Accelerate the AIREBO Bond-Order Potential on the New Sunway Supercomputer,""P. Gao"; X. Duan; B. Schmidt; W. Wan; J. Guo; W. Zhang; L. Gan; H. Fu; W. Xue; W. Liu;" G. Yang"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; School of Software, Shandong University, Jinan, China; Institute of Computer Science, Johannes Gutenberg University, Mainz, Germany; Department of Computer Science and Technology, Tsinghua University, Beijing, China; National Supercomputing Center in Wuxi, Wuxi, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Earth System Science, Ministry of Education Key Laboratory for Earth System Modeling, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Software, Shandong University, Jinan, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2023"",""2023"",""34"",""12"",""3117"",""3132"",""Molecular dynamics (MD) is one of the most crucial computer simulation methods for understanding real-world processes at the atomic level. Reactive potentials based on the bond order concept have the ability to model dynamic bond breaking and formation with close to quantum mechanical (QM) precision without actually requiring expensive QM calculations. In this article, we focus on the adaptive intermolecular reactive empirical bond-order (AIREBO) potential in LAMMPS for the simulation of carbon and hydrocarbon systems on the new Sunway supercomputer. To achieve scalable performance, we propose a parallel two-level building scheme and periodic buffering strategy for the tailored data design to explore data locality and data reuse. Furthermore, we design two optimized nearest-neighbor access algorithms: the redistribution of accumulated coefficients algorithm and the double-end search connectivity algorithm. Finally, we implement parallel force computation with an AoS data layout and hardware/software co-cache. In addition, we have designed a low-overhead atomic operation-based load balancing method and vectorization. The overall performance of AIREBO achieves a speedup of nearly $20\times$20× on a single core group (CG), and more than $5\times$5× and $4\times$4× over an Intel Xeon E5 2680 v3 core and an Intel Xeon Gold 6138 core, respectively. Compared with the Intel accelerator package in LAMMPS, our performance further achieves $3.0\times$3.0× of an Intel Xeon E5 2680 v3 core and is better than that of an Intel Xeon Gold 6138 core. We complete the validation of the results in no more than 20.5 hours on a single node with 2,000,000 running steps (i.e., 1 ns). Our experiments show that the simulation of 2,139,095,040 atoms on 798,720 ((1MPE+64CPEs) × 12,288 processes) cores exhibits a parallel efficiency of 88% under weak scaling."",""1558-2183"","""",""10.1109/TPDS.2023.3321927"",""National Natural Science Foundation of China(grant numbers:62202119,62102114,61972231,U2242210,T2125006)"; China Postdoctoral Science Foundation(grant numbers:2023M731950); National Key R&D Program of China(grant numbers:2019YFA0709400); Engineering Research Center of Digital Media Technology; Ministry of Education, China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271720"",""High-performance computing";molecular dynamics;LAMMPS;AIREBO;computational science;"the new generation sunway supercomputer"",""Carbon";Mathematical models;Hydrocarbons;Atmospheric modeling;Supercomputers;Computational modeling;"Bonding"","""","""","""",""37"",""IEEE"",""4 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Redesigning OpenKMC for Multi-Component Trillion-Atom Simulations on the New Sunway Supercomputer,""L. Xu"; H. Shang; X. Chen; Y. Zhang; L. Wang; X. Gao;" H. Song"",""Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Laboratory of Computational Physics, Institute of Applied Physics and Computational Mathematics, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Laboratory of Computational Physics, Institute of Applied Physics and Computational Mathematics, Beijing, China; Laboratory of Computational Physics, Institute of Applied Physics and Computational Mathematics, Beijing, China;" Laboratory of Computational Physics, Institute of Applied Physics and Computational Mathematics, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 May 2023"",""2023"",""34"",""7"",""1997"",""2010"",""The atomic kinetic Monte Carlo method plays an important role in material simulations by connecting the microscale mechanism with macroscale evolution. However, the long-time simulation of multi-component materials is highly challenging because it demands significant computing resources. With the advent of exascale computing, ultra-high computing power can enable kinetic Monte Carlo (KMC) simulations. In this paper, we deeply optimize OpenKMC for the new-generation Sunway supercomputer. This includes optimizing the memory access for the SW39000 architecture, eliminating various redundant computations at growing scales, and proposing a communication strategy for heterogeneous platforms. In addition, we expanded OpenKMC's simulation for multi-component alloys. Finally, the acceleration framework can produces a $37\times$37× performance enhancement on the Sunway platform. Furthermore, when powered by 10 million cores, our program can perform trillion-atom simulations of complex multi-component alloys with 85% parallel efficiency."",""1558-2183"","""",""10.1109/TPDS.2023.3269625"",""National Natural Science Foundation of China(grant numbers:12004046)"; Science Challenge Project(grant numbers:TZZT2019); State Key Lab of Processors Foundation(grant numbers:CARCH 4205,CARCH 4411);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10113812"",""Atomic kinetic Monte Carlo";many-core processor;"scalability"",""Metals";Computational modeling;Monte Carlo methods;Kinetic theory;Aging;Steel;"Silicon"","""","""","""",""51"",""IEEE"",""1 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Reliability-Aware Multi-Objective Memetic Algorithm for Workflow Scheduling Problem in Multi-Cloud System,""S. Qin"; D. Pi; Z. Shao; Y. Xu;" Y. Chen"",""College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; School of Computer Science, Shaanxi Normal University, Xi’an, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China;" College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Mar 2023"",""2023"",""34"",""4"",""1343"",""1361"",""With the development of cloud computing, multi-cloud systems have become common platforms for hosting and executing workflow applications in recent years. However, the complexity of workflow scheduling increases exponentially because of the diversified billing mechanisms, heterogeneous virtual machines, and reliability of multi-cloud systems. This article focuses on a multi-objective workflow scheduling problem in multi-cloud systems (MOWSP-MCS). The makespan, cost, and reliability are considered the optimization objectives from the perspective of users. Compared with the classical multi-objective workflow scheduling in the cloud environment, MOWSP-MCS allows users to apply the backup technique to improve reliability. To solve the MOWSP-MCS, this article proposes a reliability-aware multi-objective memetic algorithm (RA-MOMA) containing a diversification strategy and intensification strategy. In the diversification strategy, several problem-specific genetic operators are introduced to construct the diversified offspring individuals. In the intensification strategy, four problem-specific neighborhood operators are designed based on the critical path and resource utilization rate to improve the quality of the individuals in the archive set. A comprehensive numerical experiment is conducted to evaluate the effectiveness of RA-MOMA. The comparisons with several related algorithms demonstrate the superiority of RA-MOMA for solving the MOWSP-MCS."",""1558-2183"","""",""10.1109/TPDS.2023.3245089"",""Funding for Outstanding Doctoral Dissertation(grant numbers:BCXJ22-13)"; Science and Technology Innovation 2030-Key Project(grant numbers:2021ZD0113103);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10045021"",""Memetic algorithm";multi-cloud systems;multi-objective optimization;reliability;"workflow scheduling"",""Scheduling";Reliability;Costs;Task analysis;Cloud computing;Optimization;"Schedules"","""",""6"","""",""59"",""IEEE"",""15 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Replicated Versioned Data Structures for Wide-Area Distributed Systems,""N. Saquib"; C. Krintz;" R. Wolski"",""University of California, Santa Barbara, CA, USA"; University of California, Santa Barbara, CA, USA;" University of California, Santa Barbara, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2022"",""2023"",""34"",""1"",""207"",""224"",""In this work, we investigate the integration of replicated versioned data structures and append-only distributed storage systems. Doing so facilitates high availability and scalability while providing developer access to different versions of program data structures across program executions. Modern distributed systems such as the Internet of Things (IoT) often employ multi-tiered (cloud/edge/sensors) architectures consisting of a wide array of heterogeneous devices generating data frequently. Hence system availability is imperative to avoid data loss, while scalability is required for the efficient operation of the system not only within the same tier but across different tiers as well. Our proposed approach replicates, persists, and versions program data structures such as binary search trees and linked lists for use in distributed IoT applications. The versioning and persistence of these structures aid failure recovery and facilitate system debugging from its inception instead of making such considerations an afterthought. Moreover, our experiments suggest versioned data structures can perform better in applications performing high volumes of temporal queries versus traditional methods of persisting data (e.g., in a database). We empirically evaluate the overheads associated with versioning and storage persistence of program data structures, present experimental results for multiple end-to-end applications, and demonstrate the scalability of this approach."",""1558-2183"","""",""10.1109/TPDS.2022.3217969"",""National Science Foundation(grant numbers:CNS-2107101,CNS-1703560,ACI-1541215)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931978"",""Append-only logs";IoT;replication;"versioning"",""Data structures";Internet of Things;Cloud computing;Scalability;Protocols;Semantics;"Programming"","""","""","""",""71"",""IEEE"",""28 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Revenue Maximizing Online Service Function Chain Deployment in Multi-Tier Computing Network,""H. Liu"; S. Long; Z. Li; Y. Fu; Y. Zuo;" X. Zhang"",""School of Computer Science, Xiangtan University, Xiangtan, Hunan, China"; National & Local Joint Engineering Research Center of Network Security Detection and Protection Technology, Guangdong Provincial Key Laboratory of Data Security and Privacy Protection, College of Information Science and Technology, Jinan University, Guangzhou, Guangdong, China; National & Local Joint Engineering Research Center of Network Security Detection and Protection Technology, Guangdong Provincial Key Laboratory of Data Security and Privacy Protection, College of Information Science and Technology, Jinan University, Guangzhou, Guangdong, China; School of Computer Science, Xiangtan University, Xiangtan, Hunan, China; College of Electronic Science and Technology, National University of Defense Technology, Changsha, Hunan, China;" School of Computer Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Jan 2023"",""2023"",""34"",""3"",""781"",""796"",""Multi-tier computing (MC) is a promising architecture that integrates cloud computing, fog computing, and edge computing to provide users with a consistent experience of computing services by fusing computing devices within the network through virtualization technology. Although MC combines powerful computation and communication resources, the massive demand from Service Function Chain (SFC) deployments continues to make it challenging regarding resource constraints, latency satisfaction, and revenue-cost tradeoffs. To this end, in this article, we study an SFC deployment problem in MC and formulate a problem for maximizing the revenue of online SFC deployment under latency, computation resources, and communication resources constraints. To solve this online problem better, we construct a computation and communication resource cost model and transform the original online problem into a deployment cost minimization problem and a request admission problem by an alternating optimization approach. To solve the two subproblems, we propose an online approximation algorithm with a provable competitive ratio for the particular scenario with no latency requirements. Then, based on the cost model, we propose an online heuristic algorithm that adopts a binary search method for the original problem with latency requirements. Simulation experiments show that our two proposed online algorithms have advantages in total revenue, running time, and load balancing compared with other comparison algorithms."",""1558-2183"","""",""10.1109/TPDS.2022.3232205"",""Nature Science Foundation of China(grant numbers:61902336,62032020,62172350,62172349)"; National Key Research and Development Program of China(grant numbers:2021YFB3101201);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999286"",""Competitive ratio";multi-tier computing network;online algorithm;revenue maximization;"service function chain"",""Cloud computing";Costs;Edge computing;Approximation algorithms;Heuristic algorithms;Computational modeling;"Computer architecture"","""","""","""",""35"",""IEEE"",""26 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Revisiting Core Maintenance for Dynamic Hypergraphs,""Q. -S. Hua"; X. Zhang; H. Jin;" H. Huang"",""National Engineering Research Center–Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center–Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center–Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" National Engineering Research Center–Big Data Technology and System Lab, Key Laboratory of Services Computing Technology and System, Key Laboratory of Cluster and Grid Computing, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Jan 2023"",""2023"",""34"",""3"",""981"",""994"",""Core maintenance for dynamic hypergraphs has been receiving an increasing attention. However, existing works mainly focus on the insertion/deletion of hyperedges. This article revisits the problem from the view of vertices change. We study core maintenance when the vertices are inserted/deleted into/from specific hyperedges in the hypergraph, which is a challenging task since the deletion of the vertex may increase the core numbers and the insertion of the vertex may decrease the core numbers. We discuss in detail the possible changes of core numbers in different situations. For the insertion/deletion of vertices contained by a single hyperedge, we design sequential algorithms to discover the vertices whose core numbers have changed. Compared with static recomputation (Leng et al. 2013) and LYCLC (Luo et al. 2021) algorithms, our sequential algorithms can accelerate more than 1,000× and 12× at most in the processing time, respectively. For the insertion/deletion of vertices contained by different hyperedges, we find that core numbers of all vertices change 1 at most if these hyperedges form a matching. We design parallel algorithms that divide a matching into different sets based on their core numbers and allot a thread to each set. Experiments show that our parallel algorithms have good stability, scalability, and parallelism. Compared with the parallel static algorithm (Gabert et al. 2021) and the parallel dynamic algorithm GPC (Gabert et al. 2021), our parallel algorithms with 32 threads can accelerate 33× and 22× at most in the processing time, respectively."",""1558-2183"","""",""10.1109/TPDS.2023.3236669"",""National Natural Science Foundation of China(grant numbers:61972447,61832006)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10016680"",""Core maintenance";dynamic hypergraphs;"parallel algorithm"",""Maintenance engineering";Heuristic algorithms;Parallel algorithms;Task analysis;Instruction sets;Visualization;"Stability analysis"","""",""1"","""",""31"",""IEEE"",""13 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Revisiting VM-Agnostic KVM vCPU Scheduler for Mitigating Excessive vCPU Spinning,""K. Ishiguro"; N. Yasuno; P. -L. Aublin;" K. Kono"",""Hosei University, Tokyo, Japan"; Keio University, Tokyo, Japan; Internet Initiative Japan Research Laboratory, Tokyo, Japan;" Keio University, Tokyo, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Aug 2023"",""2023"",""34"",""10"",""2615"",""2628"",""In virtualized environments, virtual CPUs (vCPUs) are commonly oversubscribed on physical CPUs (pCPUs) to utilize CPU resources efficiently. However, excessive vCPU spinning, which occurs when a vCPU is waiting in a spin loop for an event from a descheduled vCPU, greatly degrades application performance in virtualized environments. VM-agnostic hypervisors aim to prevent excessive vCPU spinning by rescheduling vCPUs when an excessive spin is detected by hardware support for virtualization. We investigate the effectiveness of the KVM vCPU scheduler and show that it fails to avoid excessive vCPU spinning under various situations. We identify three problems: 1) scheduler mismatch, 2) aggressive limitation of candidate vCPUs, and 3) IPI context misuse. The first problem stems from the mismatch between the KVM vCPU scheduler and the Linux scheduler. The second and third problems come from failures in choosing candidate vCPUs to be scheduled next. Our in-depth analysis reveals simple modification to KVM (89 LoC) can mitigate excessive vCPU spinning. Our simple modification reduces excessive vCPU spinning by up to 96% and improves benchmark performance by up to 2.6×. Part of the proposed mitigation has been integrated with KVM from Linux KVM v5.13 onward."",""1558-2183"","""",""10.1109/TPDS.2023.3297688"",""JST AIP Acceleration Research(grant numbers:JPMJCR22U3)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10190175"",""Hypervisor";virtual CPU spinning;"virtualization"",""Spinning";Virtual machine monitors;Linux;Schedules;Hardware;Boosting;"Limiting"","""","""","""",""43"",""IEEE"",""21 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"RHDOFS: A Distributed Online Algorithm Towards Scalable Streaming Feature Selection,""C. Luo"; S. Wang; T. Li; H. Chen; J. Lv;" Z. Yi"",""College of Computer Science, Sichuan University, Chengdu, Sichuan, China"; College of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, Sichuan, China; School of Computing and Artificial Intelligence, Southwest Jiaotong University, Chengdu, Sichuan, China; College of Computer Science, Sichuan University, Chengdu, Sichuan, China;" College of Computer Science, Sichuan University, Chengdu, Sichuan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1830"",""1847"",""Feature selection is an important topic in data mining and machine learning, which aims to select an optimal feature subset for building effective and explainable prediction models. This article introduces Rough Hypercuboid based Distributed Online Feature Selection (RHDOFS) method to tackle two critical challenges of Volume and Velocity associated with Big Data. By exploring the class separability in the boundary region of rough hypercuboid approach, a novel integrated feature evaluation criterion is proposed by examining not only the explicit patterns contained in the positive region but also the useful implicit patterns derived from the boundary region. An efficient online feature selection method for streaming feature scenario is developed to identify relevant and nonredundant features in an incremental iterative fashion. Furthermore, a parallel optimization mechanism by combining both data and computational independence is further employed to accelerate the original sequential implementation. An efficient distributed online feature selection algorithm is presented and implemented on the Apache Spark platform to scale for massive amount of data by exploiting the computational capabilities of multicore clusters. Encouraging results of extensive experiments indicate the superiority and notable advantages of the proposed algorithm over the relevant and representative online feature selection algorithms. Empirical tests on scalability and extensibility also demonstrate our distributed implementation significantly reduces the computational times requirements while maintaining the prediction accuracy, and is capable of scaling well in volume of data and number of computing nodes."",""1558-2183"","""",""10.1109/TPDS.2023.3265974"",""National Natural Science Foundation of China(grant numbers:62076171,61573292,61976182)"; Key Program of National Natural Science Foundation of China(grant numbers:61836006); Natural Science Foundation of Sichuan Province(grant numbers:2022NSFSC0898);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098647"",""Apache spark";feature selection;online learning;parallel computing;rough hypercuboid;"scalability"",""Feature extraction";Rough sets;Heuristic algorithms;Clustering algorithms;Prediction algorithms;Partitioning algorithms;"Termination of employment"","""",""1"","""",""37"",""IEEE"",""10 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"RLPTO: A Reinforcement Learning-Based Performance-Time Optimized Task and Resource Scheduling Mechanism for Distributed Machine Learning,""X. Lu"; C. Liu; S. Zhu; Y. Mao; P. Lio;" P. Hui"",""National Engineering Center for Mobile Internet Security Technology, Beijing University of Post and Telecommunications, Beijing, China"; National Engineering Center for Mobile Internet Security Technology, Beijing University of Post and Telecommunications, Beijing, China; National Engineering Center for Mobile Internet Security Technology, Beijing University of Post and Telecommunications, Beijing, China; Alibaba Cloud Computing Company Ltd., Beijing, China; Computer Laboratory, University of Cambridge, Cambridge, U.K.;" Computational Media and Arts Thrust, Hong Kong University of Science and Technology, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Oct 2023"",""2023"",""34"",""12"",""3266"",""3279"",""With the wide application of deep learning, the amount of data required to train deep learning models is becoming increasingly larger, resulting in an increased training time and higher requirements for computing resources. To improve the throughput of a distributed learning system, task scheduling and resource scheduling are required. This article proposes to combine ARIMA and GRU models to predict the future task volume. In terms of task scheduling, multi-priority task queues are used to divide tasks into different queues according to their priorities to ensure that high-priority tasks can be completed in advance. In terms of resource scheduling, the reinforcement learning method is adopted to manage limited computing resources. The reward function of reinforcement learning is constructed based on the resources occupied by the task, the training time, the accuracy of the model. When a distributed learning model tends to converge, the computing resources of the task are gradually reduced so that they can be allocated to other learning tasks. The results of experiments demonstrate that RLPTO tends to use more compu-ting nodes when facing tasks with large data scale and has good scalability. The distributed learning system reward experiment shows that RLPTO can make the computing cluster get the largest reward."",""1558-2183"","""",""10.1109/TPDS.2023.3317388"",""National Key R&D Program of China(grant numbers:2020YFB2104700)"; National Natural Science Foundation of China(grant numbers:62136006);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256135"",""Cloud computing";scheduling algorithms;dynamic scheduling;resource management;distributed computing;"reinforcement learning"",""Task analysis";Processor scheduling;Computational modeling;Training;Machine learning;Dynamic scheduling;"Deep learning"","""","""","""",""29"",""CCBYNCND"",""20 Sep 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"RLQ: Workload Allocation With Reinforcement Learning in Distributed Queues,""A. Staffolani"; V. -A. Darvariu; P. Bellavista;" M. Musolesi"",""Department of Computer Science and Engineering, University of Bologna, Bologna, Italy"; Department of Computer Science, University College London, London, U.K.; Department of Computer Science and Engineering, University of Bologna, Bologna, Italy;" Department of Computer Science, University College London, London, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""19 Jan 2023"",""2023"",""34"",""3"",""856"",""868"",""Distributed workload queues are nowadays widely used due to their significant advantages in terms of decoupling, resilience, and scaling. Task allocation to worker nodes in distributed queue systems is typically simplistic (e.g., Least Recently Used) or uses hand-crafted heuristics that require task-specific information (e.g., task resource demands or expected time of execution). When such task information is not available and worker node capabilities are not homogeneous, the existing placement strategies may lead to unnecessarily large execution timings and usage costs. In this work, we formulate the task allocation problem in the Markov Decision Process framework, in which an agent assigns tasks to an available resource, and receives a numerical reward signal upon task completion. Our adaptive and learning-based task allocation solution, Reinforcement Learning based Queues (RLQ), is implemented and integrated with the popular Celery task queuing system for Python. We compare RLQ against traditional solutions using both synthetic and real workload traces. On average, using synthetic workloads, RLQ reduces the execution cost by approximately 70%, the execution time by a factor of at least 3×, and the waiting time by almost 7×. Using real traces, we observe an improvement of about 20% for execution cost, around 70% improvement for execution time, and a reduction of approximately 20× in waiting time. We also compare RLQ with a strategy inspired by E-PVM, a state-of-the-art solution used in Google's Borg cluster manager, showing we are able to outperform it in five out of six scenarios."",""1558-2183"","""",""10.1109/TPDS.2022.3231981"",""Alan Turing Institute"; U.K. EPSRC(grant numbers:EP/N510129/1);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10007049"",""Distributed task queuing";reinforcement learning;"task allocation"",""Task analysis";Resource management;Costs;Reinforcement learning;Hardware;Prediction algorithms;"Decision making"","""",""3"","""",""52"",""IEEE"",""5 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"RLTiering: A Cost-Driven Auto-Tiering System for Two-Tier Cloud Storage Using Deep Reinforcement Learning,""M. Liu"; L. Pan;" S. Liu"",""School of Software, Shandong University, Jinan, China"; School of Software, Shandong University, Jinan, China;" School of Software, Shandong University, Jinan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""501"",""518"",""The cloud storage boom has prompted providers to offer two storage tiers, i.e., hot and cold tiers, which are respectively purpose-built to provide the lowest cost for frequent and infrequent access patterns. However, for cloud users, it is non-trivial to determine cost-effective tiers because it is hard to obtain future access patterns in advance and is difficult to predict them exactly. The lack of future information poses a risk of increasing costs instead of saving costs. This is not the only challenge encountered when it comes to cost optimization. In this article, we take Amazon S3 as an example to analyze the pricing of two-tier cloud storage and derive several major challenges faced by cost optimization. Then, assuming a priori knowledge of future access patterns, we propose an optimal offline algorithm based on dynamic programming to determine cost-effective tiers for each time slot. Further, to handle online workload arrivals, we formulate the problem using Markov decision processes and propose RLTiering based on deep reinforcement learning. Eventually, the cost performance of RLTiering is evaluated based on real-world traces and prevalent Amazon S3 pricing, and the results show that it achieves significant cost-savings."",""1558-2183"","""",""10.1109/TPDS.2022.3224865"",""National Key R&D Program of China(grant numbers:2017YFA0700601)"; Key Technology Research and Development Program of Shandong(grant numbers:2020CXGC010102); Natural Science Foundation of Shandong Province(grant numbers:ZR2020LZH011);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9964112"",""Cloud storage";two-tier storage;cost optimization;reinforcement learning;"online algorithm"",""Costs";Cloud computing;Optimization;Reinforcement learning;Pricing;Throughput;"Media"","""",""1"","""",""36"",""IEEE"",""25 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Robustness Analysis and Enhancement of Deep Reinforcement Learning-Based Schedulers,""S. Zhang"; C. Wang;" A. Y. Zomaya"",""School of Computer Science, The University of Sydney, Camperdown, NSW, Australia"; CSIRO Data61, Eveleigh, NSW, Australia;" School of Computer Science, The University of Sydney, Camperdown, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Nov 2022"",""2023"",""34"",""1"",""346"",""357"",""Dependency-aware jobs, such as the big data analytic workflows, are commonly executed on the cloud. They are compiled to directed acyclic graphs, with tasks linked in regarding the dependency. The cloud scheduler, which maintains a large number of resources, is responsible to execute tasks in parallel. To resolve the complex dependencies, Deep Reinforcement Learning (DRL) based schedulers are widely applied. However, we find that the DRL-based schedulers are vulnerable to the perturbations in the input jobs and may generate falsified decisions to benefit a particular job while delaying the others. By perturbation, we mean a slight adjustment to the job's node features or dependencies, while not changing its functionality. In this paper, we first explore the vulnerability of DRL-based schedulers to job perturbations without accessing the information of the DRL models used in the scheduler. We devise the black-box perturbation system, in which, a proxy model is trained to mimic the DRL-based scheduling policy. We show that the high-faith proxy model can help to craft effective perturbations. The DRL-based schedulers can be as high as 60% likely to be badly affected by the perturbations. Then, we investigate the solution to improve the robustness of DRL-based schedulers to such perturbations. We propose an adversarial training framework to force the neural model to adapt to the perturbation patterns during training so as to eliminate the potential damage during applications. Experiments show that the adversarial-trained scheduler is more robust, reducing the chance of being affected to 3-fold less and the potential bad effects halved."",""1558-2183"","""",""10.1109/TPDS.2022.3218649"",""Australian Research Council(grant numbers:DP200103494)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9937194"",""Scheduling";machine learning;deep reinforcement learning;robustness;job perturbation;black-box;gradient;"vulnerability"",""Perturbation methods";Robustness;Task analysis;Training;Sparks;Scheduling;"Computational modeling"","""",""1"","""",""46"",""IEEE"",""3 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"RTGPU: Real-Time GPU Scheduling of Hard Deadline Parallel Tasks With Fine-Grain Utilization,""A. Zou"; J. Li; C. D. Gill;" X. Zhang"",""University of Michigan-Shanghai Jiao Tong University Joint Institute, Shanghai Jiao Tong University, Shanghai, China"; Department of Computer Science, New Jersey Institute of Technology College of Computing Sciences, Newark, NJ, USA; Department of Computer Science and Engineering, Washington University, Saint Louis, MO, USA;" Department of Electrical and Systems Engineering, Washington University, Saint Louis, MO, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Mar 2023"",""2023"",""34"",""5"",""1450"",""1465"",""Many emerging cyber-physical systems, such as autonomous vehicles and robots, rely heavily on artificial intelligence and machine learning algorithms to perform important system operations. Since these highly parallel applications are computationally intensive, they need to be accelerated by graphics processing units (GPUs) to meet stringent timing constraints. However, despite the wide adoption of GPUs, efficiently scheduling multiple GPU applications while providing rigorous real-time guarantees remains challenging. Each GPU application has multiple CPU execution and memory copy segments, with GPU kernels running on different hardware resources. Because of the complicated interactions between heterogeneous segments of parallel tasks, high schedulability is hard to achieve with conventional approaches. This paper proposes RTGPU, which combines fine-grain GPU partitioning on the system-side with a novel scheduling algorithm on the theory-side. We start by building a model for CPU and memory copy segments. Leveraging persistent threads, we then implement fine-grained GPU partitioning with improved performance through interleaved execution. To reap the benefits of fine-grained GPU partitioning and schedule multiple parallel GPU applications, we propose a novel real-time scheduling algorithm based on federated scheduling and grid search with uniprocessor fixed-priority scheduling. Our approach provides real-time guarantees to meet hard deadlines and achieves over 11% improvement in system throughput and up to 57% schedulability improvement compared with previous work. We validate and evaluate RTGPU on NVIDIA GPU systems. Our system-side techniques can be applied on mainstream GPUs, and the proposed scheduling theory can be used in general heterogeneous computing platforms which have a similar task execution pattern."",""1558-2183"","""",""10.1109/TPDS.2023.3235439"",""NSF(grant numbers:CNS-1739643,CNS-1948457)"; National Natural Science Foundation of China(grant numbers:62202287);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10012550"",""GPGPU";parallel real-time scheduling;persistent thread;interleaved execution;federated scheduling;fixed priority;self-suspension model;"schedulability analysis"",""Graphics processing units";Task analysis;Kernel;Instruction sets;Real-time systems;Scheduling;"Time factors"","""",""3"","""",""47"",""IEEE"",""9 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Scalable Deep Reinforcement Learning-Based Online Routing for Multi-Type Service Requirements,""C. Liu"; P. Wu; M. Xu; Y. Yang;" N. Geng"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Huawei Technology, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Jun 2023"",""2023"",""34"",""8"",""2337"",""2351"",""Emerging applications raise critical QoS requirements for the Internet. The improvements in flow classification technologies, software-defined networks (SDN), and programmable network devices make it possible to fast identify users’ requirements and control the routing for fine-grained traffic flows. Meanwhile, the problem of optimizing the forwarding paths for traffic flows with multiple QoS requirements in an online fashion is not addressed sufficiently. To address the problem, we propose DRL-OR-S, a highly scalable online routing algorithm using multi-agent deep reinforcement learning. DRL-OR-S adopts a comprehensive reward function, an efficient learning algorithm, and a novel deep neural network structure to learn appropriate routing strategies for different types of flow requirements. In order to enhance the generalization and scalability, we propose a novel graph-based actor-critic network architecture and a carefully designed input state for DRL-OR-S. To accelerate the training process and guarantee reliability, we further introduce an NN-simulator for efficient offline training and a safe learning mechanism to avoid unsafe routes during the online routing process. We implement DRL-OR-S under SDN architecture and conduct Mininet-based experiments using real network topologies and traffic traces. The results validate that DRL-OR-S can well satisfy the requirements of latency-sensitive, throughput-sensitive, latency-throughput-sensitive, and latency-loss-sensitive flows at the same time, while exhibiting great adaptiveness and reliability under the scenarios of link failure, traffic change, unseen large topology and partial deployment."",""1558-2183"","""",""10.1109/TPDS.2023.3284651"",""National Key R&D Program of China(grant numbers:2022YFB2901303)"; National Natural Science Foundation of China(grant numbers:62132004,62221003,61832013);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10149369"",""QoS routing";reinforcement learning;"scalability"",""Routing";Quality of service;Network topology;Training;Packet loss;Throughput;"Scalability"","""","""","""",""55"",""IEEE"",""13 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Scheduling Algorithms for Federated Learning With Minimal Energy Consumption,""L. L. Pilla"",""University of Bordeaux, CNRS, Bordeaux INP, Inria, LaBRI, UMR 5800, Talence, France"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Feb 2023"",""2023"",""34"",""4"",""1215"",""1226"",""Federated Learning (FL) has opened the opportunity for collaboratively training machine learning models on heterogeneous mobile or Edge devices while keeping local data private. With an increase in its adoption, a growing concern is related to its economic and environmental cost (as is also the case for other machine learning techniques). Unfortunately, little work has been done to optimize its energy consumption or emissions of carbon dioxide or equivalents, as energy minimization is usually left as a secondary objective. In this paper, we investigate the problem of minimizing the energy consumption of FL training on heterogeneous devices by controlling the workload distribution. We model this as the Minimal Cost FL Schedule problem, a total cost minimization problem with identical, independent, and atomic tasks that have to be assigned to heterogeneous resources with arbitrary cost functions. We propose a pseudo-polynomial optimal solution to the problem based on the previously unexplored Multiple-Choice Minimum-Cost Maximal Knapsack Packing Problem. We also provide four algorithms for scenarios where cost functions are monotonically increasing and follow the same behavior. These solutions are likewise applicable on the minimization of other kinds of costs, and in other one-dimensional data partition problems."",""1558-2183"","""",""10.1109/TPDS.2023.3240833"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10032558"",""Dynamic programming";energy conservation;federated learning;knapsack problems;machine learning;optimization;parallel processing;"scheduling"",""Training";Energy consumption;Costs;Biological system modeling;Computational modeling;Data models;"Task analysis"","""",""2"","""",""45"",""IEEE"",""31 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scheduling Parallel Real-Time Tasks on Virtual Processors,""X. Jiang"; H. Liang; N. Guan; Y. Tang; L. Qiao;" Y. Wang"",""Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, Liaoning, China"; Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, Liaoning, China; City University of Hong Kong, Hong Kong; Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, Liaoning, China; Beijing Institute of Control Engineering, Beijing, China;" Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, Liaoning, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""33"",""47"",""In many popular parallel programming models, e.g., OpenMP (OpenMP, 2013), applications are usually dispatched into several dedicated scheduling entities (named ”threads” in common) for which the processor time of physical platform is provided through the OS schedulers. This behavior requires for a hierarchical scheduling framework, considering each thread as a virtual processor (VP). Moreover, hierarchical scheduling allow separate applications to execute together on a common hardware platform, with each application having the “illusion” of executing on a dedicated component. However, the problem for scheduling parallel real-time tasks on virtual multiprocessor platform has not been addressed yet. An analogous approach to virtual scheduling for parallel real-time tasks is federeted scheudling, where each task exclusively executes on a set of dedicated physical processors. However, federated scheduling suffers significant resource wasting. In this article, we study the scheduling of real-time parallel task on virtual multiprocessors. As a physical processor is shared by virtual processors, tasks effectively share processors with each other. We conduct comprehensive performance evaluation to compare our proposed approach with existing methods of different types. Experiment results show that our approach consistently outperforms existing methods to a considerable extent under a wide range of parameter settings."",""1558-2183"","""",""10.1109/TPDS.2022.3213024"",""National Natural Science Foundation of China(grant numbers:NSFC 62102072)"; Research Grants Council of Hong Kong(grant numbers:GRF 15206221);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9914671"",""Real-time scheduling";vitual processors;parallel tasks;"multiprocessor"",""Task analysis";Program processors;Processor scheduling;Real-time systems;Instruction sets;Computational modeling;"Resource management"","""","""","""",""34"",""IEEE"",""10 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Securing Deployed Smart Contracts and DeFi With Distributed TEE Cluster,""Z. Li"; B. Xiao; S. Guo;" Y. Yang"",""Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong"; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; College of Computer Science, Chongqing University, Chongqing, China;" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Jan 2023"",""2023"",""34"",""3"",""828"",""842"",""Smart contract technologies can be used to implement almost arbitrary business logic. They can revolutionize many businesses such as payments, insurance, and crowdfunding. The resulting birth of decentralized finance (DeFi) has gained significant momentum. Smart contracts and DeFi are now attractive targets for attacks. An important research question is how to protect deployed smart contracts and DeFi. Smart contracts cannot be modified once deployed, namely vulnerabilities cannot be fixed by patching. In this case, vulnerabilities in deployed contracts and DeFi might cause devastating consequences. In this paper, we put forward SolSaviour, a framework for protecting deployed smart contracts and DeFi. The core of SolSaviour is to build a smart contract protection mechanism based on democratic voting using a distributed trusted execution environment (TEE) cluster. Once a vulnerability in deployed contracts or DeFi is found, SolSaviour can destroy the defective contract and redeploy a patched contract via the distributed TEE cluster. Moreover, SolSaviour can migrate funds and state variables from the destroyed contract to the patched one. Compared with previous work, our approach can protect smart contracts and DeFi in a distributed manner, avoiding reliance on privileged users or trusted third parties. Our experiment results show that SolSaviour can protect smart contracts and complex DeFi protocols with feasible overhead."",""1558-2183"","""",""10.1109/TPDS.2022.3232548"",""Key-Area Research Development Program of Guangdong Provice(grant numbers:2020B0101090003)"; HK RGC GRF(grant numbers:15209822,15217321);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9999528"",""Blockchain";DeFi security;decentralized finance (DeFi);smart contract;"trusted execution environment (TEE)"",""Smart contracts";Blockchains;Stakeholders;Computer bugs;Finance;Electronic mail;"Decentralized applications"","""",""5"","""",""49"",""IEEE"",""27 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Securing Distributed SGD Against Gradient Leakage Threats,""W. Wei"; L. Liu; J. Zhou; K. -H. Chow;" Y. Wu"",""Computer and Information Science Department, Fordham University, New York City, NY, USA"; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science and Technology, Soochow University, Suzhou, Jiangsu, China; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA;" School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""23 May 2023"",""2023"",""34"",""7"",""2040"",""2054"",""This paper presents a holistic approach to gradient leakage resilient distributed Stochastic Gradient Descent (SGD). First, we analyze two types of strategies for privacy-enhanced federated learning: (i) gradient pruning with random selection or low-rank filtering and (ii) gradient perturbation with additive random noise or differential privacy noise. We analyze the inherent limitations of these approaches and their underlying impact on privacy guarantee, model accuracy, and attack resilience. Next, we present a gradient leakage resilient approach to securing distributed SGD in federated learning, with differential privacy controlled noise as the tool. Unlike conventional methods with the per-client federated noise injection and fixed noise parameter strategy, our approach keeps track of the trend of per-example gradient updates. It makes adaptive noise injection closely aligned throughout the federated model training. Finally, we provide an empirical privacy analysis on the privacy guarantee, model utility, and attack resilience of the proposed approach. Extensive evaluation using five benchmark datasets demonstrates that our gradient leakage resilient approach can outperform the state-of-the-art methods with competitive accuracy performance, strong differential privacy guarantee, and high resilience against gradient leakage attacks."",""1558-2183"","""",""10.1109/TPDS.2023.3273490"",""National Science Foundation(grant numbers:NSF 2038029,NSF 1564097)"; CISCO; IBM;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10119168"",""Federated learning";distributed system;gradient leakage attack;"privacy analysis"",""Privacy";Federated learning;Differential privacy;Training;Sensitivity;Resilience;"Training data"","""",""1"","""",""47"",""IEEE"",""5 May 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Securing Fine-Grained Data Sharing and Erasure in Outsourced Storage Systems,""J. Wei"; X. Chen; J. Wang; X. Huang;" W. Susilo"",""State Key Laboratory of Integrated Service Networks (ISN), Xidian University, Xi’an, China"; State Key Laboratory of Integrated Service Networks (ISN), Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks (ISN), Xidian University, Xi’an, China; Fujian Provincial Key Laboratory of Network Security and Cryptology, College of Computer and Cyber Security, Fujian Normal University, Fuzhou, China;" School of Computing and IT, University of Wollongong, Wollongong, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""552"",""566"",""The wide use of internet-connected services makes massive personal data collected by service providers without the need of our consent. Although the archived data may enable them to provide better service experiences for users, it also presents serious risks to individual privacy, especially when active or unexpected data breaches have become commonplace. To mitigate this issue, several acts and regulations (e.g., the European Union general data protection regulation) have been issued and specified a lot of security requirements for personal data management. Among these various requirements, we mainly focus on the requirement of giving back the access control of personal data to data owners themselves and the right to be forgotten for data erasure. In this article, we provide a cryptographic solution of achieving these two requirements in the setting of outsourced storage. Specifically, we introduce a personal data management framework built upon a novel cryptographic primitive dubbed as forward-secure attribute-based puncturable encryption (FS-DABPE). This primitive simultaneously features of system-wide forward secrecy and practical key management as well as fine-grained access control of the encrypted personal data. Consequently, by locally puncturing, updating and erasing system-wide secret keys, it securely realizes fine-grained personal data sharing and data erasure without interactions. Furthermore, to instantiate the proposed framework, we present a concrete FS-DABPE construction, and prove its security under a well-studied complexity assumption. In addition, we provide a prototype implementation of the concrete construction, and present extensive experimental results that illustrate its feasibility and practicability."",""1558-2183"","""",""10.1109/TPDS.2022.3225274"",""National Natural Science Foundation of China(grant numbers:62172434,61960206014,62072357)"; Key Research and Development Projects of Shaanxi Province(grant numbers:2020ZDLGY08-03); China Postdoctoral Science Foundation(grant numbers:2020M673348,2021T140531);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9965746"",""Fine-grained access control";data erasure;forward secrecy;"puncturable encryption"",""Cryptography";Encryption;Security;Access control;General Data Protection Regulation;Regulation;"Intserv networks"","""",""3"","""",""41"",""IEEE"",""29 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Sentinels and Twins: Effective Integrity Assessment for Distributed Computation,""S. De Capitani di Vimercati"; S. Foresti; S. Jajodia; S. Paraboschi; P. Samarati;" R. Sassi"",""Università degli Studi di Milano, Milano, Italy"; Università degli Studi di Milano, Milano, Italy; George Mason University, Fairfax, VA, USA; Università degli Studi di Bergamo, Dalmine, Italy; Università degli Studi di Milano, Milano, Italy;" Università degli Studi di Milano, Milano, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""108"",""122"",""Distributed computing supports large scale and data-intensive computations with the cooperation of a multitude of parties, each responsible for a portion of the workload. Such parties are often not fully reliable and may return incorrect results. In this article, we address the problem of assessing the integrity of the computation results. We provide a comprehensive characterization of two techniques, sentinels and twins, evaluating their effectiveness and synergy. Sentinels are pre-computed tasks whose result is known apriori, and enable checking returned results against a ground truth. Twins are replicated tasks assigned to different workers, and enable cross-checking returned results for a same task. The analysis considers many questions that arise in the design of a concrete integrity assessment strategy and identifies the parameters that have a critical impact on the overall protection. Our model enables to tune the integrity controls so to achieve best effectiveness. The model can be applied to a variety of scenarios and offers guidelines that can find extensive application."",""1558-2183"","""",""10.1109/TPDS.2022.3215863"",""Office of Naval Research(grant numbers:N00014-20-1-2407)"; Army Research Office(grant numbers:W911NF-13-1-0421); National Science Foundation(grant numbers:CNS-1822094); EC(grant numbers:101017171 (MARSAL),101070141 (GLACIATION)); Italian MIUR; PRIN;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925639"",""Distributed data computation";probabilistic integrity guarantees;sentinels;"twins"",""Task analysis";Computational modeling;Resource management;Probabilistic logic;Behavioral sciences;Predictive models;"Machine learning"","""",""1"","""",""27"",""CCBY"",""20 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Serpens: A High Performance FaaS Platform for Network Functions,""H. Yu"; H. Zhang; J. Shen; Y. Geng; J. Wang; C. Miao;" M. Xu"",""Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China"; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Tencent, Beijing, China;" Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Jul 2023"",""2023"",""34"",""8"",""2448"",""2463"",""More and more enterprises deploy applications on Function-as-a-Service (FaaS) platforms to improve resource efficiency and save monetary costs. Network Functions (NFs) suffer from staggered peaks of traffic patterns and could benefit from fine-grained resource multiplexing in FaaS platform. However, naively exploring existing FaaS platforms to support NFs can introduce significant performance overheads in three aspects, including slow instance startup, remote state access for NFs, and costly packet delivery between NFs. To address these problems, we propose ${\sf Serpens}$Serpens, a high performance FaaS platform for NFs. First, ${\sf Serpens}$Serpens proposes a reusable NF runtime design to slash instance startup overhead. Second, ${\sf Serpens}$Serpens designs a novel state management mechanism to support local state access. Third, ${\sf Serpens}$Serpens introduces an advanced service chaining approach to avoid extra packet delivery. Besides, ${\sf Serpens}$Serpens designs an NF scaling mechanism to minimize performance fluctuation. We have implemented a prototype of ${\sf Serpens}$Serpens and conducted comprehensive experiments. Compared with the NFs and Service Function Chains (SFCs) that run on existing FaaS platforms, ${\sf Serpens}$Serpens can improve the throughput by more than 10× and reduce the latency by more than 90%."",""1558-2183"","""",""10.1109/TPDS.2023.3263272"",""National Key Research and Development Program of China(grant numbers:2020YFE0200500)"; National Natural Science Foundation of China(grant numbers:62002009);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10089180"",""FaaS Platform";high performance;"network function"",""Noise measurement";Cloud computing;Containers;Multiplexing;Runtime;Hardware;"Traffic control"","""","""","""",""84"",""IEEE"",""30 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"SFC Orchestration Method for Edge Cloud and Central Cloud Collaboration: QoS and Energy Consumption Joint Optimization Combined With Reputation Assessment,""L. Rui"; S. Chen; S. Wang; Z. Gao; X. Qiu; W. Li;" S. Guo"",""State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China"; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China;" State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Aug 2023"",""2023"",""34"",""10"",""2735"",""2748"",""Network function virtualization (NFV) is an emerging technology that uses virtualization technology to provide various services in enterprise networks and reduce costs. However, in cloud edge networks, effective virtual network function (VNF) configuration is particularly difficult, and the system design needs to consider the reliability and energy-saving while meeting the requirements of Quality of Service (QoS). This paper uses the binary integer programming (BIP) model to study the service function chain (SFC) orchestration problem, and designs a federated deep reinforcement learning SFC orchestration algorithm (FDOA). With this method, energy consumption can be reduced and the QoS of users can be improved. In addition, considering the limitations of local deep reinforcement learning (DRL) model training, this paper proposes a federated DRL algorithm to help obtain a more robust model, and simultaneously improve the convergence speed of the model. Among them, we introduce reputation theory during model training to evaluate the reliability of the nodes carrying the DRL model, avoiding the influence of unreliable models on the training effect. Finally, the simulation results show that FDOA has better performance in training time and end-to-end delay compared with other existing algorithms."",""1558-2183"","""",""10.1109/TPDS.2023.3301670"",""National Natural Science Foundation of China(grant numbers:62271072)"; Beijing Natural Science Foundation(grant numbers:4232009);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10207698"",""Federated learning";reinforcement learning;reputation theory;"SFC orchestration"",""Training";Cloud computing;Reliability;Delays;Joining processes;Energy consumption;"Quality of service"","""","""","""",""37"",""IEEE"",""3 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Shuffle Differential Private Data Aggregation for Random Population,""S. Wang"; X. Luo; Y. Qian; Y. Zhu; K. Chen; Q. Chen; B. Xin;" W. Yang"",""Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangzhou, China"; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangzhou, China; Interactive Entertainment Group, Tencent Inc., Shenzhen, Guangdong Province, China; School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu Province, China; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangzhou, China; Institute of Artificial Intelligence and Blockchain, Guangzhou University, Guangzhou, China; Department of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China;" Department of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Apr 2023"",""2023"",""34"",""5"",""1667"",""1681"",""Bridging the advantages of differential privacy in both centralized model (i.e., high accuracy) and local model (i.e., minimum trust), the shuffle privacy model has potential applications in many privacy-sensitive scenarios, such as mobile user data aggregation and federated learning. Since messages from users are anonymized by semi-trusted shufflers (e.g., anonymous channels, edge servers), every user could hide message among other users’ messages and inject only part of noises (a.k.a. privacy amplification). However, existing works assume that the participating user population is known in advance, which is unrealistic for dynamic environments (e.g., mobile computing, vehicular networks). In this work, we study the shuffle privacy model with a random participating population, and give privacy amplification bounds for population size with commonly encountered binomial, Poisson, sub-Gaussian distribution and etc. For further improving accuracy, we formulate and derive optimal dummy sizes for both non-adaptive and adaptive dummies. Finally, to break the error barrier due to the constraint of sending one single message per user, we design a multi-message shuffle private protocol supporting random population. Experiment results show that our approaches reduce more than 60% error when compared to the local model and naive approaches. We hope this work provides tailored solutions of shuffle privacy for dynamic mobile/distributed computing."",""1558-2183"","""",""10.1109/TPDS.2023.3247541"",""National Natural Science Foundation of China(grant numbers:62102108)"; National Key Research and Development(grant numbers:2022YFB3102400); Natural Science Foundation of Guangdong Province of China(grant numbers:2022A1515010061); Guangzhou Basic and Applied Basic Research Foundation(grant numbers:202201010194,622191-098); National Natural Science Foundation of China(grant numbers:61802383); Research Project of Pazhou Lab for Excellent Young Scholars(grant numbers:PZL2021KF0024); Guangzhou Basic and Applied Basic Research Foundation(grant numbers:202201010330,202201020162); National Natural Science Foundation of China for Joint Fund Project(grant numbers:U1936218); National Natural Science Foundation of China(grant numbers:62102107); National Natural Science Foundation of China(grant numbers:62072132); National Natural Science Foundation of China(grant numbers:62002074);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10049722"",""Data aggregation";data privacy;differential privacy;shuffle privacy;"statistical estimation"",""Sociology";Privacy;Differential privacy;Data models;Servers;Protocols;"Data aggregation"","""","""","""",""52"",""IEEE"",""22 Feb 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Simeuro: A Hybrid CPU-GPU Parallel Simulator for Neuromorphic Computing Chips,""H. Zhang"; N. -M. Ho; D. Y. Polat; P. Chen; M. Wahib; T. T. Nguyen; J. Meng; R. S. M. Goh; S. Matsuoka; T. Luo;" W. -F. Wong"",""Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore"; School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore; National Institute of Advanced Industrial Science and Technology, Japan, RIKEN Center for Computational Science, Tokyo, Japan; RIKEN Center for Computational Science, Kobe, Japan; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore; RIKEN Center for Computational Science, Kobe, Japan; Institute of High Performance Computing (IHPC), Agency for Science, Technology and Research (A*STAR), Singapore;" School of Computing, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2023"",""2023"",""34"",""10"",""2767"",""2782"",""With the success of deep learning, there have been numerous efforts to build hardware for it. One approach that is gaining momentum is neuromorphic computing with spiking neural networks (SNNs), which are multiplication-free and open the possibility of using analog computing via novel technologies. However, to design effective and efficient hardware for such architectures, a fast and accurate software simulator is key. This article presents Simeuro, a fast and scalable system-level simulator for SNN models used in neuromorphic accelerators. The simulator uses spike-level details and configurable architectural constraints that are independent of the underlying hardware implementation. Simeuro supports a wide range of features including analog computing, novel memory (currently, RRAM is supported), and a full network-on-chip. The simulator can provide detailed simulation results such as routing statistics, energy consumption, delay, and accuracy of arbitrarily defined SNN architectures. Our simulator leverages a CPU-GPU hybrid environment to expedite the simulation by scaling out to multi-nodes equipped with multi-GPUs. We are able to conduct core simulations for a system-scale SNN chip of 20,000 neuromorphic cores on up to 512 A100 GPUs in a few minutes."",""1558-2183"","""",""10.1109/TPDS.2023.3291795"",""Singapore Government's Research, Innovation and Enterprise 2020 Plan(grant numbers:A1687b0033,A1892b0026)"; New Energy and Industrial Technology Development Organization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10172030"",""Neuromorphic computing";chip simulation;"deep learning"",""Neurons";Hardware;Computational modeling;Task analysis;Biological neural networks;Routing;"Behavioral sciences"","""","""","""",""52"",""IEEE"",""3 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"SketchINT: Empowering INT With TowerSketch for Per-Flow Per-Switch Measurement,""K. Yang"; S. Long; Q. Shi; Y. Li; Z. Liu; Y. Wu; T. Yang;" Z. Jia"",""School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University, Beijing, China"; School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University, Beijing, China; School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University, Beijing, China; School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University, Beijing, China; School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University, Beijing, China; School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University, Beijing, China; School of Computer Science, National Key Laboratory for Multimedia Information Processing, Peking University, Beijing, China;" Huawei Cloud, Huawei Technologies, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Sep 2023"",""2023"",""34"",""11"",""2876"",""2894"",""Network measurement is indispensable to network operations. INT solutions that can provide fine-grained per-switch per-packet information serve as promising solutions for per-flow per-switch measurement. The main shortcoming of INT is its high network overhead incurred by collecting INT information, making INT impractical for production deployment. Sketches that can compactly record per-flow information with small memory footprint, are a promising choice for compressing INT information to reduce INT overhead. An ideal sketch for efficiently compressing INT information in practice should achieve both simplicity and accuracy, but no existing sketch achieves both. Motivated by this, we first design SketchINT to combine INT and sketches, aiming to obtain all per-flow per-switch information with low network overhead. Second, we design a new sketch for SketchINT, namely TowerSketch, which achieves both simplicity and accuracy. The key idea of TowerSketch is to use different-sized counters for different arrays under the property that the number of bits used for different arrays stays the same. TowerSketch can automatically record larger flows in larger counters and smaller flows in smaller counters. To further ease the configuration and give network operators more confidence on performance of TowerSketch, we propose a method for precise error bound estimation. We have fully implemented our SketchINT prototype on a testbed consisting of 10 switches. We also implement our TowerSketch on P4, single-core CPU, multi-core CPU, and FPGA platforms to verify its deployment flexibility. Extensive experimental results verify that 1) TowerSketch achieves better accuracy than prior art on various tasks, outperforming the state-of-the-art ElasticSketch up to 27.7 times in terms of error"; 2) Compared to INT, SketchINT reduces the number of packets belonging to the control plane overhead by $3 \sim 4$3∼4 orders of magnitude with an error smaller than 5%;" 3) The estimated error bound of TowerSketch can almost match the actual error bound."",""1558-2183"","""",""10.1109/TPDS.2023.3303924"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101390001)"; National Natural Science Foundation of China(grant numbers:U20A20179);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10232880"",""In-band Network Telemetry (INT)";network measurement;"sketch"",""Prototypes";Estimation;Telemetry;Task analysis;Field programmable gate arrays;Hardware;"Current measurement"","""","""","""",""89"",""IEEE"",""28 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Sparse Stream Semantic Registers: A Lightweight ISA Extension Accelerating General Sparse Linear Algebra,""P. Scheffler"; F. Zaruba; F. Schuiki; T. Hoefler;" L. Benini"",""Integrated Systems Laboratory (IIS), ETH Zurich, Zürich, Switzerland"; Zurich branch of Axelera AI, Zürich, Switzerland; SiFive, San Mateo, CA, USA; Scalable Parallel Computing Laboratory (SPCL), ETH Zurich, Zürich, Switzerland;" Integrated Systems Laboratory (IIS), ETH Zurich, Zürich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""25 Oct 2023"",""2023"",""34"",""12"",""3147"",""3161"",""Sparse linear algebra is crucial in many application domains, but challenging to handle efficiently in both software and hardware, with one- and two-sided operand sparsity handled with distinct approaches. In this work, we enhance an existing memory-streaming RISC-V ISA extension to accelerate both one- and two-sided operand sparsity on widespread sparse tensor formats like compressed sparse row (CSR) and compressed sparse fiber (CSF) by accelerating the underlying operations of streaming indirection, intersection, and union. Our extensions enable single-core speedups over an optimized RISC-V baseline of up to 7.0x, 7.7x, and 9.8x on sparse-dense multiply, sparse-sparse multiply, and sparse-sparse addition, respectively, and peak FPU utilizations of up to 80% on sparse-dense problems. On an eight-core cluster, sparse-dense and sparse-sparse matrix-vector multiply using real-world matrices are up to 4.9x and 5.9x faster and up to 2.9x and 3.0x more energy efficient. We explore further applications for our extensions, such as stencil codes and graph pattern matching. Compared to recent CPU, GPU, and accelerator approaches, our extensions enable higher flexibility on data representation, degree of sparsity, and dataflow at a minimal hardware footprint, adding only 1.8% in area to a compute cluster. A cluster with our extensions running CSR matrix-vector multiplication achieves 9.9x and 1.7x higher peak floating-point utilizations than recent highly optimized sparse data structures and libraries for CPU and GPU, respectively, even when accounting for off-chip main memory (HBM) and on-chip interconnect latency and bandwidth effects."",""1558-2183"","""",""10.1109/TPDS.2023.3322029"",""ETH Future Computing Laboratory"; Huawei Technologies; European High-Performance Computing Joint Undertaking(grant numbers:101034126); The European Pilot(grant numbers:101036168);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10271722"",""Computer architecture";hardware acceleration;linear algebra;sparse computation;"sparse tensors"",""Indexes";Hardware;Generators;Sparse matrices;Tensors;Registers;"Micromechanical devices"","""","""","""",""51"",""IEEE"",""4 Oct 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Sparse Symmetric Format for Tucker Decomposition,""S. Shivakumar"; J. Li; R. Kannan;" S. Aluru"",""Georgia Institute of Technology, Atlanta, GA, USA"; North Carolina State University, Raleigh, NC, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA;" Georgia Institute of Technology, Atlanta, GA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 May 2023"",""2023"",""34"",""6"",""1743"",""1756"",""Tensor-based methods are receiving renewed attention in recent years due to their prevalence in diverse real-world applications. There is considerable literature on tensor representations and algorithms for tensor decompositions, both for dense and sparse tensors. Many applications in hypergraph analytics, machine learning, psychometry, and signal processing result in tensors that are both sparse and symmetric, making them an important class for further study. Similar to the critical Tensor Times Matrix chain operation (TTMc) in general sparse tensors, the Sparse Symmetric Tensor Times Same Matrix chain (S$^{3}$3 TTMc) operation is compute and memory intensive due to high tensor order and the associated factorial explosion in the number of non-zeros. We present the novel Compressed Sparse Symmetric (CSS) format for sparse symmetric tensors, along with an efficient parallel algorithm for the S$^{3}$3 TTMc operation. We theoretically establish that S$^{3}$3 TTMc on CSS achieves a better memory versus run-time trade-off compared to state-of-the-art implementations, and visualize the variation of the performance gap over the parameter space. We demonstrate experimental findings that confirm these results and achieve up to $2.72 \times$2.72× speedup on synthetic and real datasets. The scaling of the algorithm on different test architectures is also showcased to highlight the effect of machine characteristics on algorithm performance."",""1558-2183"","""",""10.1109/TPDS.2023.3263124"",""NVIDIA Corporation"; U.S. Department of Energy; Pacific Northwest National Laboratory(grant numbers:532181);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10086668"",""Compressed storage";sparse tensors;symmetric tensors;"tensor times matrix chain"",""Tensors";Symmetric matrices;Sparse matrices;Indexes;Signal processing algorithms;Matrix decomposition;"Parallel algorithms"","""","""","""",""46"",""IEEE"",""29 Mar 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Spiking Neural P Systems With Communication on Request and Mute Rules,""T. Wu";" L. Pan"",""Provincial Key Laboratory for Computer Information Processing Technology, School of Computer Science and Technology, Soochow University, Suzhou, China";" Key Laboratory of Image Information Processing and Intelligent Control, Institute of Artificial Intelligence, School of Artificial Intelligence and Automation, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Jan 2023"",""2023"",""34"",""2"",""734"",""745"",""Spiking neural P systems with communication on request (SNQP systems) are neurally inspired computing devices, where a neuron actively seeks spikes from presynaptic neurons instead of passively waiting for spikes. In this work, we consider SNQP systems with mute rules (SNQPM systems), where mute rules have no communication functioning, namely the application of a mute rule only affects the number of spikes in the neuron where the rule resides, without effect on other neurons. It is demonstrated the computation capability of SNQPM systems with only mute rules does not exceed that of register machines with two registers, thereby not Turing universal. SNQPM systems are Turing universal when both mute rules and request rules are employed. Furthermore, two universal SNQPM systems with 7 neurons or 13 neurons are constructed as devices of number generating and function computing, respectively. Comparing to the universal SNQP system with 14 neurons and two types of spikes, SNQPM systems show the capability of trading-off mute rules and the types of spikes."",""1558-2183"","""",""10.1109/TPDS.2022.3228931"",""National Natural Science Foundation of China(grant numbers:62002251,62172171)"; Natural Science Foundation of Jiangsu Province(grant numbers:BK20200856); Priority Academic Program Development of Jiangsu Higher Education Institutions;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9983514"",""Bio-inspired computing";distributed architectures;neural computation;spiking neural P system;"computation capability"",""Neurons";Registers;Computational modeling;Synapses;Biological neural networks;Turing machines;"Recurrent neural networks"","""","""","""",""52"",""IEEE"",""13 Dec 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Static Algorithm Allocation With Duplication in Robotic Network Cloud Systems,""S. Alirezazadeh";" L. A. Alexandre"",""C4-Cloud Computing Competence Center, Universidade da Beira Interior, Covilhã, Portugal";" NOVA LINCS, Universidade da Beira Interior, Covilhã, Portugal"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1897"",""1908"",""Robotic networks can be used to accomplish tasks that exceed the capacity of a single robot. In a robotic network, robots can work together to accomplish a common task. Cloud robotics allows robots to benefit from the massive storage and computing power of the cloud. Previous studies mainly focus on minimizing the cost of resource retrieval by robots by knowing the resource allocation in advance. Duplicating algorithms on multiple nodes can reduce the total time required to execute a task. We address the question of which algorithms should be duplicated and where the duplicates should be placed to improve overall performance. We have developed a procedure to answer wherein a robotic network cloud system should algorithms be executed and whether they should be duplicated to achieve optimal performance in terms of overall task execution time for all robots. Our proposed duplication procedure is optimal in the sense that the number of duplicated algorithms is minimal, while the result provides minimal overall completion time for all robots."",""1558-2183"","""",""10.1109/TPDS.2023.3267293"",""European Regional Development Fund"; Programa Operacional Regional do Centro; Sistema de Apoio à Investigação Cientifíca e Tecnológica - Programas Integrados de IC&DT; NOVA LINCS(grant numbers:UIDB/04516/2020);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10102553"",""Human-robot collaboration";job completion time;monitoring;quality metric;"task scheduling"",""Task analysis";Robots;Resource management;Heuristic algorithms;Cloud computing;Dynamic scheduling;"Service robots"","""","""","""",""37"",""IEEE"",""14 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"STR: Hybrid Tensor Re-Generation to Break Memory Wall for DNN Training,""Z. Zong"; L. Lin; L. Lin; L. Wen;" Y. Sun"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; School of Computer Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Management, Capital Normal University, Beijing, China; School of Software, Tsinghua University, Beijing, China;" College of Computer Science, Nankai University, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Jul 2023"",""2023"",""34"",""8"",""2403"",""2418"",""With the growth of the depth of neural networks and the scale of data, the difficulty of network training also increases. When the GPU memory is insufficient, it is challenging to train deeper models. Recent research uses tensor swapping and recomputation techniques in a combined manner to optimize memory usage. However, complex dependencies and enormous scales of the DNN graph limit the improvement of single GPU memory optimization. Improper swap and recomputation decisions even bring negative effects on training performance. In this article, we propose a novel hybrid tensor re-generation strategy, called STR, which combines swap and recomputation techniques to find the optimal execution plan for the DNN training when the memory is limited. We formalize our memory optimization problem with constraints that describe the dependency of the operator calculation and the bandwidth usage of the swap. A host checkpoint mechanism is designed to make full use of the swapped tensors, which reduces the cost of the recomputation. We also present a recursive source tracing algorithm to improve the optimization efficiency by constraint relaxation with a performance bound. To optimize large models, we further introduce an approximation method based on a weighted graph coarsening. We implement a prototype of STR as a plugin on TensorFlow and evaluated based on 5 popular DNN models. The experimental result shows that the approximate solution of STR improves the training throughput of ResNet series of models by up to 28.1% compared to the state-of-the-art hybrid optimization strategy."",""1558-2183"","""",""10.1109/TPDS.2023.3266110"",""National Key Research and Development Program of China(grant numbers:2019YFB1704003)"; National Natural Science Foundation of China(grant numbers:62021002); R&D Program of Beijing Municipal Education Commission(grant numbers:KM202310028003); Natural Science Foundation of Tianjin City(grant numbers:22JCQNJC01520); Tsinghua BNRist and Beijing Key Laboratory of Industrial Big Data System and Application;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10098636"",""DNN training";offload memory;recomputation;rematerialization;"swap"",""Tensors";Graphics processing units;Training;Optimization;Costs;Bandwidth;"Memory management"","""","""","""",""58"",""IEEE"",""10 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Swing: Providing Long-Range Lossless RDMA via PFC-Relay,""Y. Chen"; C. Tian; J. Dong; S. Feng; X. Zhang; C. Liu; P. Yu; N. Xia; W. Dou;" G. Chen"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China; Network Information Center, Xiangya Hospital, Central South University, Changsha, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""63"",""75"",""Remote Direct Memory Access (RDMA) has been widely deployed in datacenters for its high performance. Large-scale high performance cloud services built on geographically distributed datacenters require long-range RDMA for performance requirements. However, existing RDMA solutions can hardly satisfy the stringent requirements of the emerging large-scale high-performance cloud services built on geo-distributed datacenters in terms of throughput and delay. On the one hand, lossless RDMA suffers from a deep buffer and potential suboptimal throughput for inter-datacenter traffic due to delayed response to Priority Flow Control (PFC) messages. On the other hand, lossy RDMA with selective retransmissions suffers from poor performance when multiple flows with different round-trip times (RTTs) coexist in cross-datacenter scenarios. This article proposes Swing, which expands the high-performance lossless RDMA to long-distance links through PFC-Relay. Swing ensures the throughput of long-distance links while minimizing the buffer requirement for long-range RDMA. It enables long-range RDMA without making any modifications to existing in-datacenter networks. The evaluation shows that Swing can reduce the average flow completion time (FCT) by 14%-66% in a variety of traffic scenarios."",""1558-2183"","""",""10.1109/TPDS.2022.3215517"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101390001)"; National Natural Science Foundation of China(grant numbers:92067206,62072228,61972222); Fundamental Research Funds for the Central Universities; Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9925096"",""Inter datacenter communication";Datacenter networks;Flow control;PFC;"RDMA"",""Throughput";Optical switches;Relays;Packet loss;Low latency communication;Delays;"Receivers"","""","""","""",""52"",""IEEE"",""19 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"swMPAS-A: Scaling MPAS-A to 39 Million Heterogeneous Cores on the New Generation Sunway Supercomputer,""X. Hao"; T. Fang; J. Chen; J. Gu; J. Feng; H. An;" C. Zhao"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Earth and Space Sciences, University of Science and Technology of China, Hefei, Anhui, China; School of Earth and Space Sciences, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China;" School of Earth and Space Sciences, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""141"",""153"",""With the computing power of High-Performance Computing (HPC) systems having stepped into the exascale era, more complex problems can be solved with scientific applications on a large scale. However, due to the significant performance gap between computing nodes and storage subsystems, suboptimal design for the Input/Output (I/O) module will significantly impede the efficiency of scientific applications, especially for the ubiquitous atmosphere applications. Two-phase I/O implemented in N-to-1 mode creates a serious bottleneck that hinders the scalability for the Model for Prediction Across Scales-Atmosphere (MPAS-A) on the new generation Sunway supercomputer. To address the I/O problem, we apply a custom data reorganization method to enable N-to-M I/O mode to exploit the parallel file system's performance and limit the data transfer among MPI ranks to a restricted scope to alleviate communication overhead. Moreover, we have conducted several methods to accelerate the computations, including the redesign for tracer transport, a hybrid buffering scheme, and a three-level parallelization scheme, which allows MPAS-A to use all heterogeneous computing resources efficiently. Experimental results show admirable scalability and efficiency of our I/O method, which achieves speedups of 41× and 58.9× for input and output compared with the raw I/O method on 30,000 MPI ranks. By scaling MPAS-A to 39 million heterogeneous cores, we demonstrate the necessity of a well-constructed I/O module for a real-world atmosphere application. Speed tests show that our optimization methods obtain good results for computations, and MPAS-A achieves a speed of 0.82 Simulated Day per Hour (SDPH) and 0.76 parallel efficiency of strong scaling with 600,000 MPI ranks."",""1558-2183"","""",""10.1109/TPDS.2022.3215002"",""National Natural Science Foundation of China(grant numbers:62102389)"; National Key Research and Development Program of China(grant numbers:2017YFB0202002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9920218"",""MPAS-A";atmosphere science;heterogeneous core;sunway supercomputer;"I/O"",""Atmospheric modeling";Computational modeling;Supercomputers;Predictive models;Task analysis;Computer architecture;"Scalability"","""","""","""",""45"",""IEEE"",""17 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Synapse Compression for Event-Based Convolutional-Neural-Network Accelerators,""L. Bamberg"; A. Pourtaherian; L. Waeijen; A. Chahar;" O. Moreira"",""GrAI Matter Labs, Paris, France"; GrAI Matter Labs, Paris, France; GrAI Matter Labs, Paris, France; GrAI Matter Labs, Paris, France;" GrAI Matter Labs, Paris, France"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Feb 2023"",""2023"",""34"",""4"",""1227"",""1240"",""Manufacturing-viable neuromorphic chips require novel compute architectures to achieve the massively parallel and efficient information processing the brain supports so effortlessly. The most promising architectures for that are spiking/event-based, which enables massive parallelism at low complexity. However, the large memory requirements for synaptic connectivity are a showstopper for the execution of modern convolutional neural networks (CNNs) on massively parallel, event-based architectures. The present work overcomes this roadblock by contributing a lightweight hardware scheme to compress the synaptic memory requirements by several thousand times—enabling the execution of complex CNNs on a single chip of small form factor. A silicon implementation in a 12-nm technology shows that the technique achieves a total memory-footprint reduction of up to 374× compared to the best previously published technique at a negligible area overhead."",""1558-2183"","""",""10.1109/TPDS.2023.3239517"",""EU Horizon 2020 project, ANDANTE"; ECSEL Joint Undertaking(grant numbers:876925); European Union's Horizon 2020 research and innovation program and France, Belgium, Germany, Netherlands, Portugal, Spain, Switzerland;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025357"",""CNN";compression;dataflow;event-based;hardware accelerator;near-memory compute;neuromorphic;sparsity;"spiking"",""Neurons";Computer architecture;Convolutional neural networks;Memory management;Frequency modulation;Random access memory;"Convolution"","""","""","""",""25"",""IEEE"",""24 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Tag-Sharer-Fusion Directory: A Scalable Coherence Directory With Flexible Entry Formats,""Y. Qiu"; J. Jiao; X. Zeng;" Y. Fan"",""State Key Laboratory of ASIC and System, Fudan University, Shanghai, China"; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China;" State Key Laboratory of ASIC and System, Fudan University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""262"",""274"",""In large-scale chip multiprocessors (CMPs), the scalability of a coherence directory becomes more important as the number of cores increases. However, previously proposed scalable coherence directories typically reduce the directory storage overhead at the cost of one or more aspects of performance, accuracy, and complexity. In this article, we propose the tag-sharer-fusion (TSF) directory, a scalable coherence directory with low hardware complexity, as well as with high performance and accuracy. Each directory entry has just enough bits to store a single sharer pointer and is divided into two primary formats: tag and sharer, where sharer entries store sharers but not tags. Each private block is tracked by a tag entry, and each shared block is tracked by a combination of a tag entry and a sharer entry in the same set. Simulation of a 128-core chip-multiprocessor with the PARSEC and SPLASH-2x benchmarks shows that the TSF directory requires only a quarter of the area of a non-scalable full-map sparse directory to achieve similar performance and network traffic, both with an average overhead within 1%. The TSF directory outperforms the state-of-the-art Pool and way-combining directory proposals in terms of storage overhead, performance, and network traffic."",""1558-2183"","""",""10.1109/TPDS.2022.3217956"",""National Natural Science Foundation of China(grant numbers:62031009)"; Alibaba Innovative Research; Fudan University-CIOMP Joint Fund(grant numbers:FC2019-001); Fudan-ZTE Joint Lab; Academy for Engineering and Technology, Fudan University(grant numbers:gyy2021-001); CCF-Alibaba Innovative Research Fund for Young Scholars;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9932018"",""Chip multiprocessors";cache coherence;sparse directory;"scalability"",""Coherence";Scalability;Program processors;Complexity theory;Protocols;Proposals;"Multicore processing"","""","""","""",""40"",""IEEE"",""28 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Task Placement and Resource Allocation for Edge Machine Learning: A GNN-Based Multi-Agent Reinforcement Learning Paradigm,""Y. Li"; X. Zhang; T. Zeng; J. Duan; C. Wu; D. Wu;" X. Chen"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; Department of Communications, Peng Cheng Laboratory, Shenzhen, China; Department of Computer Science, University of Hong Kong, Hong Kong; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China;" School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Oct 2023"",""2023"",""34"",""12"",""3073"",""3089"",""Machine learning (ML) tasks are one of the major workloads in today's edge computing networks. Existing edge-cloud schedulers allocate the requested amounts of resources to each task, falling short of best utilizing the limited edge resources for ML tasks. This paper proposes TapFinger, a distributed scheduler for edge clusters that minimizes the total completion time of ML tasks through co-optimizing task placement and fine-grained multi-resource allocation. To learn the tasks’ uncertain resource sensitivity and enable distributed scheduling, we adopt multi-agent reinforcement learning (MARL) and propose several techniques to make it efficient, including a heterogeneous graph attention network as the MARL backbone, a tailored task selection phase in the actor network, and the integration of Bayes’ theorem and masking schemes. We first implement a single-task scheduling version, which schedules at most one task each time. Then we generalize to the multi-task scheduling case, in which a sequence of tasks is scheduled simultaneously. Our design can mitigate the expanded decision space and yield fast convergence to optimal scheduling solutions. Extensive experiments using synthetic and test-bed ML task traces show that TapFinger can achieve up to 54.9% reduction in the average task completion time and improve resource efficiency as compared to state-of-the-art schedulers."",""1558-2183"","""",""10.1109/TPDS.2023.3313779"",""National Natural Science Foundation of China(grant numbers:62102460,61902171,U1911201,U2001209,U20A20159,61972432)"; Hong Kong RGC(grant numbers:HKU 17204619,17208920); Guangzhou Municipal Science and Technology Project(grant numbers:202201011392); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021B151520008); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2023A1515012982); Young Outstanding Award; Zhujiang Talent Plan of Guangdong Province;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10246420"",""Edge machine learning";graph neural networks;multi-agent reinforcement learning;resource allocation;"task placement"",""Task analysis";Resource management;Training;Reinforcement learning;Optimal scheduling;Graphics processing units;"Clustering algorithms"","""",""2"","""",""57"",""IEEE"",""11 Sep 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Taskgraph: A Low Contention OpenMP Tasking Framework,""C. Yu"; S. Royuela;" E. Quiñones"",""Computer Science, Barcelona Supercomputing Center, Barcelona, Spain"; Computer Architecture and Operating Systems, Centro Nacional de Supercomputacion, Barcelona, Spain;" Computer Architecture, Barcelona SuperComputing Center, Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Jun 2023"",""2023"",""34"",""8"",""2325"",""2336"",""OpenMP is the de-facto standard for shared memory systems in High-Performance Computing (HPC). It includes a tasking model that offers a high-level of abstraction to effectively exploit structured (loop-based) and highly dynamic unstructured (task-based) parallelism in an easy and flexible way. Unfortunately, the run-time overheads introduced to manage tasks are (very) high in most common OpenMP frameworks (e.g., GCC, LLVM), which defeats the potential benefits of the tasking model, and makes it suitable for coarse-grained tasks only. This paper presents taskgraph, a framework that uses a task dependency graph (TDG) to represent a region of code implemented with OpenMP tasks in order to reduce the run-time overheads associated with the management of tasks, i.e., contention and parallel orchestration, including task creation and synchronization. The TDG avoids the overheads related to the resolution of task dependencies and greatly reduces those deriving from accesses to shared resources. Moreover, the taskgraph framework introduces in OpenMP the record-and-replay execution model that accelerates the taskgraph region from its second execution. Overall, the multiple optimizations presented in this paper allow exploiting fine-grained OpenMP tasks to cope with the trend in current applications pointing to leverage massive on-node parallelism, fine-grained and dynamic scheduling paradigms. The framework is implemented on LLVM 15.0. Results show that the taskgraph implementation outperforms the vanilla OpenMP system in terms of performance and scalability, for all structured and unstructured parallelism, and considering coarse and fine grained tasks. Furthermore, the proposed framework makes the tasking model a competitive alternative to the OpenMP thread model in most cases."",""1558-2183"","""",""10.1109/TPDS.2023.3284219"",""Generalitat de Catalunya project RESPECT(grant numbers:2021 PROD 00179)"; EU H2020 project AMPERE(grant numbers:871669);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10146446"",""OpenMP tasking";run-time overhead;"fine-grained parallelism"",""Task analysis";Parallel processing;Runtime;Instruction sets;Codes;Computational modeling;"Synchronization"","""",""1"","""",""32"",""IEEE"",""8 Jun 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TDTA: Topology-Based Real-Time DAG Task Allocation on Identical Multiprocessor Platforms,""Y. Wu"; W. Zhang; N. Guan;" Y. Ma"",""School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China"; School of Cyberspace Science Faculty of Computing, Harbin Institute of Technology (Harbin & Shenzhen), Harbin, China; City University of Hong Kong, Hong Kong, China;" School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Sep 2023"",""2023"",""34"",""11"",""2895"",""2909"",""Modern real-time systems contain complex workloads, which are usually modeled as directed acyclic graph (DAG) tasks and deployed on multiprocessor platforms. The complex execution logic of DAG tasks results in excessive schedulability analysis overhead, and the current DAG task allocation strategy cannot efficiently utilize processor resources (inner parallelization of DAG tasks). In this article, an invalid-edge deletion (IED) method is proposed to reduce the execution complexity of the DAG tasks while guaranteeing the correctness of the execution logic. Besides, we bound the number of complete paths for DAG tasks, which re-limits the searching space of the schedulability analysis. Then, a topology-based DAG tasks allocation (TDTA) strategy is developed, which reduces the interference caused by higher-priority DAG tasks to enable the full utilization of the processor resources. The experimental results show that the IED method effectively reduces the overhead of DAG task analysis, and the performance of the TDTA strategy is better than the performance of other state-of-the-art strategies."",""1558-2183"","""",""10.1109/TPDS.2023.3310294"",""National Natural Science Foundation of China(grant numbers:U22A2036)"; National Natural Science Foundation of China(grant numbers:62202123); Shenzhen Science and Technology Research and Development Foundation(grant numbers:JCYJ20190806143418198); Fundamental Research Funds for the Central Universities(grant numbers:HIT.OCEF.2021007); Shenzhen Colleges and Universities Stable Support Program(grant numbers:GXWD20220817124251002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10234581"",""Real-time system";directed acyclic graph task;tasks allocation strategy;partitioned scheduling;"fixed-priority"",""Task analysis";Resource management;Processor scheduling;Job shop scheduling;Real-time systems;Vehicle dynamics;"Topology"","""","""","""",""35"",""IEEE"",""30 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"TFormer: A Transmission-Friendly ViT Model for IoT Devices,""Z. Lu"; C. Ding; F. Juefei-Xu; V. N. Boddeti; S. Wang;" Y. Yang"",""School of Software Engineering, Sun Yat-sen University, Zhuhai, China"; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; Meta AI, New York, NY, USA; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA; Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China;" Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Dec 2022"",""2023"",""34"",""2"",""598"",""610"",""Deploying high-performance vision transformer (ViT) models on ubiquitous Internet of Things (IoT) devices to provide high-quality vision services will revolutionize the way we live, work, and interact with the world. Due to the contradiction between the limited resources of IoT devices and resource-intensive ViT models, the use of cloud servers to assist ViT model training has become mainstream. However, due to the larger number of parameters and floating-point operations (FLOPs) of the existing ViT models, the model parameters transmitted by cloud servers are large and difficult to run on resource-constrained IoT devices. To this end, this article proposes a transmission-friendly ViT model, TFormer, for deployment on resource-constrained IoT devices with the assistance of a cloud server. The high performance and small number of model parameters and FLOPs of TFormer are attributed to the proposed hybrid layer and the proposed partially connected feed-forward network (PCS-FFN). The hybrid layer consists of nonlearnable modules and a pointwise convolution, which can obtain multitype and multiscale features with only a few parameters and FLOPs to improve the TFormer performance. The PCS-FFN adopts group convolution to reduce the number of parameters. The key idea of this article is to propose TFormer with few model parameters and FLOPs to facilitate applications running on resource-constrained IoT devices to benefit from the high performance of the ViT models. Experimental results on the ImageNet-1K, MS COCO, and ADE20K datasets for image classification, object detection, and semantic segmentation tasks demonstrate that the proposed model outperforms other state-of-the-art models. Specifically, TFormer-S achieves 5% higher accuracy on ImageNet-1K than ResNet18 with 1.4× fewer parameters and FLOPs."",""1558-2183"","""",""10.1109/TPDS.2022.3222765"",""Fundamental Research Funds for the Central Universities(grant numbers:2021RC272)"; National Natural Science Foundation of China(grant numbers:62202039); China Postdoctoral Science Foundation(grant numbers:2021M700364); National Natural Science Foundation of China(grant numbers:62106097); China Postdoctoral Science Foundation(grant numbers:2021M691424); National Natural Science Foundation of China(grant numbers:61922017);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954278"",""Internet of Things";cloud computing;cloud-assisted;"vision transformer"",""Internet of Things";Transformers;Servers;Computational modeling;Cloud computing;Convolution;"Analytical models"","""",""3"","""",""61"",""IEEE"",""17 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"The Doctrine of MEAN: Realizing Deduplication Storage at Unreliable Edge,""J. Xia"; G. Cheng; L. Luo; D. Guo; P. Lv;" B. Sun"",""Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China"; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National Laboratory for Parallel and Distributed Processing, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; School of Computer Electronics and Information, Guangxi University, Nanning, Guangxi, China;" Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Aug 2023"",""2023"",""34"",""10"",""2811"",""2826"",""Placing popular data at the network edge helps reduce the retrieval latency, but it also brings challenges to the limited edge storage space. Currently, using available yet not necessarily reliable edge resources is common sense for edge space expansion, while deploying deduplication storage strategies is a general method for better space utilization. However, a contradiction arises when jointly implementing data deduplication with unreliable edge resources. On the one hand, the deduplication policy stipulates that any data chunk can be stored exactly once";" on the other hand, the use of unreliable resources imposes that data should be backed up for the seek of file availability. To resolve such contradiction, we propose MEAN, a deduplication-enabled storage system using unreliable resources at the network edge. The core idea of MEAN is to place similar files together for better deduplication and maintain replicas of popular files for higher reliability. We first formulate this problem and prove its NP-hardness, then provide efficient heuristics based on similarity-aware hierarchical clustering. Three different reliability scenarios are comprehensively considered to develop our algorithms. We also implement a prototype system and evaluate the performance of MEAN with a real-world dataset. The results show that MEAN can fortify the file hit ratio under unreliable environments by 77% while reducing the file retrieval delay up to 71%, compared with the state-of-the-art approach."",""1558-2183"","""",""10.1109/TPDS.2023.3305460"",""National Natural Science Foundation of China(grant numbers:U19B2024,62002378)"; Research Funding of NUDT(grant numbers:ZK20-30);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10219047"",""Deduplication";fault tolerance;storage system;"edge computing"",""Servers";Redundancy;Clustering algorithms;Resource management;Production;Heuristic algorithms;"Delays"","""","""","""",""49"",""CCBY"",""15 Aug 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"The High Faulty Tolerant Capability of the Alternating Group Graphs,""H. Zhang"; R. -X. Hao; X. -W. Qin; C. -K. Lin;" S. -Y. Hsieh"",""School of Mathematics and Statistics, Beijing Jiaotong University, Beijing, China"; School of Mathematics and Statistics, Beijing Jiaotong University, Beijing, China; School of Mathematics and Statistics, Beijing Jiaotong University, Beijing, China; Department of Computer Science, National Yang Ming Chiao Tung University, Hsinchu, Taiwan;" Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""225"",""233"",""The matroidal connectivity and conditional matroidal connectivity are novel indicators to measure the real faulty tolerability. In this paper, for the $n$n-dimensional alternating group graph $AG_{n}$AGn, the structure properties and (conditional) matroidal connectivity are studied based on the dimensional partition of $E(AG_{n})$E(AGn). We prove that for $S\subseteq E(AG_{n})$S⊆E(AGn) under some limitation on the number of faulty edges in each dimensional edge set, if $|S|\leq (n-1)!-1$|S|≤(n-1)!-1, then $AG_{n}-S$AGn-S is connected. We study the value of matroidal connectivity and conditional matroidal connectivity of $AG_{n}$AGn. Furthermore, simulations have been carried out to compare the matroidal connectivity with other types of conditional connectivity in $AG_{n}$AGn. The simulation result shows that the matroidal connectivity significantly improves these known fault-tolerant capability of alternating group graphs."",""1558-2183"","""",""10.1109/TPDS.2022.3217415"",""National Natural Science Foundation of China(grant numbers:11971054,11731002,12161141005)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9930647"",""Matroidal connectivity";conditional matroidal connectivity;alternating group graph;"fault tolerance"",""Fault tolerant systems";Fault tolerance;Multiprocessor interconnection;Program processors;Terminology;Computer science;"Wireless communication"","""",""6"","""",""24"",""IEEE"",""26 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"The Role of Idle Waves, Desynchronization, and Bottleneck Evasion in the Performance of Parallel Programs,""A. Afzal"; G. Hager;" G. Wellein"",""Erlangen National High Performance Computing Center (NHR@FAU), Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany"; Erlangen National High Performance Computing Center (NHR@FAU), Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany;" Erlangen National High Performance Computing Center (NHR@FAU), Friedrich-Alexander-Universität Erlangen-Nürnberg, Erlangen, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Dec 2022"",""2023"",""34"",""2"",""623"",""638"",""The performance of highly parallel applications on distributed-memory systems is influenced by many factors. Analytic performance modeling techniques aim to provide insight into performance limitations and are often the starting point of optimization efforts. However, coupling analytic models across the system hierarchy (socket, node, network) fails to encompass the intricate interplay between the program code and the hardware, especially when execution and communication bottlenecks are involved. In this paper we investigate the effect of bottleneck evasionand how it can lead to automatic overlap of communication overhead with computation. Bottleneck evasion leads to a gradual loss of the initial bulk-synchronous behavior of a parallel code so that its processes become desynchronized. This occurs most prominently in memory-bound programs, which is why we choose memory-bound benchmark and application codes, specifically an MPI-augmented STREAM Triad, sparse matrix-vector multiplication, and a collective-avoiding Chebyshev filter diagonalization code to demonstrate the consequences of desynchronization on two different supercomputing platforms. We investigate the role of idle waves as possible triggers for desynchronization and show the impact of automatic asynchronous communication for a spectrum of code properties and parameters, such as saturation point, matrix structures, domain decomposition, and communication concurrency. Our findings reveal how eliminating synchronization points (such as collective communication or barriers) precipitates performance improvements that go beyond what can be expected by simply subtracting the overhead of the collective from the overall runtime."",""1558-2183"","""",""10.1109/TPDS.2022.3221085"",""KONWIHR"; Bavarian Competence Network for Scientific High Performance Computing in Bavaria;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944862"",""Bottleneck";desynchronization;parallel distributed computing;performance modeling;performance optimization;scalability;"synchronization"",""Delays";Bandwidth;Codes;Computational modeling;Bars;Topology;"Synchronization"","""","""","""",""38"",""IEEE"",""10 Nov 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"The Tiny-Tasks Granularity Trade-Off: Balancing Overhead Versus Performance in Parallel Systems,""S. Bora"; B. Walker;" M. Fidler"",""Institute of Communications Technology, Leibniz Universität Hannover, Hannover, Germany"; Institute of Communications Technology, Leibniz Universität Hannover, Hannover, Germany;" Institute of Communications Technology, Leibniz Universität Hannover, Hannover, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Mar 2023"",""2023"",""34"",""4"",""1128"",""1144"",""Models of parallel processing systems typically assume that one has $l$l workers and jobs are split into an equal number of $k=l$k=l tasks. Splitting jobs into $k > l$k>l smaller tasks, i.e. using “tiny tasks”, can yield performance and stability improvements because it reduces the variance in the amount of work assigned to each worker, but as $k$k increases, the overhead involved in scheduling and managing the tasks begins to overtake the performance benefit. We perform extensive experiments on the effects of task granularity on an Apache Spark cluster, and based on these, develop a four-parameter model for task and job overhead that, in simulation, produces sojourn time distributions that match those of the real system. We also present analytical results which illustrate how using tiny tasks improves the stability region of split-merge systems, and analytical bounds on the sojourn and waiting time distributions of both split-merge and single-queue fork-join systems with tiny tasks. Finally we combine the overhead model with the analytical models to produce an analytical approximation to the sojourn and waiting time distributions of systems with tiny tasks which include overhead. We also perform analogous tiny-tasks experiments on a hybrid multi-processor shared memory system based on MPI and OpenMP which has no load-balancing between nodes. Though no longer strict analytical bounds, our analytical approximations with overhead match both the Spark and MPI/OpenMP experimental results very well."",""1558-2183"","""",""10.1109/TPDS.2022.3233712"",""German Research Council(grant numbers:VaMoS FI 1236/7-1)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005831"",""Network calculus";parallel processing;performance bounds;processing overhead;Spark;synchronization constraints;task granularity;"tiny-tasks"",""Task analysis";Sparks;Servers;Analytical models;Cluster computing;Parallel processing;"Synchronization"","""","""","""",""58"",""CCBY"",""4 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Topology-Aware Scheduling Framework for Microservice Applications in Cloud,""X. Li"; J. Zhou; X. Wei; D. Li; Z. Qian; J. Wu; X. Qin;" S. Lu"",""Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China"; Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; Department of Computer Science, Montclair State University, Montclair, NJ, USA; State Key Laboratory of Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China; Center for Networked Computing, Temple University, Philadelphia, PA, USA; Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China;" State Key Laboratory of Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Mar 2023"",""2023"",""34"",""5"",""1635"",""1649"",""Loosely coupled and highly cohesived microservices running in containers are becoming the new paradigm for application development. Compared with monolithic applications, applications built on microservices architecture can be deployed and scaled independently, which promises to simplify software development and operation. However, the dramatic increase in the scale of microservices and east-west network traffic in the data center have made the cluster management more complex. Not only does the scale of microservices cause a great deal of pressure on cluster management, but also cascading QoS violations present a substantial risk for SLOs (Service Level Objectives). In this paper, we propose a Microservice-Oriented Topology-Aware Scheduling Framework (MOTAS), which effectively utilizes the topologies of microservices and clusters to optimize the network overhead of microservice applications through a heuristic graph mapping algorithm. The proposed framework can also guarantee the cluster resource utilization. To deal with the dynamic environment of microservice, we propose a mechanism based on distributed trace analysis to detect and handle QoS violations in microservice applications. Through real-world experiments, the framework has been proved to be effective in ensuring cluster resource utilization, reducing application end-to-end latency, improving throughput, and handling QoS violations."",""1558-2183"","""",""10.1109/TPDS.2023.3238751"",""National Key R&D Program of China(grant numbers:2019YFB2102002)"; National Natural Science Foundation of China(grant numbers:61802182); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10024362"",""Cloud computing";microservice;quality of service;"resource scheduling"",""Microservice architectures";Topology;Network topology;Containers;Resource management;Data centers;"Virtual machining"","""",""1"","""",""52"",""IEEE"",""23 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Towards Correlated Data Trading for High-Dimensional Private Data,""H. Cai"; Y. Yang; W. Fan; F. Xiao;" Y. Zhu"",""College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China"; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Jan 2023"",""2023"",""34"",""3"",""1047"",""1059"",""The commoditization of private data has become an attractive research topic with the emergence of Big Data era. In this paper, we study the trading of high-dimensional private data with differential privacy guarantee. We propose Cheap, which is a novel Correlated data trading framework for High-dimEnsionAl Private data. Cheap first models data correlations among high-dimensional user attributes, and builds an initial attribute clustering scheme. Combined with this scheme, Cheap devises a novel data perturbation mechanism by solving optimal attribute clustering (OAC) problem, in order to improve data utility of traded data and further generate a privacy-preserving high-dimensional dataset with close joint distribution with the original one. It then quantifies privacy loss based on near-optimal attribute cluster scheme due to the NP-hardness of the OAC problem, and further compensates data owners by running auction in a cost-effective way. We evaluate the performance of Cheap on UserBehavior dataset and Obesity dataset, respectively. Our evaluation and analysis demonstrate that Cheap well balances data utility and privacy protection, and achieves all desired economic properties of budget balance, individual rationality and truthfulness."",""1558-2183"","""",""10.1109/TPDS.2023.3237691"",""National Natural Science Foundation of China(grant numbers:62102195,62102196,62172236,62072304,61932013)"; National Science Fund for Distinguished Young Scholars(grant numbers:62125203); Natural Science Foundation of the Higher Education Institutions of Jiangsu Province(grant numbers:21KJB520023); China Postdoctoral Science Foundation Funded Project(grant numbers:2022M711688); Research Foundation of Jiangsu(grant numbers:BRA2020065); Jiangsu Provincial Double-Innovation Doctor Program(grant numbers:JSSCBS20210501); NUPTSF(grant numbers:NY220134);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10018836"",""Data correlation";data privacy;"data trading"",""Data privacy";Correlation;Privacy;Costs;Perturbation methods;Data models;"History"","""",""1"","""",""47"",""IEEE"",""17 Jan 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Tripartite Graph Aided Tensor Completion For Sparse Network Measurement,""X. Li"; K. Xie; X. Wang; G. Xie; K. Li; J. Cao; D. Zhang;" J. Wen"",""College of Computer Science and Electronics Engineering, Hunan University, Changsha, China"; College of Computer Science and Electronics Engineering, Hunan University, Changsha, China; Department of Electrical and Computer Engineering, State University of New York at Stony Brook, Stony Brook, NY, USA; Computer Network Information Center(CNIC), Chinese Academy of Sciences, Beijing, China; College of Computer Science and Electronics Engineering, Hunan University, Changsha, China; Department of computing, The Hong Kong Polytechnic University, Hong Kong; College of Computer Science and Electronics Engineering, Hunan University, Changsha, China;" College of Computer Science and Electronics Engineering, Hunan University, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Nov 2022"",""2023"",""34"",""1"",""48"",""62"",""Network measurements provide critical inputs for a wide range of network management. Existing network-wide monitoring methods face the challenge of incurring a high measurement cost. Some recent studies show that network-wide measurement data such as end-to-end latency and flow traffic, have hidden spatio-temporal correlations and thus low-rank features. Taking advantage of the low-rank feature, enlightened by tensor model's strong capability of information representation and extracting, this paper studies a novel sparse measurement scheduling problem which selects a proportion of Origin and Destination (OD) pairs to take measurements in the future time slots, while ensuring the data of the remaining un-measured OD pairs be accurately inferred through tensor completion. It is challenging to find the optimal sampling points (OD pairs) without knowing the structure of the future data and also infer the un-measured data in the presence of noise in the measurement samples. To conquer the challenges, we propose several techniques: a tripartite graph to illustrate the relationship between sample locations and tensor factorization, a graph-based sample selection algorithm, and a graph-based robust tensor completion algorithm. We have conducted extensive experiments based on two real network latency monitoring traces (PlanetLab and Harvard) and two other network monitoring traces (including a traffic trace Abilene and a throughput trace WS-Dream). Our results demonstrate that, even with a sampling ratio of less than 5%, our scheme can accurately obtain the complete network-wide monitoring data by inferring the missing ones based on the samples taken. To achieve similar recovery performance, the best peer tensor completion algorithm needs a significantly larger number of samples, with the sampling ratio up to 25-150 times ours."",""1558-2183"","""",""10.1109/TPDS.2022.3213259"",""National Science Fund for Distinguished Young Scholars(grant numbers:62025201)"; National Natural Science Foundation of China(grant numbers:62102138,61972144,61976087); China National Postdoctoral Program for Innovative Talents(grant numbers:BX20200120); China Postdoctoral Science Foundation(grant numbers:2020M682556); Natural Science Foundation of Hunan Province(grant numbers:2021JJ40115);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915456"",""Sampling scheduling";tensor completion;"tripartite graph"",""Tensors";Monitoring;Extraterrestrial measurements;Sparse matrices;Data models;Time measurement;"Costs"","""",""2"","""",""42"",""IEEE"",""10 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"TSC-VEE: A TrustZone-Based Smart Contract Virtual Execution Environment,""Z. Jian"; Y. Lu; Y. Qiao; Y. Fang; X. Xie; D. Yang; Z. Zhou;" T. Li"",""College of Computer Science, Nankai University, Tianjin, China"; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; Blockchain Platform Division, Ant Group, Beijing, China; Blockchain Platform Division, Ant Group, Beijing, China;" College of Computer Science, Nankai University, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1773"",""1788"",""TrustZone as a trusted execution environment (TEE) has been proven to preserve the confidentiality of blockchain transactions supported by smart contracts. Despite some academic effort, TrustZone can only support limited languages for now. The lack of the corresponding execution environment for smart contracts seriously hinders blockchain applications from directly running on TrustZone. In this paper, we design the first virtual execution environment named TSC-VEE for performing Solidity smart contracts on TrustZone, to the best of our knowledge. TSC-VEE can be decomposed into fourfold: (1) an instruction set adapted to the isolation and world switching mechanism of TrustZone. (2) a runtime memory management mechanism that provides a pair of instructions with the corresponding processing mechanism to allocate and release the work memory. (3) a hybrid granularity resource analysis algorithm which computes and records the value of maximum stack height and static gas cost through bytecode pre-execution, avoiding runtime overflow and invalid computations. (4) a cross-isolation-environment prefetching approach that supports loading and storing the storage data from the normal world into the secure world on TrustZone before execution, thus avoiding switching the world state frequently at runtime. Extensive experimental results show that TSC-VEE can perform smart contracts correctly and efficiently on TrustZone. Compared with the most commonly used Ethereum client—Geth, TSC-VEE achieves execution performance improvements by $9.29\times$9.29×. We also implement the Ethereum virtual machine—evmone on TrustZone. TSC-VEE can reduce the latency by 12.63% with our optimization techniques, and decrease the work memory footprint by 22.95% on average when executing various scale contracts."",""1558-2183"","""",""10.1109/TPDS.2023.3263882"",""CCF-AFSG Research Fund(grant numbers:CCF-AFSG RF20210031)"; CCF-Huawei Populus Grove Fund(grant numbers:CCF-HuaweiTC2022005); National Natural Science Foundation of China(grant numbers:62002175); Open Project Fund of State Key Laboratory of Computer Architecture; Institute of Computing Technology; Chinese Academy of Sciences(grant numbers:CARCHB202016); Open Project Foundation of Information Security Evaluation Center of Civil Aviation; Civil Aviation University of China(grant numbers:ISECCA-202102);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10094016"",""Blockchain";smart contract;solidity program language;TrustZone;"virtual execution environment"",""Smart contracts";Blockchains;Memory management;Switches;Codes;Public key;"Optimization"","""","""","""",""44"",""IEEE"",""6 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"TurboMGNN: Improving Concurrent GNN Training Tasks on GPU With Fine-Grained Kernel Fusion,""W. Wu"; X. Shi; L. He;" H. Jin"",""National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, University of Warwick, Coventry, U.K.;" National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2023"",""2023"",""34"",""6"",""1968"",""1981"",""Graph Neural Networks (GNN) have evolved as powerful models for graph representation learning. Many works have been proposed to support GNN training efficiently on GPU. However, these works only focus on a single GNN training task such as operator optimization, task scheduling, and programming model. Concurrent GNN training, which is needed in the applications such as neural network structure search, has not been explored yet. This work aims to improve the training efficiency of the concurrent GNN training tasks on GPU by developing fine-grained methods to fuse the kernels from different tasks. Specifically, we propose a fine-grained Sparse Matrix Multiplication (SpMM) based kernel fusion method to eliminate redundant accesses to graph data. In order to increase the fusion opportunity and reduce the synchronization cost, we further propose a novel technique to enable the fusion of the kernels in forward and backward propagation. Finally, in order to reduce the resource contention caused by the increased number of concurrent, heterogeneous GNN training tasks, we propose an adaptive strategy to group the tasks and match their operators according to resource contention. We have conducted extensive experiments, including kernel- and model-level benchmarks. The results show that the proposed methods can achieve up to 2.6X performance speedup."",""1558-2183"","""",""10.1109/TPDS.2023.3267943"",""National Key R&D Program of China(grant numbers:2020AAA0108501)"; Key R&D Program of Hubei(grant numbers:2020BAA020);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10103627"",""GNN training";concurrent multi-tasks;GPU;"kernel fusion"",""Task analysis";Training;Graphics processing units;Kernel;Graph neural networks;Computational modeling;"Fuses"","""",""1"","""",""48"",""CCBYNCND"",""17 Apr 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"UMA-MF: A Unified Multi-CPU/GPU Asynchronous Computing Framework for SGD-Based Matrix Factorization,""Y. Huang"; Y. Liu; Y. Bai; S. Chen;" R. Li"",""Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China;" Key Laboratory for Embedded and Network Computing of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Oct 2023"",""2023"",""34"",""11"",""2978"",""2993"",""Recent research has shown that collaborative computing of CPUs and GPUs in the same system can effectively accelerate large-scale SGD-based matrix factorization (MF), but it faces the problem of limited scalability due to parameter synchronization in the server. Theoretically, asynchronous methods can overcome this shortcoming. However, through a series of tests, observations, and analyses, we realize that developing an effective asynchronous multi-CPU/GPU MF framework faces several major design challenges: the underutilized CPUs, high communication overhead, and the asynchronous data safety issue. This article presents a unified multi-CPU/GPU asynchronous computing framework for SGD-based matrix factorization, named UMA-MF. UMA-MF treats CPUs and GPUs in the system as distributed workers that train matrix datasets in parallel and update feature parameters asynchronously. It provides a cache-friendly CPU external working mode, which can improve the CPU's cache hit rate, thereby promoting the efficient use of CPUs. It offers an algorithm to find the shortest communication ring topology of heterogeneous CPU/GPU workers and builds computing-communication pipelines to minimize the communication overhead. It implements a wait-free structure and load-balanced data distribution to achieve asynchronous data safety. UMA-MF can effectively accelerate SGD-based MF on multi-CPU/GPU systems in an asynchronous way. On a physical platform with configurations ranging from single processor system to 2CPUs--4CPUs system, for five common datasets Netfix, R1, R2, Goodreads, and de-dense, UMA-MF achieves up to 3.56x speedup compared with HCC-MF, which is the state-of-the-art multi-CPU/GPU synchronous computing framework for SGD-based MF. UMA-MF also shows good scalability. When the system is scaled to 2CPUs-4GPUs, the training time speedup of UMA-MF can reach 70%--97% of the ideal speedup."",""1558-2183"","""",""10.1109/TPDS.2023.3317535"",""National Natural Science Foundation of China(grant numbers:62272155,61932010)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10256153"",""Matrix factorization";multi-CPU/GPU;parallel systems;"stochastic gradient descent"",""Graphics processing units";Training;Safety;Topology;Scalability;Computer architecture;"Stochastic processes"","""","""","""",""47"",""IEEE"",""20 Sep 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"When Deduplication Meets Migration: An Efficient and Adaptive Strategy in Distributed Storage Systems,""G. Cheng"; L. Luo; J. Xia; D. Guo;" Y. Sun"",""Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China"; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China;" Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Aug 2023"",""2023"",""34"",""10"",""2749"",""2766"",""The traditional migration methods are confronted with formidable challenges when data deduplication technologies are incorporated. First, the deduplication creates data-sharing dependencies in the stored files";" breaking such dependencies in migration may attach extra space overhead. Second, the redundancy elimination makes the storage system reserves only one copy for each storage file, and heightens the risk of data unavailability. The existing methods fail to tackle them in one shot. To this end, we propose Jingwei, an efficient and adaptive data migration strategy for deduplicated storage systems. To be specific, Jingwei tries to minimize the extra space cost in migration for space efficiency. Meanwhile, Jingwei realizes the service adaptability by encouraging replicas of hot files to spread out their data access requirements. We first model such a problem as an integer linear programming (ILP) and solve it with a commercial solver when only one empty migration target server is allowed. We then extend this problem to a scenario wherein multiple non-empty target servers are available for migration. We solve it by effective heuristic algorithms based on the Bloom Filter-based data sketches. The Jingwei strategy can suffer from performance degradation when the heat degree varies significantly. Therefore, we further present incremental adjustment strategies for the two scenarios, which adjust the number of block replicas and their locations in an incremental manner. The mathematical analyses and trace-driven experiments show the effectiveness of our Jingwei strategy. To be specific, Jingwei fortifies the file replicas by 25% with only 5.7% of the extra storage space, compared with the latest “Goseed” method. With the small extra space cost, the file retrieval throughput of Jingwei can reach up to 333.5 Mbps, which is 12.3% higher than that of the Random method."",""1558-2183"","""",""10.1109/TPDS.2023.3299309"",""National Natural Science Foundation of China(grant numbers:U19B2024,62002378)"; Research Funding of NUDT(grant numbers:ZK20-3);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10195996"",""Data migration";data deduplication;replica storage;"heat variation"",""Servers";Costs;Space heating;Degradation;Mathematical analysis;Heuristic algorithms;"Big Data"","""","""","""",""46"",""CCBY"",""27 Jul 2023"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;