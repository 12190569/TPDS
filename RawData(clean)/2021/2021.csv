"3D Perception With Slanted Stixels on GPU,""D. Hernandez-Juarez"; A. Espinosa; D. Vazquez; A. M. Lopez;" J. C. Moure"",""SLAMcore Ltd., London, SE1 1JA, U.K."; Universitat Autonoma de Barcelona, Bellaterra, Spain; Element AI, Montreal, QC, Canada; Universitat Autonoma de Barcelona, Bellaterra, Spain;" Universitat Autonoma de Barcelona, Bellaterra, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Apr 2021"",""2021"",""32"",""10"",""2434"",""2447"",""This article presents a GPU-accelerated software design of the recently proposed model of Slanted Stixels, which represents the geometric and semantic information of a scene in a compact and accurate way. We reformulate the measurement depth model to reduce the computational complexity of the algorithm, relying on the confidence of the depth estimation and the identification of invalid values to handle outliers. The proposed massively parallel scheme and data layout for the irregular computation pattern that corresponds to a Dynamic Programming paradigm is described and carefully analyzed in performance terms. Performance is shown to scale gracefully on current generation embedded GPUs. We assess the proposed methods in terms of semantic and geometric accuracy as well as run-time performance on three publicly available benchmark datasets. Our approach achieves real-time performance with high accuracy for 2048 × 1024 image sizes and 4 × 4 Stixel resolution on the low-power embedded GPU of an NVIDIA Tegra Xavier."",""1558-2183"","""",""10.1109/TPDS.2021.3067836"",""Ministerio de Economía, Industria y Competitividad(grant numbers:TIN2017-84553-C2-1-R)"; Antonio M. López(grant numbers:TIN2017-88709-R);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382880"",""Stereo vision";stixel world;autonomous vehicles;scene understanding;computer vision;embedded systems;"GPU acceleration"",""Graphics processing units";Semantics;Computational modeling;Real-time systems;Mathematical model;Image segmentation;"Indexes"","""","""","""",""34"",""CCBY"",""22 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"A Case for Pricing Bandwidth: Sharing Datacenter Networks With Cost Dominant Fairness,""L. Chen"; Y. Feng; B. Li;" B. Li"",""School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA"; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada;" Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Jan 2021"",""2021"",""32"",""5"",""1256"",""1269"",""Unlike other resources such as CPU or memory in a virtual machine, inter-virtual-machine (inter-VM) bandwidth has not been explicitly priced in datacenter networks. In this article, we argue that tenants of an IaaS cloud computing platform should be given the flexibility to pay more for explicitly priced datacenter bandwidth beyond traditional virtual machines, in order to achieve better (or more predictable) application performance. We show that a much simpler design principle can be followed to allocate bandwidth fairly, and desirable properties related to fairness can be more easily achieved, compared with state-of-the-art proposals. We call such a design principle cost dominant fairness, which stipulates that bandwidth should be allocated based on the total cost that a tenant incurs for running its applications in the cloud. Guided by the principle of cost dominant fairness, we explore the design space of pricing inter-VM bandwidth, as well as achieving fair bandwidth sharing among multiple tenants. Through our study, we believe that it is best to assign per-VM-pair weights based on individualized prices. We present a distributed bandwidth allocation algorithm that is theoretically supported by a network utility maximization formulation, and practically implemented as a shim layer at each virtual machine. We are also concerned with practical issues of billing, where discounts are needed to ensure that a tenant only pays for the bandwidth share that it is allocated. Finally, we have evaluated our pricing framework and per-VM-pair weighted fair bandwidth allocation in the Mininet emulation testbed and simulations."",""1558-2183"","""",""10.1109/TPDS.2020.3045709"",""BoRSF-RCS(grant numbers:LEQSF(2019-22)-RD-A-21)"; RGC GRF(grant numbers:16206417,16207818);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9298928"",""Datacenter networks";bandwidth allocation;fairness;"pricing"",""Bandwidth";Pricing;Cloud computing;Virtual machining;Space exploration;Resource management;"Channel allocation"","""",""5"","""",""40"",""IEEE"",""18 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"A Distributed Framework for EA-Based NAS,""Q. Ye"; Y. Sun; J. Zhang;" J. Lv"",""School of Computer Science, Sichuan University, Chengdu, China"; School of Computer Science, Sichuan University, Chengdu, China; School of Computer Science, Sichuan University, Chengdu, China;" State Key Laboratory of Hydraulics and Mountain River Engineering, School of Computer Science, Sichuan University, Chengdu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1753"",""1764"",""Evolutionary Algorithms (EA) are widely applied in Neural Architecture Search (NAS) and have achieved appealing results. Different EA-based NAS algorithms may utilize different encoding schemes for network representation, while they have the same workflow. Specifically, the first step is the initialization of the population with different encoding schemes, and the second step is the evaluation of the individuals by the fitness function. Then, the EA-based NAS algorithm executes evolution operations, e.g., selection, mutation, and crossover, to eliminate weak individuals and generate more competitive ones. Lastly, evolution continues until the max generation and the best neural architectures will be chosen. Because each individual needs complete training and validation on the target dataset, the EA-based NAS always consumes significant computation and time inevitably, which results in the bottleneck of this approach. To ameliorate this issue, this article proposes a distributed framework to boost the computation of the EA-based NAS algorithm. This framework is a server/worker model where the server distributes individuals requested by the computing nodes and collects the validated individuals and hosts the evolution operations. Meanwhile, the most time-consuming phase (i.e., individual evaluation) of the EA-based NAS is allocated to the computing nodes, which send requests asynchronously to the server and evaluate the fitness values of the individuals. Additionally, a new packet structure of the message delivered in the cluster is designed to encapsulate various network representations and support different EA-based NAS algorithms. We design an EA-based NAS algorithm as a case to investigate the efficiency of the proposed framework. Extensive experiments are performed on an illustrative cluster with different scales, and the results reveal that the framework can achieve a nearly linear reduction of the search time with the increase of the computational nodes. Furthermore, the length of the exchanged messages among the cluster is tiny, which benefits the framework expansion."",""1558-2183"","""",""10.1109/TPDS.2020.3046774"",""National Key R&D Program of China(grant numbers:YFB1002201)"; National Natural Science Fund for Distinguished Young Scholar(grant numbers:61625204); National Science Foundation of China(grant numbers:61836006);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305984"",""Distributed framework";evolutionary algorithm (EA);neural architecture search (NAS);"evolutionary neural networks"",""Computer architecture";Training;Statistics;Sociology;Neural networks;Clustering algorithms;"Servers"","""",""8"","""",""61"",""IEEE"",""23 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"A Fault-Tolerant Distributed Framework for Asynchronous Iterative Computations,""T. Zhou"; L. Gao;" X. Guan"",""Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, China"; Department of Electrical and Computer Engineering, University of Massachusetts Amhers, Amherst, MA, USA;" Systems Engineering Instituteg, Xi'an Jiaotong University, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Mar 2021"",""2021"",""32"",""8"",""2062"",""2073"",""Asynchronous iterative computations (AIC) are common in machine learning and data mining systems. However, the lack of synchronization barriers in asynchronous processing brings challenges for continuous processing while workers might fail. There is no global synchronization point that all workers can roll back to. In this article, we propose a fault-tolerant framework for asynchronous iterative computations (FAIC). Our framework takes a virtual snapshot of the AIC system without halting the computation of any worker. We prove that the virtual snapshot capture by FAIC can recover the AIC system correctly. We evaluate our FAIC framework on two existing AIC systems, Maiter and NOMAD. Our experiment result shows that the checkpoint overhead of FAIC is more than 50 percent shorter than the synchronous checkpoint method. FAIC is around 10 percent faster than other asynchronous snapshot algorithms, such as the Chandy-Lamport algorithm. Our experiments on a large cluster demonstrate that FAIC scales with the number of workers."",""1558-2183"","""",""10.1109/TPDS.2021.3059420"",""National Science Foundation(grant numbers:CNS-1815412,CNS-1908536)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354533"",""Fault-tolerance";cloud computing;asynchronous iterative computation;"asynchronous snapshot"",""Computational modeling";Synchronization;Servers;Transient analysis;Fault tolerant systems;Fault tolerance;"Data models"","""","""","""",""26"",""IEEE"",""15 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Game-Based Approach for Cost-Aware Task Assignment With QoS Constraint in Collaborative Edge and Cloud Environments,""S. Long"; W. Long; Z. Li; K. Li; Y. Xia;" Z. Tang"",""Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent Network, School of Computer Science, Xiangtan University, Xiangtan, Hunan, China"; Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent Network, School of Computer Science, Xiangtan University, Xiangtan, Hunan, China; Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent Network, School of Computer Science, Xiangtan University, Xiangtan, Hunan, China; College of Information Science and Engineering, Hunan University; School of Automation, Beijing Institute of Technology, Beijing, China;" College of Information Science and Engineering, Hunan University"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1629"",""1640"",""With the development of the Internet of Things, the data that needs to be processed is increasing rapidly. Therefore, the collaboration of cloud and edge emerges as the times require. Edge nodes are mainly responsible for collecting data, and decide to process the data locally or offload to cloud data centers. Cloud data centers are suitable for data analysis, model training, and managing edge nodes. In this article, we focus on the task assignment problems in collaborative edge and cloud environments and study it in a distributed, non-cooperative environment. An M/M/1 queueing model is established to characterize the task transmission. Because of the multi-core processors, we set an M/M/C queueing model to characterize the task computation. We consider the problem from the perspective of game theory and formulate it into a non-cooperative game among multi-agents (multiple edge data centers) in which each agent is informed with incomplete information (allocation strategies) of others. For each agent, we define a function of the expected cost of tasks as the disutility function, and minimize it subject to the QoS constraint. We analyze the existence of Nash equilibrium and develop a Greedy Energy-aware Algorithm (GEA) to choose active servers using the Limit Searching Algorithm (LSA) to find the ceiling utilization. Then we propose the Best Response Algorithm (BRA) to optimize the utility function. The convergence of the BRA algorithm has been discussed. Finally, the results demonstrate that the BRA algorithm can get a solution close to Nash equilibrium and reach it quickly."",""1558-2183"","""",""10.1109/TPDS.2020.3041029"",""National Key Research and Development Program of China(grant numbers:2018YFB1003702)"; National Natural Science Foundation of China(grant numbers:62032020); Hunan Science and Technology Planning Project(grant numbers:2019RS3019); Hunan Provincial Natural Science Foundation of China for Distinguished Young Scholars(grant numbers:2018JJ1025); Hunan Province Department of Education(grant numbers:18C0107); National Natural Science Foundation of China(grant numbers:61502407,62076214); Natural Science Foundation of Hunan(grant numbers:2019JJ50618); Hunan Province Science and Technology Project(grant numbers:2018TP1036); Xiangtan University(grant numbers:11kz/kz08057);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272869"",""Data centers";game theory;mutliple agent system;QoS constraint;"queueing system"",""Task analysis";Servers;Cloud computing;Quality of service;Energy consumption;Games;"Data centers"","""",""36"","""",""41"",""IEEE"",""27 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"A Generic Stochastic Model for Resource Availability in Fog Computing Environments,""S. K. Battula"; M. M. O'Reilly; S. Garg;" J. Montgomery"",""School of Information and Communication Technology, College of Sciences and Engineering, University of Tasmania, Hobart, Australia"; School of Natural Sciences, College of Sciences and Engineering, University of Tasmania, Hobart, Australia; School of Information and Communication Technology, College of Sciences and Engineering, University of Tasmania, Hobart, Australia;" School of Information and Communication Technology, College of Sciences and Engineering, University of Tasmania, Hobart, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Dec 2020"",""2021"",""32"",""4"",""960"",""974"",""Fog computing is an increasingly popular method with which to process the huge amount of data generated by the Internet of Things (IoT) devices and applications at the edge-level, using the heterogeneous autonomous end-devices of the participating users. To meet the requirements of the IoT and time-sensitive applications, a Fog computing platform needs to select appropriate resources, the availability of which can be guaranteed during the execution of the application. For the proper selection of resources, the platform must be able to predict future availability. Hence, a proper resource availability model which provides knowledge about the future availability of resources in the Fog computing environment is required. However, designing an efficient resource availability model, in a highly distributed and mobile environment like the Fog, is a complex task due to the multidimensional characteristics of Fog devices, such as mobility, lack of centralised control, limited resources, and being battery powered. Existing resource availability models did not consider all the characteristics of a real Fog environment. Therefore, this study aims to provide a generic continuous-time Markov chain (CTMC), based resource availability model for Fog computing environments. The applicability of the model is shown by integrating the model input with the nearest-location best fit (NLBF) and Best-Fit resource selection policies."",""1558-2183"","""",""10.1109/TPDS.2020.3037247"",""Australian Research Council(grant numbers:LP140100152)"; ARC Center of Excellence for Mathematical and Statistical Frontiers;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253552"",""Fog computing";resource availability;stochastic model;Markov chain;"mobility and Internet of Things"",""Computational modeling";Edge computing;Mathematical model;Cloud computing;Internet of Things;Stochastic processes;"Dynamic scheduling"","""",""13"","""",""29"",""IEEE"",""10 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"A GPU Acceleration Framework for Motif and Discord Based Pattern Mining,""B. Zhu"; Y. Jiang; M. Gu;" Y. Deng"",""School of Software, Tsinghua University, Beijing, China"; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China;" School of Software, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2021"",""2021"",""32"",""8"",""1987"",""2004"",""With the fast digitalization of our society, mining patterns from large time series data is increasingly becoming a critical problem for a wide range of big data applications. Motif and discord discovery algorithms, which offer effective solutions to identify repeatedly appearing and abnormal patterns, respectively, are fundamental building blocks for time series processing. Both approaches, however, can be time extremely consuming when handling large time series due to the subsequence-based computations of distance similarity metrics. In this article, we show that the highly involved subsequence-based computations can actually be decomposed into a few fine-grained computing patterns for efficient data parallel computing. By developing highly efficient GPU algorithms for such basic patterns and effectively composing such patterns, we are able to solve both motif and discord discovery problems under euclidean and DTW distance metrics in a unified GPU acceleration framework. Extensive experiments prove that the proposed framework outperforms pruned CPU algorithms by up to three orders of magnitude. Our work paves the foundation of building GPU acceleration frameworks for large-scale time series datasets."",""1558-2183"","""",""10.1109/TPDS.2021.3055765"",""National Key Research and Development Program of China(grant numbers:2018YFB1702600)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343677"",""Time series";pattern mining;motif;discord;dynamic time warping (DTW);euclidean distance;GPU;"acceleration"",""Lenses";Time series analysis;Graphics processing units;Euclidean distance;Data mining;Acceleration;"Force"","""",""7"","""",""33"",""IEEE"",""1 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"A High-Throughput FPGA Accelerator for Short-Read Mapping of the Whole Human Genome,""Y. -L. Chen"; B. -Y. Chang; C. -H. Yang;" T. -D. Chiueh"",""Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan"; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Department of Electrical Engineering, Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan;" Department of Electrical Engineering, Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Feb 2021"",""2021"",""32"",""6"",""1465"",""1478"",""The mapping of DNA subsequences to a known reference genome, referred to as “short-read mapping”, is essential for next-generation sequencing. Hundreds of millions of short reads need to be aligned to a tremendously long reference sequence, making short-read mapping very time consuming. In this article, a high-throughput hardware accelerator is proposed so as to accelerate this task. A Bloom filter-based candidate mapping location (CML) generator and a folded processing element (PE) array are proposed to address CML selection and the Smith-Waterman (SW) alignment algorithm, respectively. It is shown that the proposed CML generator reduces the required memory access by 40 percent by employing a down-sampling scheme when compared to the Ferragina-Manzini index (FM-index) solution. The proposed hierarchical Bloom filter (HBF) that includes optimized parameters achieves a 1.5×104 times acceleration over the conventional Bloom filter. The proposed memory re-allocation scheme further reduces the memory access time for the HBF by a factor of 256. The proposed folded PE array delivers a 1.2-to-3.2 times higher giga cell updates per second (GCUPS). The processing time can be further reduced by 53-to-72 percent by employing a fully pipelined PE array that allows for a tailored shift amount for seeding. The accelerator is realized on a Stratix V GX FPGA with 16GB external SDRAM. Operated at 200MHz, the proposed FPGA accelerator delivers a 2.1-to-11 times higher throughput with the highest 99 percent accuracy and 98 percent sensitivity compared to the state-of-the-art FPGA-based solutions."",""1558-2183"","""",""10.1109/TPDS.2021.3051011"",""Ministry of Science and Technology(grant numbers:NSC99-2221-E-002-208-MY2,MOST 108-2218-E-002-060)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320533"",""DNA sequencing";Bloom filter;short-read mapping;Smith-Waterman alignment;"FPGA implementation"",""Genomics";Bioinformatics;Indexes;Field programmable gate arrays;DNA;Generators;"Sequential analysis"","""",""10"","""",""46"",""IEEE"",""12 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"A Hybrid Fuzzy Convolutional Neural Network Based Mechanism for Photovoltaic Cell Defect Detection With Electroluminescence Images,""C. Ge"; Z. Liu; L. Fang; H. Ling; A. Zhang;" C. Yin"",""College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China;" College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1653"",""1664"",""In the intelligent manufacturing process of solar photovoltaic (PV) cells, the automatic defect detection system using the Industrial Internet of Things (IIoT) smart cameras and sensors cooperated in IIoT has become a promising solution. Many works have been devoted to defect detection of PV cells in a data-driven way. However, because of the subjectivity and fuzziness of human annotation, the data contains a high quantity of noise and unpredictable uncertainties, which creates great difficulties in automatic defect detection. To address this problem, we propose a novel architecture named fuzzy convolution, which integrates fuzzy logic and convolution operations at microscopic level. Combining the proposed fuzzy convolution with the regular convolution, we build a network called Hybrid Fuzzy Convolutional Neural Network (HFCNN). Compared with convolutional neural networks (CNNs), HFCNN can address the uncertainties of PV cell data to improve the accuracy with fewer parameters, making it possible to apply our method in smart cameras. Experimental results on a public dataset show the superiority of our proposed method compared with CNNs."",""1558-2183"","""",""10.1109/TPDS.2020.3046018"",""National Natural Science Foundation of China(grant numbers:62032025,62076125,U20B2050)"; National Science Foundation for Post-doctoral Scientists of China(grant numbers:2019M651826); Natural Science Foundation of Jiangsu Province(grant numbers:BK20180421);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301240"",""Convolutional neural network";photovoltaic cell;defect detection;fuzzy logic;"fuzzy inference"",""Fuzzy logic";Convolution;Feature extraction;Uncertainty;Computer architecture;Deep learning;"Smart cameras"","""",""9"","""",""52"",""IEEE"",""21 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"A Machine-Learning-Based Framework for Productive Locality Exploitation,""E. Kayraklioglu"; E. Favry;" T. El-Ghazawi"",""Hewlett Packard Enterprise Company, San Jose, CA, USA"; Université Paris-Est, Champs-sur-Marne, France;" George Washington University, DC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Feb 2021"",""2021"",""32"",""6"",""1409"",""1424"",""Data locality is of extreme importance in programming distributed-memory architectures due to its implications on latency and energy consumption. Automated compiler and runtime system optimization studies have attempted to improve data locality exploitation without burdening the programmer. However, due to the difficulty of static code analysis, conservatism in compiler optimizations to avoid errors, and cost of dynamic analysis, the efficacy of automated optimizations is limited. Therefore, programmers need to spend significant effort in optimizing locality while creating applications for distributed memory parallel systems. We present a machine-learning based framework to automatically exploit locality in distributed memory applications. This framework takes application source whose time-critical blocks are marked by pragmas, and produces optimized source code that uses a regressor for efficient data movement. The regressor is trained with automatically-collected application profiles with very small input data sizes. We integrate our prototype in the Chapel language stack. In our experiments, we show that the Elastic Net model is the ideal regressor for our case and applications that utilize Elastic Net can perform very similarly to programmer-optimized versions. We also show that such regressors can be trained within few minutes on a cluster or within 30 minutes on a workstation, including data collection."",""1558-2183"","""",""10.1109/TPDS.2021.3051348"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9321696"",""Data locality";distributed memory;programming models;"machine learning"",""Optimization";Reactive power;Programming;Runtime;Program processors;Productivity;"Prefetching"","""",""2"","""",""49"",""IEEE"",""13 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Parallel Jacobi-Embedded Gauss-Seidel Method,""A. Ahmadi"; F. Manganiello; A. Khademi;" M. C. Smith"",""Holcombe Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA"; School of Mathematical and Statistical Sciences, Clemson University, Clemson, SC, USA.; Department of Industrial Engineering, Clemson University, Clemson, SC, USA;" Holcombe Department of Electrical and Computer Engineering, Clemson University, Clemson, SC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Feb 2021"",""2021"",""32"",""6"",""1452"",""1464"",""A broad range of scientific simulations involve solving large-scale computationally expensive linear systems of equations. Iterative solvers are typically preferred over direct methods when it comes to large systems due to their lower memory requirements and shorter execution times. However, selecting the appropriate iterative solver is problem-specific and dependent on the type and symmetry of the coefficient matrix. Gauss-Seidel (GS) is an iterative method for solving linear systems that are either strictly diagonally dominant or symmetric positive definite. This technique is an improved version of Jacobi and typically converges in fewer iterations. However, the sequential nature of this algorithm complicates the parallel extraction. In fact, most parallel derivatives of GS rely on the sparsity pattern of the coefficient matrix and require matrix reordering or domain decomposition. In this article, we introduce a new algorithm that exploits the convergence property of GS and adapts the parallel structure of Jacobi. The proposed method works for both dense and sparse systems and is straightforward to implement. We have examined the performance of our method on multicore and many-core architectures. Experimental results demonstrate the superior performance of the proposed algorithm compared with GS and Jacobi. Additionally, performance comparison with built-in Krylov solvers in MATLAB showed that in terms of time per iteration, Krylov methods perform faster on CPUs, but our approach is significantly better when executed on GPUs. Lastly, we apply our method to solve the power flow problem, and the results indicate a significant improvement in runtime, reaching up to 87 times faster speed compared with GS."",""1558-2183"","""",""10.1109/TPDS.2021.3052091"",""NSF-MRI(grant numbers:1725573)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325945"",""Linear systems";iterative methods;parallel;PJG;Gauss-Seidel;Jacobi;Krylov;SpMV performance;"power flow"",""Jacobian matrices";Convergence;Matrix decomposition;Sparse matrices;Mathematical model;Linear systems;"Multicore processing"","""",""6"","""",""41"",""IEEE"",""15 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"A Parallel Structured Divide-and-Conquer Algorithm for Symmetric Tridiagonal Eigenvalue Problems,""X. Liao"; S. Li; Y. Lu;" J. E. Roman"",""College of Computer Science, National University of Defense Technology, Changsha, China"; College of Computer Science, National University of Defense Technology, Changsha, China; National Supercomputer Center in Guangzhou, School of Data and Computer Science, Sun Yatsen University, Guangzhou, China;" D. Sistemes Informàtics i Computació, Universitat Politècnica de València, València, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Sep 2020"",""2021"",""32"",""2"",""367"",""378"",""In this article, a parallel structured divide-and-conquer (PSDC) eigensolver is proposed for symmetric tridiagonal matrices based on ScaLAPACK and a parallel structured matrix multiplication algorithm, called PSMMA. Computing the eigenvectors via matrix-matrix multiplications is the most computationally expensive part of the divide-and-conquer algorithm, and one of the matrices involved in such multiplications is a rank-structured Cauchy-like matrix. By exploiting this particular property, PSMMA constructs the local matrices by using generators of Cauchy-like matrices without any communication, and further reduces the computation costs by using a structured low-rank approximation algorithm. Thus, both the communication and computation costs are reduced. Experimental results show that both PSMMA and PSDC are highly scalable and scale to 4096 processes at least. PSDC has better scalability than PHDC that was proposed in [16] and only scaled to 300 processes for the same matrices. Comparing with PDSTEDC in ScaLAPACK, PSDC is always faster and achieves 1.4x-1.6x speedup for some matrices with few deflations. PSDC is also comparable with ELPA, with PSDC being faster than ELPA when using few processes and a little slower when using many processes."",""1558-2183"","""",""10.1109/TPDS.2020.3019471"",""National Natural Science Foundation of China(grant numbers:NNW2019ZT6-B20,NNW2019ZT6-B21,NNW2019ZT5-A10,U1611261,61872392,U1811461)"; National Key RD Program of China(grant numbers:2018YFB0204303); NSF of Hunan(grant numbers:2019JJ40339); NSF of NUDT(grant numbers:ZK18-03-01); Natural Science Foundation of Guangdong Province(grant numbers:2018B030312002); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); Spanish Agencia Estatal de Investigación(grant numbers:PID2019-107379RB-I00);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9177298"",""PSMMA";PUMMA algorithm;ScaLAPACK;divide-and-conquer;rank-structured matrix;"cauchy-like matrix"",""Approximation algorithms";Symmetric matrices;Generators;Eigenvalues and eigenfunctions;Matrix decomposition;Complexity theory;"Scalability"","""",""9"","""",""47"",""IEEE"",""25 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"A Probabilistic Machine Learning Approach to Scheduling Parallel Loops With Bayesian Optimization,""K. -R. Kim"; Y. Kim;" S. Park"",""Department of Electronics Engineering, Sogang University, Seoul, Republic of Korea"; Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea;" Department of Computer Science and Engineering, Sogang University, Seoul, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1815"",""1827"",""This article proposes Bayesian optimization augmented factoring self-scheduling (BO FSS), a new parallel loop scheduling strategy. BO FSS is an automatic tuning variant of the factoring self-scheduling (FSS) algorithm and is based on Bayesian optimization (BO), a black-box optimization algorithm. Its core idea is to automatically tune the internal parameter of FSS by solving an optimization problem using BO. The tuning procedure only requires online execution time measurement of the target loop. In order to apply BO, we model the execution time using two Gaussian process (GP) probabilistic machine learning models. Notably, we propose a locality-aware GP model, which assumes that the temporal locality effect resembles an exponentially decreasing function. By accurately modeling the temporal locality effect, our locality-aware GP model accelerates the convergence of BO. We implemented BO FSS on the GCC implementation of the OpenMP standard and evaluated its performance against other scheduling algorithms. Also, to quantify our method's performance variation on different workloads, or workload-robustness in our terms, we measure the minimax regret. According to the minimax regret, BO FSS shows more consistent performance than other algorithms. Within the considered workloads, BO FSS improves the execution time of FSS by as much as 22% and 5% on average."",""1558-2183"","""",""10.1109/TPDS.2020.3046461"",""National Research Foundation of Korea(grant numbers:2017M3C4A7080245)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303443"",""Parallel loop scheduling";Bayesian optimization;parallel computing;"OpenMP"",""Frequency selective surfaces";Task analysis;Heuristic algorithms;Dynamic scheduling;Optimization;Scheduling algorithms;"Copper"","""",""6"","""",""58"",""IEEE"",""22 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Quantum Approach Towards the Adaptive Prediction of Cloud Workloads,""A. K. Singh"; D. Saxena; J. Kumar;" V. Gupta"",""Department of Computer Applications, NIT Kurukshetra, Haryana, India"; Department of Computer Applications, NIT Kurukshetra, Haryana, India; Department of Computer Applications, NIT Tiruchirappalli, Tamilnadu, India;" Department of Electronics & Communication Engineering, NIT Kurukshetra, Haryana, India"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Jun 2021"",""2021"",""32"",""12"",""2893"",""2905"",""This work presents a novel Evolutionary Quantum Neural Network (EQNN) based workload prediction model for Cloud datacenter. It exploits the computational efficiency of quantum computing by encoding workload information into qubits and propagating this information through the network to estimate the workload or resource demands with enhanced accuracy proactively. The rotation and reverse rotation effects of the Controlled-NOT (C-NOT) gate serve activation function at the hidden and output layers to adjust the qubit weights. In addition, a Self Balanced Adaptive Differential Evolution (SB-ADE) algorithm is developed to optimize qubit network weights. The accuracy of the EQNN prediction model is extensively evaluated and compared with seven state-of-the-art methods using eight real world benchmark datasets of three different categories. Experimental results reveal that the use of the quantum approach to evolutionary neural network substantially improves the prediction accuracy up to 91.6 percent over the existing approaches."",""1558-2183"","""",""10.1109/TPDS.2021.3079341"",""National Institute of Technology Kurukshetra";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428529"",""Cloud computing";differential evolution;quantum neural network;"workload forecasting"",""Qubit";Predictive models;Neural networks;Hidden Markov models;Prediction algorithms;Logic gates;"Cloud computing"","""",""30"","""",""46"",""IEEE"",""11 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"A Resource and Performance Optimization Reduction Circuit on FPGAs,""L. Tang"; G. Cai; Y. Zheng;" J. Chen"",""Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China"; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China; Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China;" Aerospace Information Research Institute, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Sep 2020"",""2021"",""32"",""2"",""355"",""366"",""Reduce is a fundamental computing pattern, which is widely involved in scientific and engineering applications. For example, accumulation, the most common example of reduce pattern, is the core of applications such as dot product, matrix multiplication, and finite impulse response (FIR) filter. However, there is a trade-off between performance and area in the hardware implementation of the reduce pattern. To solve this problem, we propose an optimized reduction method that can handle multiple arbitrary-length sets. The performance of the proposed method is evaluated for both a single data set and numerous data sets. Moreover, to quickly differentiate the data of different sets in the reduction circuit, individual modules are designed to manage the data. We implement the design on FPGAs and present the experimental results. The proposed design with high performance and low resource consumption can achieve at least 1.59 times improvement on area-time product compared with the reported methods."",""1558-2183"","""",""10.1109/TPDS.2020.3020117"",""National Natural Science Foundation of China(grant numbers:61901440)"; Beijing Municipal Natural Science Foundation(grant numbers:4202080); Chinese Academy of Sciences;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9180054"",""Pipelined processors";vector reduction;accumulator;"field programmable gate arrays"",""Adders";Field programmable gate arrays;Merging;Clocks;Binary trees;Upper bound;"Time-frequency analysis"","""",""2"","""",""20"",""IEEE"",""28 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"A Runtime and Non-Intrusive Approach to Optimize EDP by Tuning Threads and CPU Frequency for OpenMP Applications,""J. Schwarzrock"; C. C. de Oliveira; M. Ritt; A. F. Lorenzon;" A. C. S. Beck"",""Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, RS, Brazil"; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, RS, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, RS, Brazil; Federal University of Pampa, Bagé, RS, Brazil;" Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, RS, Brazil"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1713"",""1724"",""Efficiently exploiting thread-level parallelism has been challenging. Many parallel applications are not sufficiently balanced or CPU-bound to take advantage of the increasing number of cores and the highest possible operating frequency. Moreover, many variables may change according to the system (input set, microarchitecture, and number of cores) or during execution, influencing each parallel region in different ways. Therefore, the task of rightly choosing the ideal configuration (number of threads and DVFS) for each parallel region to deliver the best Energy-Delay Product (EDP) is not straightforward. While the significant number of variables prevents the use of exhaustive search methods, the changing nature of the problem precludes offline strategies. Few solutions are online and synergistically consider thread throttling and DVFS. However, they lack transparency (demand changes in the original code) and/or adaptability (do not automatically adjust to applications at run-time). Our proposed Hoder covers all the characteristics above, optimizing at run-time any dynamically linked OpenMP application, without requiring any code transformation or recompilation. We show Hoder's efficiency by comparing it to two exhaustive offline and two online search approaches, three state-of-the-art techniques, and regular OpenMP execution, considering different setups (Intel 44-, 16- and 12-core";" AMD 8- and 12-core)."",""1558-2183"","""",""10.1109/TPDS.2020.3046537"",""Coordenação de Aperfeiçoamento de Pessoal de Nível Superior";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303387"",""Thread throttling";DVFS;"OpenMP applications"",""Libraries";Runtime;Message systems;Minimization;Instruction sets;Tuning;"Software"","""",""13"","""",""44"",""IEEE"",""22 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"A Scalable Multi-Layer PBFT Consensus for Blockchain,""W. Li"; C. Feng; L. Zhang; H. Xu; B. Cao;" M. A. Imran"",""James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom"; James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom; James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom; James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom; Beijing University of Posts and Telecommunications, Beijing, China;" James Watt School of Engineering, University of Glasgow, Glasgow, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Dec 2020"",""2021"",""32"",""5"",""1146"",""1160"",""Practical Byzantine Fault Tolerance (PBFT) consensus mechanism shows a great potential to break the performance bottleneck of the Proof-of-Work (PoW)-based blockchain systems, which typically support only dozens of transactions per second and require minutes to hours for transaction confirmation. However, due to frequent inter-node communications, PBFT mechanism has a poor node scalability and thus it is typically adopted in small networks. To enable PBFT in large systems such as massive Internet of Things (IoT) ecosystems and blockchain, in this article, a scalable multi-layer PBFT-based consensus mechanism is proposed by hierarchically grouping nodes into different layers and limiting the communication within the group. We first propose an optimal double-layer PBFT and show that the communication complexity is significantly reduced. Specifically, we prove that when the nodes are evenly distributed within the sub-groups in the second layer, the communication complexity is minimized. The security threshold is analyzed based on faulty probability determined (FPD) and faulty number determined (FND) models, respectively. We also provide a practical protocol for the proposed double-layer PBFT system. Finally, the results are extended to arbitrary-layer PBFT systems with communication complexity and security analysis. Simulation results verify the effectiveness of the analytical results."",""1558-2183"","""",""10.1109/TPDS.2020.3042392"",""UK EPSRC(grant numbers:EP/S02476X/1)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9279277"",""PBFT";communication complexity;node scalability;consensus mechanism;"blockchain"",""Blockchain";Complexity theory;Scalability;Security;Peer-to-peer computing;Throughput;"Internet of Things"","""",""154"","""",""34"",""CCBY"",""3 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"A Scalable Platform for Distributed Object Tracking Across a Many-Camera Network,""A. Khochare"; A. Krishnan;" Y. Simmhan"",""Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India"; Indian Institute of Science, Bangalore, India;" Department of Computational and Data Sciences, Indian Institute of Science, Bangalore, India"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Feb 2021"",""2021"",""32"",""6"",""1479"",""1493"",""Advances in deep neural networks (DNN) and computer vision (CV) algorithms have made it feasible to extract meaningful insights from large-scale deployments of urban cameras. Tracking an object of interest across the camera network in near real-time is a canonical problem. However, current tracking platforms have two key limitations: 1) They are monolithic, proprietary and lack the ability to rapidly incorporate sophisticated tracking models, and 2) They are less responsive to dynamism across wide-area computing resources that include edge, fog, and cloud abstractions. We address these gaps using Anveshak, a runtime platform for composing and coordinating distributed tracking applications. It provides a domain-specific dataflow programming model to intuitively compose a tracking application, supporting contemporary CV advances like query fusion and re-identification, and enabling dynamic scoping of the camera network's search space to avoid wasted computation. We also offer tunable batching and data-dropping strategies for dataflow blocks deployed on distributed resources to respond to network and compute variability. These balance the tracking accuracy, its real-time performance, and the active camera-set size. We illustrate the concise expressiveness of the programming model for four tracking applications. Our detailed experiments for a network of 1000 camera-feeds on modest resources exhibit the tunable scalability, performance, and quality trade-offs enabled by our dynamic tracking, batching, and dropping strategies."",""1558-2183"","""",""10.1109/TPDS.2021.3049450"",""Ministry of Electronic and Information Technology(grant numbers:4(16)/2019-ITEA)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314091"",""Big data platform";edge and fog computing;video analytics;distributed stream processing;"Internet of Things"",""Cameras";Streaming media;Urban areas;Tracking;Cloud computing;Target tracking;"Scalability"","""",""10"","""",""49"",""IEEE"",""5 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"A Scalable Stateful Approach for Virtual Security Functions Orchestration,""N. Moradi"; A. Shameli-Sendi;" A. Khajouei"",""Faculty of Computer Science and Engineering, Shahid Beheshti University (SBU), Tehran, Iran"; Faculty of Computer Science and Engineering, Shahid Beheshti University (SBU), Tehran, Iran;" Faculty of Computer Science and Engineering, Shahid Beheshti University (SBU), Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Jan 2021"",""2021"",""32"",""6"",""1383"",""1394"",""Previous works suggested different approaches to implementing service chaining. Their goal is to enhance the performance of the middleboxes and satisfy the expectations of the cloud providers and users. To meet these expectations, the delay factor, i.e., flow through the low-cost paths, as well as the best node processing factor, are considered. Achieving these two goals simultaneously turns the middlebox optimal placement into an NP-hard problem. Therefore, when the problem size is large, it is infeasible to obtain an optimal solution at a reasonable time. One of the important issues which has not been considered in the previous works is stateful optimal placement when receiving a new request. Due to resource constraints as well as financial costs for the customers, it is not possible to create functions for all requests. Therefore, not only it is possible to integrate the same network functions between new flows, but it will also be examined between new on-demand network functions as well as existing ones. Our proposed approach not only reduces the creation of network functions that can be cost-effective for the customer but also because of the migration of previous network functions (integration with on-demand network functions) to optimize new requests, overall, it will optimize the entire network cost over time. We formulated the problem as 0-1 programming problem. The results of this article are based on a fat-tree data center. To show that our stateful solution is scalable in large networks, we use network zoning and topology partitioning heuristics. Our simulations show that we were able to scale our placement model to a network with 54K nodes and 1.5M edges."",""1558-2183"","""",""10.1109/TPDS.2021.3049804"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316976"",""NFV";security functions;service chaining;optimal placement;"stateful placement"",""Middleboxes";Security;Delays;Cloud computing;Topology;Network topology;"Scalability"","""",""6"","""",""31"",""IEEE"",""8 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Split Execution Model for SpTRSV,""N. Ahmad"; B. Yilmaz;" D. Unat"",""Department of Computer Science and Engineering, Koç University, Istanbul, Turkey"; Department of Computer Science, Istinye University, Istanbul, Turkey;" Department of Computer Science and Engineering, Koç University, Istanbul, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""24 May 2021"",""2021"",""32"",""11"",""2809"",""2822"",""Sparse Triangular Solve (SpTRSV) is an important and extensively used kernel in scientific computing. Parallelism within SpTRSV depends upon matrix sparsity pattern and, in many cases, is non-uniform from one computational step to the next. In cases where the SpTRSV computational steps have contrasting parallelism characteristics- some steps are more parallel, others more sequential in nature, the performance of an SpTRSV algorithm may be limited by the contrasting parallelism characteristics. In this work, we propose a split-execution model for SpTRSV to automatically divide SpTRSV computation into two sub-SpTRSV systems and an SpMV, such that one of the sub-SpTRSVs has more parallelism than the other. Each sub-SpTRSV is then computed using different SpTRSV algorithms, which are possibly executed on different platforms (CPU or GPU). By analyzing the SpTRSV Directed Acyclic Graph (DAG) and matrix sparsity features, we use a heuristics-based approach to (i) automatically determine the suitability of an SpTRSV for split-execution, (ii) find the appropriate split-point, and (iii) execute SpTRSV in a split fashion using two SpTRSV algorithms while managing any required inter-platform communication. Experimental evaluation of the execution model on two CPU-GPU machines with a matrix dataset of 327 matrices from the SuiteSparse Matrix Collection shows that our approach correctly selects the fastest SpTRSV method (split or unsplit) for 88 percent of matrices on the Intel Xeon Gold (6148) + NVIDIA Tesla V100 and 83 percent on the Intel Core I7 + NVIDIA G1080 Ti platform achieving speedups up to 10x and 6.36x respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3074501"",""Aramco Overseas Company"; Saudi Aramco;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9409717"",""Sparse triangular solve";CPU-GPU computing;heterogeneous computing;sparse linear systems;SpTRSV;"SpTS"",""Sparse matrices";Parallel algorithms;Computational modeling;Kernel;Graphics processing units;Fats;"Phased arrays"","""",""4"","""",""45"",""IEEE"",""20 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"A Survey of System Architectures and Techniques for FPGA Virtualization,""M. H. Quraishi"; E. B. Tavakoli;" F. Ren"",""School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA"; School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA;" School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Mar 2021"",""2021"",""32"",""9"",""2216"",""2230"",""FPGA accelerators are gaining increasing attention in both cloud and edge computing because of their hardware flexibility, high computational throughput, and low power consumption. However, the design flow of FPGAs often requires specific knowledge of the underlying hardware, which hinders the wide adoption of FPGAs by application developers. Therefore, the virtualization of FPGAs becomes extremely important to create a useful abstraction of the hardware suitable for application developers. Such abstraction also enables the sharing of FPGA resources among multiple users and accelerator applications, which is important because, traditionally, FPGAs have been mostly used in single-user, single-embedded-application scenarios. There are many works in the field of FPGA virtualization covering different aspects and targeting different application areas. In this article, we review the system architectures used in the literature for FPGA virtualization. In addition, we identify the primary objectives of FPGA virtualization, based on which we summarize the techniques for realizing FPGA virtualization. This article helps researchers to efficiently learn about FPGA virtualization research by providing a comprehensive review of the existing literature."",""1558-2183"","""",""10.1109/TPDS.2021.3063670"",""Cisco Research Center(grant numbers:CG#1490376)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369140"",""FPGA";virtualization;architecture;accelerator;"reconfiguration"",""Field programmable gate arrays";Virtualization;Hardware;Cloud computing;Computer architecture;Edge computing;"Systems architecture"","""",""15"","""",""84"",""IEEE"",""3 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"A Thread Level SLO-Aware I/O Framework for Embedded Virtualization,""X. Gong"; D. Cao; Y. Li; X. Liu; Y. Li; J. Zhang;" T. Li"",""College of Computer Science, Nankai University, Tianjin, China"; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China;" College of Computer Science, Nankai University, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2020"",""2021"",""32"",""3"",""500"",""513"",""With the development of virtualization technology, it is practical and necessary to integrate virtual machine software into embedded systems. I/O scheduling is important for embedded systems, because embedded systems always face different situations and their requests have more diversity on the requirement of real-time and importance. However, the semantic information associated with the I/O data is completely lost when crossing the virtualized I/O software stack. Here, we present an I/O scheduling framework to connect the semantic gap between the application threads in virtual machines and hardware schedulers in the host machine. Therefore, the details for the I/O request can be passed through the layers of the software stack and each layer can get the specific information about the device environment. Also, various scheduling points have been provided to implement different I/O strategies. Our framework was implemented based on Linux operating system, KVM, QEMU and virtio protocol. A prototype scheduler, Orthrus, was implemented to evaluate the effectiveness of the framework. Comprehensive experiments were conducted and the results show that our framework can guarantee the real-time requirements, and reserve more system resources for critical tasks, with negligible memory consumption and throughput overhead."",""1558-2183"","""",""10.1109/TPDS.2020.3026042"",""National Key Research and Development Program of China(grant numbers:2018YFB1003405)"; National Natural Science Foundation of China(grant numbers:61702286); Natural Science Foundation of Tianjin, China(grant numbers:18JCYBJC15600);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9204842"",""Embedded system";I/O virtualization;I/O scheduling;SLO-Aware;"semantic gap"",""Virtualization";Semantics;Real-time systems;Virtual machining;Hardware;"Embedded systems"","""",""2"","""",""36"",""IEEE"",""23 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"A Two-Phase Dynamic Throughput Optimization Model for Big Data Transfers,""M. S. Q. Z. Nine";" T. Kosar"",""Department of Computer Science and Engineering, University at Buffalo, Buffalo, USA";" Department of Computer Science and Engineering, University at Buffalo, Buffalo, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Aug 2020"",""2021"",""32"",""2"",""269"",""280"",""The amount of data transferred over dedicated and non-dedicated network links has been increasing much faster than the increase in the network capacity. On the other hand, the current data transfer solutions fail to guarantee even the promised achievable transfer throughput. In this article, we propose a novel two-phase dynamic throughput optimization model based on mathematical modeling with offline knowledge discovery/analysis and adaptive online decision making. In the offline analysis, we mine historical transfer logs to perform knowledge discovery about the transfer characteristics. The online phase uses the discovered knowledge from the offline analysis along with the real-time investigation of the network condition to optimize the protocol parameters. As the real-time investigation is expensive and provides partial knowledge about the current network status, our model uses historical knowledge about the network and data characteristics to reduce the real-time investigation overhead while ensuring near-optimal throughput for each transfer. Our novel approach is tested over different networks with different datasets, and it has outperformed its closest competitor by 1.7x and the default case by 5x. It also achieved up to 93 percent accuracy compared to the optimal achievable throughput possible on those networks."",""1558-2183"","""",""10.1109/TPDS.2020.3012929"",""National Science Foundation(grant numbers:OAC-1724898)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9153004"",""Throughput optimization";big data transfers;offline analysis;dynamic learning;"protocol tuning"",""Throughput";Protocols;Data transfer;Data models;Bandwidth;Optimization;"Real-time systems"","""",""5"","""",""38"",""IEEE"",""31 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Unified Framework for Flexible Playback Latency Control in Live Video Streaming,""G. Zhang"; J. Y. B. Lee; K. Liu; H. Hu;" V. Aggarwal"",""Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Kowloon, Hong Kong"; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, NT, Hong Kong; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Electronic and Information Engineering, The Hong Kong Polytechnic University, Kowloon, Hong Kong;" School of Industrial Engineering, Purdue University, West Lafayette, IN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jun 2021"",""2021"",""32"",""12"",""3024"",""3037"",""Live video streaming has seen tremendous growth in the past decade. An important fact in live streaming is that the demand for low playback-latency inherently conflicts with the desire for high QoE. This requires different types of live services to seek different latency-QoE tradeoffs according to their service-requirements. However, our investigations revealed that it is fundamentally difficult for existing streaming algorithms to keep consistent latency in changing network conditions, let alone achieve the service-desired latency-QoE tradeoff. To tackle the challenge, this article develops a novel framework called Flexible Latency Aware Streaming (FLAS) that not only can achieve consistent low latency, but also control the latency-QoE tradeoff flexibly. Specifically, FLAS generates a set of adaptation logics offline, each optimized for a candidate tradeoff point, then selects the most appropriate one to run online. We first show how FLAS can be applied to optimizing the existing algorithms, then developed a novel Genetic Programming approach to fully exploit FLAS's potential. Extensive evaluations show that FLAS can precisely control latency all the way down to 1s and achieve substantially higher QoE than state-of-the-arts. FLAS can be readily implemented into real streaming platforms, offering a practical and reliable solution for live-streaming services."",""1558-2183"","""",""10.1109/TPDS.2021.3083202"",""Centre for Advances in Reliability and Safety Limited"; National Natural Science Foundation of China(grant numbers:62072439); National Key Research and Development Program of China(grant numbers:2016YFB1000200); Natural Science Foundation of Shandong Province(grant numbers:ZR2019LZH004); Beijing Municipal Natural Science Foundation(grant numbers:4212028); State Key Laboratory of Computer Architecture Innovation Fund(grant numbers:carch4503);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439873"",""Video streaming";genetic programming;quality-of-experience;"video reliability"",""Streaming media";Quality of experience;Throughput;Bit rate;3G mobile communication;"Video recording"","""",""2"","""",""43"",""IEEE"",""24 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Accelerating Binarized Neural Networks via Bit-Tensor-Cores in Turing GPUs,""A. Li";" S. Su"",""High-Performance Computing Group, Pacific Northwest National Laboratory (PNNL), Richland, WA, USA";" U.S. Army Research Laboratory (ARL), DoD Supercomputing Resource Center, Aberdeen Proving Ground, MD, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1878"",""1891"",""Despite foreseeing tremendous speedups over conventional deep neural networks, the performance advantage of binarized neural networks (BNNs) has merely been showcased on general-purpose processors such as CPUs and GPUs. In fact, due to being unable to leverage bit-level-parallelism with a word-based architecture, GPUs have been criticized for extremely low utilization (1 percent) when executing BNNs. Consequently, the latest tensorcores in NVIDIA Turing GPUs start to experimentally support bit computation. In this article, we look into this brand new bit computation capability and characterize its unique features. We show that the stride of memory access can significantly affect performance delivery and a data-format co-design is highly desired to support the tensorcores for achieving superior performance than existing software solutions without tensorcores. We realize the tensorcore-accelerated BNN design, particularly the major functions for fully-connect and convolution layers - bit matrix multiplication and bit convolution. Evaluations on two NVIDIA Turing GPUs show that, with ResNet-18, our BTC-BNN design can process ImageNet at a rate of 5.6K images per second, 77 percent faster than state-of-the-art. Our BNN approach is released on https://github.com/pnnl/TCBNN."",""1558-2183"","""",""10.1109/TPDS.2020.3045828"",""PNNL's DMC-CFA"; DS-HPC LDRD projects; U.S. DOE SC, ASCR(grant numbers:66150);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303370"","""",""Graphics processing units";Hardware;Convolution;Synchronization;Tensors;Libraries;"Field programmable gate arrays"","""",""13"","""",""75"",""IEEE"",""22 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating End-to-End Deep Learning Workflow With Codesign of Data Preprocessing and Scheduling,""Y. Cheng"; D. Li; Z. Guo; B. Jiang; J. Geng; W. Bai; J. Wu;" Y. Xiong"",""Tsinghua University, Beijing, China"; Tsinghua University, Beijing, China; Beihang University, Beijing, China; Microsoft Research, Beijing, China; Tsinghua University, Beijing, China; Microsoft Research, Beijing, China; Tsinghua University, Beijing, China;" Microsoft Research, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1802"",""1814"",""In this article, we investigate the performance bottleneck of existing deep learning (DL) systems and propose DLBooster to improve the running efficiency of deploying DL applications on GPU clusters. At its core, DLBooster leverages two-level optimizations to boost the end-to-end DL workflow. On the one hand, DLBooster selectively offloads some key decoding workloads to FPGAs to provide high-performance online data preprocessing services to the computing engine. On the other hand, DLBooster reorganizes the computational workloads of training neural networks with the backpropagation algorithm and schedules them according to their dependencies to improve the utilization of GPUs at runtime. Based on our experiments, we demonstrate that compared with baselines, DLBooster can improve the image processing throughput by 1.4× - 2.5× and reduce the processing latency by 1/3 in several real-world DL applications and datasets. Moreover, DLBooster consumes less than 1 CPU core to manage FPGA devices at runtime, which is at least 90 percent less than the baselines in some cases. DLBooster shows its potential to accelerate DL workflows in the cloud."",""1558-2183"","""",""10.1109/TPDS.2020.3047966"",""National Key Research and Development Program of China(grant numbers:2018YFB1800500)"; Key Areas of Guangdong Province(grant numbers:2018B010113001); National Natural Science Foundation of China(grant numbers:61772305);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9310194"",""Deep learning";data preprocessing;workload offloading;computation scheduling;"FPGAs"",""Data preprocessing";Training;Field programmable gate arrays;Task analysis;Graphics processing units;Artificial neural networks;"Hardware"","""",""4"","""",""62"",""IEEE"",""29 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Accelerating Federated Learning Over Reliability-Agnostic Clients in Mobile Edge Computing Systems,""W. Wu"; L. He; W. Lin;" R. Mao"",""Department of Computer Science, University of Warwick, Coventry, U.K."; Department of Computer Science, University of Warwick, Coventry, U.K.; School of Computer Science and Engineering, South China University of Technology, Guangzhou, Guangdong Province, China;" College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong Province, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1539"",""1551"",""Mobile Edge Computing (MEC), which incorporates the Cloud, edge nodes, and end devices, has shown great potential in bringing data processing closer to the data sources. Meanwhile, Federated learning (FL) has emerged as a promising privacy-preserving approach to facilitating AI applications. However, it remains a big challenge to optimize the efficiency and effectiveness of FL when it is integrated with the MEC architecture. Moreover, the unreliable nature (e.g., stragglers and intermittent drop-out) of end devices significantly slows down the FL process and affects the global model's quality in such circumstances. In this article, a multi-layer federated learning protocol called HybridFL is designed for the MEC architecture. HybridFL adopts two levels (the edge level and the cloud level) of model aggregation enacting different aggregation strategies. Moreover, in order to mitigate stragglers and end device drop-out, we introduce regional slack factors into the stage of client selection performed at the edge nodes using a probabilistic approach without identifying or probing the state of end devices (whose reliability is agnostic). We demonstrate the effectiveness of our method in modulating the proportion of clients selected and present the convergence analysis for our protocol. We have conducted extensive experiments with machine learning tasks in different scales of MEC system. The results show that HybridFL improves the FL training process significantly in terms of shortening the federated round length, speeding up the global model's convergence (by up to 12×) and reducing end device energy consumption (by up to 58 percent)."",""1558-2183"","""",""10.1109/TPDS.2020.3040867"",""Worldwide Byte Security Information Technology Company Ltd."; Guangdong Project(grant numbers:2018B030325002); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164003); Guangzhou Science and Technology Program key projects(grant numbers:202007040002,201902010040); Guangzhou Development Zone Science and Technology(grant numbers:2018GH17);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272671"",""Federated learning";mobile edge computing;distributed computing;"machine learning"",""Protocols";Data models;Training;Cloud computing;Performance evaluation;Distributed databases;"Computer architecture"","""",""38"","""",""25"",""IEEE"",""26 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Accelerating Gossip-Based Deep Learning in Heterogeneous Edge Computing Platforms,""R. Han"; S. Li; X. Wang; C. H. Liu; G. Xin;" L. Y. Chen"",""Beijing Institute of Technology, Beijing, P. R. China"; Beijing Institute of Technology, Beijing, P. R. China; Beijing Institute of Technology, Beijing, P. R. China; Beijing Institute of Technology, Beijing, P. R. China; Beijing Institute of Technology, Beijing, P. R. China;" Delft University of Technology. Mekelweg 5, Delft, The Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1591"",""1602"",""With the exponential growth of data created at the network edge, decentralized and Gossip-based training of deep learning (DL) models on edge computing (EC) gains tremendous research momentum, owing to its capability to learn from resource-strenuous edge nodes with limited network connectivity. Today's edge devices are extremely heterogeneous, e.g., hardware and software stacks, and result in high performance variation of training time and inducing extra delay to synchronize and converge. The large body of prior art accelerates DL, being data or model parallelization, via a centralized server, e.g., parameter server scheme, which may easily turn into the system bottleneck or single point of failure. In this artice, we propose EdgeGossip, a framework specifically designed to accelerate the training process of decentralized and Gossip-based DL training for heterogeneous EC platforms. EdgeGossip features on: (i) low performance variation among multiple EC platforms during iterative training, and (ii) accuracy-aware training to fastly obtain best possible model accuracy. We implement EdgeGossip based on popular Gossip algorithms and demonstrate its effectiveness using real-world DL workloads, i.e., considerably reducing model training time by an average of 2.70 times while only incurring accuracy losses of 0.78 percent."",""1558-2183"","""",""10.1109/TPDS.2020.3046440"",""National Key Research and Development Plan of China(grant numbers:2018YFB1003701,2018YFB1003700)"; National Natural Science Foundation of China(grant numbers:61872337); Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung(grant numbers:407540_167266);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303468"",""Deep learning";decentralized training;gossip;"edge computing"",""Training";Data models;Computational modeling;Peer-to-peer computing;Distributed databases;Acceleration;"Servers"","""",""15"","""",""49"",""IEEE"",""22 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Accelerating Large-Scale Prioritized Graph Computations by Hotness Balanced Partition,""S. Gong"; Y. Zhang;" G. Yu"",""School of Computer Science and Engineering, Northeastern University, Shenyang, China"; School of Computer Science and Engineering, Northeastern University, Shenyang, China;" School of Computer Science and Engineering, Northeastern University, Shenyang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2020"",""2021"",""32"",""4"",""746"",""759"",""Prioritized computation is shown promising performance for a large class of graph algorithms. It prioritizes the execution of some vertices that play important roles in determining convergence. For large-scale distributed graph processing, graph partitioning is an important preprocessing step that aims to balance workload and to reduce communication costs between workers. However, existing graph partitioning methods are designed for round-robin synchronous distributed frameworks. They balance workload without distinction of vertex importance and fail to consider the characteristics of priority-based scheduling, which may limit the benefit of prioritized graph computation. In this article, to accelerate prioritized iterative graph computations, we propose Hotness Balanced Partition (HBP). In prioritized graph computation, high priority vertices are likely to be executed more frequently and are likely to pass more messages, which result in hot vertices. Based on this observation, we partition graph by distributing vertices with distinction according to their hotness rather than blindly distributing vertices with equal weights, aiming to evenly distribute the hot vertices among workers. We further provide two HBP algorithms: a streaming-based algorithm for efficient one-pass processing and a distributed algorithm for distributed processing. Our results show that our proposed partitioning methods outperform the state-of-the-art partitioning methods, Fennel, HotGraph, and SNE."",""1558-2183"","""",""10.1109/TPDS.2020.3032709"",""National Key R&D Program of China(grant numbers:2018YFB1003404)"; National Natural Science Foundation of China(grant numbers:62072082,61672141,U1811261); Fundamental Research Funds for the Central Universities(grant numbers:N181605017,N181604016); Key R&D Program of Liaoning Province(grant numbers:2020JH 2/10100037);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234696"",""Hotness balanced partition";graph partitioning;"distributed computing"",""Processor scheduling";Partitioning algorithms;Runtime;Scheduling;Computational modeling;Google;"Optimal scheduling"","""",""3"","""",""50"",""IEEE"",""21 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating the Bron-Kerbosch Algorithm for Maximal Clique Enumeration Using GPUs,""Y. -W. Wei"; W. -M. Chen;" H. -H. Tsai"",""Department of Electronic Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan"; Department of Electronic Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan;" Department of Electronic Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Apr 2021"",""2021"",""32"",""9"",""2352"",""2366"",""Maximal clique enumeration (MCE) is a classic problem in graph theory to identify all complete subgraphs in a graph. In prior MCE work, the Bron-Kerbosch algorithm is one of the most popular solutions, and there are several improved algorithms proposed on CPU platforms. However, while few studies have focused on the related issue of parallel implementation, recently, there have been numerous explorations of the acceleration of general purpose applications using a graphics processing unit (GPU) to reduce the computing power consumption. In this article, we develop a GPU-based Bron-Kerbosch algorithm that efficiently solves the MCE problem in parallel by optimizing the process of subproblem decomposition and computing resource usage. To speed up the computations, we use coalesced memory accesses and warp reductions to increase bandwidth and reduce memory latency. Our experimental results show that the proposed algorithm can fully exploit the resources of GPU architectures, allowing for the vast acceleration of operations to solve the MCE problem."",""1558-2183"","""",""10.1109/TPDS.2021.3067053"",""Ministry of Science Technology(grant numbers:MOST 107-2221-E-011-015-MY2)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9381690"",""Maximal clique";parallel computing;"GPU"",""Graphics processing units";Computer architecture;Time complexity;Task analysis;Central Processing Unit;Acceleration;"Upper bound"","""",""6"","""",""28"",""IEEE"",""18 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accurate Differentially Private Deep Learning on the Edge,""R. Han"; D. Li; J. Ouyang; C. H. Liu; G. Wang; D. Wu;" L. Y. Chen"",""Beijing Institute of Technology, Beijing, China"; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; University of Florida, Gainesville, FL, USA;" TU Delft, Delft, The Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Mar 2021"",""2021"",""32"",""9"",""2231"",""2247"",""Deep learning (DL) models are increasingly built on federated edge participants holding local data. To enable insight extractions without the risk of information leakage, DL training is usually combined with differential privacy (DP). The core theme is to tradeoff learning accuracy by adding statistically calibrated noises, particularly to local gradients of edge learners, during model training. However, this privacy guarantee unfortunately degrades model accuracy due to edge learners' local noises, and the global noise aggregated at the central server. Existing DP frameworks for edge focus on local noise calibration via gradient clipping techniques, overlooking the heterogeneity and dynamic changes of local gradients, and their aggregated impact on accuracy. In this article, we present a systematical analysis that unveils the influential factors capable of mitigating local and aggregated noises, and design PrivateDL to leverage these factors in noise calibration so as to improve model accuracy while fulfilling privacy guarantee. PrivateDL features on: (i) sampling-based sensitivity estimation for local noise calibration and (ii) combining large batch sizes and critical data identification in global training. We implement PrivateDL on the popular Laplace/Gaussian DP mechanisms and demonstrate its effectiveness using Intel BigDL workloads, i.e., considerably improving model accuracy by up to 5X when comparing against existing DP frameworks."",""1558-2183"","""",""10.1109/TPDS.2021.3064345"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9372811"",""Deep learning";differential privacy;federated learning;"model accuracy"",""Training";Privacy;Data models;Sensitivity;Differential privacy;Biological system modeling;"Servers"","""",""5"","""",""65"",""IEEE"",""8 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Achieving Fine-Grained Flow Management Through Hybrid Rule Placement in SDNs,""G. Zhao"; H. Xu; J. Fan; L. Huang;" C. Qiao"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science & Engineering, University at Buffalo, Buffalo, NY, USA; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China;" Department of Computer Science & Engineering, University at Buffalo, Buffalo, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2020"",""2021"",""32"",""3"",""728"",""742"",""Fine-grained flow management is useful in many practical applications, e.g., resource allocation, anomaly detection and traffic engineering. However, it is difficult to provide fine-grained management for a large number of flows in SDNs due to switches' limited flow table capacity. While using wildcard rules can reduce the number of flow entries needed, it cannot fully ensure fine-grained management for all the flows without degrading application performance. In this article, we design and implement hybrid rule placement for fine-grained flow management (to be referred to as HiFi here after). HiFi achieves fine-grained management with a minimal number of flow entries through taking a two-step approach: wildcard entry installment and application-specific exact-match entry installment. How to optimally install wildcard and exact-match flow entries, however, is intractable. Therefore, we design approximation algorithms with bounded factors to solve these problems. We consider how to achieve network-wide load balancing via fine-grained flow management as a case study. Both experiment on a testbed built with open virtual switches and extensive simulation show that HiFi can reduce the number of required flow entries by about 45-69 percent and reduce the control overhead by about 28-50 percent compared with the state-of-the-art approaches for achieving fine-grained flow management."",""1558-2183"","""",""10.1109/TPDS.2020.3030630"",""National Science Foundation of China(grant numbers:61822210,61936015,U1709217)"; Anhui Initiative in Quantum Information Technologies(grant numbers:AHY150300);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222349"",""Software defined networks";fine-grained management;wildcard entry;exact-match entry;"approximation"",""Control systems";Software;Approximation algorithms;Resource management;Monitoring;Mice;"Erbium"","""",""14"","""",""55"",""IEEE"",""13 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Achieving Probabilistic Atomicity With Well-Bounded Staleness and Low Read Latency in Distributed Datastores,""L. Ouyang"; Y. Huang; H. Wei;" J. Lu"",""Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, China"; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, China;" Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Nov 2020"",""2021"",""32"",""4"",""815"",""829"",""Although it has been commercially successful to deploy weakly consistent but highly-responsive distributed datastores, the tension between developing complex applications and obtaining only weak consistency guarantees becomes more and more severe. The almost strong consistency tradeoff aims at achieving both strong consistency and low latency in the common case. In distributed storage systems, we investigate the generic notion of almost strong consistency in terms of designing fast read algorithms while guaranteeing Probabilistic Atomicity with well-Bounded staleness (PAB). This problem has been explored in the case where only one client can write the data. However, the more general case where multiple clients can write the data has not been studied. In this article, we study the fast read algorithm for PAB in the multi-writer case. We show the bound of data staleness and the probability of atomicity violation by decomposing inconsistent reads into the read inversion and the write inversion patterns. We implement the fast read algorithm and evaluate the consistency-latency tradeoffs based on the instrumentation of Cassandra and the YCSB benchmark framework. The theoretical analysis and the experimental evaluations show that our fast read algorithm guarantees PAB, even when faced with dynamic changes in the computing environment."",""1558-2183"","""",""10.1109/TPDS.2020.3034328"",""National Natural Science Foundation of China(grant numbers:61932021,61772258)"; Fundamental Research Funds for the Central Universities(grant numbers:14380063); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9242251"",""Probabilistic atomicity";well-bounded staleness;fast read algorithm;"quorum-replicated datastore"",""Registers";Heuristic algorithms;Servers;History;Probabilistic logic;Distributed databases;"Collaboration"","""","""","""",""68"",""CCBY"",""28 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Adaptive Preference-Aware Co-Location for Improving Resource Utilization of Power Constrained Datacenters,""P. Pang"; Q. Chen; D. Zeng;" M. Guo"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science, China University of Geosciences, Wuhan, China;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Sep 2020"",""2021"",""32"",""2"",""441"",""456"",""Large-scale datacenters often host latency-sensitive services that have stringent Quality-of-Service requirement and experience diurnal load pattern. Co-locating best-effort applications that have no QoS requirement with the latency-sensitive services has been widely used to improve the resource utilization of datacenters with careful shared resource management. However, existing co-location techniques tend to result in the power overload problem on power constrained servers due to the ignorance of the power consumption. To this end, we propose Sturgeon, a runtime system proactively manages resources between co-located applications in a power constrained environment, to ensure the QoS of latency-sensitive services while maximizing the throughput of best-effort applications. Our investigation shows that, at a given load, there are multiple feasible resource configurations to meet both QoS requirement and power budget, while one of them yields the maximum throughput of best-effort applications. To find such a configuration, we establish models to accurately predict the performance and power consumption of the co-located applications. Sturgeon monitors the QoS of the services periodically, in order to eliminate the potential QoS violation caused by the unpredictable interference. Besides, when the datacenter hosts different types of applications to perform co-location, Sturgeon places applications with their preferable candidates to improve the overall throughput. The experimental results show that at server level Sturgeon improves the throughput of the best-effort application by 25.43 percent compared to the state-of-the-art technique, while guaranteeing the 95%-ile latency within the QoS target";" at cluster level, Sturgeon improves the overall throughput of best-effort applications by 13.74 percent compared to the baseline."",""1558-2183"","""",""10.1109/TPDS.2020.3023997"",""National R&D Program of China(grant numbers:2018YFB1004800)"; National Natural Science Foundation of China(grant numbers:61632017,61772480,61872240,61832006,61702328);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197666"",""QoS";improved utilization;"power constrained datacenters"",""Quality of service";Throughput;Resource management;Power demand;Servers;Load modeling;"Runtime"","""",""5"","""",""49"",""IEEE"",""15 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Adaptive SpMV/SpMSpV on GPUs for Input Vectors of Varied Sparsity,""M. Li"; Y. Ao;" C. Yang"",""Chinese Academy of Sciences, Institute of Software, Beijing, China"; School of Mathematical Sciences, Peking University, Beijing, China;" School of Mathematical Sciences, Peking University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1842"",""1853"",""Despite numerous efforts for optimizing the performance of Sparse Matrix and Vector Multiplication (SpMV) on modern hardware architectures, few works are done to its sparse counterpart, Sparse Matrix and Sparse Vector Multiplication (SpMSpV), not to mention dealing with input vectors of varied sparsity. The key challenge is that depending on the sparsity levels, distribution of data, and compute platform, the optimal choice of SpMV/SpMSpV kernel can vary, and a static choice does not suffice. In this article, we propose an adaptive SpMV/SpMSpV framework, which can automatically select the appropriate SpMV/SpMSpV kernel on GPUs for any sparse matrix and vector at the runtime. Based on systematic analysis on key factors such as computing pattern, workload distribution and write-back strategy, eight candidate SpMV/SpMSpV kernels are encapsulated into the framework to achieve high performance in a seamless manner. A comprehensive study on machine learning-based kernel selector is performed to choose the kernel and adapt with the varieties of both the input and hardware from both accuracy and overhead perspectives. Experiments demonstrate that the adaptive framework can substantially outperform the previous state-of-the-art in real-world applications on NVIDIA Tesla K40m, P100, and V100 GPUs."",""1558-2183"","""",""10.1109/TPDS.2020.3040150"",""National Key R&D Plan of China(grant numbers:2016YFB0200603)"; Guangdong Key-Area R&D Program(grant numbers:2019B121204008); Beijing Natural Science Foundation(grant numbers:JQ18001); Beijing Academy of Artificial Intelligence;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268964"",""Sparse matrix and vector multiplication (SpMV)";sparse matrix and sparse vector multiplication (SpMSpV);GPU computing;adaptive performance optimization;"machine learning"",""Kernel";Sparse matrices;Hardware;Machine learning;Computational modeling;Adaptation models;"Runtime"","""",""3"","""",""49"",""IEEE"",""24 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"ADRL: A Hybrid Anomaly-Aware Deep Reinforcement Learning-Based Resource Scaling in Clouds,""S. Kardani-Moghaddam"; R. Buyya;" K. Ramamohanarao"",""Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia"; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia;" Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, The University of Melbourne, Parkville, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Oct 2020"",""2021"",""32"",""3"",""514"",""526"",""The virtualization concept and elasticity feature of cloud computing enable users to request resources on-demand and in the pay-as-you-go model. However, the high flexibility of the model makes the on-time resource scaling problem more complex. A variety of techniques such as threshold-based rules, time series analysis, or control theory are utilized to increase the efficiency of dynamic scaling of resources. However, the inherent dynamicity of cloud-hosted applications requires autonomic and adaptable systems that learn from the environment in real-time. Reinforcement Learning (RL) is a paradigm that requires some agents to monitor the surroundings and regularly perform an action based on the observed states. RL has a weakness to handle high dimensional state space problems. Deep-RL models are a recent breakthrough for modeling and learning in complex state space problems. In this article, we propose a Hybrid Anomaly-aware Deep Reinforcement Learning-based Resource Scaling (ADRL) for dynamic scaling of resources in the cloud. ADRL takes advantage of anomaly detection techniques to increase the stability of decision-makers by triggering actions in response to the identified anomalous states in the system. Two levels of global and local decision-makers are introduced to handle the required scaling actions. An extensive set of experiments for different types of anomaly problems shows that ADRL can significantly improve the quality of service with less number of actions and increased stability of the system."",""1558-2183"","""",""10.1109/TPDS.2020.3025914"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9203981"",""Cloud computing";anomaly detection;deep reinforcement learning;performance management;"vertical scaling"",""Cloud computing";Adaptation models;Resource management;Computational modeling;Quality of service;Dynamic scheduling;"Decision making"","""",""29"","""",""25"",""IEEE"",""22 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Automatic Synthesizer of Advising Tools for High Performance Computing,""H. Guan"; X. Shen;" H. Krim"",""University of Massachusetts Amherst, USA"; North Carolina State University, Raleigh, USA;" North Carolina State University, Raleigh, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Sep 2020"",""2021"",""32"",""2"",""330"",""341"",""This article presents Egeria, the first automatic synthesizer of advising tools for High-Performance Computing (HPC). When one provides it with some HPC programming guides as inputs, Egeria automatically constructs a text retrieval tool that can advise on what to do to improve the performance of a given program. The advising tool provides a concise list of essential rules automatically extracted from the documents and can retrieve relevant optimization knowledge for optimization questions. Egeria is built based on a distinctive multi-layered design that leverages natural language processing (NLP) techniques and extends them with HPC-specific knowledge and considerations. This article presents the design, implementation, and both quantitative and qualitative evaluation results of Egeria."",""1558-2183"","""",""10.1109/TPDS.2020.3018636"",""DOE Early Career Award(grant numbers:DE-SC0013700)"; National Science Foundation(grant numbers:1455404,1455733 (CAREER),1525609);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173796"",""Performance tools";natural language processing;"code optimization"",""Tools";Optimization;Programming;Syntactics;Semantics;Guidelines;"Natural language processing"","""",""3"","""",""47"",""IEEE"",""21 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Efficiency-Boosting Client Selection Scheme for Federated Learning With Fairness Guarantee,""T. Huang"; W. Lin; W. Wu; L. He; K. Li;" A. Y. Zomaya"",""School of Computer Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China"; School of Computer Science and Engineering, South China University of Technology, Guangzhou, Guangdong, China; Department of Computer Science, University of Warwick, Coventry, United Kingdom; Department of Computer Science, University of Warwick, Coventry, United Kingdom; Department of Computer Science, State University of New York, New Paltz, NY, USA;" School of Computer Science, The University of Sydney, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1552"",""1564"",""The issue of potential privacy leakage during centralized AI's model training has drawn intensive concern from the public. A Parallel and Distributed Computing (or PDC) scheme, termed Federated Learning (FL), has emerged as a new paradigm to cope with the privacy issue by allowing clients to perform model training locally, without the necessity to upload their personal sensitive data. In FL, the number of clients could be sufficiently large, but the bandwidth available for model distribution and re-upload is quite limited, making it sensible to only involve part of the volunteers to participate in the training process. The client selection policy is critical to an FL process in terms of training efficiency, the final model's quality as well as fairness. In this article, we will model the fairness guaranteed client selection as a Lyapunov optimization problem and then a C2MAB-based method is proposed for estimation of the model exchange time between each client and the server, based on which we design a fairness guaranteed algorithm termed RBCS-F for problem-solving. The regret of RBCS-F is strictly bounded by a finite constant, justifying its theoretical feasibility. Barring the theoretical results, more empirical data can be derived from our real training experiments on public datasets."",""1558-2183"","""",""10.1109/TPDS.2020.3040887"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164003)"; National Natural Science Foundation of China(grant numbers:62072187,61872084,61772205); Guangdong Major Project of Basic and Applied Basic Research(grant numbers:2019B030302002); Guangzhou Science and Technology Program key projects(grant numbers:202007040002,201902010040,201907010001); Fundamental Research Funds for the Central Universities(grant numbers:2019ZD26);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272649"",""Client selection";contextual combinatorial multi-arm bandit;fairness scheduling;federated learning;"lyapunov optimization"",""Training";Data models;Collaborative work;Mathematical model;Servers;Optimization;"Computer science"","""",""72"","""",""27"",""IEEE"",""26 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"An Efficient Parallel Secure Machine Learning Framework on GPUs,""F. Zhang"; Z. Chen; C. Zhang; A. C. Zhou; J. Zhai;" X. Du"",""Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China"; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China; Guangdong Province Engineering Center of China-Made High Performance Data Computing System, Shenzhen University, Shenzhen, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Mar 2021"",""2021"",""32"",""9"",""2262"",""2276"",""Machine learning is widely used in our daily lives. Large amounts of data have been continuously produced and transmitted to the cloud for model training and data processing, which raises a problem: how to preserve the security of the data. Recently, a secure machine learning system named SecureML has been proposed to solve this issue using two-party computation. However, due to the excessive computation expenses of two-party computation, the secure machine learning is about 2× slower than the original machine learning methods. Previous work on secure machine learning mostly focused on novel protocols or improving accuracy, while the performance metric has been ignored. In this article, we propose a GPU-based framework ParSecureML to improve the performance of secure machine learning algorithms based on two-party computation. The main challenges of developing ParSecureML lie in the complex computation patterns, frequent intra-node data transmission between CPU and GPU, and complicated inter-node data dependence. To handle these challenges, we propose a series of novel solutions, including profiling-guided adaptive GPU utilization, fine-grained double pipeline for intra-node CPU-GPU cooperation, and compressed transmission for inter-node communication. Moreover, we integrate architecture specific optimizations, such as Tensor Cores, into ParSecureML. As far as we know, this is the first GPU-based secure machine learning framework. Compared to the state-of-the-art framework, ParSecureML achieves an average of 33.8× speedup. ParSecureML can also be applied to inferences, which achieves 31.7× speedup on average."",""1558-2183"","""",""10.1109/TPDS.2021.3059108"",""National R&D Program of China(grant numbers:2020AAA0105200)"; National Natural Science Foundation of China(grant numbers:U20A20226,61802412,61802260,61972403,61732014); Beijing Natural Science Foundation(grant numbers:4202031,L192027); Beijing Academy of Artificial Intelligence (BAAI); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program; Shenzhen Science and Technology Foundation(grant numbers:JCYJ20180305125737520); Tencent “Rhinoceros Birds” project of Scientific Research Foundation for Young Teachers of Shenzhen University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354058"",""Two-party computation";GPU acceleration;secure training;secure inference;"machine learning"",""Machine learning";Servers;Graphics processing units;Acceleration;Machine learning algorithms;Optimization;"Task analysis"","""",""22"","""",""73"",""IEEE"",""12 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"An Elastic Task Scheduling Scheme on Coarse-Grained Reconfigurable Architectures,""L. Chen"; J. Zhu; Y. Deng; Z. Li; J. Chen; X. Jiang; S. Yin; S. Wei;" L. Liu"",""School of Integrated Circuits, Tsinghua University, Beijing, China"; School of Integrated Circuits, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Integrated Circuits, Tsinghua University, Beijing, China; Alibaba Group, Sunnyvale, CA, USA; Alibaba Group, Sunnyvale, CA, USA; School of Integrated Circuits, Tsinghua University, Beijing, China; School of Integrated Circuits, Tsinghua University, Beijing, China;" School of Integrated Circuits, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Jul 2021"",""2021"",""32"",""12"",""3066"",""3080"",""Coarse-grained reconfigurable architectures (CGRAs) are increasingly employed as domain-specific accelerators due to their efficiency and flexibility. A CGRA typically relies on compilers to perform task scheduling. The longstanding problem of static scheduling is that it suffers from insufficient parallelism in handling irregularities due to over-serialization and workload imbalance, which leads to severe resource underutilization and performance loss. To counteract the limitations of static scheduling in CGRAs, it is essential to exploit dynamic parallelism automatically and manage hardware resources adaptively. However, existing dynamic scheduling mechanisms, e.g., work stealing, often reschedule aggressively for instant performance but sacrifice efficiency, which is unfavorable to CGRAs that emphasize efficiency and fewer reconfigurations. This article proposes an elastic task scheduling scheme that enables lightweight dynamic scheduling in CGRAs. Tasks are rescheduled at runtime according to the classic tagged-token dataflow paradigm to enable dynamic task-level parallelism. Meanwhile, tasks are dynamically resized according to run-time throughputs via duplication, combination, and substitution operators for balanced multitask execution. We implement the elastic task scheduling scheme on a well-known reconfigurable architecture - triggered instruction architecture (TIA). Evaluation on the MachSuite benchmarks shows that the proposed scheme is effective in improving performance and energy efficiency. The average speedup is 2× over the baseline. Also, our design attains a 57 percent improvement in the area-normalized performance and a 49 percent better energy efficiency. Compared with a state-of-the-art dynamic scheduling method, our scheme achieves 1.6× speedup and 1.6× energy efficiency than work-stealing mechanism on the same substrate."",""1558-2183"","""",""10.1109/TPDS.2021.3084804"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444171"",""Task schedule";elastic task schedule;reconfigurable architectures;dynamic issue;"dynamic mapping"",""Task analysis";Dynamic scheduling;Parallel processing;Reconfigurable architectures;Processor scheduling;Computer architecture;"Job shop scheduling"","""",""4"","""",""65"",""IEEE"",""28 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"An Incremental Iterative Acceleration Architecture in Distributed Heterogeneous Environments With GPUs for Deep Learning,""X. Zhang"; Z. Tang; L. Du;" L. Yang"",""College of Computer Science and Electronic Engineering, National Supercomputing Center in Changsha, Hunan University, Changsha, China"; College of Computer Science and Electronic Engineering, National Supercomputing Center in Changsha, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, National Supercomputing Center in Changsha, Hunan University, Changsha, China;" College of Computer and Communication Engineering, Changsha University of Science and Technology, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 May 2021"",""2021"",""32"",""11"",""2823"",""2837"",""The parallel computing capabilities of GPUs have a significant impact on computationally intensive iterative tasks. Offloading part or all of the deep learning tasks from the CPU to the GPU for execution is mainstream. However, a large number of redundant iterative calculations exist in the iterative process of computing tasks. Therefore, we propose a GPU-based distributed incremental iterative computing architecture that can make full use of distributed parallel computing and GPU memory structure. The architecture supports deep learning and other computationally intensive iterative applications by optimizing data placement and reducing redundant iterative calculations. To support block-based data partitioning and coalesced memory access on GPUs, we propose GDataSet, an abstract data set. The GPU incremental iteration manager called GTracker is designed to be responsible for GDataSet cache management on the GPU. In order to solve the limitation of on-chip memory size, we propose a variable sliding window mechanism. It improves the hit rate of cache access and the speed of data access by realizing the best block arrangement between on-chip memory and off-chip memory. Besides, a communication channel based on an incremental iterative model is designed to support data transmission and task communication in cluster computing. Finally, we implement the proposed architecture based on Spark 2.4.1 and CUDA 10.0. Comparative experiments with widely used computationally intensive iterative applications (K-means, LSTM, etc.) show that the incremental iterative acceleration architecture can significantly improve the efficiency of iterative computing."",""1558-2183"","""",""10.1109/TPDS.2021.3078254"",""National Key Research and Development Program of China(grant numbers:2018YFB1701400,2018YFB0203804,2017YFB0202201)"; National Natural Science Foundation of China(grant numbers:92055213,61873090,L1924056,62002114); China Knowledge Centre for Engineering Sciences and Technology(grant numbers:CKCEST-2020-2-5);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426412"",""Deep learning";distributed computing;incremental iteration;"GPU"",""Graphics processing units";Computational modeling;Deep learning;Training;Task analysis;Sparks;"Parallel processing"","""",""3"","""",""36"",""IEEE"",""7 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"An Optimized Weighted Average Makespan in Fault-Tolerant Heterogeneous MPSoCs,""H. Youness"; A. Omar;" M. Moness"",""Department of Computers and Systems Engineering, Minia University, Minia, CO, Egypt"; Department of Computers and Systems Engineering, Minia University, Minia, CO, Egypt;" Department of Computers and Systems Engineering, Minia University, Minia, CO, Egypt"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2021"",""2021"",""32"",""8"",""1933"",""1946"",""The multiprocessor system on chips (MPSoCs) are considered today the core of most modern systems. Most of the applications of these heterogeneous MPSoCs include critical systems and hence terms of fault tolerance and reliability have become essential. Task replication is a technique to carry out fault tolerance and can help for reducing the schedule length by increasing locality. It introduces an upper and lower bound for the makespan of each schedule while each task is replicated more than once. If a fault occurs during execution, the expected makespan will be some value between the upper bound and the lower bound based on when and where the fault has occurred. In this research a new performance parameter namely the weighted average makespan is introduced. It is calculated as the average of the lower and upper bounds of makespan using the probability of occurrence of each. Two scheduling algorithms are presented for fault tolerant scheduling based on directed acyclic graphs. These algorithms are the list scheduling algorithm and the optimizing of the weighted average makespan based on simulated annealing method. The simulation results show that the techniques can improve the schedule length and increase the system reliability without compromising the performance."",""1558-2183"","""",""10.1109/TPDS.2021.3053150"",""Hassan Youness";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9329106"",""Bi-criteria scheduling";DAGs;fault tolerance;heterogeneous MPSoC;reliability;scheduling;"simulated annealing"",""Fault tolerant systems";Reliability;Task analysis;Schedules;Scheduling algorithms;Simulated annealing;"Redundancy"","""",""4"","""",""100"",""IEEE"",""20 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Analysis of Global and Local Synchronization in Parallel Computing,""F. Cicirelli"; A. Giordano;" C. Mastroianni"",""ICAR-CNR, Rende, Italy"; ICAR-CNR, Rende, Italy;" ICAR-CNR, Rende, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Dec 2020"",""2021"",""32"",""5"",""988"",""1000"",""In a parallel computing scenario, the synchronization overhead, needed to coordinate the execution on the parallel computing nodes, can significantly impair the overall execution performance. Typically, synchronization is achieved by adopting a global synchronization schema involving all the nodes. In many application domains, though, a looser synchronization schema, namely, local synchronization, can be exploited, in which each node needs to synchronize only with a subset of the other nodes. In this work, we compare the performance of global and local synchronization using the efficiency, i.e., the ratio between the useful computing time and the total computing time, including the synchronization overhead, as a key performance indicator. We present an analytical study of the asymptotic behavior of the efficiency when the number of nodes increases. As an original contribution, we prove, using the Max-Plus algebra, that there is a non-zero lower bound on the efficiency in the case of local synchronization and we present a statistical procedure to find a value of this bound. This outcome marks a significant advantage of local synchronization with respect to global synchronization, for which the efficiency tends to zero when increasing the number of nodes."",""1558-2183"","""",""10.1109/TPDS.2020.3037469"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9257592"",""Parallel computing";efficiency;"synchronization"",""Algebra";Computational modeling;Simulation;Parallel processing;Probabilistic logic;Random variables;"Synchronization"","""",""6"","""",""63"",""IEEE"",""12 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Analysis of GPU Data Access Patterns on Complex Geometries for the D3Q19 Lattice Boltzmann Algorithm,""G. Herschlag"; S. Lee; J. S. Vetter;" A. Randles"",""Department of Biomedical Engineering, Duke University, Durham, NC, USA"; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA;" Department of Biomedical Engineering, Duke University, Durham, NC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Apr 2021"",""2021"",""32"",""10"",""2400"",""2414"",""GPU performance of the lattice Boltzmann method (LBM) depends heavily on memory access patterns. When implemented with GPUs on complex domains, typically, geometric data is accessed indirectly and lattice data is accessed lexicographically. Although there are a variety of other options, no study has examined the relative efficacy between them. Here, we examine a suite of memory access schemes via empirical testing and performance modeling. We find strong evidence that semi-direct is often better suited than the more common indirect addressing, providing increased computational speed and reducing memory consumption. For the layout, we find that the Collected Structure of Arrays (CSoA) and bundling layouts outperform the common Structure of Array layout";" on V100 and P100 devices, CSoA consistently outperforms bundling, however the relationship is more complicated on K40 devices. When compared to state-of-the-art practices, our recommendations lead to speedups of 10-40 percent and reduce memory consumption up to 17 percent. Using performance modeling and computational experimentation, we determine the mechanisms behind the accelerations. We demonstrate that our results hold across multiple GPUs on two leadership class systems, and present the first near-optimal strong results for LBM with arterial geometries run on GPUs."",""1558-2183"","""",""10.1109/TPDS.2021.3061895"",""National Institutes of Health(grant numbers:DP5OD019876)"; UT-Battelle(grant numbers:DE-AC05-00OR22725);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373915"",""Lattice boltzmann";graphics processing units;memory access;complex geometries;"high performance computing"",""Lattices";Layout;Geometry;Memory management;Computational modeling;Acceleration;"Standards"","""",""8"","""",""54"",""IEEE"",""9 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Architectural Adaptation and Performance-Energy Optimization for CFD Application on AMD EPYC Rome,""L. Szustak"; R. Wyrzykowski; L. Kuczynski;" T. Olas"",""Department of Computer Science, Czestochowa University of Technology, Czestochowa, Poland"; Department of Computer Science, Czestochowa University of Technology, Czestochowa, Poland; Department of Computer Science, Czestochowa University of Technology, Czestochowa, Poland;" Department of Computer Science, Czestochowa University of Technology, Czestochowa, Poland"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2021"",""2021"",""32"",""12"",""2852"",""2866"",""The advantages of the second-generation AMD EPYC Rome processors can be successfully used in the race to Exascale. However, the novel architecture's complexity makes it challenging to adapt demanding scientific codes - like stencil ones - to platforms with Rome CPUs. This article tackles this challenge by exploring the adaptation of the stencil-based CFD (computational fluid dynamics) application called MPDATA to these processors' influential features. We show that the previously proposed parametric adaptation methodology can be profitably applied to extend the performance portability of the memory-bound MPDATA on the AMD EPYC architecture. The extension of the parametric adaptation on the novel architecture requires careful consideration of two relevant aspects that reflect splitting the Rome architecture into multiple dies - features of the cache hierarchy and partitioning cores into work teams. The article also investigates the correlation between the performance optimizations and energy efficiency for a ccNUMA platform powered by top-of-the-line 64-core AMD Rome 7742 CPUs, comparing the results against two servers with Intel Xeon Scalable processors of different generations. Even without appealing to prices, the achieved performance and energy efficiency results are a solid argument confirming the competitiveness of AMD Rome processors against Intel Xeon CPUs in scientific applications."",""1558-2183"","""",""10.1109/TPDS.2021.3078153"",""National Science Center Poland(grant numbers:UMO-2017/26/D/ST6/00687)"; Polish Minister of Science and Higher Education; Regional Initiative of Excellence(grant numbers:020/RID/2018/19);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9426456"",""CFD";MPDATA;AMD EPYC Rome;shared-memory programming;performance portability;"energy efficiency"",""Program processors";Computer architecture;Optimization;Servers;Memory management;"Energy effficiency"","""",""6"","""",""47"",""CCBY"",""7 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"ARENA: Asynchronous Reconfigurable Accelerator Ring to Enable Data-Centric Parallel Computing,""C. Tan"; C. Xie; T. Geng; A. Marquez; A. Tumeo; K. Barker;" A. Li"",""Pacific Northwest National Laboratory, Richland, WA, USA"; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA; Pacific Northwest National Laboratory, Richland, WA, USA;" Pacific Northwest National Laboratory, Richland, WA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Jun 2021"",""2021"",""32"",""12"",""2880"",""2892"",""The next generation HPC and data centers are likely to be reconfigurable and data-centric due to the trend of hardware specialization and the emergence of data-driven applications. In this article, we propose ARENA - an asynchronous reconfigurable accelerator ring architecture as a potential scenario on how the future HPC and data centers will be like. Despite using the coarse-grained reconfigurable arrays (CGRAs) as the substrate platform, our key contribution is not only the CGRA-cluster design itself, but also the ensemble of a new architecture and programming model that enables asynchronous tasking across a cluster of reconfigurable nodes, so as to bring specialized computation to the data rather than the reverse. We presume distributed data storage without asserting any prior knowledge on the data distribution. Hardware specialization occurs at runtime when a task finds the majority of data it requires are available at the present node. In other words, we dynamically generate specialized CGRA accelerators where the data reside. The asynchronous tasking for bringing computation to data is achieved by circulating the task token, which describes the dataflow graphs to be executed for a task, among the CGRA cluster connected by a fast ring network. Evaluations on a set of HPC and data-driven applications across different domains show that ARENA can provide better parallel scalability with reduced data movement (53.9 percent). Compared with contemporary compute-centric parallel models, ARENA can bring on average 4.37× speedup. The synthesized CGRAs and their task-dispatchers only occupy 2.93mm$^2$2<";" inline-graphic xlink:href=""""tan-ieq1-3081074.gif""""/> chip area under 45nm process technology and can run at 800MHz with on average 759.8mW power consumption. ARENA also supports the concurrent execution of multi-applications, offering ideal architectural support for future high-performance parallel computing and data analytics systems."",""1558-2183"","""",""10.1109/TPDS.2021.3081074"",""Compute-Flow-Architecture"; U.S. Department of Energy(grant numbers:66150); U.S. Department of Energy(grant numbers:DE-AC05-76RL01830);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9436051"",""Compute-flow-architecture";runtime reconfiguration;asynchronous parallel execution;"abstract machine model"",""Task analysis";Computational modeling;Data models;Runtime;Programming;Computer architecture;"Data centers"","""",""11"","""",""79"",""IEEE"",""19 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Auditing Cache Data Integrity in the Edge Computing Environment,""B. Li"; Q. He; F. Chen; H. Jin; Y. Xiang;" Y. Yang"",""School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia"; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia;" School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Jan 2021"",""2021"",""32"",""5"",""1210"",""1223"",""Edge computing allows app vendors to deploy their applications and relevant data on distributed edge servers to serve nearby users. Caching data on edge servers can minimize users' data retrieval latency. However, such cache data are subject to both intentional and accidental corruption in the highly distributed, dynamic, and volatile edge computing environment. Given a large number of edge servers and their limited computing resources, how to effectively and efficiently audit the integrity of app vendors' cache data is a critical and challenging problem. This article makes the first attempt to tackle this Edge Data Integrity (EDI) problem. We first analyze the threat model and the audit objectives, then propose a lightweight sampling-based probabilistic approach, namely EDI-V, to help app vendors audit the integrity of their data cached on a large scale of edge servers. We propose a new data structure named variable Merkle hash tree (VMHT) for generating the integrity proofs of those data replicas during the audit. VMHT can ensure the audit accuracy of EDI-V by maintaining sampling uniformity. EDI-V allows app vendors to inspect their cache data and locate the corrupted ones efficiently and effectively. Both theoretical analysis and comprehensively experimental evaluation demonstrate the efficiency and effectiveness of EDI-V."",""1558-2183"","""",""10.1109/TPDS.2020.3043755"",""Australian Research Council(grant numbers:DP180100212,DP200102491)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9290124"",""Edge computing";data integrity;data cache;data replica;integrity audit;"merkle hash tree"",""Servers";Data integrity;Edge computing;Cloud computing;Image edge detection;Distributed databases;"Computer hacking"","""",""73"","""",""50"",""IEEE"",""10 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"BALS: Blocked Alternating Least Squares for Parallel Sparse Matrix Factorization on GPUs,""J. Chen"; J. Fang; W. Liu;" C. Yang"",""College of Computer, National University of Defense Technology, Changsha, Hunan, China"; College of Computer, National University of Defense Technology, Changsha, Hunan, China; Department of Computer Science and Technology, Super Scientific Software Laboratory, China University of Petroleum, Beijing, China;" College of Computer, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Mar 2021"",""2021"",""32"",""9"",""2291"",""2302"",""Matrix factorization on sparse matrices has been proven to be an effective approach for data mining and machine learning. However, the prior parallel implementations for matrix factorization fail to capture the internal social property embedded in real-world use cases. This article presents an efficient implementation of the alternative least squares (ALS) algorithm called BALSbuilt on top of a new sparse matrix format for parallel matrix factorization. The BALS storage format organizes the sparse matrix into 2D tiles to avoid repeated data loads and improve data reuses. We further propose a data reordering technique to sort sparse matrices according to nonzeros. The experimental results show that BALS can yield a superior performance than state-of-the-art implementations, i.e., our BALS generally runs faster than Gates’ implementation over different latent feature sizes, with a speedup of up to 2.08× on K20C, 3.72× on TITAN X and 3.13× on TITAN RTX. When compared with alternative matrix factorization algorithms, our BALS consistently outperforms CDMF, cuMF_CCD, and cuMF_SGD over various latent feature sizes and datasets. The reordering technique can provide an extra improvement of up to 23.68 percent on K20C, 19.87 percent on TITAN X and 20.38 percent on TITAN RTX."",""1558-2183"","""",""10.1109/TPDS.2021.3064942"",""National Key R&D Program of China(grant numbers:2018YFB0204301)"; National Natural Science Foundation of China(grant numbers:61972408,61972415); Science Challenge Project(grant numbers:TZZT2016002); Science Foundation of China University of Petroleum, Beijing(grant numbers:2462019YJRC004,2462020XKJS03);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373912"",""Matrix factorization";alternating least squares;data reuse;data reordering;performance evaluation;"GPGPUs"",""Sparse matrices";Motion pictures;Two dimensional displays;Graphics processing units;Artificial neural networks;Matrix decomposition;"Logic gates"","""",""2"","""",""35"",""IEEE"",""9 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Bi-Objective Optimization of Data-Parallel Applications on Heterogeneous HPC Platforms for Performance and Energy Through Workload Distribution,""H. Khaleghzadeh"; M. Fahad; A. Shahid; R. R. Manumachu;" A. Lastovetsky"",""School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland"; School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland; School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland; School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland;" School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Oct 2020"",""2021"",""32"",""3"",""543"",""560"",""Performance and energy are the two most important objectives for optimization on modern parallel platforms. In this article, we show that moving from single-objective optimization for performance or energy to their bi-objective optimization on heterogeneous processors results in a tremendous increase in the number of optimal solutions (workload distributions) even for the simple case of linear performance and energy profiles. We then study full performance and energy profiles of two real-life data-parallel applications and find that they exhibit shapes that are non-linear and complex enough to prevent good approximation of them as analytical functions for input to exact algorithms or optimization software for determining the Pareto front. We, therefore, propose a solution method solving the bi-objective optimization problem on heterogeneous processors. The method's novel component is an efficient and exact global optimization algorithm that takes as an input performance and energy profiles as arbitrary discrete functions of workload size, which accurately and realistically take into account resource contention and NUMA inherent in modern parallel platforms, and returns the Pareto-optimal solutions (generally speaking, load imbalanced). To construct the input discrete energy functions, the method employs a methodology that accurately models the energy consumption by a hybrid data-parallel application executing on a heterogeneous HPC platform containing different computing devices using system-level power measurements provided by power meters. We experimentally analyse the proposed solution method using three data-parallel applications, matrix multiplication, 2D fast Fourier transform (2D-FFT), and gene sequencing, on two connected heterogeneous servers consisting of multicore CPUs, GPUs, and Intel Xeon Phi. We show that it determines a superior Pareto front containing the best load balanced solutions and all the load imbalanced solutions that are ignored by load balancing methods."",""1558-2183"","""",""10.1109/TPDS.2020.3027338"",""Science Foundation Ireland(grant numbers:14/IA/2474)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207974"",""Heterogeneous platforms";data-parallel applications;workload partitioning;performance optimization;energy optimization;bi-objective optimization;workload distribution;multicore CPU;GPU;"Intel Xeon Phi"",""Optimization";Program processors;Energy consumption;Heuristic algorithms;Multicore processing;Software algorithms;"Sequential analysis"","""",""19"","""",""62"",""IEEE"",""28 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Biscotti: A Blockchain System for Private and Secure Federated Learning,""M. Shayan"; C. Fung; C. J. M. Yoon;" I. Beschastnikh"",""University of British Columbia, Vancouver, BC, Canada"; University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada;" University of British Columbia, Vancouver, BC, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1513"",""1525"",""Federated Learning is the current state-of-the-art in supporting secure multi-party machine learning (ML): data is maintained on the owner's device and the updates to the model are aggregated through a secure protocol. However, this process assumes a trusted centralized infrastructure for coordination, and clients must trust that the central service does not use the byproducts of client data. In addition to this, a group of malicious clients could also harm the performance of the model by carrying out a poisoning attack. As a response, we propose Biscotti: a fully decentralized peer to peer (P2P) approach to multi-party ML, which uses blockchain and cryptographic primitives to coordinate a privacy-preserving ML process between peering clients. Our evaluation demonstrates that Biscotti is scalable, fault tolerant, and defends against known attacks. For example, Biscotti is able to both protect the privacy of an individual client's update and maintain the performance of the global model at scale when 30 percent adversaries are present in the system."",""1558-2183"","""",""10.1109/TPDS.2020.3044223"",""Natural Sciences and Engineering Research Council of Canada(grant numbers:2014-04870)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9292450"",""Distributed machine learning";blockchain;privacy;"security"",""Peer-to-peer computing";Data models;Collaborative work;Training;Privacy;Machine learning;"Training data"","""",""86"","""",""90"",""IEEE"",""11 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Blockchain at the Edge: Performance of Resource-Constrained IoT Networks,""S. Misra"; A. Mukherjee; A. Roy; N. Saurabh; Y. Rahulamathavan;" M. Rajarajan"",""Department of Computer Science, Engineering at Indian Institute of Technology Kharagpur, Kharagpur, India"; Department of Computer Science, Engineering at Indian Institute of Technology Kharagpur, Kharagpur, India; Advanced technology Development Center, Indian Institute of Technology Kharagpur, Kharagpur, India; Department of Electronics and Communication Engineering, National Institute of Technology Patna, Patna, India; Institute for Digital Technologies, Loughborough University London, London, UK;" Information Security Group, School of Engineering and Mathematical Sciences, City University London, London, UK"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Aug 2020"",""2021"",""32"",""1"",""174"",""183"",""The proliferation of IoT in various technological realms has resulted in the massive spurt of unsecured data. The use of complex security mechanisms for securing these data is highly restricted owing to the low-power and low-resource nature of most of the IoT devices, especially at the Edge. In this article, we propose to use blockchains for extending security to such IoT implementations. We deploy a Ethereum blockchain consisting of both regular and constrained devices connecting to the blockchain through wired and wireless heterogeneous networks. We additionally implement a secure and encrypted networked clock mechanism to synchronize the non-real-time IoT Edge nodes within the blockchain. Further, we experimentally study the feasibility of such a deployment and the bottlenecks associated with it by running necessary cryptographic operations for blockchains in IoT devices. We study the effects of network latency, increase in constrained blockchain nodes, data size, Ether, and blockchain node mobility during transaction and mining of data within our deployed blockchain. This study serves as a guideline for designing secured solutions for IoT implementations under various operating conditions such as those encountered for static IoT nodes and mobile IoT devices."",""1558-2183"","""",""10.1109/TPDS.2020.3013892"",""University Grants Commission"; UK-India Education and Research Initiative; Joint Research Programme(grant numbers:184-17/2017(IC));" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158540"",""Internet of Things";blockchain;edge nodes;ethereum;"constrained-networks"",""Ecosystems";Synchronization;Logic gates;Data privacy;"Reliability"","""",""57"","""",""23"",""IEEE"",""4 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Boosting Parallel Influence-Maximization Kernels for Undirected Networks With Fusing and Vectorization,""G. Göktürk";" K. Kaya"",""Computer Science and Engineering, Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, TR, Turkey";" Computer Science and Engineering, Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, TR, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Dec 2020"",""2021"",""32"",""5"",""1001"",""1013"",""Influence maximization (IM) is the problem of finding a seed vertex set which is expected to incur the maximum influence spread on a graph. It has various applications in practice such as devising an effective and efficient approach to disseminate information, news or ad within a social network. The problem is shown to be NP-hard and approximation algorithms with provable quality guarantees exist in the literature. However, these algorithms are computationally expensive even for medium-scaled graphs. Furthermore, graph algorithms usually suffer from spatial and temporal irregularities during memory accesses, and this adds an extra cost on top of the already expensive IM kernels. In this article we leverage fused sampling, memoization, and vectorization to restructure, parallelize and boost their performance on undirected networks. The proposed approach employs a pseudo-random function and performs multiple Monte-Carlo simulations in parallel to exploit the SIMD lanes effectively and efficiently. In addition, it significantly reduces the number of edge traversals, hence the amount of data brought from the memory, which is critical for almost all memory-bound graph kernels. We apply the proposed approach to the traditional MIXGREEDY algorithm and propose INFUSER-MG which is more than 3000χ fasterthan the greedy approaches and can run on large graphs that have been considered as too large in the literature. For instance, the new algorithm runs in 2.09, 0.08, 0.36 seconds on graphs Amazon, NetHEP, NetPhy with 16 threads where the sequential baseline takes 141.3, 259.1 and 1725.2 seconds, respectively. To compare INFUSER-MG with the state-of-the-art approximation algorithms, we conduct a thorough experimental analysis with various influence settings. The results on real-life, undirected networks show that on 16 threads, INFUSER-MG is 2:3χ-173:8χ faster than state-of-the-art while being superior in terms of influence scores, and using a comparable amount of memory."",""1558-2183"","""",""10.1109/TPDS.2020.3038376"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9261128"","""",""Monte Carlo methods";Social networking (online);Instruction sets;Memory management;Approximation algorithms;Boosting;"Kernel"","""",""5"","""",""38"",""IEEE"",""16 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"BOSSA: A Decentralized System for Proofs of Data Retrievability and Replication,""D. Chen"; H. Yuan; S. Hu; Q. Wang;" C. Wang"",""Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China"; School of Computer Science, Wuhan University, Wuhan, Hubei, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China;" Department of Computer Science, City University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2020"",""2021"",""32"",""4"",""786"",""798"",""Proofs of retrievability and proofs of replication are two cryptographic tools that enable a remote server to prove that the users' data has been correctly stored. Nevertheless, the literature either requires the users themselves to perform expensive verification jobs, or relies on a “fully trustworthy” third party auditor (TPA) to execute the public verification. In addition, none of existing solutions consider the underlying incentive issues behind a rational server who is motivated to collect users' data but tries to evade the replication checking in order to save storage resources. In this article, we propose the first decentralized system for proofs of data retrievability and replication-BOSSA, which is incentive-compatible for each party and realizes automated auditing atop off-the-shelf blockchain platforms. We deal with issues such as proof enforcements to catch malicious behaviors, new metrics to measure the contributions, and reward distributions to create a fair reciprocal environment. BOSSA also incorporates privacy-enhancing techniques to prevent decentralized peers (including blockchain nodes) from inferring private information about the outsourced data. Security analysis is presented in the context of integrity, privacy, and reliability. We implement a prototype based on BOSSA leveraging the smart contracts of Ethereum blockchain. Our extensive experimental evaluations demonstrate the practicality of our proposal."",""1558-2183"","""",""10.1109/TPDS.2020.3030063"",""National Key Research and Development Program of China(grant numbers:2020YFB1005500)"; National Natural Science Foundation of China(grant numbers:61822207,U1636219); Outstanding Youth Foundation of Hubei Province(grant numbers:2017CFA047); Fundamental Research Funds for the Central Universities(grant numbers:2042019kf0210); Research Grants Council of Hong Kong(grant numbers:CityU 11212717,CityU 11217819); Innovation and Technology Commission of Hong Kong(grant numbers:ITS/145/19); Fundamental Research Funds for the Central Universities(grant numbers:2020kfyXJJS075);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9220842"",""Proofs of retrievability";proofs of replication;decentralized system;"blockchain"",""Blockchain";Cryptography;Servers;Reliability;Peer-to-peer computing;"Smart contracts"","""",""16"","""",""52"",""IEEE"",""12 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Breaking (Global) Barriers in Parallel Stochastic Optimization With Wait-Avoiding Group Averaging,""S. Li"; T. Ben-Nun; G. Nadiradze; S. D. Girolamo; N. Dryden; D. Alistarh;" T. Hoefler"",""Department of Computer Science, ETH Zurich, Zürich, Switzerland"; Department of Computer Science, ETH Zurich, Zürich, Switzerland; IST Austria, Klosterneuburg, Austria; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Zürich, Switzerland; IST Austria, Klosterneuburg, Austria;" Department of Computer Science, ETH Zurich, Zürich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1725"",""1739"",""Deep learning at scale is dominated by communication time. Distributing samples across nodes usually yields the best performance, but poses scaling challenges due to global information dissemination and load imbalance across uneven sample lengths. State-of-the-art decentralized optimizers mitigate the problem, but require more iterations to achieve the same accuracy as their globally-communicating counterparts. We present Wait-Avoiding Group Model Averaging (WAGMA) SGD, a wait-avoiding stochastic optimizer that reduces global communication via subgroup weight exchange. The key insight is a combination of algorithmic changes to the averaging scheme and the use of a group allreduce operation. We prove the convergence of WAGMA-SGD, and empirically show that it retains convergence rates similar to Allreduce-SGD. For evaluation, we train ResNet-50 on ImageNet"; Transformer for machine translation;" and deep reinforcement learning for navigation at scale. Compared with state-of-the-art decentralized SGD variants, WAGMA-SGD significantly improves training throughput (e.g., 2.1× on 1,024 GPUs for reinforcement learning), and achieves the fastest time-to-solution (e.g., the highest score using the shortest training time for Transformer)."",""1558-2183"","""",""10.1109/TPDS.2020.3040606"",""European Research Council(grant numbers:678880)"; EPiGRAM-HS(grant numbers:801039); ERC Starting Grant ScaleML(grant numbers:805223); Swiss National Science Foundation(grant numbers:185778); ETH Postdoctoral;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9271898"",""Stochastic gradient descent";distributed deep learning;"decentralized optimization"",""Computational modeling";Training;Convergence;Program processors;Stochastic processes;Deep learning;"Task analysis"","""",""3"","""",""75"",""IEEE"",""25 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Burst Load Evacuation Based on Dispatching and Scheduling In Distributed Edge Networks,""S. Deng"; C. Zhang; C. Li; J. Yin; S. Dustdar;" A. Y. Zomaya"",""College of Computer Science, Zhejiang University, Hangzhou, PR China"; College of Computer Science, Zhejiang University, Hangzhou, PR China; College of Computer Science, Zhejiang University, Hangzhou, PR China; College of Computer Science, Zhejiang University, Hangzhou, PR China; Distributed Systems Group, TU Wien, Vienna, Austria;" School of Computer Science, The University of Sydney, Sydney, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2021"",""2021"",""32"",""8"",""1918"",""1932"",""Edge computing, a fast evolving computing paradigm, has spawned a variety of new system architectures and computing methods discussed in both academia and industry. Edge servers are directly deployed near users’ equipment or devices owned by telecommunications companies. This allows for offloading computing tasks of various devices nearby to edge servers. Due to the shortage of computing resources in edge computing networks, they are often not as sufficient as the computing resources in a cloud computing center. This leads to the problem of service load imbalance once the load in the edge computing network increases suddenly. To solve the problem of “load evacuation” in edge environments, we introduce a strategy when the number of service requests for mobile devices or IoT devices increases rapidly within a short period of time. Therefore, to prevent poor QoS in edge computing, service load should be migrated to other edge servers to reduce the overall delay of these service requests. In this article, we have introduced a strategy with two stages during the burst load evacuation. Based on an optimal routing search at the dispatching stage, tasks will be migrated from the server in which the burst load occurs to other servers as soon as possible. Subsequently, with the assistance of the remote server and edge servers, these tasks are processed with the highest efficiency through the proposed parallel structure at the scheduling stage. Finally, we conduct numerical experiments to clarify the superiority of our algorithm in an edge environment simulation."",""1558-2183"","""",""10.1109/TPDS.2021.3052236"",""National Natural Science Foundation of China(grant numbers:U20A20173,61772461)"; Natural Science Foundation of Zhejiang Province(grant numbers:LR18F020003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328221"",""Edge computing";routing search;"online scheduling"",""Job shop scheduling";Uncertainty;Processor scheduling;Dispatching;Servers;Task analysis;"Edge computing"","""",""32"","""",""44"",""IEEE"",""18 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Canary: Decentralized Distributed Deep Learning Via Gradient Sketch and Partition in Multi-Interface Networks,""Q. Zhou"; K. Wang; H. Lu; W. Xu; Y. Sun;" S. Guo"",""School of Automation and School of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China"; Department of Electrical and Computer Engineering, University of California, Los Angeles, Los Angeles, CA, USA; School of Internet of Things, Nanjing University of Posts and Telecommunications, Nanjing, China; Department of Computer Science and Engineering, University at Buffalo, the State University of New York, Buffalo, NY, USA; School of Automation and School of Artificial Intelligence, Nanjing University of Posts and Telecommunications, Nanjing, China;" Department of Computing, Hong Kong Polytechnic University, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Nov 2020"",""2021"",""32"",""4"",""900"",""917"",""The multi-interface networks are efficient infrastructures to deploy distributed Deep Learning (DL) tasks as the model gradients generated by each worker can be exchanged to others via different links in parallel. Although this decentralized parameter synchronization mechanism can reduce the time of gradient exchange, building a high-performance distributed DL architecture still requires the balance of communication efficiency and computational utilization, i.e., addressing the issues of traffic burst, data consistency, and programming convenience. To achieve this goal, we intend to asynchronously exchange gradient pieces without the central control in multi-interface networks. We propose the Piece-level Gradient Exchange and Multi-interface Collective Communication to handle parameter synchronization and traffic transmission, respectively. Specifically, we design the gradient sketch approach based on 8-bit uniform quantization to compress gradient tensors and introduce the colayerabstraction to better handle gradient partition, exchange and pipelining. Also, we provide general programming interfaces to capture the synchronization semantics and build the Gradient Exchange Index (GEI) data structures to make our approach online applicable. We implement our algorithms into a prototype system called Canary by using PyTorch-1.4.0. Experiments conducted in Alibaba Cloud demonstrate that Canary reduces 56.28 percent traffic on average and completes the training by up to 1.61x, 2.28x, and 2.84x faster than BML, Ako on PyTorch, and PS on TensorFlow, respectively."",""1558-2183"","""",""10.1109/TPDS.2020.3036738"",""Hong Kong RGC Research Impact Fund(grant numbers:R5060-19,R5034-18)"; General Research Fund(grant numbers:152221/19E); Collaborative Research Fund(grant numbers:C5026-18G); National Natural Science Foundation of China(grant numbers:61872310,61772286,61802208,61872195); Jiangsu Key Research and Development Program(grant numbers:BE2019742); Natural Science Foundation of Jiangsu Province(grant numbers:BK20191381); Natural Science Foundation for Distinguished Young Scholar of Jiangsu(grant numbers:BK2020010062);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252115"",""Distributed systems";multi-interface network;deep learning;gradient sketch;"decentralized architecture"",""Training";Synchronization;Computer architecture;Convergence;Tensors;Task analysis;"Partitioning algorithms"","""",""3"","""",""62"",""IEEE"",""9 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"CASpMV: A Customized and Accelerative SpMV Framework for the Sunway TaihuLight,""G. Xiao"; K. Li; Y. Chen; W. He; A. Y. Zomaya;" T. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; State Key Laboratory of Mathematic Engineering and Advance Computing, Jiangnan Institute of Computing Technology, Wuxi, China; School of Information Technologies, University of Sydney, Sidney, Australia;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Sep 2020"",""2021"",""32"",""1"",""131"",""146"",""The Sunway TaihuLight, equipped with 10 million cores, is currently the world's third fastest supercomputer. SpMV is one of core algorithms in many high-performance computing applications. This paper implements a fine-grained design for generic parallel SpMV based on the special Sunway architecture and finds three main performance limitations, i.e., storage limitation, load imbalance, and huge overhead of irregular memory accesses. To address these problems, this paper introduces a customized and accelerative framework for SpMV (CASpMV) on the Sunway. The CASpMV customizes an auto-tuning four-way partition scheme for SpMV based on the proposed statistical model, which describes the sparse matrix structure characteristics, to make it better fit in with the computing architecture and memory hierarchy of the Sunway. Moreover, the CASpMV provides an accelerative method and customized optimizations to avoid irregular memory accesses and further improve its performance on the Sunway. Our CASpMV achieves a performance improvement that ranges from 588.05 to 2118.62 percent over the generic parallel SpMV on a CG (which corresponds to an MPI process) of the Sunway on average and has good scalability on multiple CGs. The performance comparisons of the CASpMV with state-of-the-art methods on the Sunway indicate that the sparsity and irregularity of data structures have less impact on CASpMV."",""1558-2183"","""",""10.1109/TPDS.2019.2907537"",""National Key R&D Program of China(grant numbers:2018YFB0203800,2016YFB0200201)"; National Natural Science Foundation of China(grant numbers:61625202); National Natural Science Foundation of China(grant numbers:61432005); National Natural Science Foundation of China(grant numbers:61806077); Hunan Provincial Innovation Foundation for Postgraduate(grant numbers:CX2018B230); China Postdoctoral Council(grant numbers:OCPC2017032); China Scholarship Council;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8674603"",""Heterogeneous many-core processor";matrix partition;optimization;parallelism;SpMV;"Sunway TaihuLight supercomputer"",""Sparse matrices";Computer architecture;Parallel processing;Acceleration;Supercomputers;Kernel;"Graphics processing units"","""",""62"","""",""32"",""IEEE"",""26 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Co-Active: A Workload-Aware Collaborative Cache Management Scheme for NVMe SSDs,""H. Sun"; S. Dai; J. Huang;" X. Qin"",""School of Science and Technology, Anhui University, Hefei, Anhui, China"; School of Science and Technology, Anhui University, Hefei, Anhui, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China;" Department of Computer Science and Software Engineering, Auburn University, Auburn, AL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Feb 2021"",""2021"",""32"",""6"",""1437"",""1451"",""When it comes to NAND Flash-based solid-state disks (SSDs), cache can narrow the performance gap between user-level I/Os and flash memory. Cache management schemes impose relentless impacts on the endurance and performance of flash memory. A vast majority of existing cache management techniques adopt a passive data-update style (e.g., GCaR, LCR), thereby undermining response times in burst I/O requests-based applications11.Burst I/O requests must be served in a real-time manner. This type of I/O access pattern is prevalent in data-intensive workloads.. To address this issue, we propose a collaborative active write-back cache management scheme, called Co-Active, customized for I/O access patterns and the usage status of a flash chip. We design a hot/cold separation module to determine whether data is cold or hot in workload. When a flash chip is idle, cold and dirty data in the cache is flushed into the idle flash chip to produce clean data. To curtail cache replacement cost, clean data are preferentially evicted amid the procedure of cache replacement. A maximum write-back threshold is configured according to the level of burst I/O requests in workload. This threshold is intended to avert redundant write I/Os flushing into flash memory, thereby boosting the endurance of flash memory. The experiments are conducted to validate the advantages of Co-Active in terms of average response time, write amplification, and erase count. The findings unveil that compared with the six popular cache management schemes (LRU, CFLRU, GCaR_CFLRU, LCR, and MQSim), Co-Active (1) slashes the average response time by up to 83.89 percent with an average of 32.7 percent"; (2) drives up the performance cliff degree by up to 76.4 percent with an average of 42.3 percent;" and (3) improves write amplification rate by up to 60.5 percent with an average of 5.4 percent."",""1558-2183"","""",""10.1109/TPDS.2021.3052028"",""National Natural Science Foundation of China(grant numbers:61702004,62072001,61821003)"; State Key Laboratory of Computer Architecture(grant numbers:CARCH201915); Natural Science Research Projects at Higher Institutions in Anhui Province(grant numbers:KJ2017A015); National Science Foundation(grant numbers:IIS-1618669,OAC-1642133,CCF-0845257);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9325932"",""Cache management";NAND flash;solid state disks;"proactive write back"",""Time factors";Nonvolatile memory;Memory management;Parallel processing;Sun;Protocols;"Degradation"","""",""11"","""",""49"",""IEEE"",""15 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Coarse-Grained Parallel Routing With Recursive Partitioning for FPGAs,""M. Shen"; G. Luo;" N. Xiao"",""School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China"; School of Electronics Engineering and Computer Science, Peking University, Beijing, China;" School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Nov 2020"",""2021"",""32"",""4"",""884"",""899"",""Routing is a very time-consuming stage in the FPGA design flow, significantly hindering the productivity. This article proposes CPRS, a coarse-grained parallel routing scheme in a distributed computing environment. First, we partition entire routing region to guide the assignment of nets for parallel processing. The partitioning is a recursive fashion, and at each recursive partitioning, the region is partitioned into two subregions forming three subsets of nets. The first subset consists of potentially dependent nets and they are distributed in different subregions. The remaining two subsets consist of potentially independent nets and they are distributed in their own subregions. Second, we route the nets of first subset in serial and process the remaining two subsets in parallel. The parallel processing is a coarse-grained fashion, which is implemented by MPI parallel programming model. Finally, we explore the optimization of both partitioning and parallel processing to further improve the overall speedup of parallel routing. In addition, we adopt MPI message to synchronize the intermediate results between different cores in parallel routing for a feasible solution. Experiments use a set of commonly used benchmarks to demonstrate the effectiveness of CPRS. Notably, CPRS achieves about 18× speedup on average using 32 processor cores with minor loss of quality, compared with the VTR 7.0 serial router. There is about 1.6× improvement over the state-of-the-art parallel router."",""1558-2183"","""",""10.1109/TPDS.2020.3035787"",""National Natural Science Foundation of China(grant numbers:62072479,61433019,61802446)"; Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); Guangdong Basic and Application Basic Research Teams(grant numbers:2018B030312002); Fundamental Research Funds for the Central Universities(grant numbers:19lgpy215);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248621"",""Distributed computing";field programmable gate array (FPGA);routing;parallel processing;"recursive partitioning"",""Routing";Field programmable gate arrays;Pins;Nickel;Parallel processing;Optimization;"Synchronization"","""",""5"","""",""44"",""IEEE"",""4 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Coflow Scheduling in Data Centers: Routing and Bandwidth Allocation,""L. Shi"; Y. Liu; J. Zhang;" T. Robertazzi"",""Snap Inc., Santa Monica, CA, USA"; Google Inc., Mountain View, CA, USA; Microsoft, Redmond, WA, USA;" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2021"",""2021"",""32"",""11"",""2661"",""2675"",""In distributed computing frameworks like MapReduce, Spark, and Dyrad, a coflow is a set of flows transferring data between two stages of a job. The job cannot start its next stage unless all flows in the coflow finish. To improve the execution performance of such a job, it is crucial to reduce the completion time of a coflow, as it can contribute more than 50 percent of the job completion time. While several coflow schedulers have been proposed, we observe that routing, as a factor greatly impacting the Coflow Completion Time (CCT), has not been well considered. In this article, we focus on the coflow scheduling problem and jointly consider routing and bandwidth allocation. We begin by providing an analytical solution to the problem of optimal bandwidth allocation with pre-determined routes. In the following, we formulate the problem of scheduling a single coflow as a Non-linear Mixed Integer Programming problem and present its relaxed convex optimization problem. We further propose two algorithms, CoRBA and its simplified version: CoRBA-fast that solve the single coflow scheduling problem with a joint consideration of routing and bandwidth allocation. Lastly, to address multiple coflows in online scheduling, we propose an online scheduler named OnCoRBA. By comparing with the start-of-the-art algorithms and schedulers via simulations, we demonstrate that CoRBA and CoRBA-fast reduce the CCT by 30-400 percent and the OnCoRBA scheduler reduces the average online CCT by 20-230 percent. In addition, CoRBA-fast can be hundreds times faster than CoRBA with around 8 percent performance degradation compared to CoRBA, which makes the use of CoRBA-fast very appropriate in practice."",""1558-2183"","""",""10.1109/TPDS.2021.3068424"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384326"",""Coflow scheduling";distributed computing;non-linear programming;"convex optimization"",""Routing";Bandwidth;Schedules;Channel allocation;Data centers;Job shop scheduling;"Optimal scheduling"","""",""7"","""",""27"",""IEEE"",""23 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Collaborative Heterogeneity-Aware OS Scheduler for Asymmetric Multicore Processors,""T. Yu"; R. Zhong; V. Janjic; P. Petoumenos; J. Zhai; H. Leather;" J. Thomson"",""Tsinghua University, Beijing, China"; Tsinghua University, Beijing, China; University of Dundee, Dundee, United Kingdom; University of Manchester, Manchester, United Kingdom; Tsinghua University, Beijing, China; University of Edinburgh, Edinburgh, United Kingdom;" University of St Andrews, St Andrews, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Dec 2020"",""2021"",""32"",""5"",""1224"",""1237"",""Asymmetric multicore processors (AMP) offer multiple types of cores under the same programming interface. Extracting the full potential of AMPs requires intelligent scheduling decisions, matching each thread with the right kind of core, the core that will maximize performance or minimize wasted energy for this thread. Existing OS schedulers are not up to this task. While they may handle certain aspects of asymmetry in the system, none can handle all runtime factors affecting AMPs for the general case of multi-threaded multi-programmed workloads. We address this problem by introducing COLAB, a general purpose asymmetry-aware scheduler targeting multi-threaded multi-programmed workloads. It estimates the performance and power of each thread on each type of core and identifies communication patterns and bottleneck threads. With this information, the scheduler makes coordinated core assignment and thread selection decisions that still provide each application its fair share of the processor's time. We evaluate our approach using both the GEM5 simulator on four distinct big.LITTLE configurations and a development board with ARM Cortex-A73/A53 processors and mixed workloads composed of PARSEC and SPLASH2 benchmarks. Compared to the state-of-the art Linux CFS and AMP-aware schedulers, we demonstrate performance gains of up to 25 and 5 to 15 percent on average, together with an average 5 percent energy saving depending on the hardware setup."",""1558-2183"","""",""10.1109/TPDS.2020.3045279"",""China Postdoctoral Science Foundation(grant numbers:2020TQ0169)"; ShuiMu Tsinghua Scholar fellowship(grant numbers:2019SM131); National Key R&D Program of China(grant numbers:2020AAA0105200); National Natural Science Foundation of China(grant numbers:U20A20226); Natural Science Foundation of Beijing Municipality(grant numbers:4202031); Beijing Academy of Artificial Intelligence(grant numbers:EP/P020631/1); Royal Academy of Engineering;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9296343"",""Asymmetric multicore processors";operating system;scheduling;performance model;"energy efficiency"",""Instruction sets";Multicore processing;Runtime;Message systems;Acceleration;Sensitivity;"Processor scheduling"","""",""11"","""",""35"",""IEEE"",""16 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Comment on “Circuit Ciphertext-Policy Attribute-Based Hybrid Encryption With Verifiable Delegation in Cloud Computing”,""Z. Cao";" O. Markowitch"",""Department of Mathematics, Shanghai University, Shanghai, China";" Computer Sciences Department, Université Libre de Bruxelles, Bruxelles, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Sep 2020"",""2021"",""32"",""2"",""392"",""393"",""The scheme [1] is flawed because: (1) its circuit access structure is confusingly described"; (2) the cloud server cannot complete the related computations;" (3) some users can conspire to generate new decryption keys, without the help of the key generation authority."",""1558-2183"","""",""10.1109/TPDS.2020.3021683"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186640"",""Ciphertext-policy attribute-based encryption";verifiable delegation;multilinear map;"hybrid encryption"",""Authorization";Encryption;Cloud computing;"Cryptography"","""",""8"","""",""1"",""IEEE"",""4 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Constructing Completely Independent Spanning Trees in Data Center Network Based on Augmented Cube,""G. Chen"; B. Cheng;" D. Wang"",""School of Computer Science and Technology, Soochow University, Suzhou, China"; School of Computer Science and Technology, Soochow University, Suzhou, China;" Department of Computer Science, Montclair State University, Upper Montclair, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2020"",""2021"",""32"",""3"",""665"",""673"",""A set of spanning trees T1"; T2;. .. ;" Tk in a network G are Completely Independent Spanning Trees (CISTs) if for any two nodes u and v in V (G), the paths between u and v in any two trees have no common edges and no common internal nodes. CISTs have important applications in data center networks, such as fault-tolerant multi-node broadcasting, fault-tolerant one-to-all broadcasting, reliable broadcasting, secure message distribution, and so on. The augmented cube AQn is a prominent variant of the well-known hypercube Qn, and having the important property of scalability, and both Qn and AQn have been proposed as the underlying structure for a data center network. The data center network based on AQn is denoted by AQDNn, and the logic graph of AQDNn is denoted by L-AQDNn. In this article, we study how to construct n - 1 CISTs in L-AQDNn. The constructed n - 1 CISTs are optimal in the sense that n - 1 is the maximally allowed CISTs in L-AQDNn. The correctness of our construction algorithm is proved. It is the first time a direct relationship is established between the dimension of a hypercube-family network and the number of CISTs it can host."",""1558-2183"","""",""10.1109/TPDS.2020.3029654"",""National Natural Science Foundation of China(grant numbers:U1905211)"; National Natural Science Foundation of China(grant numbers:61572337); Natural Science Foundation of the Jiangsu Higher Education Institutions of China(grant numbers:18KJA520009); China Postdoctoral Science Foundation(grant numbers:2015M581858); Jiangsu Planned Projects for Postdoctoral Research Funds(grant numbers:1501089B); Priority Academic Program Development of Jiangsu Higher Education Institutions;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217977"",""Augmented cube";hamiltonian cycle;edge-disjoint hamiltonian paths;data center network;"completely independent spanning trees"",""Data centers";Hypercubes;Broadcasting;Servers;Scalability;Fault tolerance;"Fault tolerant systems"","""",""27"","""",""34"",""IEEE"",""8 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Cost-Effective App Data Distribution in Edge Computing,""X. Xia"; F. Chen; Q. He; J. C. Grundy; M. Abdelrazek;" H. Jin"",""School of Information Technology, Deakin University, Geelong, Australia"; School of Information Technology, Deakin University, Geelong, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia; School of Information Technology, Deakin University, Geelong, Australia;" Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technolgoy, HuaZhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Jul 2020"",""2021"",""32"",""1"",""31"",""44"",""Edge computing, as an extension of cloud computing, distributes computing and storage resources from centralized cloud to distributed edge servers, to power a variety of applications demanding low latency, e.g., IoT services, virtual reality, real-time navigation, etc. From an app vendor's perspective, app data needs to be transferred from the cloud to specific edge servers in an area to serve the app users in the area. However, according to the pay-as-you-go business model, distributing a large amount of data from the cloud to edge servers can be expensive. The optimal data distribution strategy must minimize the cost incurred, which includes two major components, the cost of data transmission between the cloud to edge servers and the cost of data transmission between edge servers. In the meantime, the delay constraint must be fulfilled - the data distribution must not take too long. In this article, we make the first attempt to formulate this Edge Data Distribution (EDD) problem as a constrained optimization problem from the app vendor's perspective and prove its NP-hardness. We propose an optimal approach named EDD-IP to solve this problem exactly with the Integer Programming technique. Then, we propose an O(k)-approximation algorithm named EDD-A for finding approximate solutions to largescale EDD problems efficiently. EDD-IP and EDD-A are evaluated on a real-world dataset and the results demonstrate that they significantly outperform three representative approaches."",""1558-2183"","""",""10.1109/TPDS.2020.3010521"",""Australian Research Council(grant numbers:DP180100212,DP200102491)"; Laureate Fellowship(grant numbers:FL190100035);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9145634"",""Edge computing";optimization;data distribution;cost-effectiveness;"edge server network"",""Servers";Cloud computing;Data communication;Videos;Facebook;Distributed databases;"Edge computing"","""",""106"","""",""41"",""IEEE"",""21 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"CPDE: A Methodology for the Transparent Distribution of Centralized Smart Grid Programs,""T. T. Q. Nguyen"; C. Bobineau; V. Debusschere; Q. H. Giap;" N. Hadjsaid"",""G2Elab, University Grenoble Alpes, CNRS, Grenoble INP (Institute of Engineering University Grenoble Alpes), Grenoble, France"; University Grenoble Alpes, CNRS, Grenoble INP (Institute of Engineering University Grenoble Alpes), LIG, Grenoble, France; G2Elab, University Grenoble Alpes, CNRS, Grenoble INP (Institute of Engineering University Grenoble Alpes), Grenoble, France; Faculty of Electrical Engineering, The University of Danang - University of Science and Technology, Danang, Vietnam;" G2Elab, University Grenoble Alpes, CNRS, Grenoble INP (Institute of Engineering University Grenoble Alpes), Grenoble, France"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Sep 2020"",""2021"",""32"",""2"",""342"",""354"",""Control and management in smart grids are facing many challenges such as scalability, heterogeneity and technology innovation. This requires a transformation from the traditional centralised paradigm into a distributed one. In this article, a new distributed programming methodology, called Centralised Programming and Distributed Execution (CPDE), is proposed. CPDE relies on (i) the abstraction of the whole system as a distributed database"; (ii) the use of the Smartlog declarative and reactive rule based language for expressing data manipulation;" and (iii) the automatic Smartlog rule distribution according to data distribution. It thus provides a simple and straightforward mean for distributed programming. A centralised algorithm of fair over-voltage regulation of PV systems is used as a typical smart grids study case to validate the methodology and to compare it with centralized implementations. The experiments are implemented in a real-time simulation platform with a network of Raspberry Pis. In addition to showing its correctnes and ease of use, the performance of the CPDE implementation is studied, as well as its sensitivity to the increasing number of computing units and the data distribution. Results are promising and show the clear benefits of this methodology compared to more classical implementations."",""1558-2183"","""",""10.1109/TPDS.2020.3019759"",""Foundation Grenoble INP"; French Embassy in Vietnam;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9178461"",""Distributed programming";CPDE methodology;distributed database;declarative language;Smartlog;"smart grid"",""Programming";Smart grids;Distributed databases;Computer architecture;Real-time systems;"Actuators"","""",""1"","""",""28"",""IEEE"",""26 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Cryptomining Detection in Container Clouds Using System Calls and Explainable Machine Learning,""R. R. Karn"; P. Kudva; H. Huang; S. Suneja;" I. M. Elfadel"",""Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE"; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA; IBM Research, Yorktown Heights, NY, USA;" Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Oct 2020"",""2021"",""32"",""3"",""674"",""691"",""The use of containers in cloud computing has been steadily increasing. With the emergence of Kubernetes, the management of applications inside containers (or pods) is simplified. Kubernetes allows automated actions like self-healing, scaling, rolling back, and updates for the application management. At the same time, security threats have also evolved with attacks on pods to perform malicious actions. Out of several recent malware types, cryptomining has emerged as one of the most serious threats with its hijacking of server resources for cryptocurrency mining. During application deployment and execution in the pod, a cryptomining process, started by a hidden malware executable can be run in the background, and a method to detect malicious cryptomining software running inside Kubernetes pods is needed. One feasible strategy is to use machine learning (ML) to identify and classify pods based on whether or not they contain a running process of cryptomining. In addition to such detection, the system administrator will need an explanation as to the reason(s) of the ML's classification outcome. The explanation will justify and support disruptive administrative decisions such as pod removal or its restart with a new image. In this article, we describe the design and implementation of an ML-based detection system of anomalous pods in a Kubernetes cluster by monitoring Linux-kernel system calls (syscalls). Several types of cryptominers images are used as containers within an anomalous pod, and several ML models are built to detect such pods in the presence of numerous healthy cloud workloads. Explainability is provided using SHAP, LIME, and a novel auto-encoding-based scheme for LSTM models. Seven evaluation metrics are used to compare and contrast the explainable models of the proposed ML cryptomining detection engine."",""1558-2183"","""",""10.1109/TPDS.2020.3029088"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9215018"",""Cryptomining";docker;kubernetes;containers;machine learning;explainability;pod;"anomaly"",""Containers";Cloud computing;Malware;Machine learning;Cryptocurrency;"Data mining"","""",""38"","""",""93"",""CCBY"",""6 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Cuttlefish: Neural Configuration Adaptation for Video Analysis in Live Augmented Reality,""N. Chen"; S. Quan; S. Zhang; Z. Qian; Y. Jin; J. Wu; W. Li;" S. Lu"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Center for Networked Computing, Temple University, Philadelphia, PA, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Nov 2020"",""2021"",""32"",""4"",""830"",""841"",""Instead of relying on remote clouds, today's Augmented Reality (AR) applications usually send videos to nearby edge servers for analysis (such as objection detection) so as to optimize the user's quality of experience (QoE), which is often determined by not only detection latency but also detection accuracy, playback fluency, etc. Therefore, many studies have been conducted to help adaptively choose best video configuration, e.g., resolution and frame per second (fps), based on network bandwidth to further improve QoE. However, we notice that the video content itself has significant impacts on the configuration selection, e.g., the videos with high-speed objects must be encoded with a high fps to meet the user's fluency requirement. In this article, we aim to adaptively select configurations that match the time-varying network condition as well as the video content. We design Cuttlefish, a system that generates video configuration decisions using reinforcement learning (RL). Cuttlefish trains a neural network model that picks a configuration for the next encoding slot based on observations collected by AR devices. Cuttlefish does not rely on any pre-programmed models or specific assumptions on the environments. Instead, it learns to make configuration decisions solely through observations of the resulting performance of historical decisions. Cuttlefish automatically learns the adaptive configuration policy for diverse AR video streams and obtains a gratifying QoE. We compared Cuttlefish to several state-of-the-art bandwidth-based and velocity-based methods using trace-driven and real world experiments. The results show that Cuttlefish achieves a 18.4-25.8 percent higher QoE than the others."",""1558-2183"","""",""10.1109/TPDS.2020.3035044"",""National Key R&D Program of China(grant numbers:2017YFB1001801)"; National Natural Science Foundation of China(grant numbers:61872175,61832008); Natural Science Foundation of Jiangsu Province(grant numbers:BK20181252); Jiangsu Key R&D Program(grant numbers:BE2018116); Fundamental Research Funds for the Central Universities(grant numbers:14380060); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246260"",""Augmented reality";reinforcement learning;"configuration adaption"",""Bandwidth";Streaming media;Quality of experience;Adaptation models;Image edge detection;Real-time systems;"Servers"","""",""11"","""",""47"",""IEEE"",""30 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Data Life Aware Model Updating Strategy for Stream-Based Online Deep Learning,""W. Rang"; D. Yang; D. Cheng;" Y. Wang"",""Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA"; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA;" Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 May 2021"",""2021"",""32"",""10"",""2571"",""2581"",""Many deep learning applications deployed in dynamic environments change over time, in which the training models are supposed to be continuously updated with streaming data to guarantee better descriptions of data trends. However, most state-of-the-art learning frameworks support well in offline training methods while omitting online model updating strategies. In this work, we propose and implement iDlaLayer, a thin middleware layer on top of existing training frameworks that streamlines the support and implementation of online deep learning applications. In pursuit of good model quality and fast data incorporation, we design a Data Life Aware model updating strategy (DLA), which builds training data samples according to contributions of data from different life stages, and considers the training cost consumed in model updating. We evaluate iDlaLayer's performance through simulations and experiments based on TensorflowOnSpark with three representative online learning workloads. Our experimental results demonstrate that iDlaLayer reduces the overall elapsed time of ResNet, DeepFM and PageRank by 11.3, 28.2, and 15.2 percent compared to the periodic update strategy, respectively. It further achieves an average 20 percent decrease in training cost and brings about a 5 percent improvement in model quality against the traditional continuous training method."",""1558-2183"","""",""10.1109/TPDS.2021.3071939"",""National Science Foundation(grant numbers:CCF-1908843,CNS-2008265)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9399304"",""Online learning";model updating strategy;"data life cycle"",""Training";Data models;Predictive models;Distributed databases;Deep learning;Middleware;"Inference algorithms"","""",""4"","""",""35"",""IEEE"",""8 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Decentralized Dual Proximal Gradient Algorithms for Non-Smooth Constrained Composite Optimization Problems,""H. Li"; J. Hu; L. Ran; Z. Wang; Q. Lü; Z. Du;" T. Huang"",""Chongqing Key Laboratory of Nonlinear Circuits and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China"; Chongqing Key Laboratory of Nonlinear Circuits and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China; Chongqing Key Laboratory of Nonlinear Circuits and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China; School of Electrical Engineering and Telecommunications, University of New South Wales, Sydney, NSW, Australia; College of Computer, Chongqing University, Chongqing, China; Chongqing Key Laboratory of Nonlinear Circuits and Intelligent Information Processing, College of Electronic and Information Engineering, Southwest University, Chongqing, China;" Science Program, Texas A&M University at Qatar, Doha, Qatar"",""IEEE Transactions on Parallel and Distributed Systems"",""5 May 2021"",""2021"",""32"",""10"",""2594"",""2605"",""Decentralized dual methods play significant roles in large-scale optimization, which effectively resolve many constrained optimization problems in machine learning and power systems. In this article, we focus on studying a class of totally non-smooth constrained composite optimization problems over multi-agent systems, where the mutual goal of agents in the system is to optimize a sum of two separable non-smooth functions consisting of a strongly-convex function and another convex (not necessarily strongly-convex) function. Agents in the system conduct parallel local computation and communication in the overall process without leaking their private information. In order to resolve the totally non-smooth constrained composite optimization problem in a fully decentralized manner, we devise a synchronous decentralized dual proximal (SynDe-DuPro) gradient algorithm and its asynchronous version (AsynDe-DuPro) based on the randomized block-coordinate method. Both SynDe-DuPro and AsynDe-DuPro algorithms are theoretically proved to achieve the globally optimal solution to the totally non-smooth constrained composite optimization problem relied on the quasi-Fejér monotone theorem. As a main result, AsynDe-DuPro algorithm attains the globally optimal solution without requiring all agents to be activated at each iteration and thus is more robust than most existing synchronous algorithms. The practicability of the proposed algorithms and correctness of the theoretical findings are demonstrated by the experiments on a constrained Decentralized Sparse Logistic Regression (DSLR) problem in machine learning and a Decentralized Energy Resources Coordination (DERC) problem in power systems."",""1558-2183"","""",""10.1109/TPDS.2021.3072373"",""Fundamental Research Funds for the Central Universities(grant numbers:XDJK2019AC001)"; Venture and Innovation Support Program for Chongqing Overseas Returnees(grant numbers:cx2019005); National Natural Science Foundation of China(grant numbers:61773321);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9400765"",""Convex optimization";synchronous and asynchronous decentralized algorithms;multi-agent systems;non-smooth constrained composite optimization problems;decentralized machine learning;DSLR problems;"DERC problems"",""Optimization";Signal processing algorithms;Power systems;Machine learning;Machine learning algorithms;Linear programming;"Multi-agent systems"","""",""13"","""",""44"",""IEEE"",""12 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"DeepSlicing: Collaborative and Adaptive CNN Inference With Low Latency,""S. Zhang"; S. Zhang; Z. Qian; J. Wu; Y. Jin;" S. Lu"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Center for Networked Computing, Temple University, Philadelphia, PA, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2021"",""2021"",""32"",""9"",""2175"",""2187"",""The booming of Convolutional Neural Networks (CNNs) has empowered lots of computer-vision applications. Due to its stringent requirement for computing resources, substantial research has been conducted on how to optimize its deployment and execution on resource-constrained devices. However, previous works have several weaknesses, including limited support for various CNN structures, fixed scheduling strategies, overlapped computations, high synchronization overheads, etc. In this article, we present DeepSlicing, a collaborative and adaptive inference system that adapts to various CNNs and supports customized flexible fine-grained scheduling. As a built-in functionality, DeepSlicing has supported typical CNNs including GoogLeNet, ResNet, etc. By partitioning both model and data, we also design an efficient scheduler, Proportional Synchronized Scheduler (PSS), which achieves the trade-off between computation and synchronization. Based on PyTorch, we have implemented DeepSlicing on the testbed with real-world edge settings that consists of 8 heterogeneous Raspberry Pi's. The results indicate that DeepSlicing with PSS outperforms the existing systems dramatically, e.g., the inference latency and memory footprint are reduced up to 5.79× and 14.72×, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3058532"",""National Key Research and Development Program of China(grant numbers:2017YFB1001801)"; National Natural Science Foundation of China(grant numbers:61872175,61832008); Natural Science Foundation of Jiangsu Province(grant numbers:BK20181252); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9353250"",""CNN inference";edge computing;scheduling;"synchronization"",""Feature extraction";Task analysis;Synchronization;Computational modeling;Processor scheduling;Performance evaluation;"Scheduling"","""",""30"","""",""36"",""IEEE"",""11 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Design and Evaluation of a Risk-Aware Failure Identification Scheme for Improved RAS in Erasure-Coded Data Centers,""W. Huang"; J. Fang; S. Wan; C. Xie;" X. He"",""Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China"; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China;" Department of Computer and Information Sciences, Temple University, Philadelphia, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Jul 2020"",""2021"",""32"",""1"",""16"",""30"",""Data reliability and availability, and serviceability (RAS) of erasure-coded data centers are highly affected by data repair induced by node failures. In a traditional failure identification scheme, all chunks share the same identification time threshold, thus losing opportunities to further improve the RAS. To solve this problem, we propose RAFI, a novel risk-aware failure identification scheme. In RAFI, chunk failures in stripes experiencing different numbers of failed chunks are identified using different time thresholds. For those chunks in a high-risk stripe, a shorter identification time is adopted, thus improving the overall data reliability and availability. For those chunks in a low-risk stripe, a longer identification time is adopted, thus reducing the repair network traffic. Therefore, RAS can be improved simultaneously. We also propose three optimization techniques to reduce the additional overhead that RAFI imposes on management nodes and to ensure that RAFI can work properly under large-scale clusters. We use simulation, emulation, and prototyping implementation to evaluate RAFI from multiple aspects. Simulation and prototype results prove the effectiveness and correctness of RAFI, and the performance improvement of the optimization techniques on RAFI is demonstrated by running the emulator."",""1558-2183"","""",""10.1109/TPDS.2020.3010048"",""National Natural Science Foundation of China(grant numbers:61972445,61300046)"; National Science Foundation(grant numbers:CCF-1717660,CNS-1702474);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143488"",""Distributed storage system";erasure coding;"failure identification"",""Maintenance engineering";Reliability;Data centers;Heart beat;Silicon;Optimization;"Encoding"","""",""2"","""",""49"",""IEEE"",""17 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Design and Implementation of a Criticality- and Heterogeneity-Aware Runtime System for Task-Parallel Applications,""M. Han"; J. Park;" W. Baek"",""Department of Computer Science and Engineering, UNIST, Ulsan, Republic of Korea"; Department of Computer Science and Engineering, UNIST, Ulsan, Republic of Korea;" Department of Computer Science and Engineering, Graduate School of Artificial Intelligence, UNIST, Ulsan, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Dec 2020"",""2021"",""32"",""5"",""1117"",""1132"",""Heterogeneous multiprocessing (HMP) is an emerging technology for high-performance and energy-efficient computing. While task parallelism is widely used in various computing domains, such as embedded, big-data, and machine-learning computing domains, it still remains unexplored to investigate the efficient runtime support that effectively utilizes the criticality of the tasks of the target application and the heterogeneity of the underlying HMP system with full resource management. To bridge this gap, we propose CHRT, a criticality- and heterogeneity-aware runtime system for task-parallel applications. CHRT dynamically estimates the performance and power consumption of the target task-parallel application and robustly manages the full HMP system resources (i.e., core types, counts, and voltage/frequency levels) to maximize the overall efficiency. Our quantitative evaluation based on widely-used task parallel benchmarks and two full HMP systems (i.e., the XU3 and HiKey970 HMP systems) demonstrates the effectiveness of CHRT in that CHRT achieves significantly higher energy (e.g., 60.4 and 57.2 percent on average on the XU3 system) and energy-delay product (e.g., 52.2 and 44.0 percent on average on the HiKey970 system) efficiency than the baseline runtime system that employs the breadth-first scheduler and the state-of-the-art criticality-aware runtime system and incurs low performance overheads."",""1558-2183"","""",""10.1109/TPDS.2020.3031911"",""Institute of Information & Communications Technology Planning & Evaluation(grant numbers:2018-0-00769)"; National Research Foundation of Korea(grant numbers:NRF-2018R1C1B6005961); Institute of Information & Communications Technology Planning & Evaluation(grant numbers:2020-0-01336);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9266082"",""Criticality- and heterogeneity-aware runtime system";"task-parallel applications"",""Task analysis";Runtime;Parallel processing;Heart beat;Benchmark testing;Prototypes;"Power demand"","""",""3"","""",""45"",""IEEE"",""23 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Adaptive Consensus Tracking Control for Multi-Agent System With Communication Constraints,""P. Zhang"; H. Xue; S. Gao;" J. Zhang"",""School of Automation, Northwestern Polytechnical University, Xi'an, China"; School of Automation, Northwestern Polytechnical University, Xi'an, China; School of Automation, Northwestern Polytechnical University, Xi'an, China;" School of Automation, Northwestern Polytechnical University, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Jan 2021"",""2021"",""32"",""6"",""1293"",""1306"",""Aiming at a class of high-order strict feedback nonlinear multi-agent systems with communication constraints, a novel distributed adaptive back-stepping control method is proposed to cooperatively track the moving targets. First, five agents are used as controlled objects, and all five agents form a “leader-follower” mode with a distributed control structure. Meanwhile, the leader's moving velocity is considered the forward velocity of the whole formation system, and remaining agents follow the leader's movement. Then, each agent tracks the desired formation with a time-varying reference trajectory, thereby achieving the consensus tracking purpose of multi-agent system. Moreover, the multi-agent formation system avoids obstacles with the optimal trajectory and maintains the desired formation movement. Finally, the simulation results show that the designed controller can achieve the lateral and horizontal tracking errors of the multi-agent system to converge quickly, and then keep the system asymptotically stable during tracking process."",""1558-2183"","""",""10.1109/TPDS.2020.3048383"",""National Defense Research Program(grant numbers:N2015KD0152,11504429)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311825"",""Weak communication";multi-agent system;back-stepping technology;"rapid convergence"",""Multi-agent systems";Topology;Target tracking;Process control;Convergence;Trajectory;"Time-varying systems"","""",""25"","""",""31"",""IEEE"",""31 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed and Collective Deep Reinforcement Learning for Computation Offloading: A Practical Perspective,""X. Qiu"; W. Zhang; W. Chen;" Z. Zheng"",""School of Data and Computer Science, National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China"; School of Data and Computer Science, National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China;" School of Data and Computer Science, National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Dec 2020"",""2021"",""32"",""5"",""1085"",""1101"",""Mobile edge computing (MEC) is a promising solution to support resource-constrained devices by offloading tasks to the edge servers. However, traditional approaches (e.g., linear programming and game-theory methods) for computation offloading mainly focus on the immediate performance, potentially leading to performance degradation in the long run. Recent breakthroughs regarding deep reinforcement learning (DRL) provide alternative methods, which focus on maximizing the cumulative reward. Nonetheless, there exists a large gap to deploy real DRL applications in MEC. This is because: 1) training a well-performed DRL agent typically requires data with large quantities and high diversity, and 2) DRL training is usually accompanied by huge costs caused by trial-and-error. To address this mismatch, we study the applications of DRL on the multi-user computation offloading problem from a more practical perspective. In particular, we propose a distributed and collective DRL algorithm called DC-DRL with several improvements: 1) a distributed and collective training scheme that assimilates knowledge from multiple MEC environments, which not only greatly increases data amount and diversity but also spreads the exploration costs, 2) an updating method called adaptive n-step learning, which can improve training efficiency without suffering from the high variance caused by distributed training, and 3) combining the advantages of deep neuroevolution and policy gradient to maximize the utilization of multiple environments and prevent the premature convergence. Lastly, evaluation results demonstrate the effectiveness of our proposed algorithm. Compared with the baselines, the exploration costs and final system costs are reduced by at least 43 and 9.4 percent, respectively."",""1558-2183"","""",""10.1109/TPDS.2020.3042599"",""National Key Research and Development Plan(grant numbers:2018YFB1003803)"; National Natural Science Foundation of China(grant numbers:61802450,61722214); Natural Science Foundation of Guangdong Province(grant numbers:2018A030313005); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Guangdong Provincial Pearl River Talents Program(grant numbers:2019QN01X130);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288861"",""Mobile edge computing";computation offloading;deep reinforcement learning;distributed and collective training;n-step return;deep neuroevolution;"policy gradient"",""Training";Task analysis;Computational modeling;Optimization;Resource management;Performance evaluation;"Games"","""",""59"","""",""42"",""IEEE"",""9 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Distributed and Dynamic Service Placement in Pervasive Edge Computing Networks,""Z. Ning"; P. Dong; X. Wang; S. Wang; X. Hu; S. Guo; T. Qiu; B. Hu;" R. Y. K. Kwok"",""School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China"; School of Software, Dalian University of Technology, Dalian, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China,; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, China;" Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jan 2021"",""2021"",""32"",""6"",""1277"",""1292"",""The explosive growth of mobile devices promotes the prosperity of novel mobile applications, which can be realized by service offloading with the assistance of edge computing servers. However, due to limited computation and storage capabilities of a single server, long service latency hinders the continuous development of service offloading in mobile networks. By supporting multi-server cooperation, Pervasive Edge Computing (PEC) is promising to enable service migration in highly dynamic mobile networks. With the objective of maximizing the system utility, we formulate the optimization problem by jointly considering the constraints of server storage capability and service execution latency. To enable dynamic service placement, we first utilize Lyapunov optimization method to decompose the long-term optimization problem into a series of instant optimization problems. Then, a sample average approximation-based stochastic algorithm is proposed to approximate the future expected system utility. Afterwards, a distributed Markov approximation algorithm is utilized to determine the service placement configurations. Through theoretical analysis, the time complexity of our proposed algorithm is linear to the number of users, and the backlog queue of PEC servers is stable. Performance evaluations are conducted based on both synthetic and real trace-driven scenarios, with numerical results demonstrating the effectiveness of our proposed algorithm from various aspects."",""1558-2183"","""",""10.1109/TPDS.2020.3046000"",""National Natural Science Foundation of China(grant numbers:61971084,61931019,62001073)"; Chongqing Talent Program(grant numbers:CQYC2020058659); National Natural Science Foundation of China(grant numbers:cstc2019jcyjmsxmX0208);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301260"",""Pervasive edge computing";service migration;Lyapunov optimization;"distributed Markov approximation"",""Servers";Markov processes;Heuristic algorithms;Edge computing;Quality of service;Approximation algorithms;"Task analysis"","""",""77"","""",""42"",""IEEE"",""21 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Distributed Task Migration Optimization in MEC by Extending Multi-Agent Deep Reinforcement Learning Approach,""C. Liu"; F. Tang; Y. Hu; K. Li; Z. Tang;" K. Li"",""College of Information Science and Engineering, Hunan University, Changsha, Hunan, China"; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China;" College of Information Science and Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1603"",""1614"",""Closer to mobile users geographically, mobile edge computing (MEC) can provide some cloud-like capabilities to users more efficiently. This enables it possible for resource-limited mobile users to offload their computation-intensive and latency-sensitive tasks to MEC nodes. For its great benefits, MEC has drawn wide attention and extensive works have been done. However, few of them address task migration problem caused by distributed user mobility, which can't be ignored with quality of service (QoS) consideration. In this article, we study task migration problem and try to minimize the average completion time of tasks under migration energy budget. There are multiple independent users and the movement of each mobile user is memoryless with a sequential decision-making process, thus reinforcement learning algorithm based on Markov chain model is applied with low computation complexity. To further facilitate cooperation among users, we devise a distributed task migration algorithm based on counterfactual multi-agent (COMA) reinforcement learning approach to solve this problem. Extensive experiments are carried out to assess the performance of this distributed task migration algorithm. Compared with no migrating (NM) and single-agent actor-critic (AC) algorithms, the proposed distributed task migration algorithm can achieve up 30-50 percent reduction about average completion time."",""1558-2183"","""",""10.1109/TPDS.2020.3046737"",""National Key Research and Development Program of China(grant numbers:2018YFB1701403)"; National Natural Science Foundation of China(grant numbers:62072165,61876061,U19A2058); Zhijiang Lab, China(grant numbers:2020KE0AB01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305956"",""Energy";mobile edge computing;mobility;multi-agent reinforcement learning;"task migration"",""Task analysis";Reinforcement learning;Quality of service;Energy consumption;Optimization;Markov processes;"Computational modeling"","""",""61"","""",""32"",""IEEE"",""23 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"DL2: A Deep Learning-Driven Scheduler for Deep Learning Clusters,""Y. Peng"; Y. Bao; Y. Chen; C. Wu; C. Meng;" W. Lin"",""University of Hong Kong, Hong Kong, China"; University of Hong Kong, Hong Kong, China; University of Hong Kong, Hong Kong, China; University of Hong Kong, Hong Kong, China; NAOC, Beijing, China;" Alibaba Inc., Hanzhou, Zhejiang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2021"",""2021"",""32"",""8"",""1947"",""1960"",""Efficient resource scheduling is essential for maximal utilization of expensive deep learning (DL) clusters. Existing cluster schedulers either are agnostic to machine learning (ML) workload characteristics, or use scheduling heuristics based on operators' understanding of particular ML framework and workload, which are less efficient or not general enough. In this article, we show that DL techniques can be adopted to design a generic and efficient scheduler. Specifically, we propose DL2, a DL-driven scheduler for DL clusters, targeting global training job expedition by dynamically resizing resources allocated to jobs. DL2 advocates a joint supervised learning and reinforcement learning approach: a neural network is warmed up via offline supervised learning based on job traces produced by the existing cluster scheduler";" then the neural network is plugged into the live DL cluster, fine-tuned by reinforcement learning carried out throughout the training progress of the DL jobs, and used for deciding job resource allocation in an online fashion. We implement DL2 on Kubernetes and enable dynamic resource scaling in DL jobs on MXNet. Extensive evaluation shows that DL2 outperforms fairness scheduler (i.e., DRF) by 44.1 percent and expert heuristic scheduler (i.e., Optimus) by 17.5 percent in terms of average job completion time."",""1558-2183"","""",""10.1109/TPDS.2021.3052895"",""Alibaba Innovative Research"; Hong Kong RGC(grant numbers:HKU 17204619,17208920);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328612"",""Deep learning";resource allocation;"distributed training"",""Training";Resource management;Graphics processing units;Adaptation models;Supervised learning;Servers;"Reinforcement learning"","""",""27"","""",""67"",""IEEE"",""19 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"DTransE: Distributed Translating Embedding for Knowledge Graph,""D. Song"; F. Zhang; M. Lu; S. Yang;" H. Huang"",""Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China"; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), and the School of Information, Renmin University of China, Beijing, China; Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China;" Engineering Research Center of High Volume Language Information Processing and Cloud Computing Applications, School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Apr 2021"",""2021"",""32"",""10"",""2509"",""2523"",""Knowledge graphs play an important role in many applications, such as link prediction and question answering. Translating embedding for knowledge graphs is done with the aim of encoding structured information on entities and their rich relations in a low-dimensional embedding space. TransE is one of the most important methods in translation-based models, and uses translation invariance to implement translating embedding for knowledge graphs. In this line of work, translating embedding models represent the relation as a translation from the head entity to the tail entity and have achieved impressive results. Currently, the TransE model is only developed on single-node machines. Unfortunately, the computing and storage capacities of a single machine can easily reach their limits as knowledge graphs become larger and more complex, which limits the application scope of TransE. In order to solve this problem, we propose a distributed TransE method, known as DTransE, which can utilize distributed computing resources to calculate knowledge graph embeddings. However, building a distributed TransE is complicated and involves challenges of knowledge graph partitioning and computation. To solve these challenges, we provide a high-quality edge partitioning algorithm for the power-law graph by considering the high-degree and low-degree vertices with adaptive weights, which can balance the workload. By using the unactivated Gather-Apply-Scatter model on TransE, the processes periodically exchange messages in a loop. The irregular data distribution among the processes is also optimized to further accelerate communication. As far as we know, this is the first work on a distributed TransE method. We use link prediction to evaluate the DTransE in a distributed environment. Experiments show that, compared to the original TransE method, our proposed DTransE is, on average, 24.5 times faster with a minimum loss of accuracy";" compared to the state-of-the-art parallel TransE implementation, DTransE is two times faster on average."",""1558-2183"","""",""10.1109/TPDS.2021.3066442"",""National Key Research and Development Program of China(grant numbers:2020YFC0832606)"; National Natural Science Foundation of China(grant numbers:61976021,61802412,U1811262); Beijing Natural Science Foundation(grant numbers:L192027); State Key Laboratory of Computer Architecture (ICT, CAS)(grant numbers:CARCHA202007);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380698"",""TransE";distributed computing;workload partitioning;"knowledge graph and representation"",""Computational modeling";Training;Partitioning algorithms;Mathematical model;Parallel processing;Memory management;"Prediction algorithms"","""",""9"","""",""42"",""IEEE"",""17 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Dynamic Load Balancing in Parallel Execution of Cellular Automata,""A. Giordano"; A. De Rango; R. Rongo; D. D'Ambrosio;" W. Spataro"",""ICAR-CNR, Rende (CS), Italy"; DIATIC UNICAL, Rende (CS), Italy; DeMACS UNICAL, Rende (CS), Italy; DeMACS UNICAL, Rende (CS), Italy;" DeMACS UNICAL, Rende (CS), Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Sep 2020"",""2021"",""32"",""2"",""470"",""484"",""The allocation of the computational load across different processing elements is an important issue in parallel computing. Indeed, an unbalanced load distribution can strongly affect the performances of a parallel system caused by an excess of synchronization idle times due to less loaded processes waiting for more loaded ones. In this article, we focus on the load balancing issues in the context of the parallel execution of spatial-related applications where the domain space is partitioned in regions that are assigned to different processing elements. In particular, without loss of generality, we consider the well-known spatial-related Cellular Automata computational paradigm for evaluating the proposed dynamic load balancing approach. The main contribution of this article is the derivation of simple closed-form expressions that allow to compute the optimal workload assignment in a dynamic fashion, with the goal of guaranteeing a fully balanced workload distribution during the parallel execution. Based on these expressions, an algorithm for balanced execution of cellular automata is presented and implemented using the MPI technology. Eventually, an experimental section practically shows the behaviour of the proposed dynamic load balancing approach and proves its performance improvement, compared to the not-balanced version, as witnessed by the appreciable reduction of execution times."",""1558-2183"","""",""10.1109/TPDS.2020.3025102"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200755"",""Parallel computing";load balancing;"cellular automata"",""Load management";Automata;Heuristic algorithms;Computational modeling;Parallel processing;Task analysis;"Load modeling"","""",""13"","""",""36"",""IEEE"",""18 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"e-PoS: Making Proof-of-Stake Decentralized and Fair,""M. Saad"; Z. Qin; K. Ren; D. Nyang;" D. Mohaisen"",""University of Central Florida, Orlando, FL, USA"; Zhejiang University, Zhejiang, China; Zhejiang University, Zhejiang, China; Ewha Womans University, Seoul, South Korea;" University of Central Florida, Orlando, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2021"",""2021"",""32"",""8"",""1961"",""1973"",""Blockchain applications that rely on the Proof-of-Work (PoW) have increasingly become energy inefficient with a staggering carbon footprint. In contrast, energy efficient alternative consensus protocols such as Proof-of-Stake (PoS) may cause centralization and unfairness in the blockchain system. To address these challenges, we propose a modular version of PoS-based blockchain systems called e-PoS that resists the centralization of network resources by extending mining opportunities to a wider set of stakeholders. Moreover, e-PoS leverages the in-built system operations to promote fair mining practices by penalizing malicious entities. We validate e-PoS 's achievable objectives through theoretical analysis and simulations. Our results show that e-PoS ensures fairness and decentralization, and can be applied to existing blockchain applications."",""1558-2183"","""",""10.1109/TPDS.2020.3048853"",""NRF(grant numbers:NRF-2016K1A1A2912757)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9312484"",""Blockchains";consensus protocols;"bitcoin"",""Bitcoin";Cryptography;Smart contracts;Stakeholders;Data mining;Consensus protocol;"Complexity theory"","""",""38"","""",""51"",""IEEE"",""1 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"E2bird: Enhanced Elastic Batch for Improving Responsiveness and Throughput of Deep Learning Services,""W. Cui"; Q. Chen; H. Zhao; M. Wei; X. Tang;" M. Guo"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, Shanghai University of Finance and Economics, Shanghai, China;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jan 2021"",""2021"",""32"",""6"",""1307"",""1321"",""We aim to tackle existing problems about deep learning serving on GPUs in the view of the system. GPUs have been widely adopted to serve online deep learning-based services that have stringent QoS(Quality-of-Service) requirements. However, emerging deep learning serving systems often result in poor responsiveness and low throughput of the inferences that damage user experience and increase the number of GPUs required to host an online service. Our investigation shows that the poor batching operation and the lack of data transfer-computation overlap are the root causes of the poor responsiveness and low throughput. To this end, we propose E2bird, a deep learning serving system that is comprised of a GPU-resident memory pool, a multi-granularity inference engine, and an elastic batch scheduler. The memory pool eliminates the unnecessary waiting of the batching operation and enables data transfer-computation overlap. The inference engine enables concurrent execution of different batches, improving the GPU resource utilization. The batch scheduler organizes inferences elasticallyto guarantee the QoS. Our experimental results on an Nvidia Titan RTXGPU show that E2bird reduces the response latency of inferences by up to 82.4 percent and improves the throughput by up to 62.8 percent while guaranteeing the QoS target compared with TensorFlow Serving."",""1558-2183"","""",""10.1109/TPDS.2020.3047638"",""National R&D Program of China(grant numbers:2018YFB1004800)"; National Natural Science Foundation of China(grant numbers:62022057,61632017,61832006);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309326"",""GPUs";DL serving;latency;throughput;"responsiveness"",""Graphics processing units";Throughput;Deep learning;Birds;Quality of service;Kernel;"Engines"","""",""13"","""",""45"",""IEEE"",""28 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"EDGES: An Efficient Distributed Graph Embedding System on GPU Clusters,""D. Yang"; J. Liu;" J. Lai"",""NVIDIA, Beijing, China"; NVIDIA, Beijing, China;" NVIDIA, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1892"",""1902"",""Graph embedding training models access parameters sparsely in a “one-hot” manner. Currently, the distributed graph embedding neural network is learned by data parallel with the parameter server, which suffers significant performance and scalability problems. In this article, we analyze the problems and characteristics of training this kind of models on distributed GPU clusters for the first time, and find that fixed model parameters scattered among different machine nodes are a major limiting factor for efficiency. Based on our observation, we develop an efficient distributed graph embedding system called EDGES, which can utilize GPU clusters to train large graph models with billions of nodes and trillions of edges using data and model parallelism. Within the system, we propose a novel dynamic partition architecture for training these models, achieving at least one half of communication reduction compared to existing training systems. According to our evaluations on real-world networks, our system delivers a competitive accuracy for the trained embeddings, and significantly accelerates the training process of the graph node embedding neural network, achieving a speedup of 7.23x and 18.6x over the existing fastest training system on single node and multi-node, respectively. As for the scalability, our experiments show that EDGES obtains a nearly linear speedup."",""1558-2183"","""",""10.1109/TPDS.2020.3041219"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272876"",""Large-scale distributed training";graph node embedding;GPU clusters;parallel algorithm;"scalability"",""Training";Data models;Servers;Graphics processing units;Computational modeling;Neural networks;"Training data"","""",""3"","""",""34"",""IEEE"",""27 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Buffer Overflow Detection on GPU,""B. Di"; J. Sun; H. Chen;" D. Li"",""College of Computer Science and Electrical Engineering, Hunan University, Changsha, China"; College of Computer Science and Electrical Engineering, Hunan University, Changsha, China; College of Computer Science and Electrical Engineering, Hunan University, Changsha, China;" The Department of Electrical Engineering and Computer Science, University of California Merced, Merced, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Jan 2021"",""2021"",""32"",""5"",""1161"",""1177"",""Rich thread-level parallelism of GPU has motivated co-running GPU kernels on a single GPU. However, when GPU kernels co-run, it is possible that one kernel can leverage buffer overflow to attack another kernel running on the same GPU. There is very limited work aiming to detect buffer overflow for GPU. Existing work has either large performance overhead or limited capability in detecting buffer overflow. In this article, we introduce GMODx, a runtime software system that can detect GPU buffer overflow. GMODx performs always-on monitoring on allocated memory based on a canary-based design. First, for the fine-grained memory management, GMODx introduces a set of byte arrays to store buffer information for overflow detection. Techniques, such as lock-free accesses to the byte arrays, delayed memory free, efficient memory reallocation, and garbage collection for the byte arrays, are proposed to achieve high performance. Second, for the coarse-grained memory management, GMODx utilizes unified memory to delegate the always-on monitoring to the CPU. To reduce performance overhead, we propose several techniques, including customized list data structure and specific optimizations against the unified memory. For micro-benchmarking, our experiments show that GMODx is capable of detecting buffer overflow for the fine-grained memory management without performance loss, and that it incurs small runtime overhead (4.2 percent on average and up to 9.7 percent) for the coarse-grained memory management. For real workloads, we deploy GMODx on the TensorFlow framework, it only causes 0.8 percent overhead on average (up to 1.8 percent)."",""1558-2183"","""",""10.1109/TPDS.2020.3042965"",""National Natural Science Foundation of China(grant numbers:61972137,61772183)"; Natural Science Foundation of Hunan Province(grant numbers:2016JJ3042);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286775"",""Buffer overflows";CUDA;GPGPU;"unified memory"",""Graphics processing units";Kernel;Memory management;Runtime;Resource management;Performance evaluation;"Instruction sets"","""",""1"","""",""57"",""IEEE"",""8 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Data Loader for Fast Sampling-Based GNN Training on Large Graphs,""Y. Bai"; C. Li; Z. Lin; Y. Wu; Y. Miao; Y. Liu;" Y. Xu"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Apr 2021"",""2021"",""32"",""10"",""2541"",""2556"",""Emerging graph neural networks (GNNs) have extended the successes of deep learning techniques against datasets like images and texts to more complex graph-structured data. By leveraging GPU accelerators, existing frameworks combine mini-batch and sampling for effective and efficient model training on large graphs. However, this setup faces a scalability issue since loading rich vertex features from CPU to GPU through a limited bandwidth link usually dominates the training cycle. In this article, we propose PaGraph, a novel, efficient data loader that supports general and efficient sampling-based GNN training on single-server with multi-GPU. PaGraph significantly reduces the data loading time by exploiting available GPU resources to keep frequently-accessed graph data with a cache. It also embodies a lightweight yet effective caching policy that takes into account graph structural information and data access patterns of sampling-based GNN training simultaneously. Furthermore, to scale out on multiple GPUs, PaGraph develops a fast GNN-computation-aware partition algorithm to avoid cross-partition access during data-parallel training and achieves better cache efficiency. Finally, it overlaps data loading and GNN computation for further hiding loading costs. Evaluations on two representative GNN models, GCN and GraphSAGE, using two sampling methods, Neighbor and Layer-wise, show that PaGraph could eliminate the data loading time from the GNN training pipeline, and achieve up to 4.8× performance speedup over the state-of-the-art baselines. Together with preprocessing optimization, PaGraph further delivers up to 16.0× end-to-end speedup."",""1558-2183"","""",""10.1109/TPDS.2021.3065737"",""National Key R&D Program of China(grant numbers:2018YFB1003204)"; National Natural Science Foundation of China(grant numbers:61 802 358,61 772 484); USTC Research Funds of the Double First-Class Initiative(grant numbers:YD2150002006); Huawei and Microsoft;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376972"",""Graph neural network";cache;large graph;graph partition;pipeline;"multi-GPU"",""Training";Graphics processing units;Loading;Computational modeling;Load modeling;Partitioning algorithms;"Deep learning"","""",""9"","""",""60"",""IEEE"",""12 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Efficient Forwarding Anomaly Detection in Software-Defined Networks,""Q. Li"; Y. Liu; Z. Liu; P. Zhang;" C. Pang"",""Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China"; Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; School of Computer Science, Xi'an Jiaotong University, Xi'an, China;" Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2021"",""2021"",""32"",""11"",""2676"",""2690"",""Data centers, the critical infrastructure underpinning Cloud computing, often employ Software-Defined Networks (SDN) to manage cluster, wide-area and enterprise networks. As the network forwarding in SDN is dynamically programmed by controllers, it is crucial to ensure that the controller intent is correctly translated into underlying forwarding rules. Therefore, detecting and locating forwarding anomalies in SDN is a fundamental problem in production networks. Existing research proposals, roughly categorized into probing-based, packet piggybacking-based, and flow statistics analysis-based, either impose significant overhead or do not provide sufficient coverage for certain forwarding anomalies. In this article, we propose ${\sf FADE}$FADE, a controllable and passive measuring scheme to simultaneously deliver detection efficiency and accuracy. ${\sf FADE}$FADE first analyzes the entire network topology and flow rules, and then computes a minimal set of flows that can cover all forwarding rules. For each selected network flow, ${\sf FADE}$FADE decides the optimal number of monitoring positions on its path (much less than total number of hops), and installs dedicated rules to collect flow statistics. ${\sf FADE}$FADE controls the installation and expiration of these rules, along with unique flow labels, to guarantee the accuracy of collected statistics, based on which ${\sf FADE}$FADE algorithmically decides whether a forwarding anomaly is detected, and if so it further locates the anomaly. On top of ${\sf FADE}$FADE, we propose ${\sf iFADE}$iFADE (a more scalable version of ${\sf FADE}$FADE) to further optimize the usage and deployment of dedicated measurement rules. ${\sf iFADE}$iFADE achieves over 40 percent rule reduction compared with ${\sf FADE}$FADE . We implement a prototype of both ${\sf FADE}$FADE and ${\sf iFADE}$iFADE in about 12000 lines of code and evaluate the prototype extensively. The experiment results demonstrate ${\sf (i)}$(i) ${\sf FADE}$FADE and ${\sf iFADE}$iFADE are accurate, e.g., they achieve over 95 percent true positive rate and 99 percent true negative rate in anomaly detection";" ${\sf (ii)}$(ii) ${\sf FADE}$FADE and ${\sf iFADE}$iFADE are lightweight, e.g., they reduce the overhead of control messages compared with state-of-the-art by about 50 and 90 percent, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3068135"",""National Key Research and Development Program of China(grant numbers:2018YFB1800304)"; National Natural Science Foundation of China(grant numbers:61572278,61772412); BNRist(grant numbers:BNR2020RC01013); K. C. Wong Education Foundation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387572"",""Software defined networking";cross-plane consistency check;"forwarding anomaly"",""Anomaly detection";Probes;Network topology;Data centers;Prototypes;Production;"Hardware"","""",""12"","""",""49"",""CCBY"",""26 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Efficient Methods for Mapping Neural Machine Translator on FPGAs,""Q. Li"; X. Zhang; J. Xiong; W. -M. Hwu;" D. Chen"",""Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Champaign, IL, USA"; Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Champaign, IL, USA; IBM T.J. Watson Research Center, NY, USA; Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Champaign, IL, USA;" Department of Electrical and Computer Engineering, University of Illinois Urbana-Champaign, Champaign, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Feb 2021"",""2021"",""32"",""7"",""1866"",""1877"",""Neural machine translation (NMT) is one of the most critical applications in natural language processing (NLP) with the main idea of converting text in one language to another using deep neural networks. In recent year, we have seen continuous development of NMT by integrating more emerging technologies, such as bidirectional gated recurrent units (GRU), attention mechanisms, and beam-search algorithms, for improved translation quality. However, with the increasing problem size, the real-life NMT models have become much more complicated and difficult to implement on hardware for acceleration opportunities. In this article, we aim to exploit the capability of FPGAs to deliver highly efficient implementations for real-life NMT applications. We map the inference of a large-scale NMT model with total computation of 172 GFLOP to a highly optimized high-level synthesis (HLS) IP and integrate the IP into Xilinx VCU118 FPGA platform. The model has widely used key features for NMTs, including the bidirectional GRU layer, attention mechanism, and beam search. We quantize the model to mixed-precision representation in which parameters and portions of calculations are in 16-bit half precision, and others remain as 32-bit floating-point. Compared to the float NMT implementation on FPGA, we achieve 13.1× speedup with an end-to-end performance of 22.0 GFLOPS without any accuracy degradation. Based on our knowledge, this is the first work that successfully implements a real-life end-to-end NMT model to an FPGA on board."",""1558-2183"","""",""10.1109/TPDS.2020.3047371"",""IBM-Illinois Center for Cognitive Computing System Research";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309170"",""Hardware-efficient inference";neural machine translation;FPGA;"high level synthesis"",""Computational modeling";Field programmable gate arrays;Decoding;Task analysis;Hardware;IP networks;"Dictionaries"","""",""8"","""",""35"",""CCBY"",""25 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Efficient Virtual Network Embedding of Cloud-Based Data Center Networks into Optical Networks,""W. Fan"; F. Xiao; X. Chen; L. Cui;" S. Yu"",""College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China"; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China; College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China; School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia;" School of Computer Science, University of Technology Sydney, Ultimo, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""24 May 2021"",""2021"",""32"",""11"",""2793"",""2808"",""The demand for data center bandwidth has exploded due to the continuous development of cloud computing, causing the use of network resources close to saturation. Optical network has become an encouraging technology for many burgeoning networks and parallel/distributed computing applications because of its huge bandwidth. This article focuses on efficient embedding of data centers into optical networks, which aims to reduce complexity of the network topology by using the parallel transmission characteristics of optical fiber. We first present a novel virtual network embedding (VNE) mathematical model used for optical data center networks. Then we derive a priority of location VNE algorithm according to node proximity sensing and path comprehensive evaluation. Furthermore, we propose routing and wavelength assignment for DCNs into optical networks, and identify the lower bound of the required number of wavelengths. Extensive evaluations show that the proposed embedding algorithm can reduce the average waiting time of virtual network requests by 20 percent, increase the request acceptance rate and revenue-overhead ratio by 13 percent, as compared to the latest VNE algorithm."",""1558-2183"","""",""10.1109/TPDS.2021.3075296"",""National Natural Science Foundation of China(grant numbers:61932013)"; Research Foundation of Jiangsu(grant numbers:BRA2020065); Natural Science Foundation of Jiangsu Province(grant numbers:BK20200753); NUPTSF(grant numbers:NY219151);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415134"",""Data center network";network virtualization;optical network;embedding;"performance evaluation"",""Optical fiber networks";Bandwidth;Heuristic algorithms;Optical sensors;Approximation algorithms;Multicast algorithms;"Data centers"","""",""19"","""",""42"",""IEEE"",""23 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Elastic Scheduling for Microservice Applications in Clouds,""S. Wang"; Z. Ding;" C. Jiang"",""Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University"; Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University;" Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2020"",""2021"",""32"",""1"",""98"",""115"",""Microservices are widely used for flexible software development. Recently, containers have become the preferred deployment technology for microservices because of fast start-up and low overhead. However, the container layer complicates task scheduling and auto-scaling in clouds. Existing algorithms do not adapt to the two-layer structure composed of virtual machines and containers, and they often ignore streaming workloads. To this end, this article proposes an Elastic Scheduling for Microservices (ESMS) that integrates task scheduling with auto-scaling. ESMS aims to minimize the cost of virtual machines while meeting deadline constraints. Specifically, we define the task scheduling problem of microservices as a cost optimization problem with deadline constraints and propose a statistics-based strategy to determine the configuration of containers under a streaming workload. Then, we propose an urgency-based workflow scheduling algorithm that assigns tasks and determines the type and quantity of instances for scale-up. Finally, we model the mapping of new containers to virtual machines as a variable-sized bin-packing problem and solve it to achieve integrated scaling of the virtual machines and containers. Via simulation-based experiments with well-known workflow applications, the ability of ESMS to improve the success ratio of meeting deadlines and reduce the cost is verified through comparison with existing algorithms."",""1558-2183"","""",""10.1109/TPDS.2020.3011979"",""National Basic Research Program of China (973 Program)(grant numbers:2019YFB1704102)"; National Natural Science Foundation of China(grant numbers:61672381); Fundamental Research Funds for the Central Universities(grant numbers:22120180508);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9149819"",""Auto-scaling";cloud computing;containers;microservice;"task scheduling"",""Task analysis";Scheduling;Containers;Cloud computing;Scheduling algorithms;"Virtual machining"","""",""52"","""",""53"",""IEEE"",""27 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Energy-Aware Inference Offloading for DNN-Driven Applications in Mobile Edge Clouds,""Z. Xu"; L. Zhao; W. Liang; O. F. Rana; P. Zhou; Q. Xia; W. Xu;" G. Wu"",""School of Software, Dalian University of Technology, Dalian, China"; School of Software, Dalian University of Technology, Dalian, China; Research School of Computer Science, The Australian National University, Canberra, ACT, Australia; Cardiff University, Cardiff, United Kingdom; Hubei Engineering Research Center on Big Data Security, School of Cyber Science and Engineering, Huazhong University of Science and Technology, Wuhan, China; International School of Information Science and Engineering, Dalian University of Technology, Dalian, China; School of Sichuan University, Chengdu, China;" School of Software, Dalian University of Technology, Dalian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Nov 2020"",""2021"",""32"",""4"",""799"",""814"",""With increasing focus on Artificial Intelligence (AI) applications, Deep Neural Networks (DNNs) have been successfully used in a number of application areas. As the number of layers and neurons in DNNs increases rapidly, significant computational resources are needed to execute a learned DNN model. This ever-increasing resource demand of DNNs is currently met by large-scale data centers with state-of-the-art GPUs. However, increasing availability of mobile edge computing and 5G technologies provide new possibilities for DNN-driven AI applications, especially where these application make use of data sets that are distributed in different locations. One fundamental process of a DNN-driven application in mobile edge clouds is the adoption of “inferencing” - the process of executing a pre-trained DNN based on newly generated image and video data from mobile devices. We investigate offloading DNN inference requests in a 5G-enabled mobile edge cloud (MEC), with the aim to admit as many inference requests as possible. We propose exact and approximate solutions to the problem of inference offloading in MECs. We also consider dynamic task offloading for inference requests, and devise an online algorithm that can be adapted in real time. The proposed algorithms are evaluated through large-scale simulations and using a real world test-bed implementation. The experimental results demonstrate that the empirical performance of the proposed algorithms outperform their theoretical counterparts and other similar heuristics reported in literature."",""1558-2183"","""",""10.1109/TPDS.2020.3032443"",""National Natural Science Foundation of China(grant numbers:61802048,61802047)"; Fundamental Research Funds for the Central Universities(grant numbers:DUT19RC(4)035); DUT-RU Co-Research Center of Advanced ICT for Active Life; Dalian University of Technology; Australian Research Council(grant numbers:DP200101985); National Natural Science Foundation of China(grant numbers:61972448);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234011"",""Inference offloading";mobile edge clouds;"approximation and online algorithms"",""Artificial intelligence";Cloud computing;5G mobile communication;Base stations;Task analysis;Mobile handsets;"Heuristic algorithms"","""",""47"","""",""59"",""IEEE"",""20 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Energy-Efficient Hardware-Accelerated Synchronization for Shared-L1-Memory Multiprocessor Clusters,""F. Glaser"; G. Tagliavini; D. Rossi; G. Haugou; Q. Huang;" L. Benini"",""Department of Information Technology and Electrical Engineering (D-ITET), ETH, Zürich, Switzerland"; Department of Electrical, Electronic, and Information Engineering, University of Bologna, Italy; Department of Electrical, Electronic, and Information Engineering, University of Bologna, Italy; Department of Information Technology and Electrical Engineering (D-ITET), ETH, Zürich, Switzerland; Department of Information Technology and Electrical Engineering (D-ITET), ETH, Zürich, Switzerland;" Department of Information Technology and Electrical Engineering (D-ITET), ETH, Zürich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Oct 2020"",""2021"",""32"",""3"",""633"",""648"",""The steeply growing performance demands for highly power- and energy-constrained processing systems such as end-nodes of the Internet-of-Things (IoT) have led to parallel near-threshold computing (NTC), joining the energy-efficiency benefits of low-voltage operation with the performance typical of parallel systems. Shared-L1-memory multiprocessor clusters are a promising architecture, delivering performance in the order of GOPS and over 100 GOPS/W of energy-efficiency. However, this level of computational efficiency can only be reached by maximizing the effective utilization of the processing elements (PEs) available in the clusters. Along with this effort, the optimization of PE-to-PE synchronization and communication is a critical factor for performance. In this article, we describe a light-weight hardware-accelerated synchronization and communication unit (SCU) for tightly-coupled clusters of processors. We detail the architecture, which enables fine-grain per-PE power management, and its integration into an eight-core cluster of RISC-V processors. To validate the effectiveness of the proposed solution, we implemented the eight-core cluster in advanced 22 nm FDX technology and evaluated performance and energy-efficiency with tunable microbenchmarks and a set of real-life applications and kernels. The proposed solution allows synchronization-free regions as small as 42 cycles, over 41× smaller than the baseline implementation based on fast test-and-set access to L1 memory when constraining the microbenchmarks to 10 percent synchronization overhead. When evaluated on the real-life DSP-applications, the proposed SCU improves performance by up to 92 and 23 percent on average and energy efficiency by up to 98 and 39 percent on average."",""1558-2183"","""",""10.1109/TPDS.2020.3028691"",""EU Horizon 2020 Projects OPRECOMP(grant numbers:732631)"; Eurostars(grant numbers:10691); WiPLASH(grant numbers:863337);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214436"",""Energy-efficient embedded parallel computing";fine-grain parallelism;"tightly memory-coupled multiprocessors"",""Synchronization";Hardware;Complexity theory;Parallel processing;Kernel;Benchmark testing;"Computer architecture"","""",""11"","""",""46"",""IEEE"",""6 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"ETICA: Efficient Two-Level I/O Caching Architecture for Virtualized Platforms,""S. Ahmadian"; R. Salkhordeh; O. Mutlu;" H. Asadi"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; ETH Zurich, Zurich, Switzerland;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Apr 2021"",""2021"",""32"",""10"",""2415"",""2433"",""In recent years, increased I/O demand of Virtual Machines (VMs) in large-scale data centers and cloud computing has encouraged system architects to design high-performance storage systems. One common approach to improving performance is to employ fast storage devices such as Solid-State Drives (SSDs) as an I/O caching layer for slower storage devices. SSDs provide high performance, especially on random requests, but they also have limited endurance: they support only a limited number of write operations and can therefore wear out relatively fast due to write operations. In addition to the write requests generated by the applications, each read miss in the SSD cache is served at the cost of imposing a write operation to the SSD (to copy the data block into the cache), resulting in an even larger number of writes into the SSD. Previous I/O caching schemes on virtualized platforms only partially mitigate the endurance limitations of SSD-based I/O caches";" they mainly focus on assigning efficient cache write policies and cache space to the VMs. Moreover, existing cache space allocation schemes have inefficiencies: they do not take into account the impact of cache write policy in reuse distance calculation of the running workloads and hence, reserve cache blocks for accesses that would not be served by cache. In this article, we propose an Efficient Two-Level I/O Caching Architecture (ETICA) for virtualized platforms that can significantly improve I/O latency, endurance, and cost (in terms of cache size) while preserving the reliability of write-pending data blocks. As opposed to previous one-level I/O caching schemes in virtualized platforms, our proposed architecture 1) provides two levels of cache by employing both Dynamic Random-Access Memory (DRAM) and SSD in the I/O caching layer of virtualized platforms and 2) effectively partitions the cache space between running VMs to achieve maximum performance and minimum cache size. To manage the two-level cache, unlike the previous reuse distance calculation schemes such as Useful Reuse Distance (URD), which only consider the request type and neglect the impact of cache write policy, we propose a new metric, Policy Optimized reuse Distance (POD). The key idea of POD is to effectively calculate the reuse distance and estimate the amount of two-level DRAM+SSD cache space to allocate by considering both 1) the request type and 2) the cache write policy. Doing so results in enhanced performance and reduced cache size due to the allocation of cache blocks only for the requests that would be served by the I/O cache. ETICA maintains the reliability of write-pending data blocks and improves performance by 1) assigning an effective and fixed write policy at each level of the I/O cache hierarchy and 2) employing effective promotion and eviction methods between cache levels. Our extensive experiments conducted with a real implementation of the proposed two-level storage caching architecture show that ETICA provides 45 percent higher performance, compared to the state-of-the-art caching schemes in virtualized platforms, while improving both cache size and SSD endurance by 51.7 and 33.8 percent, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3066308"",""Sharif University of Technology"; Eidgenössische Technische Hochschule Zürich;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380565"",""Virtualization";cloud computing;I/O caching;solid-state drives;data storage systems;performance;endurance;"reuse distance"",""Random access memory";Reliability;Measurement;Performance evaluation;Resource management;Computer architecture;"Power system reliability"","""",""6"","""",""89"",""IEEE"",""17 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Failure-Atomic Byte-Addressable R-tree for Persistent Memory,""S. Cho"; W. Kim; S. Oh; C. Kim; K. Koh;" B. Nam"",""Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea"; Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea; Ulsan National Institute of Science and Technology (UNIST), Ulsan, South Korea; Data Centric Computing Systems, ETRI, Daejeon, South Korea; SW Fundamental Research, ETRI, Daejeon, South Korea;" Sungkyunkwan University, Seoul"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Oct 2020"",""2021"",""32"",""3"",""601"",""614"",""In this article, we propose Failure-atomic Byte-addressable R-tree (FBR-tree) that leverages the byte-addressability, persistence, and high performance of persistent memory while guaranteeing the crash consistency. We carefully control the order of store and cacheline flush instructions and prevent any single store instruction from making an FBR-tree inconsistent and unrecoverable. We also develop a non-blocking lock-free range query algorithm for FBR-tree. Since FBR-tree allows read transactions to detect and ignore any transient inconsistent states, multiple read transactions can concurrently access tree nodes without using shared locks while other write transactions are making changes to them. Our performance study shows that FBR-tree successfully reduces the legacy logging overhead and the lock-free range query algorithm shows up to 2.6x higher query processing throughput than the shared lock-based crabbing concurrency protocol."",""1558-2183"","""",""10.1109/TPDS.2020.3028699"",""National Research Foundation of Korea(grant numbers:NRF-2016M3C4A7952587,NRF-2018R1A2B3006681)"; Institute for Information and Communications Technology Promotion(grant numbers:2018-0-00549); Electronics and Telecommunications Research Institute(grant numbers:20ZS1310);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214450"",""R-tree";persistent memory;failure-atomicity;"multidimensional indexing structure"",""Data structures";Metadata;Indexing;Computer crashes;Transient analysis;"Concurrent computing"","""",""4"","""",""60"",""IEEE"",""6 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Fast Adaptive Task Offloading in Edge Computing Based on Meta Reinforcement Learning,""J. Wang"; J. Hu; G. Min; A. Y. Zomaya;" N. Georgalas"",""Department of Computer Science, University of Exeter, Exeter, United Kingdom"; Department of Computer Science, University of Exeter, Exeter, United Kingdom; Department of Computer Science, University of Exeter, Exeter, United Kingdom; School of Information Technologies, The University of Sydney, Sydney, Australia;" Applied Research Department, British Telecom, Edinburgh, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Aug 2020"",""2021"",""32"",""1"",""242"",""253"",""Multi-access edge computing (MEC) aims to extend cloud service to the network edge to reduce network traffic and service latency. A fundamental problem in MEC is how to efficiently offload heterogeneous tasks of mobile applications from user equipment (UE) to MEC hosts. Recently, many deep reinforcement learning (DRL)-based methods have been proposed to learn offloading policies through interacting with the MEC environment that consists of UE, wireless channels, and MEC hosts. However, these methods have weak adaptability to new environments because they have low sample efficiency and need full retraining to learn updated policies for new environments. To overcome this weakness, we propose a task offloading method based on meta reinforcement learning, which can adapt fast to new environments with a small number of gradient updates and samples. We model mobile applications as Directed Acyclic Graphs (DAGs) and the offloading policy by a custom sequence-to-sequence (seq2seq) neural network. To efficiently train the seq2seq network, we propose a method that synergizes the first order approximation and clipped surrogate objective. The experimental results demonstrate that this new offloading method can reduce the latency by up to 25 percent compared to three baselines while being able to adapt fast to new environments."",""1558-2183"","""",""10.1109/TPDS.2020.3014896"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9161406"",""Multi-access edge computing";task offloading;meta reinforcement learning;"deep learning"",""Task analysis";Training;Neural networks;Heuristic algorithms;Mobile applications;Learning (artificial intelligence);"Edge computing"","""",""159"","""",""40"",""IEEE"",""6 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Fast, Accurate Processor Evaluation Through Heterogeneous, Sample-Based Benchmarking,""P. Prieto"; P. Abad; J. A. Gregorio;" V. Puente"",""Computer Engineering Group, University of Cantabria, Santander, Spain"; Computer Engineering Group, University of Cantabria, Santander, Spain; Computer Engineering Group, University of Cantabria, Santander, Spain;" Computer Engineering Group, University of Cantabria, Santander, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jun 2021"",""2021"",""32"",""12"",""2983"",""2995"",""Performance evaluation is a key task in computing and communication systems. Benchmarking is one of the most common techniques for evaluation purposes, where the performance of a set of representative applications is used to infer system responsiveness in a general usage scenario. Unfortunately, most benchmarking suites are limited to a reduced number of applications, and in some cases, rigid execution configurations. This makes it hard to extrapolate performance metrics for a general-purpose architecture, supposed to have a multi-year lifecycle, running dissimilar applications concurrently. The main culprit of this situation is that current benchmark-derived metrics lack generality, statistical soundness and fail to represent general-purpose environments. Previous attempts to overcome these limitations through random app mixes significantly increase computational cost (workload population shoots up), making the evaluation process barely affordable. To circumvent this problem, in this article we present a more elaborate performance evaluation methodology named BenchCast. Our proposal provides more representative performance metrics, but with a drastic reduction of computational cost, limiting app execution to a small and representative fraction marked through code annotation. Thanks to this labeling and making use of synchronization techniques, we generate heterogeneous workloads where every app runs simultaneously inside its Region Of Interest, making a few execution seconds highly representative of full application execution."",""1558-2183"","""",""10.1109/TPDS.2021.3080702"",""Agencia Estatal de Investigacion(grant numbers:PID2019-110051GB-I00)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432746"",""C.1 Processor Architectures";C.4 Performance of Systems;C.4.c Measurement techniques;C.4.g Measurement;evaluation;modeling;"simulation of multiple-processor systems"",""Benchmark testing";Simulation;Performance evaluation;Microarchitecture;Computational efficiency;Program processors;"Task analysis"","""",""1"","""",""39"",""IEEE"",""17 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"FedSCR: Structure-Based Communication Reduction for Federated Learning,""X. Wu"; X. Yao;" C. -L. Wang"",""Department of Computer Science, University of Hong Kong, Hong Kong, China"; Department of Computer Science, University of Hong Kong, Hong Kong, China;" Department of Computer Science, University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1565"",""1577"",""Federated Learning allows edge devices to collaboratively train a shared model on their local data without leaking user privacy. The non-independent-and-identically-distributed (Non-IID) property of data distribution, which leads to severe accuracy degradation, and enormous communication overhead for aggregating parameters should be tackled in federated learning. In this article, we conduct a detailed analysis of parameter updates on the Non-IID datasets and compare the difference with the IID setting. Experimental results exhibit that parameter update matrices are structure-sparse and show that more gradients could be identified as negligible updates on the Non-IID data. As a result, we propose a structure-based communication reduction algorithm, called FedSCR, that reduces the number of parameters transported through the network while maintaining the model accuracy. FedSCR aggregates the parameter updates over channels and filters, identifies and removes the redundant updates by comparing the aggregated values with a threshold. Unlike the traditional structured pruning methods, FedSCR retains the complete model that does not require to be retrained and fine-tuned. The local loss and weight divergence on each device vary a lot because of the unbalanced data distribution. We further propose an adaptive FedSCR, that dynamically changes the bounded threshold, to enhance the model robustness on the Non-IID data. Evaluation results show that our proposed strategies achieve almost 50 percent upstream communication reduction without loss of accuracy. FedSCR can be integrated into state-of-the-art federated learning algorithms to dramatically reduce the number of parameters pushed to the global server with a tolerable accuracy reduction."",""1558-2183"","""",""10.1109/TPDS.2020.3046250"",""Hong Kong RGC Research Impact Fund(grant numbers:R5060-19)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303442"",""Federated learning";communication reduction;"Non-IID data"",""Training";Servers;Data models;Collaborative work;Adaptation models;Performance evaluation;"Linear programming"","""",""13"","""",""39"",""IEEE"",""22 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Feluca: A Two-Stage Graph Coloring Algorithm With Color-Centric Paradigm on GPU,""Z. Zheng"; X. Shi; L. He; H. Jin; S. Wei; H. Dai;" X. Peng"",""National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, University of Warwick, Coventry, United Kingdom; National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China;" National Engineering Research Center for Big Data Technology and System / Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Aug 2020"",""2021"",""32"",""1"",""160"",""173"",""There are great challenges in performing graph coloring on GPU in general. First, the long-tail problem exists in the recursion algorithm because the conflict (i.e., different threads assign the adjacent nodes to the same color) becomes more likely to occur as the number of iterations increases. Second, it is hard to parallelize the sequential spread algorithm because the color allocation depends on the adjoining iteration. Third, the atomic operation is widely used on GPU to maintain the color list, which can greatly reduce the efficiency of GPU threads. In this article, we propose a two-stage high-performance graph coloring algorithm, called Feluca, aiming to address the above challenges. Feluca combines the recursion-based method with the sequential spread-based method. In the first stage, Feluca uses a recursive routine to color a majority of vertices in the graph. Then, it switches to the sequential spread method to color the remaining vertices in order to avoid the conflicts of the recursive algorithm. Moreover, the following techniques are proposed to further improve the graph coloring performance. i) A new method is proposed to eliminate the cycles in the graph"; ii) a top-down scheme is developed to avoid the atomic operation originally required for color selection;" and iii) a novel color-centric coloring paradigm is designed to improve the degree of parallelism for the sequential spread part. All these newly developed techniques, together with further GPU-specific optimizations such as coalesced memory access, comprise an efficient parallel graph coloring solution in Feluca. We have conducted extensive experiments on NVIDIA GPU. The results show that Feluca can achieve 1.19 - 8.39× speedup over the state-of-the-art algorithms."",""1558-2183"","""",""10.1109/TPDS.2020.3014173"",""National Key R&D Program of China(grant numbers:2017YFC0803700)"; National Natural Science Foundation of China(grant numbers:61772218);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162529"",""Graph coloring";GPGPU;parallelism;color-centric paradigm;"pipeline"",""Color";Graphics processing units;Image color analysis;Task analysis;Parallel processing;Computational modeling;"Synchronization"","""",""1"","""",""54"",""IEEE"",""7 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Fine-Grained Multi-Query Stream Processing on Integrated Architectures,""F. Zhang"; C. Zhang; L. Yang; S. Zhang; B. He; W. Lu;" X. Du"",""Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"; Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Information Systems Technology, and Design Pillar, Singapore University of Technology, and Design, Singapore; School of Computing, National University of Singapore, Singapore; Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China;" Key Laboratory of Data Engineering, and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Apr 2021"",""2021"",""32"",""9"",""2303"",""2320"",""Exploring the sharing opportunities among multiple stream queries is crucial for high-performance stream processing. Modern stream processing necessitates accelerating multiple queries by utilizing heterogeneous coprocessors, such as GPUs, and this has shown to be an effective method. Emerging CPU-GPU integrated architectures 6integrate CPU and GPU on the same chip and eliminate PCI-e bandwidth bottleneck. Such a novel architecture provides new opportunities for improving multi-query performance in stream processing but has not been fully explored by existing systems. We introduce a stream processing engine, called FineStream, for efficient multi-query window-based stream processing on CPU-GPU integrated architectures. FineStream's key contribution is a novel fine-grained workload scheduling mechanism between CPU and GPU to take advantage of both architectures. Particularly, FineStream is able to efficiently handle multiple queries in both static and dynamic streams. Our experimental results show that 1) on integrated architectures, FineStream achieves an average 52 percent throughput improvement and 36 percent lower latency over the state-of-the-art stream processing engine"; 2) compared to the coarse-grained strategy of applying different devices for multiple queries, FineStream achieves 32 percent throughput improvement;" 3) compared to the stream processing engine on the discrete architecture, FineStream on the integrated architecture achieves 10.4× price-throughput ratio, 1.8× energy efficiency, and can enjoy lower latency benefits."",""1558-2183"","""",""10.1109/TPDS.2021.3066407"",""National Natural Science Foundation of China(grant numbers:61802412)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380479"",""Fine-grained";multi-query;stream processing;CPU;GPU;"integrated architectures"",""Computer architecture";Graphics processing units;Structured Query Language;Performance evaluation;Throughput;Engines;"Bandwidth"","""",""14"","""",""68"",""IEEE"",""17 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Fine-Grained Powercap Allocation for Power-Constrained Systems Based on Multi-Objective Machine Learning,""M. Hao"; W. Zhang; Y. Wang; G. Lu; F. Wang;" A. V. Vasilakos"",""School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China"; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China; School of Computer Science and Technology, Harbin Institute of Technology, Harbin, China;" School of Electrical and Data Engineering, University Technology Sydney, Ultimo, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1789"",""1801"",""Power capping is an important solution to keep the system within a fixed power constraint. However, for the over-provisioned and power-constrained systems, especially the future exascale supercomputers, powercap needs to be reasonably allocated according to the workloads of compute nodes to achieve trade-offs among performance, energy and powercap. Thus it is necessary to model performance and energy and to predict the optimal powercap allocation strategies. Existing power allocation approaches have insufficient granularity within nodes. Modeling approaches usually model performance and energy separately, ignoring the correlation between objectives, and do not expose the Pareto-optimal powercap configurations. Therefore, this article combines the powercap with uncore frequency scaling and proposes an approach to predict the Pareto-optimal powercap configurations on the power-constrained system for input MPI and OpenMP parallel applications. Our approach first uses the elaborately designed micro-benchmarks and a small number of existing benchmarks to build the training set, and then applies a multi-objective machine learning algorithm which combines the stacked single-target method with extreme gradient boosting to build multi-objective models of performance and energy. The models can be used to predict the optimal processor and memory powercap settings, helping compute nodes perform fine-grained powercap allocation. When the optimal powercap configuration is determined, the uncore frequency scaling is used to further optimize the energy consumption. Compared with the reference powercap configuration, the predicted optimal configurations predicted by our method can achieve an average powercap reduction of 31.35 percent, an average energy reduction of 12.32 percent, and average performance degradation of only 2.43 percent."",""1558-2183"","""",""10.1109/TPDS.2020.3045983"",""National Key Research and Development Program of China(grant numbers:2017YFB0202901)"; Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010136001); National Natural Science Foundation of China(grant numbers:61672186); Shenzhen Science and Technology Research and Development Foundation(grant numbers:JCYJ20190806143418198);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301369"",""Power capping";performance and energy modeling;pareto front;"multi-objective machine learning"",""Resource management";Random access memory;Mathematical model;Computational modeling;Energy consumption;Analytical models;"Benchmark testing"","""",""9"","""",""41"",""IEEE"",""21 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"FRATO: Fog Resource Based Adaptive Task Offloading for Delay-Minimizing IoT Service Provisioning,""H. Tran-Dang";" D. -S. Kim"",""Department of IT Convergence Engineering, Kumoh National Institute of Technology, Gumi, Korea";" Department of IT Convergence Engineering, Kumoh National Institute of Technology, Gumi, Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Apr 2021"",""2021"",""32"",""10"",""2491"",""2508"",""In the IoT-based systems, the fog computing allows the fog nodes to offload and process tasks requested from IoT-enabled devices in a distributed manner instead of the centralized cloud servers to reduce the response delay. However, achieving such a benefit is still challenging in the systems with high rate of requests, which imply long queues of tasks in the fog nodes, thus exposing probably an inefficiency in terms of latency to offload the tasks. In addition, a complicated heterogeneous degree in the fog environment introduces an additional issue that many of single fogs can not process heavy tasks due to lack of available resources or limited computing capabilities. To cope with the situation, this article introduces FRATO (Fog Resource aware Adaptive Task Offloading) - a framework for the IoT-fog-cloud systems to offer the minimal service provisioning delay through an adaptive task offloading mechanism. Fundamentally, FRATO is based on the fog resource to select flexibly the optimal offloading policy, which in particular includes a collaborative task offloading solution based on the data fragment concept. In addition, two distributed fog resource allocation algorithms, namely TPRA and MaxRU are developed to deploy the optimized offloading solutions efficiently in cases of resource competition. Through the extensive simulation analysis, the FRATO-based service provisioning approaches show potential advantages in reducing the average delay significantly in the systems with high rate of service requests and heterogeneous fog environment compared with the existing solutions."",""1558-2183"","""",""10.1109/TPDS.2021.3067654"",""Ministry of Science and ICT, South Korea(grant numbers:IITP-2020-2020-0-01612)"; National Research Foundation of Korea (NRF)(grant numbers:2018R1A6A1A03024003,NRF-2020R1I1A1A01073019);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382907"",""IoT-fog-cloud systems";IoT services;fog computing;task offloading;"task division"",""Task analysis";Delays;Internet of Things;Cloud computing;Servers;Resource management;"Quality of service"","""",""48"","""",""43"",""IEEE"",""22 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"FT-CNN: Algorithm-Based Fault Tolerance for Convolutional Neural Networks,""K. Zhao"; S. Di; S. Li; X. Liang; Y. Zhai; J. Chen; K. Ouyang; F. Cappello;" Z. Chen"",""Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA"; Argonne National Laboratory, Mathematics and Computer Science Division, Lemont, IL, USA; Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA; Oak Ridge National Laboratory, Computer Science and Mathematics Division, Oak Ridge, TN, USA; Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA; Oak Ridge National Laboratory, Computer Science and Mathematics Division, Oak Ridge, TN, USA; Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA; Argonne National Laboratory, Mathematics and Computer Science Division, Lemont, IL, USA;" Department of Computer Science and Engineering, University of California, Riverside, Riverside, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1677"",""1689"",""Convolutional neural networks (CNNs) are becoming more and more important for solving challenging and critical problems in many fields. CNN inference applications have been deployed in safety-critical systems, which may suffer from soft errors caused by high-energy particles, high temperature, or abnormal voltage. Of critical importance is ensuring the stability of the CNN inference process against soft errors. Traditional fault tolerance methods are not suitable for CNN inference because error-correcting code is unable to protect computational components, instruction duplication techniques incur high overhead, and existing algorithm-based fault tolerance (ABFT) techniques cannot protect all convolution implementations. In this article, we focus on how to protect the CNN inference process against soft errors as efficiently as possible, with the following three contributions. (1) We propose several systematic ABFTschemes based on checksum techniques and analyze their fault protection ability and runtime thoroughly. Unlike traditional ABFT based on matrix-matrix multiplication, our schemes support any convolution implementations. (2) We design a novel workflow integrating all the proposed schemes to obtain a high detection/correction ability with limited total runtime overhead. (3) We perform our evaluation using ImageNet with well-known CNN models including AlexNet, VGG-19, ResNet-18, and YOLOv2. Experimental results demonstrate that our implementation can handle soft errors with very limited runtime overhead (4%~8% in both error-free and error-injected situations)."",""1558-2183"","""",""10.1109/TPDS.2020.3043449"",""Exascale Computing Project(grant numbers:17-SC-20-SC)"; National Nuclear Security Administration; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:CCF-1513201,CCF-1619253,OAC-2034169);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311863"",""Algorithm-based fault tolerance";deep learning;silent data corruption;reliability;"high-performance computing"",""Convolution";Runtime;Kernel;Fault tolerant systems;Fault tolerance;Error correction codes;"Mathematical model"","""",""26"","""",""50"",""IEEE"",""31 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"gIM: GPU Accelerated RIS-Based Influence Maximization Algorithm,""S. Shahrouz"; S. Salehkaleybar;" M. Hashemi"",""Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran"; Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran;" Department of Electrical Engineering, Learning and Intelligent Systems Laboratory, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Apr 2021"",""2021"",""32"",""10"",""2386"",""2399"",""Given a social network modeled as a weighted graph GG, the influence maximization problem seeks kk vertices to become initially influenced, to maximize the expected number of influenced nodes under a particular diffusion model. The influence maximization problem has been proven to be NP-hard, and most proposed solutions to the problem are approximate greedy algorithms, which can guarantee a tunable approximation ratio for their results with respect to the optimal solution. The state-of-the-art algorithms are based on Reverse Influence Sampling (RIS) technique, which can offer both computational efficiency and non-trivial (1-1/e-ε)(1-1/e-ε)-approximation ratio guarantee for any ε > 0ε>0. RIS-based algorithms, despite their lower computational cost compared to other methods, still require long running times to solve the problem in large-scale graphs with low values of ε. In this article, we present a novel and efficient parallel implementation of a RIS-based algorithm, namely IMM, on GPU. The proposed GPU-accelerated influence maximization algorithm, named gIM, can significantly reduce the running time on large-scale graphs with low values of ε. Furthermore, we show that gIM algorithm can solve other variations of the IM problem, only by applying minor modifications. Experimental results show that the proposed solution reduces the runtime by a factor up to 220 ×. The source code of gIM is publicly available online."",""1558-2183"","""",""10.1109/TPDS.2021.3066215"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380701"",""CUDA";GPGPU;graph diffusion process;influence maximization (IM);parallel processing;"reverse influence sampling (RIS)"",""Graphics processing units";Integrated circuit modeling;Computational modeling;Acceleration;Greedy algorithms;Diffusion processes;"Social networking (online)"","""",""4"","""",""45"",""IEEE"",""17 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"GML: Efficiently Auto-Tuning Flink's Configurations Via Guided Machine Learning,""Y. Guo"; H. Shan; S. Huang; K. Hwang; J. Fan;" Z. Yu"",""Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Science (CAS), Shenzhen, China"; JD.com American Technologies Corporation, Mountain View, CA, USA; Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Science (CAS), Shenzhen, China; Computer Science and Engineering, Chinese University of Hong Kong (CUHK), Shenzhen, China; Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Science (CAS), Shenzhen, China;" Shenzhen Institute of Advanced Technology (SIAT), Chinese Academy of Science (CAS), Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Jun 2021"",""2021"",""32"",""12"",""2921"",""2935"",""The increasingly popular fused batch-streaming big data framework, Apache Flink, has many performance-critical as well as untamed configuration parameters. However, how to tune them for optimal performance has not yet been explored. Machine learning (ML) has been chosen to tune the configurations for other big data frameworks (e.g., Apache Spark), showing significant performance improvements. However, it needs a long time to collect a large amount of training data by nature. In this article, we propose a guided machine learning (GML) approach to tune the configurations of Flink with significantly shorter time for collecting training data compared to traditional ML approaches. GML innovates two techniques. First, it leverages generative adversarial networks (GANs) to generate a part of training data, reducing the time needed for training data collection. Second, GML guides a ML algorithm to select configurations that the corresponding performance is higher than the average performance of random configurations. We evaluate GML on a lab cluster with 4 servers and a real production cluster in an internet company. The results show that GML significantly outperforms the state-of-the-art, DAC (Datasize-Aware-Configuration) (Z. Yu et al. 2018) for tuning the configurations of Spark, with 2.4× of reduced data collection time but with 30 percent reduced 99th percentile latency. When GML is used in the internet company, it reduces the latency by up to 57.8× compared to the configurations made by the company."",""1558-2183"","""",""10.1109/TPDS.2021.3081600"",""Key R&D Program of Guangdong Province(grant numbers:2019B010155003)"; National Natural Science Foundation of China(grant numbers:61672511,61702495,61802384); Shenzhen Institute of Artificial Intelligence and Robotics for Society; Chinese University of Hong Kong;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9435010"",""Big data systems";batch-stream fused processing;flink;configuration optimization;"generative adversarial networks (GAN)"",""Training data";Machine learning;Generative adversarial networks;Big Data;Optimization;"Tuning"","""",""10"","""",""43"",""IEEE"",""18 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"GPU Tensor Cores for Fast Arithmetic Reductions,""C. A. Navarro"; R. Carrasco; R. J. Barrientos; J. A. Riquelme;" R. Vega"",""Institute of Informatics, Universidad Austral de Chile, Valdivia, Chile"; Institute of Informatics, Universidad Austral de Chile, Valdivia, Chile; Laboratory of Technological Research in Pattern Recognition (LITRP), Department of DCI, Faculty of Engineering Science, Universidad Católica del Maule, San Miguel, Chile; Laboratory of Technological Research in Pattern Recognition (LITRP), Department of DCI, Faculty of Engineering Science, Universidad Católica del Maule, San Miguel, Chile;" Institute of Informatics, Universidad Austral de Chile, Valdivia, Chile"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2020"",""2021"",""32"",""1"",""72"",""84"",""This article proposes a parallel algorithm for computing the arithmetic reduction of n numbers as a set of matrix-multiply accumulate (MMA) operations that are executed simultaneously by GPU tensor cores. The analysis, assuming tensors of size m x m, shows that the proposed algorithm has a parallel running time of T(n) = 5logm2n and a speedup of S = 45log2m2 over a canonical parallel reduction. Experimental performance results on a Tesla V100 GPU show that the tensor-core based approach is energy efficient and runs up to ~ 3:2× and 2× faster than a standard GPU-based reduction and Nvidia's CUB library, respectively, while keeping the numerical error below 1 percent with respect to a double precision CPU reduction. The chained design of the algorithm allows a flexible configuration of GPU thread-blocks and the optimal values found through experimentation agree with the theoretical ones. The results obtained in this work show that GPU tensor cores are relevant not only for Deep Learning or Linear Algebra computations, but also for applications that require the acceleration of large summations."",""1558-2183"","""",""10.1109/TPDS.2020.3011893"",""FONDECYT(grant numbers:11180881)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9147055"",""Arithmetic reduction";GPU computing;tensor cores;matrix multiply accumulate;"parallel reduction"",""Graphics processing units";Tensile stress;Programming;Machine learning;Computational modeling;Acceleration;"Instruction sets"","""",""17"","""",""41"",""IEEE"",""24 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Group Reassignment for Dynamic Edge Partitioning,""H. Li"; H. Yuan; J. Huang; J. Cui; X. Ma; S. Wang; J. Yoo;" P. S. Yu"",""School of Computer Science and Technology, Xidian University, Xi'an, China"; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Engineering, Central South University, Changsha, China; Department of Information and Communication Engineering, Chungbuk National University, Cheongju, Korea;" University of Illinois at Chicago, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Apr 2021"",""2021"",""32"",""10"",""2477"",""2490"",""Graph partitioning is a mandatory step in large-scale distributed graph processing. When partitioning real-world power-law graphs, the edge partitioning algorithm performs better than the traditional vertex partitioning algorithm, because it can cut a single vertex into multiple replicas to apportion the computation. Many advanced edge partitioning methods are designed for partitioning a static graph from scratch. However, the real-world graph structure changes continuously, which leads to a decrease in partition quality and affects the performance of the graph applications. Some studies are devoted to offline repartitioning or batch incremental partitioning, but how to deal with dynamics in real-time is still worthy of in-depth study. In this article, we discuss the impact of dynamic change on partition and discover that both insertion and deletion will lead to local suboptimal partitioning, which is the reason for the degradation of partition quality. As a solution, a dynamic edge partitioning algorithm is proposed to partition dynamics in real-time. Specifically, we deal with dynamics by a distributed stream and improve partition quality by reassigning some closely connected edges. Experiments show that it is robust to initial partition quality, dynamic scale and type, and distributed scale. Compared with the state-of-the-art dynamic partitioner, it can reduce vertex-cuts by 29.5 percent. Compared with the repartitioning algorithms, it can save the partitioning time by 91.0 percent. Applied on the graph task, it can reduce the increase of communication cost and the increase of the total time of task by 41.5 and 71.4 percent."",""1558-2183"","""",""10.1109/TPDS.2021.3069292"",""Natural Science Foundation of China(grant numbers:61602354,61876138,61976168,61772394)"; Natural Science Foundation of Shaanxi Province(grant numbers:2019JM-227); National Science Foundation(grant numbers:III-1763325,III-1909323,SaTC-1930941); National Research Foundation of Korea(grant numbers:2019R1A2C2084257);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9388864"",""Dynamic graph";edge partitioning;distributed system;"edge group"",""Heuristic algorithms";Partitioning algorithms;Real-time systems;Task analysis;Electronic mail;Aerodynamics;"Social networking (online)"","""",""4"","""",""49"",""IEEE"",""29 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Hardware Accelerator Integration Tradeoffs for High-Performance Computing: A Case Study of GEMM Acceleration in N-Body Methods,""M. Asri"; D. Malhotra; J. Wang; G. Biros; L. K. John;" A. Gerstlauer"",""Electrical and Computer Engineering Department, The University of Texas at Austin, Austin, TX, USA"; Flatiron Institute, New York, USA; Electrical and Computer Engineering Department, The University of Texas at Austin, Austin, TX, USA; Institute for Computational Engineering and Sciences, The University of Texas at Austin, Austin, TX, USA; Electrical and Computer Engineering Department, The University of Texas at Austin, Austin, TX, USA;" Electrical and Computer Engineering Department, The University of Texas at Austin, Austin, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2021"",""2021"",""32"",""8"",""2035"",""2048"",""In this article, we study performance and energy saving benefits of hardware acceleration under different hardware configurations and usage scenarios for a state-of-the-art Fast Multipole Method (FMM), which is a popular N-body method. We use a dedicated Application Specific Integrated Circuit (ASIC) to accelerate General Matrix-Matrix Multiply (GEMM) operations. FMM is widely used in applications and is representative example of the workload for many HPC applications. We compare architectures that integrate the GEMM ASIC next to, in or near main memory with an on-chip coupling aimed at minimizing or avoiding repeated round-trip transfers through DRAM for communication between accelerator and CPU. We study tradeoffs using detailed and accurately calibrated x86 CPU, accelerator and DRAM simulations. Our results show that simply moving accelerators closer to the chip does not necessarily lead to performance/energy gains. We demonstrate that, while careful software blocking and on-chip placement optimizations can reduce DRAM accesses by 2X over a naive on-chip integration, these dramatic savings in DRAM traffic do not automatically translate into significant total energy or runtime savings. This is chiefly due to the application characteristics, the high idle power and effective hiding of memory latencies in modern systems. Only when more aggressive co-optimizations such as software pipelining and overlapping are applied, additional performance and energy savings can be unlocked by 37 and 35 percent respectively over baseline acceleration. When similar optimizations (pipelining and overlapping) are applied with an off-chip integration, on-chip integration delivers up to 20 percent better performance and 17 percent less total energy consumption than off-chip integration."",""1558-2183"","""",""10.1109/TPDS.2021.3056045"",""National Science Foundation(grant numbers:CCF-1817048,CCF-1725743,CCF-1337393)"; DOE(grant numbers:DE-SC0019393,DE-NA0003969); Air Force Office of Scientific Research(grant numbers:FA9550-17-1-0190);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9343690"","""",""System-on-chip";Acceleration;Random access memory;Optimization;Couplings;Computer architecture;"Software"","""",""2"","""",""54"",""IEEE"",""1 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Hierarchical Multi-Agent Optimization for Resource Allocation in Cloud Computing,""X. Gao"; R. Liu;" A. Kaushik"",""School of Electronic and Information Engineering, Beihang University, Beijing, China"; School of Electronic and Information Engineering, Beihang University, Beijing, China;" Department of Electronic and Electrical Engineering, University College London (UCL), London, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""22 Oct 2020"",""2021"",""32"",""3"",""692"",""707"",""In cloud computing, an important concern is to allocate the available resources of service nodes to the requested tasks on demand and to make the objective function optimum, i.e., maximizing resource utilization, payoffs, and available bandwidth. This article proposes a hierarchical multi-agent optimization (HMAO) algorithm in order to maximize the resource utilization and make the bandwidth cost minimum for cloud computing. The proposed HMAO algorithm is a combination of the genetic algorithm (GA) and the multi-agent optimization (MAO) algorithm. With maximizing the resource utilization, an improved GA is implemented to find a set of service nodes that are used to deploy the requested tasks. A decentralized-based MAO algorithm is presented to minimize the bandwidth cost. We study the effect of key parameters of the HMAO algorithm by the Taguchi method and evaluate the performance results. The results demonstrate that the HMAO algorithm is more effective than two baseline algorithms of genetic algorithm (GA) and fast elitist non-dominated sorting genetic algorithm (NSGA-II) in solving the large-scale optimization problem of resource allocation. Furthermore, we provide the performance comparison of the HMAO algorithm with two heuristic Greedy and Viterbi algorithms in on-line resource allocation."",""1558-2183"","""",""10.1109/TPDS.2020.3030920"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9224163"",""Cloud computing";resource allocation;resource utilization;bandwidth cost;genetic algorithm;"multi-agent optimization"",""Resource management";Task analysis;Optimization;Bandwidth;Cloud computing;Genetic algorithms;"Signal processing algorithms"","""",""30"","""",""40"",""IEEE"",""14 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"High Performance Multivariate Geospatial Statistics on Manycore Systems,""M. L. O. Salvaña"; S. Abdulah; H. Huang; H. Ltaief; Y. Sun; M. G. Genton;" D. E. Keyes"",""Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia;" Extreme Computing Research Center, Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jun 2021"",""2021"",""32"",""11"",""2719"",""2733"",""Modeling and inferring spatial relationships and predicting missing values of environmental data are some of the main tasks of geospatial statisticians. These routine tasks are accomplished using multivariate geospatial models and the cokriging technique. The latter requires the evaluation of the expensive Gaussian log-likelihood function, which has impeded the adoption of multivariate geospatial models for large multivariate spatial datasets. However, this large-scale cokriging challenge provides a fertile ground for supercomputing implementations for the geospatial statistics community as it is paramount to scale computational capability to match the growth in environmental data coming from the widespread use of different data collection technologies. In this article, we develop and deploy large-scale multivariate spatial modeling and inference on parallel hardware architectures. To tackle the increasing complexity in matrix operations and the massive concurrency in parallel systems, we leverage low-rank matrix approximation techniques with task-based programming models and schedule the asynchronous computational tasks using a dynamic runtime system. The proposed framework provides both the dense and the approximated computations of the Gaussian log-likelihood function. It demonstrates accuracy robustness and performance scalability on a variety of computer systems. Using both synthetic and real datasets, the low-rank matrix approximation shows better performance compared to exact computation, while preserving the application requirements in both parameter estimation and prediction accuracy. We also propose a novel algorithm to assess the prediction accuracy after the online parameter estimation. The algorithm quantifies prediction performance and provides a benchmark for measuring the efficiency and accuracy of several approximation techniques in multivariate spatial modeling."",""1558-2183"","""",""10.1109/TPDS.2021.3071423"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397281"",""Gaussian log-likelihood";geospatial statistics;high-performance computing;large multivariate spatial data;low-rank approximation;"multivariate modeling/prediction"",""Geospatial analysis";Computational modeling;Meteorology;Predictive models;Numerical models;Mathematical model;"Graphics processing units"","""",""7"","""",""79"",""IEEE"",""6 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"High-Performance Computing Implementations of Agent-Based Economic Models for Realizing 1:1 Scale Simulations of Large Economies,""A. Gill"; M. Lalith; S. Poledna; M. Hori; K. Fujita;" T. Ichimura"",""Department of Civil Engineering, The University of Tokyo, Bunkyo City, Tokyo, Japan"; Department of Civil Engineering and the Earthquake Research Institute, The University of Tokyo, Bunkyo City, Tokyo, Japan; International Institute for Applied Systems Analysis, Laxenburg, Austria; Japan Agency for Marine-Earth Science and Technology, Research Institute for Value-Added-Information Generation, Yokohama, Kanagawa, Japan; Department of Civil Engineering and the Earthquake Research Institute, The University of Tokyo, Bunkyo City, Tokyo, Japan;" Department of Civil Engineering and the Earthquake Research Institute, The University of Tokyo, Bunkyo City, Tokyo, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Mar 2021"",""2021"",""32"",""8"",""2101"",""2114"",""We present a scalable high-performance computing implementation of an agent-based economic model using distributed + shared-memory hybrid parallelization paradigms, capable of simulating 1:1 scale models of large economies like the eurozone. Agent-based economic models consist of millions of agents interacting over several graphs, which are either centralized or scale-free in nature. While most of the interactions are bi-directional, the interaction graphs are dense and random and keep evolving as the simulation progresses. These characteristics cause a very large and unknown number of random communications among MPI processes, posing challenges to developing scalable parallel extensions. Further, random access to large volume of data makes the algorithms highly memory-bound, severely degrading computational performance. Adopting various strategies inspired by the real-world functioning of economies, we reduce the large unknown number of communications to a known handful number. Memory-intensive algorithms are improved to make these cache-efficient, and advanced MPI functions are used to minimize communication overhead, thereby attaining higher performance and scalability. Further, an MPI + OpenMP hybrid model is developed to best utilize modern many-core computing nodes with low per-core memory capacity. It is demonstrated that our implementation can simulate a full fledged economic model with 331 million agents within 108 seconds using 128 CPU cores attaining 70 percent strong scalability."",""1558-2183"","""",""10.1109/TPDS.2021.3060462"",""Japan Society for the Promotion of Science(grant numbers:18H01675)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359510"",""Agent-based economic models";high-performance computing;one-to-one scale simulations;large economies;scale-free graphs;message passing interface;"OpenMP"",""Biological system modeling";Economics;Computational modeling;Government;Finance;Scalability;"Distributed databases"","""",""4"","""",""15"",""IEEE"",""19 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"High-Performance Routing With Multipathing and Path Diversity in Ethernet and HPC Networks,""M. Besta"; J. Domke; M. Schneider; M. Konieczny; S. D. Girolamo; T. Schneider; A. Singla;" T. Hoefler"",""Department of Computer Science, ETH Zurich, Zürich, Switzerland"; RIKEN Center for Computational Science (R-CCS), Kobe, Hyogo, Japan; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Faculty of Computer Science, Electronics and Telecommunications, AGH-UST, Kraków, Poland; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Zürich, Switzerland;" Department of Computer Science, ETH Zurich, Zürich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Dec 2020"",""2021"",""32"",""4"",""943"",""959"",""The recent line of research into topology design focuses on lowering network diameter. Many low-diameter topologies such as Slim Fly or Jellyfish that substantially reduce cost, power consumption, and latency have been proposed. A key challenge in realizing the benefits of these topologies is routing. On one hand, these networks provide shorter path lengths than established topologies such as Clos or torus, leading to performance improvements. On the other hand, the number of shortest paths between each pair of endpoints is much smaller than in Clos, but there is a large number of non-minimal paths between router pairs. This hampers or even makes it impossible to use established multipath routing schemes such as ECMP. In this article, to facilitate high-performance routing in modern networks, we analyze existing routing protocols and architectures, focusing on how well they exploit the diversity of minimal and non-minimal paths. We first develop a taxonomy of different forms of support for multipathing and overall path diversity. Then, we analyze how existing routing schemes support this diversity. Among others, we consider multipathing with both shortest and non-shortest paths, support for disjoint paths, or enabling adaptivity. To address the ongoing convergence of HPC and “Big Data” domains, we consider routing protocols developed for both HPC systems and for data centers as well as general clusters. Thus, we cover architectures and protocols based on Ethernet, InfiniBand, and other HPC networks such as Myrinet. Our review will foster developing future high-performance multipathing routing protocols in supercomputers and data centers."",""1558-2183"","""",""10.1109/TPDS.2020.3035761"",""JSPS KAKENHI(grant numbers:JP19H04119)"; Eidgenössische Technische Hochschule Zürich; Google Doctoral European Fellowship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9248644"",""Routing";multipath routing;high-performance routing;path diversity;network architectures;high-performance networks;data center networks;ethernet;TCP/IP;"InfiniBand"",""Routing";Topology;Network topology;Routing protocols;Data centers;Fats;"Ethernet"","""",""20"","""",""176"",""IEEE"",""4 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Homomorphic Sorting With Better Scalability,""G. S. Çetin"; E. Savaş;" B. Sunar"",""Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA"; Faculty of Engineering and Natural Sciences, Sabanci University, Istanbul, Turkey;" Department of Electrical and Computer Engineering, Worcester Polytechnic Institute, Worcester, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2020"",""2021"",""32"",""4"",""760"",""771"",""Homomorphic sorting is an operation that blindly sorts a given set of encrypted numbers without decrypting them (thus, there is no need for the secret key). In this article, we propose a new, efficient, and scalable method for homomorphic sorting of numbers: polynomial rank sort algorithm. To put the new algorithm in a comparative perspective, we provide an extensive survey of classical sorting algorithms and networks that are not directly suitable for homomorphic computation. We also include, in our discussions, two of our previous algorithms specifically designed for homomorphic sorting operation: direct and greedy sort, and explain how they evolve from classical sorting networks. We theoretically show that the new algorithm is superior in terms of multiplicative depth when compared with all other algorithms. When batched implementation is used, the number of comparisons is reduced from O(N2) to O(N) provided that the number of slots is larger than or equal to the number of elements in the set. Our software implementation results confirm that the new algorithm is several orders of magnitude faster than many methods in the literature. Also, the polynomial sort algorithm scales better than the fastest algorithm in the literature to the best our knowledge although for small sets the execution times are comparable. The proposed algorithm is amenable to parallel implementation as most time consuming operations in the algorithm can naturally be performed concurrently."",""1558-2183"","""",""10.1109/TPDS.2020.3030748"",""National Science Foundation(grant numbers:#1561536)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222337"",""Private computation";encrypted computing;fully homomorphic encryption;"homomorphic sorting"",""Sorting";Encryption;Software algorithms;Optimization;Indexes;"Logic gates"","""",""3"","""",""33"",""IEEE"",""13 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Hone: Mitigating Stragglers in Distributed Stream Processing With Tuple Scheduling,""W. Li"; D. Liu; K. Chen; K. Li;" H. Qi"",""iSING Laboratory, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong"; iSING Laboratory, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; iSING Laboratory, Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong; Tianjin Key Laboratory of Advanced Networking, College of Intelligence and Computing, Tianjin University, Tianjin, China;" School of Computer Science and Technology, Dalian University of Technology, Dalian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2021"",""2021"",""32"",""8"",""2021"",""2034"",""Low latency stream processing on large clusters consisting of hundreds to thousands of servers is an increasingly important challenge. A crucial barrier to tackling this challenge is stragglers, i.e., tasks that are significantly straggling behind others in processing the stream data. However, prior straggler mitigation solutions have significant limitations. They balance streaming workloads among tasks but may incur imbalanced backlogs when the workloads exhibit variance, causing stragglers as well. Fortunately, we observe that carefully scheduling the outgoing tuples of different tasks can yield benefits for balancing backlogs, and thus avoids stragglers. To this end, we present Hone, a tuple scheduler that aims to minimize the maximum queue backlog of all tasks over time. Hone leverages an online Largest-Backlog-First (LBF) algorithm with a provable good competitive ratio to perform efficient tuple scheduling. We have implemented Hone based on Apache Storm and evaluated it extensively via both simulations and testbed experiments. Our results show that under the same workload balancing strategy-shuffle grouping, Hone outperforms the original Storm significantly, with the end-to-end tuple processing latency reduced by 78.7 percent on average."",""1558-2183"","""",""10.1109/TPDS.2021.3051059"",""Hong Kong RGC TRS(grant numbers:T41-603/20-R,GRF-16215119)"; National Natural Science Foundation of China(grant numbers:62002259,62032017,61772251,61772112); Science Innovation Foundation of Dalian(grant numbers:2019J12GX037);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9320519"",""Distributed stream processing";tuple scheduling;straggler task;"backlog balancing"",""Task analysis";Storms;Scheduling;Instruction sets;Schedules;Computer science;"Technological innovation"","""",""7"","""",""55"",""IEEE"",""12 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Identifying Degree and Sources of Non-Determinism in MPI Applications Via Graph Kernels,""D. Chapp"; N. Tan; S. Bhowmick;" M. Taufer"",""University of Tennessee at Knoxville, Knoxville, TN, USA"; University of Tennessee at Knoxville, Knoxville, TN, USA; University of North Texas, Denton, TX, USA;" University of Tennessee at Knoxville, Knoxville, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Jun 2021"",""2021"",""32"",""12"",""2936"",""2952"",""As the scientific community prepares to deploy an increasingly complex and diverse set of applications on exascale platforms, the need to assess reproducibility of simulations and identify the root causes of reproducibility failures increases correspondingly. One of the greatest challenges facing reproducibility issues at exascale is the inherent non-determinism at the level of inter-process communication. The use of non-deterministic communication constructs is necessary to boost performance, but communication non-determinism can also hamper software correctness and result reproducibility. To address this challenge, we propose a software framework for identifying the percentage and sources of communication non-determinism. We model parallel executions as directed graphs and leverage graph kernels to characterize run-to-run variations in inter-process communication. We demonstrate the effectiveness of graph kernel similarity as a proxy for non-determinism, by showing that these kernels can quantify the type and degree of non-determinism present in communication patterns. To demonstrate our framework’s ability to link and quantify runtime non-determinism to root sources, demonstrate with present for an adaptive mesh refinement application, where our framework automatically quantifies the impact of function calls on non-determinism, and a Monte Carlo application, where our framework automatically quantifies the impact of parameter configurations on non-determinism."",""1558-2183"","""",""10.1109/TPDS.2021.3081530"",""National Science Foundation(grant numbers:1900888,1900765)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9435018"",""Non-determinism";reproducibility;debugging;trace analysis;"graph similarity"",""Reproducibility of results";Software engineering;Program processors;Computer bugs;"Runtime"","""",""4"","""",""46"",""CCBYNCND"",""18 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"iMLBench: A Machine Learning Benchmark Suite for CPU-GPU Integrated Architectures,""C. Zhang"; F. Zhang; X. Guo; B. He; X. Zhang;" X. Du"",""Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; School of Computing, National University of Singapore, Singapore; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China;" Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1740"",""1752"",""Utilizing heterogeneous accelerators, especially GPUs, to accelerate machine learning tasks has shown to be a great success in recent years. GPUs bring huge performance improvements to machine learning and greatly promote the widespread adoption of machine learning. However, the discrete CPU-GPU architecture design with high PCIe transmission overhead decreases the GPU computing benefits in machine learning training tasks. To overcome such limitations, hardware vendors release CPU-GPU integrated architectures with shared unified memory. In this article, we design a benchmark suite for machine learning training on CPU-GPU integrated architectures, called iMLBench, covering a wide range of machine learning applications and kernels. We mainly explore two features on integrated architectures: 1) zero-copy, which means that the PCIe overhead has been eliminated for machine learning tasks and 2) co-running, which means that the CPU and the GPU co-run together to process a single machine learning task. Our experimental results on iMLBench show that the integrated architecture brings an average 7.1× performance improvement over the original implementations. Specifically, the zero-copy design brings 4.65× performance improvement, and co-running brings 1.78× improvement. Moreover, integrated architectures exhibit promising results from both performance-per-dollar and energy perspectives, achieving 6.50× performance-price ratio while 4.06× energy efficiency over discrete GPUs. The benchmark is open-sourced at https://github.com/ChenyangZhang-cs/iMLBench."",""1558-2183"","""",""10.1109/TPDS.2020.3046870"",""National Key Research and Development Program of China(grant numbers:2018YFB1004401)"; National Natural Science Foundation of China(grant numbers:61802412,61732014,61972403,62072459,U1911203); State Key Laboratory of Computer Architecture(grant numbers:CARCHA202007); MoE AcRF Tier 1(grant numbers:T1 251RES1824); Tier 2(grant numbers:MOE2017-T2-1-122);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305972"",""Machine learning";benchmark;CPU;GPU;"integrated architectures"",""Computer architecture";Machine learning;Benchmark testing;Graphics processing units;Task analysis;Hardware;"Training"","""",""8"","""",""58"",""IEEE"",""23 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Improved MPC Algorithms for Edit Distance and Ulam Distance,""M. Boroujeni"; M. Ghodsi;" S. Seddighin"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran;" Toyota Technological Institute at Chicago, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""24 May 2021"",""2021"",""32"",""11"",""2764"",""2776"",""Edit distance is one of the most fundamental problems in combinatorial optimization to measure the similarity between strings. Ulam distance is a special case of edit distance where no character is allowed to appear more than once in a string. Recent developments have been very fruitful for obtaining fast and parallel algorithms for both edit distance and Ulam distance. In this work, we present an almost optimal MPC (massively parallel computation) algorithm for Ulam distance and improve MPC algorithms for edit distance. Our algorithm for Ulam distance is almost optimal in the sense that (1) the approximation factor of our algorithm is 1+ε1+ε, (2) the round complexity of our algorithm is constant, (3) the total memory of our algorithm is almost linear (~Oε (n)Õε(n)), and (4) the overall running time of our algorithm is almost linear which is the best known for Ulam distance. We also improve the work of Hajiaghayi et al. for edit distance in terms of total memory. The best previously known MPC algorithm for edit distance requires ~O(n2x)Õ(n2x) machines when the memory of each machine is bounded by ~O(n1-x)Õ(n1-x). In this work, we improve the number of machines to ~O(n(9/5)x)Õ(n(9/5)x) while keeping the memory limit intact. Moreover, the round complexity of our algorithm is constant and the total running time of our algorithm is truly subquadratic. However, our improvement comes at the expense of a constant factor in the approximation guarantee of the algorithm. This improvement is inspired by the recent techniques of Boroujeni et al. and Chakraborty et al. for obtaining truly subquadratic time algorithms for edit distance."",""1558-2183"","""",""10.1109/TPDS.2021.3076534"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419758"",""MapReduce";parallel algorithms;approximation algorithms;ulam distance;"edit distance"",""Approximation algorithms";Optimized production technology;Computer science;Computational modeling;Complexity theory;Transforms;"Distributed databases"","""",""1"","""",""31"",""IEEE"",""29 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving HW/SW Adaptability for Accelerating CNNs on FPGAs Through A Dynamic/Static Co-Reconfiguration Approach,""L. Gong"; C. Wang; X. Li;" X. Zhou"",""School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China;" School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1854"",""1865"",""With the continuous evolution of Convolutional Neural Networks (CNNs) and the improvement of the computing capability of FPGAs, the deployment of CNN accelerator based on FPGA has become more and more popular in various computing scenarios. The key element of implementing these accelerators is to take full advantage of underlying hardware characteristics to adapt to the computational features of the software-level CNN model. To achieve this goal, however, previous designs mainly focus on the static hardware reconfiguration pattern, which is not flexible enough and can hardly make the accelerator architecture and the CNN features fully fit, resulting in inefficient computations and data communications. By leveraging the dynamic partial reconfiguration technology equipped in the modern FPGA devices, in this article, we propose a new accelerator architecture for implementing CNNs on FPGAs in which static and dynamic reconfigurabilities of the hardware are cooperatively utilized to maximize the acceleration efficiency. Based on this architecture, we further present a systematic design and optimization methodology for implementing the specific CNN model in the particular computing scenario, in which a static design space exploration method and a reinforcement learning-based decision method are proposed to obtain the optimal static hardware configuration and run-time reconfiguration strategy respectively. We evaluate our proposal by implementing three widely used CNN models, AlexNet, VGG16C, and ResNet34, on the Xilinx ZCU102 FPGA platform. Experimental results show that our implementations on average can achieve 683 GOPS under 16-bit fixed data type and 1.37 TOPS under 8-bit fixed data type for three targeted CNN models, and improve the computational density from 1.1× to 1.91× compared with previous implementations on the same type of FPGA platform."",""1558-2183"","""",""10.1109/TPDS.2020.3046762"",""National Science Foundation of China(grant numbers:61976200,61772482)"; Jiangsu Provincial Natural Science Foundation(grant numbers:BK20181193); Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2017497); Fundamental Research Funds for the Central Universities(grant numbers:WK2150110003); 2019 Youth Innovation Fund of University of Science and Technology of China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305957"",""Convolutional neural networks";FPGA;hardware accelerator;computing adapativity;"dynamic partial reconfiguration"",""Hardware";Field programmable gate arrays;Computational modeling;Acceleration;Mathematical model;Convolution;"Accelerator architectures"","""",""9"","""",""22"",""IEEE"",""23 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Improving the Performance of Deduplication-Based Storage Cache via Content-Driven Cache Management Methods,""Y. Tan"; C. Xu; J. Xie; Z. Yan; H. Jiang; W. Srisa-an; X. Chen;" D. Liu"",""College of Computer Science, Chongqing University, Chongqing, China"; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; HewlettPackard Enterprise, San Jose, USA; University of Texas Arlington, Arlington, USA; University of Nebraska Lincoln, Lincoln, USA; College of Computer Science, Chongqing University, Chongqing, China;" College of Computer Science, Chongqing University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Aug 2020"",""2021"",""32"",""1"",""214"",""228"",""Data deduplication, as a proven technology for effective data reduction in backup and archiving storage systems, is also showing promises in increasing the logical space capacity for storage caches by removing redundant data. However, our in-depth evaluation of the existing deduplication-aware caching algorithms reveals that they only work well when the cached block size is set to 4 KB. Unfortunately, modern storage systems often set the block size to be much larger than 4 KB, and in this scenario, the overall performance of these caching schemes drops below that of the conventional replacement algorithms without any deduplication. There are several reasons for this performance degradation. The first reason is the deduplication overhead, which is the time spent on generating the data fingerprints and their use to identify duplicate data. Such overhead offsets the benefits of deduplication. The second reason is the extremely low cache space utilization caused by read and write alignment. The third reason is that existing algorithms only exploit access locality to identify block replacement. There is a lost opportunity to effectively leverage the content usage patterns such as intensity of content redundancy and sharing in deduplication-based storage caches to further improve performance. We propose CDAC, a Content-driven Deduplication-Aware Cache, to address this problem. CDAC focuses on exploiting the content redundancy in blocks and intensity of content sharing among source addresses in cache management strategies. We have implemented CDAC based on LRU and ARC algorithms, called CDAC-LRU and CDAC-ARC respectively. Our extensive experimental results show that CDAC-LRU and CDAC-ARC outperform the state-of-the-art deduplication-aware caching algorithms, D-LRU, and D-ARC, by up to 23.83X in read cache hit ratio, with an average of 3.23X, and up to 53.3 percent in IOPS, with an average of 49.8 percent, under a real-world mixed workload when the cache size ranges from 20 to 50 percent of the workload size and the block size ranges from 4KB to 32 KB."",""1558-2183"","""",""10.1109/TPDS.2020.3012704"",""Wuhan National Laboratory for Optoelectronics(grant numbers:2019WNLOKF009)"; Fundamental Research Funds for the Central Universities(grant numbers:2019CDJGFJSJ001); National Natural Science Foundation of China(grant numbers:61402061,61672116,61802038); Chongqing High-Tech Research Program(grant numbers:cstc2016jcyjA0274,cstc2016jcyjA0332); China Postdoctoral Science Foundation(grant numbers:2017M620412); Chongqing Postdoctoral Special Science Foundation(grant numbers:XmT2018003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9152152"",""Data deduplication";storage cache;"content sharing"",""Metadata";Cache storage;Redundancy;Performance evaluation;Distributed databases;Degradation;"Indexes"","""",""9"","""",""39"",""IEEE"",""29 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Investigating the Adoption of Hybrid Encrypted Cloud Data Deduplication With Game Theory,""X. Liang"; Z. Yan; R. H. Deng;" Q. Zheng"",""State Key Lab on Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China"; State Key Lab on Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, China; School of Information Systems, Singapore Management University, Singapore;" School of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Oct 2020"",""2021"",""32"",""3"",""587"",""600"",""Encrypted data deduplication, along with different preferences in data access control, brings the birth of hybrid encrypted cloud data deduplication (H-DEDU for short). However, whether H-DEDU can be successfully deployed in practice has not been seriously investigated. Obviously, the adoption of H-DEDU depends on whether it can bring economic benefits to all stakeholders. But existing economic models of cloud storage fail to support H-DEDU due to complicated interactions among stakeholders. In this article, we establish a formal economic model of H-DEDU by formulating the utilities of all involved stakeholders, i.e., data holders, data owners, and Cloud Storage Providers (CSPs). Then, we construct a multi-stage Stackelberg game, which consists of Holder Participation Game, Owner Online Game, and CSP Pricing Game, to capture the interactions among all system stakeholders. We further analyze the conditions of the existence of a sub-game perfect Nash Equilibrium and propose a gradient-based algorithm to help the stakeholders choose near-optimal strategies. Extensive experiments show the feasibility of the proposed algorithm in achieving the Nash Equilibrium of the Stackelberg game. Additionally, we investigate the effects of parameters related to CSP, data owners and data holders on H-DEDU adoption. Our study advises all stakeholders the best strategies to adopt H-DEDU."",""1558-2183"","""",""10.1109/TPDS.2020.3028685"",""National Natural Science Foundation of China(grant numbers:61672410,62072351,61802293)"; Academy of Finland(grant numbers:308087,314203,335262); Shaanxi Innovation Team Project(grant numbers:2018TD-007); Higher Education Discipline Innovation Project(grant numbers:B16037);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214531"",""Cloud computing";deduplication;gradient-based algorithm;"multi-stage stackelberg game"",""Games";Stakeholders;Economics;Biological system modeling;Cloud computing;Cryptography;"Game theory"","""",""10"","""",""42"",""IEEE"",""6 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"IPPTS: An Efficient Algorithm for Scientific Workflow Scheduling in Heterogeneous Computing Systems,""H. Djigal"; J. Feng; J. Lu;" J. Ge"",""School of Computer and Information, Hohai University, Nanjing, China"; School of Computer and Information, Hohai University, Nanjing, China; School of Computer and Information, Hohai University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Dec 2020"",""2021"",""32"",""5"",""1057"",""1071"",""Efficient scheduling algorithms are key for attaining high performance in heterogeneous computing systems. In this article, we propose a new list scheduling algorithm for assigning task graphs to fully connected heterogeneous processors with an aim to minimize the scheduling length. The proposed algorithm, called Improved Predict Priority Task Scheduling (IPPTS) algorithm has two phases: task prioritization phase, which gives priority to tasks, and processor selection phase, which selects a processor for a task. The IPPTS algorithm has a quadratic time complexity as the related algorithms for the same goal, that is $O(t^{2} \times p)$O(t2×p), for $t$t tasks and $p$p processors. Our algorithm reduces the scheduling length significantly by looking ahead in both task prioritization phase and processor selection phase. In this way, the algorithm is looking ahead to schedule a task and its heaviest successor task to the optimistic processor, i.e., the processor that minimizes their computation and communication costs. The experiments based on both randomly generated graphs and graphs of real-world applications show that the IPPTS algorithm significantly outperforms previous list scheduling algorithms in terms of makespan, speedup, makespan standard deviation, efficiency, and frequency of best results."",""1558-2183"","""",""10.1109/TPDS.2020.3041829"",""National Key R&D Program of China(grant numbers:2018YFC0407901)"; National Natural Science Foundation of China(grant numbers:61370091,61602151);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9276443"",""Workflow scheduling";list scheduling;random graphs generator;scientific workflows;"heterogeneous systems"",""Task analysis";Program processors;Scheduling algorithms;Prediction algorithms;Heuristic algorithms;Heterogeneous networks;"Dynamic scheduling"","""",""32"","""",""41"",""IEEE"",""2 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Joint SFC Deployment and Resource Management in Heterogeneous Edge for Latency Minimization,""Y. Liu"; X. Shang;" Y. Yang"",""Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA"; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA;" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Mar 2021"",""2021"",""32"",""8"",""2131"",""2143"",""With the advancement of edge computing and network function virtualization, it is promising to provide flexible and low-latency network services at the network edge. However, due to resource limitation and heterogeneity of servers at the edge, it is unlikely to achieve an efficient service function chain deployment without considering the resource management of edge servers jointly. In this article, we consider the Joint Service function chain Deployment and Resource Management problem (JSDRM) in heterogeneous edge environments with the goal of minimizing the total system latency. We prove the NP-hardness of JSDRM and propose a scheme called JOint service function chain deployment and resource management Scheme (JOS) based on a game-theoretic approach to deploy service function chains and manage resources. We prove that JOS has a constant approximation ratio of 2.62 Extensive simulation results show that our scheme performs comparably to the optimal solution and much better than the baselines. The simulation results also show that the proposed scheme is time-efficient."",""1558-2183"","""",""10.1109/TPDS.2021.3062341"",""National Science Foundation(grant numbers:CCF-1717731)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9363567"",""Network function virtualization";edge computing;"latency minimization"",""Servers";Resource management;Service function chaining;Routing;Minimization;Bandwidth;"Simulation"","""",""19"","""",""30"",""IEEE"",""25 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Joint Task Scheduling and Containerizing for Efficient Edge Computing,""J. Zhang"; X. Zhou; T. Ge; X. Wang;" T. Hwang"",""UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China"; UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China; UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China; UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China;" Yonsei University, Seoul, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Mar 2021"",""2021"",""32"",""8"",""2086"",""2100"",""Container-based operation system (OS) level virtualization has been adopted by many edge-computing platforms. However, for an edge server, inter-container communications, and container management consume significant CPU resources. Given an application composed of interdependent tasks, the number of such operations is closely related to the dependency between the scheduled tasks. Thus, to improve the execution efficiency of an application in an edge server, task scheduling and task containerizing need to be considered together. To this end, a joint task scheduling and containerizing (JTSC) scheme is developed in this article. Experiments are first carried out to quantify the resource utilization of container operations. System models are then built to capture the features of task execution in containers in an edge server with multiple processors. With these models, joint task scheduling and containerizing is conducted as follows. First, tasks are scheduled without considering containerization, which results in initial schedules. Second, based on system models and guidelines gained from the initial schedules, several containerization algorithms are designed to map tasks to containers. Third, task execution durations are updated by adding the time for inter-container communications, and then the task schedules are updated accordingly. The JTSC scheme is evaluated through extensive simulations. The results show that it reduces inefficient container operations and enhances the execution efficiency of applications by 60 percent."",""1558-2183"","""",""10.1109/TPDS.2021.3059447"",""National Natural Science Foundation of China(grant numbers:61771312)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9355003"",""Container";task scheduling;resource consumption;execution efficiency;"edge computing"",""Containers";Task analysis;Servers;Schedules;Image edge detection;Edge computing;"Virtualization"","""",""38"","""",""56"",""IEEE"",""16 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"K-Athena: A Performance Portable Structured Grid Finite Volume Magnetohydrodynamics Code,""P. Grete"; F. W. Glines;" B. W. O'Shea"",""Department of Physics and Astronomy, Michigan State University, East Lansing, USA"; Department of Physics and Astronomy, Michigan State University, East Lansing, USA;" Department of Physics and Astronomy, Michigan State University, East Lansing, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2020"",""2021"",""32"",""1"",""85"",""97"",""Large scale simulations are a key pillar of modern research and require ever-increasing computational resources. Different novel manycore architectures have emerged in recent years on the way towards the exascale era. Performance portability is required to prevent repeated non-trivial refactoring of a code for different architectures. We combine ATHENA++, an existing magnetohydrodynamics (MHD) CPU code, with KOKKOS, a performance portable on-node parallel programming paradigm, into K-ATHENA to allow efficient simulations on multiple architectures using a single codebase. We present profiling and scaling results for different platforms including Intel Skylake CPUs, Intel Xeon Phis, and NVIDIA GPUs. K-ATHENA achieves > 108 cell-updates/s on a single V100 GPU for second-order double precision MHD calculations, and a speedup of 30 on up to 24 576 GPUs on Summit (compared to 172,032 CPU cores), reaching 1:94 × 1012 total cell-updates/s at 76 percent parallel efficiency. Using a roofline analysis we demonstrate that the overall performance is currently limited by DRAM bandwidth and calculate a performance portability metric of 62.8 percent. Finally, we present the implementation strategies used and the challenges encountered in maximizing performance. This will provide other research groups with a straightforward approach to prepare their own codes for the exascale era. K-ATHENA is available at https://gitlab.com/pgrete/kathena."",""1558-2183"","""",""10.1109/TPDS.2020.3010016"",""NASA Astrophysics Theory(grant numbers:#NNX15AP39G)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9143480"",""D.2.8.b performance measures";D.3.2.d concurrent, distributed and parallel languages;I.6.8.h parallel;J.2.i physics;"J.2.c astronomy"",""Magnetohydrodynamics";Computer architecture;Graphics processing units;Message systems;Programming;C++ languages;"Kernel"","""",""18"","""",""35"",""IEEE"",""17 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Large-Scale Analysis of Docker Images and Performance Implications for Container Storage Systems,""N. Zhao"; V. Tarasov; H. Albahar; A. Anwar; L. Rupprecht; D. Skourtis; A. K. Paul; K. Chen;" A. R. Butt"",""Key Laboratory of Big Data Storage and Management of MIIT, and National Engineering Laboratory for Integrated Aero-Space-Ground-Ocean Big Data Application Technology, School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China"; IBM Research-Almaden, San Jose, CA, USA; Department of Computer Science, Virginia Tech., Blacksburg, VA, USA; IBM Research-Almaden, San Jose, CA, USA; IBM Research-Almaden, San Jose, CA, USA; IBM Research-Almaden, San Jose, CA, USA; Department of Computer Science, Virginia Tech., Blacksburg, VA, USA; Department of Computer Science, Virginia Tech., Blacksburg, VA, USA;" Department of Computer Science, Virginia Tech., Blacksburg, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Dec 2020"",""2021"",""32"",""4"",""918"",""930"",""Docker containers have become a prominent solution for supporting modern enterprise applications due to the highly desirable features of isolation, low overhead, and efficient packaging of the application’s execution environment. Containers are created from images which are shared between users via a registry. The amount of data registries store is massive. For example, Docker Hub, a popular public registry, stores at least half a million public images. In this article, we analyze over 167 TB of uncompressed Docker Hub images, characterize them using multiple metrics and evaluate the potential of file-level deduplication. Our analysis helps to make conscious decisions when designing storage for containers in general and Docker registries in particular. For example, only 3 percent of the files in images are unique while others are redundant file copies, which means file-level deduplication has a great potential to save storage space. Furthermore, we carry out a comprehensive analysis of both small I/O request performance and copy-on-write performance for multiple popular container storage drivers. Our findings can motivate and help improve the design of data reduction and caching methods for images, pulling optimizations for registries, and storage drivers."",""1558-2183"","""",""10.1109/TPDS.2020.3034517"",""National Science Foundation(grant numbers:CCF-1919113,CNS-1405697,CNS-1615411,OAC-2004751)"; Chinese National Key Research and Development Program(grant numbers:2018YFB1004401); Natural Science Foundation of Beijing Municipality(grant numbers:L192027);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9242268"",""Containers";Docker;container images;container registry;deduplication;Docker hub;"container storage drivers"",""Containers";Image coding;Cows;Crawlers;Measurement;Libraries;"Ecosystems"","""",""24"","""",""49"",""IEEE"",""28 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Learning Spatiotemporal Failure Dependencies for Resilient Edge Computing Services,""A. Aral";" I. Brandić"",""Institute of Information Systems Engineering, Vienna University of Technology, Vienna, Austria";" Institute of Information Systems Engineering, Vienna University of Technology, Vienna, Austria"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Feb 2021"",""2021"",""32"",""7"",""1578"",""1590"",""Edge computing services are exposed to infrastructural failures due to geographical dispersion, ad hoc deployment, and rudimentary support systems. Two unique characteristics of the edge computing paradigm necessitate a novel failure resilience approach. First, edge servers, contrary to cloud counterparts with reliable data center networks, are typically connected via ad hoc networks. Thus, link failures need more attention to ensure truly resilient services. Second, network delay is a critical factor for the deployment of edge computing services. This restricts replication decisions to geographical proximity and necessitates joint consideration of delay and resilience. In this article, we propose a novel machine learning based mechanism that evaluates the failure resilience of a service deployed redundantly on the edge infrastructure. Our approach learns the spatiotemporal dependencies between edge server failures and combines them with the topological information to incorporate link failures. Ultimately, we infer the probability that a certain set of servers fails or disconnects concurrently during service runtime. Furthermore, we introduce Dependency- and Topology-aware Failure Resilience (DTFR), a two-stage scheduler that minimizes either failure probability or redundancy cost, while maintaining low network delay. Extensive evaluation with various real-world failure traces and workload configurations demonstrate superior performance in terms of availability, number of failures, network delay, and cost with respect to the state-of-the-art schedulers."",""1558-2183"","""",""10.1109/TPDS.2020.3046188"",""Rucon project"; Austrian Science Fund; City of Vienna;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9303420"",""Edge computing";failure resilience;quality of service;dependency learning;"dynamic Bayesian networks"",""Servers";Resilience;Edge computing;Delays;Task analysis;Spatiotemporal phenomena;"Reliability"","""",""21"","""",""58"",""CCBY"",""22 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Learning-Driven Interference-Aware Workload Parallelization for Streaming Applications in Heterogeneous Cluster,""H. Zhang"; X. Geng;" H. Ma"",""Beijing Key Lab of Intelligent Telecomm. Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China"; Beijing Key Lab of Intelligent Telecomm. Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China;" Beijing Key Lab of Intelligent Telecomm. Software and Multimedia, Beijing University of Posts and Telecommunications, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Jul 2020"",""2021"",""32"",""1"",""1"",""15"",""In the past few years, with the rapid development of CPU-GPU heterogeneous computing, the issue of task scheduling in the heterogeneous cluster has attracted a great deal of attention. This problem becomes more challenging with the need for efficient co-execution of tasks on the GPUs. However, the uncertainty of heterogeneous cluster and the interference caused by resource contention among co-executing tasks can lead to the unbalanced use of computing resource and further cause the degradation in performance of computing platform. In this article, we propose a two-stage task scheduling approach for streaming applications based on deep reinforcement learning and neural collaborative filtering, which considers fine-grained task division and task interference on the GPU. Specifically, the Learning-Driven Workload Parallelization (LDWP) method selects an appropriate execution node for the mutually independent tasks. By using the deep Q-network, the cluster-level scheduling model is online learned to perform the current optimal scheduling actions according to the runtime status of cluster environments and characteristics of tasks. The Interference-Aware Workload Parallelization (IAWP) method assigns subtasks with dependencies to the appropriate computing units, taking into account the interference of subtasks on the GPU by using neural collaborative filtering. For making the learning of neural network more efficient, we use pre-training in the two-stage scheduler. Besides, we use transfer learning technology to efficiently rebuild task scheduling model referring to the existing model. We evaluate our learning-driven and interference-aware task scheduling approach on a prototype platform with other widely used methods. The experimental results show that the proposed strategy can averagely improve the throughout for distributed computing system by 26.9 percent and improve the GPU resource utilization by around 14.7 percent."",""1558-2183"","""",""10.1109/TPDS.2020.3008725"",""National Natural Science Foundation of China(grant numbers:61720106007,61921003)"; Higher Education Discipline Innovation Project(grant numbers:B18008);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9139412"",""Parallel computing";heterogeneous computing;task scheduling;deep reinforcement learning;neural collaborative filtering;"interference aware"",""Task analysis";Graphics processing units;Processor scheduling;Interference;Optimal scheduling;Collaboration;"Throughput"","""",""12"","""",""40"",""IEEE"",""13 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Lewat: A Lightweight, Efficient, and Wear-Aware Transactional Persistent Memory System,""K. Huang"; S. Li; L. Huang; K. -L. Tan;" H. Mei"",""School of Computing, Shanghai Jiao Tong University, Shanghai, China"; Computer and Science, Shanghai Jiao Tong University, Shanghai, China; Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computing, National University of Singapore, Singapore;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Oct 2020"",""2021"",""32"",""3"",""649"",""664"",""Emerging non-volatile memory (also termed as persistent memory, PM) technologies promise persistence, byte-addressability, and DRAM-like read/write latency. A proliferation of persistent memory systems have been proposed to leverage PM for fast data persistence and expose malloc-like persistent APIs. By eliminating disk I/Os, these systems gain low-latency and high-throughput access performance for persistent data. However, there still exist non-negligible limitations in these systems, such as frequent context switches, inefficient allocation, heavy logging overhead, and lack of wear-leveling techniques. To solve these problems, we develop Lewat, a lightweight, efficient, and wear-aware transactional persistent memory system. Lewat is built in user-layer to avoid kernel/user layer context switches and enables lightweight persistent data access. We decouple the data space into slot zone and page zone. Based on this, we design different allocators in these two zones to achieve efficient allocation performance for both small-sized data and large-sized data. To minimize logging overhead, we propose an efficient adaptive logging framework. The main idea is to utilize different logging techniques for different workloads. We also propose a suite of system-coupled wear-leveling techniques that contain wear-aware allocation, wear-aware update, and write reduction. We evaluate Lewat on a real non-volatile memory platform and the experimental results show that compared with state-of-the-art persistent memory systems, Lewat has much lower latency and higher throughput."",""1558-2183"","""",""10.1109/TPDS.2020.3028385"",""National Key Research & Development Program of China(grant numbers:2018YFB1003302)"; China Scholarship Council(grant numbers:201906230180); SJTU-Huawei Innovation Research Lab Funding(grant numbers:FA2018091021-202004);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9211773"",""Non-volatile memory";memory management;persistence;consistency;crash recovery;"wear-leveling"",""Resource management";Computer crashes;Nonvolatile memory;Memory management;Micromechanical devices;Random access memory;"Libraries"","""",""4"","""",""71"",""IEEE"",""2 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"LightChain: Scalable DHT-Based Blockchain,""Y. Hassanzadeh-Nazarabadi"; A. Küpçü;" Ö. Özkasap"",""Department of Computer Engineering, Koç University, İstanbul, Turkey"; Department of Computer Engineering, Koç University, İstanbul, Turkey;" Department of Computer Engineering, Koç University, İstanbul, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""5 May 2021"",""2021"",""32"",""10"",""2582"",""2593"",""As an append-only distributed database, blockchain is utilized in a vast variety of applications including the cryptocurrency and Internet-of-Things (IoT). The existing blockchain solutions show downsides in communication and storage scalability, as well as decentralization. In this article, we propose LightChain, which is the first blockchain architecture that operates over a Distributed Hash Table (DHT) of participating peers. LightChain is a permissionless blockchain that provides addressable blocks and transactions within the network, which makes them efficiently accessible by all peers. Each block and transaction is replicated within the DHT of peers and is retrieved in an on-demand manner. Hence, peers in LightChain are not required to retrieve or keep the entire ledger. LightChain is fair as all of the participating peers have a uniform chance of being involved in the consensus regardless of their influence such as hashing power or stake. We provide formal mathematical analysis and experimental results (simulations and cloud deployment) to demonstrate the security, efficiency, and fairness of LightChain, and show that LightChain is the only existing blockchain that can provide integrity under the corrupted majority power of peers. As we experimentally demonstrate, compared to the mainstream blockchains such as Bitcoin and Ethereum, LightChain requires around 66 times smaller per node storage, and is around 380 times faster on bootstrapping a new node to the system, and each LightChain node is rewarded equally likely for participating in the protocol."",""1558-2183"","""",""10.1109/TPDS.2021.3071176"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9397334"",""Blockchain";permissionless;DHT;consensus;storage efficiency;communication efficiency;scalability;"skip graph"",""Peer-to-peer computing";Blockchain;Complexity theory;Decision making;Security;Time complexity;"Scalability"","""",""25"","""",""53"",""IEEE"",""6 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Logically Parallel Communication for Fast MPI+Threads Applications,""R. Zambre"; D. Sahasrabudhe; H. Zhou; M. Berzins; A. Chandramowlishwaran;" P. Balaji"",""Department of Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA"; Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Scientific Computing and Imaging Institute, University of Utah, Salt Lake City, UT, USA; Department of Electrical Engineering and Computer Science, University of California Irvine, Irvine, CA, USA;" Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Jun 2021"",""2021"",""32"",""12"",""3038"",""3052"",""Supercomputing applications are increasingly adopting the MPI+threads programming model over the traditional “MPI everywhere” approach to better handle the disproportionate increase in the number of cores compared with other on-node resources. In practice, however, most applications observe a slower performance with MPI+threads primarily because of poor communication performance. Recent research efforts on MPI libraries address this bottleneck by mapping logically parallel communication, that is, operations that are not subject to MPI's ordering constraints to the underlying network parallelism. Domain scientists, however, typically do not expose such communication independence information because the existing MPI-3.1 standard's semantics can be limiting. Researchers had initially proposed user-visible endpoints to combat this issue, but such a solution requires intrusive changes to the standard (new APIs). The upcoming MPI-4.0 standard, on the other hand, allows applications to relax unneeded semantics and provides them with many opportunities to express logical communication parallelism. In this article, we show how MPI+threads applications can achieve high performance with logically parallel communication. Through application case studies, we compare the capabilities of the new MPI-4.0 standard with those of the existing one and user-visible endpoints (upper bound). Logical communication parallelism can boost the overall performance of an application by over 2×."",""1558-2183"","""",""10.1109/TPDS.2021.3075157"",""U.S. Department of Energy(grant numbers:DE-AC02-06CH11357)"; National Science Foundation(grant numbers:1750549); University of Utah;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9411740"",""MPI+threads";MPI+OpenMP;exascale MPI;MPI_THREAD_MULTIPLE;MPI endpoints;Uintah;HYPRE;wombat;"Legion"",""Parallel processing";Supercomputing;Programming;Semantics;"Upper bound"","""",""2"","""",""66"",""IEEE"",""22 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"MCFsyn: A Multi-Party Set Reconciliation Protocol With the Marked Cuckoo Filter,""L. Luo"; D. Guo; Y. Zhao; O. Rottenstreich; R. T. B. Ma;" X. Luo"",""Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China"; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; China Electronic Equipment System Engineering Company, Beijing, China; Israel Institute of Technology and ORBS Research, Haifa, Israel; School of Computing, National University of Singapore, Singapore;" Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2021"",""2021"",""32"",""11"",""2705"",""2718"",""Multi-party set reconciliation is a key component in distributed and networking systems. It naturally contains two dimensions, i.e., set representation and reconciliation protocol. However, existing sketch data structures are insufficient to satisfy the new needs brought by the multi-party scenario simultaneously, including space-efficiency, mergeability, and completeness. The current reconciliation protocols, on the other hand, fail to achieve the global optimization of communication cost. To this end, in this article, we propose the marked cuckoo filter (MCF), a data structure for representing set members. Grounded on MCF, we implement the MCFsyn protocol to reconcile multiple sets. MCFsyn aggregates and distributes sets information represented by MCFs along with an underlying minimum spanning tree among the participants. The participants then identify the different elements by traversing the overall MCF which contains the information of all elements in the union set. For the identified missing elements, MCFsyn helps the participants to choose the optimal senders to fetch with the minimum communication cost. Comprehensive evaluations indicate that MCFsyn significantly outperforms existing alternatives in terms of both reconciliation accuracy and communication cost."",""1558-2183"","""",""10.1109/TPDS.2021.3074440"",""National Key Research and Development Program of China(grant numbers:2018YFB1800203)"; National Natural Science Foundation of China(grant numbers:62002378); Research Funding of NUDT(grant numbers:ZK20-3);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9409946"",""Set reconciliation";minimum spanning tree;marked cuckoo filter;accuracy;"communication cost"",""Protocols";Data structures;Cloud computing;Aggregates;Servers;Hash functions;"Relays"","""",""9"","""",""51"",""CCBY"",""21 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Memory-Side Prefetching Scheme Incorporating Dynamic Page Mode in 3D-Stacked DRAM,""M. M. Rafique";" Z. Zhu"",""Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA";" Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""27 May 2021"",""2021"",""32"",""11"",""2734"",""2747"",""Modern multiprocessor systems running multiple applications concurrently exhibit irregular memory access pattern during different phases of execution. The principle of locality is hard to exploit in the presence of such irregular memory requests and may result in additional delays due to resource conflicts throughout memory hierarchy. Prefetching is a promising technique to reduce the memory access latency where data is speculatively fetched ahead of time and stored in a faster memory structure like cache or dedicated prefetch buffer. The emergence of 3D-stacked DRAM provides huge internal bandwidth that makes memory-side prefetching an effective approach to improving system performance. Leveraging the unique architecture of 3D-stacked DRAM, we introduce a memory-side prefetching scheme that works in conjunction with dynamic page mode to reduce memory access latency. We introduce a novel prefetch buffer management scheme that makes intelligent replacement decision based on the utilization and recency of the prefetched data, which also serves as a guidance for future prefetching. Simulation results indicate that our approach improves performance by 21.8 percent on average, compared to a baseline scheme that prefetches a whole row on consecutive hits and implements static open page policy. Our scheme also outperforms an existing memory-side prefetching scheme by 13.2 percent on average, which dynamically adjusts the prefetch degree based on the usefulness of prefetched data."",""1558-2183"","""",""10.1109/TPDS.2020.3044856"",""National Science Foundation(grant numbers:CCF-1513899)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9293400"",""3D-stacked DRAM";memory bandwidth;memory-side prefetching;"page mode"",""Prefetching";Random access memory;Memory management;Bandwidth;Program processors;Through-silicon vias;"Three-dimensional displays"","""","""","""",""57"",""IEEE"",""14 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"MG-WFBP: Merging Gradients Wisely for Efficient Communication in Distributed Deep Learning,""S. Shi"; X. Chu;" B. Li"",""Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong, China"; Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong, China;" Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Kowloon, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2021"",""2021"",""32"",""8"",""1903"",""1917"",""Distributed synchronous stochastic gradient descent has been widely used to train deep neural networks (DNNs) on computer clusters. With the increase of computational power, network communications generally limit the system scalability. Wait-free backpropagation (WFBP) is a popular solution to overlap communications with computations during the training process. In this article, we observe that many DNNs have a large number of layers with only a small amount of data to be communicated at each layer in distributed training, which could make WFBP inefficient. Based on the fact that merging some short communication tasks into a single one can reduce the overall communication time, we formulate an optimization problem to minimize the training time in pipelining communications and computations. We derive an optimal solution that can be solved efficiently without affecting the training performance. We then apply the solution to propose a distributed training algorithm named merged-gradient WFBP (MG-WFBP) and implement it in two platforms Caffe and PyTorch. Extensive experiments in three GPU clusters are conducted to verify the effectiveness of MG-WFBP. We further exploit trace-based simulations of 4 to 2048 GPUs to explore the potential scaling efficiency of MG-WFBP. Experimental results show that MG-WFBP achieves much better scaling performance than existing methods."",""1558-2183"","""",""10.1109/TPDS.2021.3052862"",""Hong Kong RGC GRF(grant numbers:HKBU 12200418,HKUST 16206417,16207818)"; RGC CRF(grant numbers:C7036-15G);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9328614"",""Deep learning";GPU;distributed stochastic gradient descent;gradient communication;"merged-gradient"",""Training";Backpropagation;Hardware;Graphics processing units;Tensors;Neural networks;"Data models"","""",""17"","""",""46"",""IEEE"",""19 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Middleware to Manage Fault Tolerance Using Semi-Coordinated Checkpoints,""A. Wong"; E. Heymann; D. Rexachs;" E. Luque"",""Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain"; Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain; Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain;" Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Aug 2020"",""2021"",""32"",""2"",""254"",""268"",""Compute node failures are becoming a normal event for many long-running and scalable MPI applications. Keeping within the MPI standards and applying some of the methods developed so far in terms of fault tolerance, we developed a methodology that allows applications to tolerate failures through the creation of semi-coordinated checkpoints within the RADIC architecture. To do this, we developed the ULSC2-RADIC middleware that divides the application into independent MPI worlds where each MPI world would correspond to a compute node and make use of the DMTCP checkpoint library in a semi-coordinated environment. We performed experimental results using scientific applications and the NAS Parallel Benchmarks to assess the overhead and also the functionality in case of a node failure. We evaluated the computational cost of the semi-coordinated checkpoints compared with the coordinated checkpoints."",""1558-2183"","""",""10.1109/TPDS.2020.3015615"",""Agencia Estatal de Investigación(grant numbers:TIN2017-84875-P)"; European Regional Development Fund; Fundación Escuelas Universitarias Gimbernat;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9165191"",""Fault tolerance";checkpoint-restart libraries;MPI;"checkpoint scalability"",""Libraries";Middleware;Computer architecture;Standards;Computational efficiency;Fault tolerance;"Fault tolerant systems"","""","""","""",""41"",""IEEE"",""11 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Minimizing Coflow Completion Time in Optical Circuit Switched Networks,""T. Zhang"; F. Ren; J. Bao; R. Shu;" W. Cheng"",""College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China"; Department of Computer Science and Techonolgy, Tsinghua University, Beijing, China; North Information Control Research Academy Group Company, Ltd, Nanjing, China; Microsoft Research, Beijing, China;" Microsoft Research, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Sep 2020"",""2021"",""32"",""2"",""457"",""469"",""Nowadays, optical circuit switching is becoming an increasingly favored technology in scaling data center networks for its definitive advantages in data rate, power consumption, and device cost. Concurrently, reducing coflow completion time (CCT) is of great significance for improving application-level performance. However, minimizing CCT in circuit switched networks is totally different from that in traditional packet switched networks due to port constraints and circuit reconfiguration delays. To address this issue, this article proposes Grouped Optimization-based Scheduling (GOS), a CCT minimization algorithm for circuit switched networks integrating circuit and coflow scheduling. We first formalize the CCT minimization problem into a 0-1 programming problem, then relax and solve the problem in 2 steps to obtain the coflow order and flow grouping decisions on each circuit. Thus intra-group reconfiguration delays are saved, and small coflows can be prioritized at the group level. Theoretical analysis proves GOS is a 4-approximation algorithm in average CCT. To reduce computing overheads, we further propose a heuristic approximation algorithm. Extensive simulations show that the heuristic algorithm has satisfactory CCT performance (0.12× Varys, 0.36× Sunflow) as well as high throughput (16.74× Varys, 1.32× Sunflow), and well adapts to a wide range of reconfiguration delays and algorithm decision time."",""1558-2183"","""",""10.1109/TPDS.2020.3025145"",""National Natural Science Foundation of China(grant numbers:62002165,61872208)"; Natural Science Foundation of Jiangsu Province(grant numbers:SBK2020041090); National Key Research and Development Program of China(grant numbers:2018YFB1700203); Nanjing University of Aeronautics and Astronautics(grant numbers:90YAH19095);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200719"",""Data center";optical circuit switch;coflow completion time;circuit scheduling;"coflow scheduling"",""Optical switches";Switching circuits;Optical packet switching;Scheduling;Data centers;Delays;"Optical fiber networks"","""",""5"","""",""39"",""IEEE"",""18 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"MO-Tree: An Efficient Forwarding Engine for Spatiotemporal-Aware Pub/Sub Systems,""T. Ding"; S. Qian; J. Cao; G. Xue; Y. Zhu; J. Yu;" M. Li"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China;" College of Mathematics and Computer Science, Zhejiang Normal University, Zhejiang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Dec 2020"",""2021"",""32"",""4"",""855"",""866"",""For large-scale spatiotemporal-aware publish/subscribe systems, it is critical to design an efficient forwarding engine to achieve fast matching and maintenance of events and subscriptions. For this goal, we propose a novel data structure called MO-Tree to index both subscriptions and events in a unified way. The design philosophy behind MO-Tree is to keep the data structure concise, which manifests in three aspects: limiting the height of MO-Tree, trading space for time, and avoiding node merging and splitting. The difficulty in designing MO-Tree is how to efficiently index width-variable intervals. We present a multi-level cell-overlapping partition scheme and build a theoretical model to optimize the cell width in each level. To evaluate the performance of MO-Tree, a series of experiments is conducted on real-world trace datasets. The experiment results show MO-Tree significantly outperforms the state-of-the-art in terms of matching speed and maintenance cost."",""1558-2183"","""",""10.1109/TPDS.2020.3036014"",""National Key R&D Program of China(grant numbers:2018YFB2101100)"; National Natural Science Foundation of China(grant numbers:61772334,61702151); National Natural Science Foundation of China(grant numbers:U1736207);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9250646"",""Spatiotemporal-aware";publish/subscribe;event matching;event maintenance;subscription matching;"subscription maintenance"",""Indexes";Spatiotemporal phenomena;Data structures;Maintenance engineering;Engines;Computational modeling;"Limiting"","""",""5"","""",""26"",""IEEE"",""6 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Model Parallelism Optimization for Distributed Inference Via Decoupled CNN Structure,""J. Du"; X. Zhu; M. Shen; Y. Du; Y. Lu; N. Xiao;" X. Liao"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China;" School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1665"",""1676"",""It is promising to deploy CNN inference on local end-user devices for high-accuracy and time-sensitive applications. Model parallelism has the potential to provide high throughput and low latency in distributed CNN inference. However, it is non-trivial to use model parallelism as the original CNN model is inherently tightly-coupled structure. In this article, we propose DeCNN, a more effective inference approach that uses decoupled CNN structure to optimize model parallelism for distributed inference on end-user devices. DeCNN is novel consisting of three schemes. Scheme-1 is structure-level optimization. It exploits group convolution and channel shuffle to decouple the original CNN structure for model parallelism. Scheme-2 is partition-level optimization. It is based on channel group to partition the convolutional layers, and then leverages input-based method to partition the fully connected layers, further exposing high degree of parallelism. Scheme-3 is communication-level optimization. It uses inter-sample parallelism to hide communications for better performance and robustness, especially in the weak network connections. We use ImageNet classification task to evaluate the effectiveness of DeCNN on a distributed multi-ARM platform. Notably, when using the number of devices from 1 to 4, DeCNN can accelerate the inference of large-scale ResNet-50 by 3.21×, and reduce 65.3 percent memory footprint, with 1.29 percent accuracy improvement."",""1558-2183"","""",""10.1109/TPDS.2020.3041474"",""National Key R&D Program of China(grant numbers:2018YFB0204303)"; Natural Science Foundation of China(grant numbers:U1811464,62072479,61433019,61802446); Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2016ZT06D211); Guangdong Basic and Application Basic Research Teams(grant numbers:2018B030312002); Fundamental Research Funds for the Central Universities(grant numbers:19lgpy215);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9275375"",""Intelligent applications";distributed deep learning;distributed inference;model parallelism;"decoupled CNN structure"",""Parallel processing";Kernel;Convolution;Computational modeling;Optimization;Performance evaluation;"Task analysis"","""",""9"","""",""36"",""IEEE"",""1 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Modeling and Analyzing Waiting Policies for Cloud-Enabled Schedulers,""P. Ambati"; N. Bashir; D. Irwin;" P. Shenoy"",""University of Massachusetts Amherst, Amherst, MA, USA"; University of Massachusetts Amherst, Amherst, MA, USA; University of Massachusetts Amherst, Amherst, MA, USA;" University of Massachusetts Amherst, Amherst, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Jun 2021"",""2021"",""32"",""12"",""3081"",""3100"",""Cloud platforms have popularized the Infrastructure-as-a-Service (IaaS) purchasing model, which enables users to rent computing resources on demand to execute their jobs. However, buying fixed resources is still much cheaper than renting if their resource utilization is high. Thus, to optimize cost, users must decide how many fixed resources to provision versus rent “on demand” based on their workload. In this article, we introduce the concept of a waiting policy for cloud-enabled schedulers and show that the optimal cost depends on it. The waiting policy explicitly controls how long jobs wait for resources, as jobs never need to wait, since cloud platforms provide the illusion of infinite scalability. A waiting policy is the dual of a scheduling policy: while a scheduling policy determines which jobs should run when fixed resources are available, a waiting policy determines which jobs should wait when fixed resources are not available. We define multiple waiting policies and develop simple and general analytical models to reveal their tradeoff between fixed resource provisioning, cost, and job waiting time. We evaluate the impact of different waiting policies on a real year-long batch workload consisting of 14M jobs run on a 14.3k-core cluster. We show that a compound waiting policy, which forces jobs with long running times or short waiting times to wait for fixed resources, offers the best tradeoff. The policy decreases both the cost (by 5 percent) and mean job waiting time (by 7×) compared to the current cluster, and also decreases the cost (by 43 percent) compared to renting on-demand resources for a modest increase in mean job waiting time (at 1.74 hours)."",""1558-2183"","""",""10.1109/TPDS.2021.3086270"",""National Science Foundation(grant numbers:CNS-1802523,CNS-1908536)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446629"",""Cloud computing";job scheduling;batch scheduling;"waiting policy"",""Analytical models";Cloud computing;Pricing;Computational modeling;Queueing analysis;Dynamic scheduling;"Resource management"","""",""3"","""",""35"",""IEEE"",""3 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Modeling and Optimization of Performance and Cost of Serverless Applications,""C. Lin";" H. Khazaei"",""Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada";" Department of Electrical Engineering and Computer Science, York University, Toronto, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Oct 2020"",""2021"",""32"",""3"",""615"",""632"",""Function-as-a-Service (FaaS) and serverless applications have proliferated significantly in recent years because of their high scalability, ease of resource management, and pay-as-you-go pricing model. However, cloud users are facing practical problems when they migrate their applications to the serverless pattern, which are the lack of analytical performance and billing model and the trade-off between limited budget and the desired quality of service of serverless applications. In this article, we fill this gap by proposing and answering two research questions regarding the prediction and optimization of performance and cost of serverless applications. We propose a new construct to formally define a serverless application workflow, and then implement analytical models to predict the average end-to-end response time and the cost of the workflow. Consequently, we propose a heuristic algorithm named Probability Refined Critical Path Greedy algorithm (PRCP) with four greedy strategies to answer two fundamental optimization questions regarding the performance and the cost. We extensively evaluate the proposed models by conducting experimentation on AWS Lambda and Step Functions. Our analytical models can predict the performance and cost of serverless applications with more than 98 percent accuracy. The PRCP algorithms can achieve the optimal configurations of serverless applications with 97 percent accuracy on average."",""1558-2183"","""",""10.1109/TPDS.2020.3028841"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9214428"",""Cloud serverless computing";performance modeling;performance optimization;cost modeling;"cost optimization"",""FAA";Cloud computing;Optimization;Time factors;Analytical models;"Computational modeling"","""",""58"","""",""51"",""IEEE"",""6 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Multi-Agent Imitation Learning for Pervasive Edge Computing: A Decentralized Computation Offloading Algorithm,""X. Wang"; Z. Ning;" S. Guo"",""Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China"; School of Software, Dalian University of Technology, Dalian, China;" Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Sep 2020"",""2021"",""32"",""2"",""411"",""425"",""Pervasive edge computing refers to one kind of edge computing that merely relies on edge devices with sensing, storage and communication abilities to realize peer-to-peer offloading without centralized management. Due to lack of unified coordination, users always pursue profits by maximizing their own utilities. However, on one hand, users may not make appropriate scheduling decisions based on their local observations. On the other hand, how to guarantee the fairness among different edge devices in the fully decentralized environment is rather challenging. To solve the above issues, we propose a decentrailized computation offloading algorithm with the purpose of minimizing average task completion time in the pervasive edge computing networks. We first derive a Nash equilibrium among devices by stochastic game theories based on the full observations of system states. After that, we design a traffic offloading algorithm based on partial observations by integrating general adversarial imitation learning. Multiple experts can provide demonstrations, so that devices can mimic the behaviors of corresponding experts by minimizing the gaps between the distributions of their observation-action pairs. At last, theoretical and performance results show that our solution has a significant advantage compared with other representative algorithms."",""1558-2183"","""",""10.1109/TPDS.2020.3023936"",""Hong Kong RGC Research Impact Fund(grant numbers:R5060-19)"; General Research Fund(grant numbers:152221/19E); National Natural Science Foundation of China(grant numbers:61872310,61971084,62001073);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197692"",""Pervasive edge computing";computation offloading;imitation learning;"decentralized execution"",""Edge computing";Task analysis;Performance evaluation;Computational modeling;Games;Processor scheduling;"Cloud computing"","""",""75"","""",""42"",""IEEE"",""15 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Multi-GPU Design and Performance Evaluation of Homomorphic Encryption on GPU Clusters,""A. Al Badawi"; B. Veeravalli; J. Lin; N. Xiao; M. Kazuaki;" A. Khin Mi Mi"",""A*STAR, Institute for Infocomm Research (I2R), Singapore"; Department of Electrical and Computer Engineering, National University of Singapore, Singapore; A*STAR, Institute for Infocomm Research (I2R), Singapore; A*STAR, Institute for Infocomm Research (I2R), Singapore; Barcelona Supercomputing Center (BSC), Barcelona, Spain;" A*STAR, Institute for Infocomm Research (I2R), Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Sep 2020"",""2021"",""32"",""2"",""379"",""391"",""We present a multi-GPU design, implementation and performance evaluation of the Halevi-Polyakov-Shoup (HPS) variant of the Fan-Vercauteren (FV) levelled Fully Homomorphic Encryption (FHE) scheme. Our design follows a data parallelism approach and uses partitioning methods to distribute the workload in FV primitives evenly across available GPUs. The design is put to address space and runtime requirements of FHE computations. It is also suitable for distributed-memory architectures, and includes efficient GPU-to-GPU data exchange protocols. Moreover, it is user-friendly as user intervention is not required for task decomposition, scheduling or load balancing. We implement and evaluate the performance of our design on two homogeneous and heterogeneous NVIDIA GPU clusters: K80, and a customized P100. We also provide a comparison with a recent shared-memory-based multi-core CPU implementation using two homomorphic circuits as workloads: vector addition and multiplication. Moreover, we use our multi-GPU Levelled-FHE to implement the inference circuit of two Convolutional Neural Networks (CNNs) to perform homomorphically image classification on encrypted images from the MNIST and CIFAR - 10 datasets. Our implementation provides 1 to 3 orders of magnitude speedup compared with the CPU implementation on vector operations. In terms of scalability, our design shows reasonable scalability curves when the GPUs are fully connected."",""1558-2183"","""",""10.1109/TPDS.2020.3021238"",""Agency for Science, Technology and Research(grant numbers:RIE2020)"; Advanced Manufacturing and Engineering(grant numbers:A19E3b0099);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9185077"",""Homomorphic encryption";parallel algorithms;multi-GPU clusters;"performance evaluation"",""Graphics processing units";Task analysis;Encryption;Computational modeling;Parallel processing;"Field programmable gate arrays"","""",""20"","""",""47"",""IEEE"",""2 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Multi-GPU Parallelization of the NAS Multi-Zone Parallel Benchmarks,""M. González";" E. Morancho"",""Computer Architecture Department, Universitat Politècnica de Catalunya - Barcelona Tech, Barcelona, Spain";" Computer Architecture Department, Universitat Politècnica de Catalunya - Barcelona Tech, Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Aug 2020"",""2021"",""32"",""1"",""229"",""241"",""GPU-based computing systems have become a widely accepted solution for the high-performance-computing (HPC) domain. GPUs have shown highly competitive performance-per-watt ratios and can exploit an astonishing level of parallelism. However, exploiting the peak performance of such devices is a challenge, mainly due to the combination of two essential aspects of multi-GPU execution. On one hand, the workload should be distributed evenly among the GPUs. On the other hand, communications between GPU devices are costly and should be minimized. Therefore, a trade-of between work-distribution schemes and communication overheads will condition the overall performance of parallel applications run on multi-GPU systems. In this article we present a multi-GPU implementation of NAS Multi-Zone Parallel Benchmarks (which execution alternate communication and computational phases). We propose several work-distribution strategies that try to evenly distribute the workload among the GPUs. Our evaluations show that performance is highly sensitive to this distribution strategy, as the the communication phases of the applications are heavily affected by the work-distribution schemes applied in computational phases. In particular, we consider Static, Dynamic, and Guided schedulers to find a trade-off between both phases to maximize the overall performance. In addition, we compare those schedulers with an optimal scheduler computed offline using IBM CPLEX. On an evaluation environment composed of 2 x IBM Power9 8335-GTH and 4 x GPU NVIDIA V100 (Volta), our multi-GPU parallelization outperforms single-GPU execution from 1.48x to 1.86x (2 GPUs) and from 1.75x to 3.54x (4 GPUs). This article analyses these improvements in terms of the relationship between the computational and communication phases of the applications as the number of GPUs is increased. We prove that Guided schedulers perform at similar level as optimal schedulers."",""1558-2183"","""",""10.1109/TPDS.2020.3015148"",""Ministerio de Ciencia y Tecnología(grant numbers:TIN2015-65316-P)"; Generalitat de Catalunya(grant numbers:2014-SGR-1051);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9162505"",""Multi-GPU parallelization";load balancing;static;dynamic;"guided schedulings"",""Benchmark testing";Graphics processing units;Parallel processing;Dynamic scheduling;Performance evaluation;Optimal scheduling;"Load management"","""",""1"","""",""27"",""IEEE"",""7 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Multi-Hop Multi-Task Partial Computation Offloading in Collaborative Edge Computing,""Y. Sahni"; J. Cao; L. Yang;" Y. Ji"",""Department of Computing, The Hong Kong Polytechnic University, Hong Kong"; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Software Engineering, South China University of Technology, Guangzhou, China;" Information Systems Architecture Research Division, National Institute of Informatics, Tokyo, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Dec 2020"",""2021"",""32"",""5"",""1133"",""1145"",""Collaborative edge computing (CEC) is a recent popular paradigm where different edge devices collaborate by sharing data and computation resources. One of the fundamental issues in CEC is to make task offloading decision. However, it is a challenging problem to solve as tasks can be offloaded to a device at multi-hop distance leading to conflicting network flows due to limited bandwidth constraint. There are some works on multi-hop computation offloading problem in the literature. However, existing works have not jointly considered multi-hop partial computation offloading and network flow scheduling that can cause network congestion and inefficient performance in terms of completion time. This article formulates the joint multi-task partial computation offloading and network flow scheduling problem to minimize the average completion time of all tasks. The formulated problem optimizes several dependent decision variables including partial offloading ratio, remote offloading device, start time of tasks, routing path, and start time of network flows. The problem is formulated as an MINLP optimization problem and shown to be NP-hard. We propose a joint partial offloading and flow scheduling heuristic (JPOFH) that decides partial offloading ratio by considering both waiting times at the devices and start time of network flows. We also do the relaxation of formulated MINLP problem to an LP problem using McCormick envelope to give a lower bound solution. Performance comparison done using simulation shows that JPOFH leads to up to 32 percent improvement in average completion time compared to benchmark solutions which do not make a joint decision."",""1558-2183"","""",""10.1109/TPDS.2020.3042224"",""Research Grants Council(grant numbers:PolyU 15217919)"; RGC Research Impact Fund(grant numbers:R5034-18);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9279235"",""Scheduling and task partitioning";network flow scheduling;collaborative edge computing;"Internet of Things"",""Task analysis";Processor scheduling;Collaboration;Edge computing;Spread spectrum communication;Routing;"Cloud computing"","""",""47"","""",""35"",""IEEE"",""3 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Multi-Queue Request Scheduling for Profit Maximization in IaaS Clouds,""S. Wang"; X. Li; Q. Z. Sheng; R. Ruiz; J. Zhang;" A. Beheshti"",""School of Computer Science and Engineering, Southeast University, Nanjing, China"; School of Computer Science and Engineering, Southeast University, Nanjing, China; Department of Computing, Macquarie University, Sydney, NSW, Australia; Grupo de Sistemas de Optimización Aplicada, Universitat Politècnica de València, Camino de Vera s/n, València, Spain; School of Computer Science and Engineering, Southeast University, Nanjing, China;" Department of Computing, Macquarie University, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""26 May 2021"",""2021"",""32"",""11"",""2838"",""2851"",""In cloud computing, service providers rent heterogeneous servers from cloud providers, i.e., Infrastructure as a Service (IaaS), to meet requests of consumers. The heterogeneity of servers and impatience of consumers pose great challenges to service providers for profit maximization. In this article, we transform this problem into a multi-queue model where the optimal expected response time of each queue is theoretically analyzed. A multi-queue request scheduling algorithm framework is proposed to maximize the total profit of service providers, which consists of three components: request stream splitting, requests allocation, and server assignment. A request stream splitting algorithm is designed to split the arriving requests to minimize the response time in the multi-queue system. An allocation algorithm, which adopts a one-step improvement strategy, is developed to further optimize the response time of the requests. Furthermore, an algorithm is developed to determine the appropriate number of required servers of each queue. After statistically calibrating parameters and algorithm components over a comprehensive set of random instances, the proposed algorithms are compared with the state-of-the-art over both simulated and real-world instances. The results indicate that the proposed multi-queue request scheduling algorithm outperforms the other algorithms with acceptable computational time."",""1558-2183"","""",""10.1109/TPDS.2021.3075254"",""National Key Research and Development Program of China(grant numbers:2017YFB1400800)"; National Natural Science Foundation of China(grant numbers:61872077,61832004); Collaborative Innovation Center of Wireless Communications Technology; Australian Research Council Future Fellowship(grant numbers:FT140101247); Discovery Project(grant numbers:DP180102378); Spanish Ministry of Science, Innovation; OPTEP-Port Terminal Operations Optimization(grant numbers:RTI2018-094940-B-I00);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9415144"",""Profit maximization";consumer impatience;queue;scheduling;"cloud computing"",""Servers";Time factors;Cloud computing;Task analysis;Queueing analysis;Resource management;"Scheduling algorithms"","""",""5"","""",""43"",""IEEE"",""23 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Mutual Information Driven Federated Learning,""M. P. Uddin"; Y. Xiang; X. Lu; J. Yearwood;" L. Gao"",""Deakin Blockchain Innovation Lab, School of Information Technology, Deakin University, Geelong, VIC, Australia"; Deakin Blockchain Innovation Lab, School of Information Technology, Deakin University, Geelong, VIC, Australia; Deakin Blockchain Innovation Lab, School of Information Technology, Deakin University, Geelong, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia;" Deakin Blockchain Innovation Lab, School of Information Technology, Deakin University, Geelong, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1526"",""1538"",""Federated Learning (FL) is an emerging research field that yields a global trained model from different local clients without violating data privacy. Existing FL techniques often ignore the effective distinction between local models and the aggregated global model when doing the client-side weight update, as well as the distinction of local models for the server-side aggregation. In this article, we propose a novel FL approach with resorting to mutual information (MI). Specifically, in client-side, the weight update is reformulated through minimizing the MI between local and aggregated models and employing Negative Correlation Learning (NCL) strategy. In server-side, we select top effective models for aggregation based on the MI between an individual local model and its previous aggregated model. We also theoretically prove the convergence of our algorithm. Experiments conducted on MNIST, CIFAR-10, ImageNet, and the clinical MIMIC-III datasets manifest that our method outperforms the state-of-the-art techniques in terms of both communication and testing performance."",""1558-2183"","""",""10.1109/TPDS.2020.3040981"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272656"",""Distributed learning";federated learning;parallel optimization;data parallelism;information theory;mutual information;communication bottleneck;"data heterogeneity"",""Data models";Training;Computational modeling;Servers;Mathematical model;Convergence;"Analytical models"","""",""12"","""",""35"",""IEEE"",""26 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Network-Aware Locality Scheduling for Distributed Data Operators in Data Centers,""L. Cheng"; Y. Wang; Q. Liu; D. H. J. Epema; C. Liu; Y. Mao;" J. Murphy"",""School of Control and Computer Engineering, North China Electric Power University, Beijing, China"; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Information Technology Group, Wageningen University & Research, Wageningen, The Netherlands; Distributed Systems Group, Delft University of Technology, Delft, The Netherlands; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Computer and Information Science, Fordham University, New York City, NY, USA;" School of Computer Science, University College Dublin, Dublin, Ireland"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Feb 2021"",""2021"",""32"",""6"",""1494"",""1510"",""Large data centers are currently the mainstream infrastructures for big data processing. As one of the most fundamental tasks in these environments, the efficient execution of distributed data operators (e.g., join and aggregation) are still challenging current data systems, and one of the key performance issues is network communication time. State-of-the-art methods trying to improve that problem focus on either application-layer data locality optimization to reduce network traffic or on network-layer data flow optimization to increase bandwidth utilization. However, the techniques in the two layers are totally independent from each other, and performance gains from a joint optimization perspective have not yet been explored. In this article, we propose a novel approach called NEAL (NEtwork-Aware Locality scheduling) to bridge this gap, and consequently to further reduce communication time for distributed big data operators. We present the detailed design and implementation of NEAL, and our experimental results demonstrate that NEAL always performs better than current approaches for different workloads and network bandwidth configurations."",""1558-2183"","""",""10.1109/TPDS.2021.3053241"",""Beijing Municipal Science and Technology Commission(grant numbers:Z181100005118016)"; National Natural Science Foundation of China(grant numbers:61874124,61876173); European Union’s Horizon 2020 research and innovation programme(grant numbers:799066);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9329172"",""Data locality";coflow scheduling;distributed operators;data centers;big data;SDN;"metaheuristic"",""Distributed databases";Bandwidth;Scheduling;Data centers;Optimization;Processor scheduling;"Big Data"","""",""27"","""",""62"",""IEEE"",""20 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"O3BNN-R: An Out-of-Order Architecture for High-Performance and Regularized BNN Inference,""T. Geng"; A. Li; T. Wang; C. Wu; Y. Li; R. Shi; W. Wu;" M. Herbordt"",""Department of Electrical and Computer Engineering, Boston University, Boston, USA"; PCSD, Pacific Northwest National Laboratory, Richland, USA; Department of Electrical and Computer Engineering, Boston University, Boston, USA; Department of Electrical and Computer Engineering, Boston University, Boston, USA; College of Information Science and Electronic Engineering, Hangzhou, China; Department of Electrical and Electronic Engineering, Hong Kong University, Hong Kong; Program Model Team, Los Alamos National Laboratory (LANL), Los Alamos, USA;" Department of Electrical and Computer Engineering, Boston University, Boston, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Aug 2020"",""2021"",""32"",""1"",""199"",""213"",""Binarized Neural Networks (BNN), which significantly reduce computational complexity and memory demand, have shown potential in cost- and power-restricted domains, such as IoT and smart edge-devices, where reaching certain accuracy bars is sufficient and real-time is highly desired. In this article, we demonstrate that the highly-condensed BNN model can be shrunk significantly by dynamically pruning irregular redundant edges. Based on two new observations on BNN-specific properties, an out-of-order (OoO) architecture, O3BNN-R, which can curtail edge evaluation in cases where the binary output of a neuron can be determined early at runtime during inference, is proposed. Similar to instruction level parallelism (ILP), fine-grained, irregular, and runtime pruning opportunities are traditionally presumed to be difficult to exploit. To further enhance the pruning opportunities, we conduct an algorithm/architecture co-design approach where we augment the loss function during the training stage with specialized regularization terms favoring edge pruning. We evaluate our design on an embedded FPGA using networks that include VGG-16, AlexNet for ImageNet, and a VGG-like network for Cifar-10. Results show that O3BNN-R without regularization can prune, on average, 30 percent of the operations, without any accuracy loss, bringing 2.2× inference-speedup, and on average 34× energy-efficiency improvement over state-of-the-art BNN implementations on FPGA/GPU/CPU. With regularization at training, the performance is further improved, on average, by 15 percent."",""1558-2183"","""",""10.1109/TPDS.2020.3013637"",""National Science Foundation(grant numbers:CNS-1405695,CCF-1618303/7960)"; National Institutes of Health(grant numbers:1R41GM128533); Microsoft; Red Hat; Xilinx; Intel Corporation; Pacific Northwest National Laboratory; U.S. DOE Office of Science, Office of Advanced Scientific Computing Research(grant numbers:66150);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9154597"",""Machine learning";BNN;high-performance computing;neural network pruning;"out-of-order architecture"",""Neurons";Training;Out of order;Computer architecture;Convolution;Integrated circuits;"Computational complexity"","""",""23"","""",""38"",""IEEE"",""3 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Octans: Optimal Placement of Service Function Chains in Many-Core Systems,""H. Yu"; Z. Zheng; J. Shen; C. Miao; C. Sun; H. Hu; J. Bi; J. Wu;" J. Wang"",""Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China"; Alibaba Group, Hangzhou, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Alibaba Group, Hangzhou, China; Department of Computer Science and Engineering, University at Buffalo, the State University of New York, Buffalo, NY, USA; Beijing National Research Center for Information Science and Technology, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China;" Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2021"",""2021"",""32"",""9"",""2202"",""2215"",""Network Function Virtualization (NFV) offers service delivery flexibility and reduces overall costs by running service function chains (SFCs) on commodity servers with many cores. Existing solutions for placing SFCs in one server treat all CPU cores as equal and allocate isolated CPU cores to network functions (NFs). However, advanced servers often adopt Non-Uniform Memory Access (NUMA) architecture to improve the scalability of many-core systems. CPU cores are grouped into nodes, incurring performance degradation due to cross-node memory access and intra-node resource contention. Our evaluation shows that randomly selecting cores to place NFs in an SFC could suffer from 39.2 percent lower throughput comparing to an optimal placement solution. In this article, we propose Octans, an NFV orchestrator to achieve maximum aggregate throughput of all SFCs in many-core systems. Octans first formulates the optimization problem as a Non-Linear Integer Programming (NLIP) Model. Then we identify the key factor for problem solving as evaluating the throughput drop of an NF caused by other NFs in the same SFC or different SFCs, i.e., performance drop index, and propose a formal and accurate prediction model based on system level performance metrics. Finally, we propose two online algorithms to quickly find near-optimal placement solutions for one-time and incremental deployment. Extensive evaluation on a prototype implementation shows that Octans significantly improves the aggregate throughput comparing to two state-of-the-art placement solutions by 27.1 ~ 45.2 percent for one-time deployment and by 20.9 ~ 38.1 percent for incremental deployment, with very low prediction errors. Moreover, Octans could quickly find a near-optimal placement solution with tiny optimality gap."",""1558-2183"","""",""10.1109/TPDS.2021.3063613"",""National Key Research and Development Program of China(grant numbers:2020YFE0200500)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9369135"",""Many-core system";network function virtualization;service function chain;"optimal placement"",""Servers";Throughput;Noise measurement;Indexes;Aggregates;Service function chaining;"Optimization"","""",""12"","""",""45"",""IEEE"",""3 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Offloading Tasks With Dependency and Service Caching in Mobile Edge Computing,""G. Zhao"; H. Xu; Y. Zhao; C. Qiao;" L. Huang"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science & Engineering, University at Buffalo, Buffalo, NY, USA;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2021"",""2021"",""32"",""11"",""2777"",""2792"",""In Mobile Edge Computing (MEC), many tasks require specific service support for execution and in addition, have a dependent order of execution among the tasks. However, previous works often ignore the impact of having limited services cached at the edge nodes on (dependent) task offloading, thus may lead to an infeasible offloading decision or a longer completion time. To bridge the gap, this article studies how to efficiently offload dependent tasks to edge nodes with limited (and predetermined) service caching. We formally define the problem of offloading dependent tasks with service caching (ODT-SC), and prove that there exists no algorithm with constant approximation for this hard problem. Then, we design an efficient convex programming based algorithm (CP) to solve this problem. Moreover, we study a special case with a homogeneous MEC and propose a favorite successor based algorithm (FS) to solve this special case with a competitive ratio of O(1)O(1)<";" inline-graphic xlink:href=""""zhao-ieq1-3076687.gif""""/>. Extensive simulation results using Google data traces show that our proposed algorithms can significantly reduce applications' completion time by about 21-47 percent compared with other alternatives."",""1558-2183"","""",""10.1109/TPDS.2021.3076687"",""National Natural Science Foundation of China(grant numbers:61822210,61936015,U1709217)"; Anhui Initiative in Quantum Information Technologies(grant numbers:AHY150300);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9419755"",""Mobile edge computing";task offloading;service caching;dependency;"approximation"",""Task analysis";Approximation algorithms;Face recognition;Edge computing;Feature extraction;Optimization;"Mobile handsets"","""",""49"","""",""48"",""IEEE"",""29 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"On Consortium Blockchain Consistency: A Queueing Network Model Approach,""T. Meng"; Y. Zhao; K. Wolter;" C. -Z. Xu"",""Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China"; School of Microelectronics Science and Technology, Sun Yat-Sen University, Zhuhai, China; Department of Mathematics and Computer Science, Freie Universität Berlin, Berlin, Germany;" State Key Lab of IoTSC & Department of Computer and Information Science, University of Macau, Macau, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Jan 2021"",""2021"",""32"",""6"",""1369"",""1382"",""Analyzing blockchain protocols is a notoriously difficult task due to the underlying large scale distributed networks. To address this problem, stochastic model-based approaches are often utilized. However, the abstract models in prior work turn out not to be adoptable to consortium blockchains as the consensus of such a blockchain often consists of multiple processes. To address the lack of efficient analysis tools, we propose a queueing network-based method for analyzing consistency properties of consortium blockchain protocols in this article. Our method provides a way to evaluate the performance of the main stages in blockchain consensus. We apply our framework to the Hyperledger Fabric system and recover key properties of the blockchain network. Using our method, we analyze the security properties of the ordering mechanism and the impact of delaying endorsement messages in consortium blockchain protocols. Then an upper bound is derived of the damage an attacker could cause who is capable of delaying the honest players' messages. Based on the proposed method, we employ analytical derivations to investigate both the security and performance features, and corroborate close agreement with measurements on a wide-area network testbed running the Hyperledger Fabric blockchain. With the proposed method, designers of future blockchains can provide a more rigorous analysis of their consortium blockchain schemes."",""1558-2183"","""",""10.1109/TPDS.2021.3049915"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164002)"; National Natural Science Foundation of China(grant numbers:61801306); Science and Technology Development Fund(grant numbers:0015/2019/AKP); Shenzhen Basic Research Program(grant numbers:JCYJ20170818153016513); Shenzhen Fundamental Research(grant numbers:JCYJ20180302145755311,JCYJ20180302145731531); Guangdong Special Fund for Science and Technology Development(grant numbers:2019A050503001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316952"",""Consortium blockchain";analyzing framework;consistency;delay attack;queueing networks;"permissoned blockchain"",""Blockchain";Protocols;Queueing analysis;Fabrics;Delays;Analytical models;"Distributed ledger"","""",""42"","""",""49"",""IEEE"",""8 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"On the Effective Parallelization and Near-Optimal Deployment of Service Function Chains,""J. Luo"; J. Li; L. Jiao;" J. Cai"",""School of Cyber Security, Guangdong Polytechnic Normal University, Guangzhou, China"; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA;" School of Cyber Security, Guangdong Polytechnic Normal University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Dec 2020"",""2021"",""32"",""5"",""1238"",""1255"",""Network operators compose Service Function Chains (SFCs) by tying different network functions (e.g., packet inspection, flow shaping, network address translation) together and process traffic flows in the order the network functions are chained. Leveraging the technique of Network Function Virtualization (NFV), each network function can be “virtualized” and decoupled from its dedicated hardware, and therefore can be deployed flexibly for better performance at any appropriate location of the underlying network infrastructure. However, an SFC often incurs high latency as traffic goes through the virtual network functions one after another. In this article, we first design an algorithm that leverages virtual network function dependency to convert an original SFC into a parallelized SFC (p-SFC). Then, to deploy multiple p-SFCs over the network for serving a large number of users, we model the deployment problem as an Integer Linear Program and propose a heuristic, ParaSFC, based on the Viterbi dynamic programming algorithm to estimate each p-SFC’s occupation of the bottleneck resources and adjust the processing order of the p-SFCs in order to approximate the optimal solution. Finally, we conduct extensive trace-driven evaluations and exhibit that, compared to the Greedy method and the state-of-the-art CoordVNF method, ParaSFC reduces the average service latency of all the deployed p-SFCs by about 15 percent through parallelization while accommodating more SFC deployment requests over resource-limited networks."",""1558-2183"","""",""10.1109/TPDS.2020.3043768"",""National Key Research and Development Program of China(grant numbers:SQ2019YFB180098)"; National Natural Science Foundation of China(grant numbers:61972104,61902080,61702120); Key Areas of Guangdong Province(grant numbers:2019B010118001); science and technology project in Guangzhou(grant numbers:201803010081); National key R & D plan(grant numbers:SQ2019YFB180098,2018YFB1802200); Foshan Science and Technology Innovation Project, China(grant numbers:2018IT100283); Science and Technology Program of Guangzhou, China(grant numbers:202002020035); Ripple Faculty Fellowship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9300184"",""Network function virtualization";service function chain;parallelization;deployment;"quality of service"",""Heuristic algorithms";Servers;Quality of service;Parallel processing;Network function virtualization;Approximation algorithms;"Hardware"","""",""33"","""",""55"",""IEEE"",""21 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Online Collaborative Data Caching in Edge Computing,""X. Xia"; F. Chen; Q. He; J. Grundy; M. Abdelrazek;" H. Jin"",""School of Information Technology, Deakin University, Geelong, Australia"; School of Information Technology, Deakin University, Geelong, Australia; School of Software and Electrical Engineering, Swinburne University of Technology, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia; School of Information Technology, Deakin University, Geelong, Australia;" Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technolgoy, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Aug 2020"",""2021"",""32"",""2"",""281"",""294"",""In the edge computing (EC) environment, edge servers are deployed at base stations to offer highly accessible computing and storage resources to nearby app users. From the app vendor's perspective, caching data on edge servers can ensure low latency in app users' retrieval of app data. However, an edge server normally owns limited resources due to its limited size. In this article, we investigate the collaborative caching problem in the EC environment with the aim to minimize the system cost including data caching cost, data migration cost, and quality-of-service (QoS) penalty. We model this collaborative edge data caching problem (CEDC) as a constrained optimization problem and prove that it is NP-complete. We propose an online algorithm, called CEDC-O, to solve this CEDC problem during all time slots. CEDC-O is developed based on Lyapunov optimization, works online without requiring future information, and achieves provable close-to-optimal performance. CEDC-O is evaluated on a real-world data set, and the results demonstrate that it significantly outperforms four representative approaches."",""1558-2183"","""",""10.1109/TPDS.2020.3016344"",""Australian Research Council(grant numbers:DP180100212,DP200102491)"; Laureate Fellowship(grant numbers:FL190100035);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9166756"",""Edge computing";data caching;"online algorithm"",""Servers";Mobile handsets;Cloud computing;Distributed databases;Collaboration;Edge computing;"Data models"","""",""126"","""",""47"",""IEEE"",""13 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Online Scheduling Technique To Handle Data Velocity Changes in Stream Workflows,""M. Barika"; S. Garg; A. Y. Zomaya;" R. Ranjan"",""Discipline of ICT — School of Technology, Environments and Design, University of Tasmania, Hobart, TAS, Australia"; Discipline of ICT — School of Technology, Environments and Design, University of Tasmania, Hobart, TAS, Australia; School of IT, University of Sydney, New South Wales, Sydney, NSW, Australia;" School of Computing, Newcastle University, Newcastle Upon Tyne, U.K"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Mar 2021"",""2021"",""32"",""8"",""2115"",""2130"",""Many IoT applications and services such as smart parking and smart traffic control contain a network of different analytical components, which are composed in the form of a workflow to make better decisions. These workflows are also known as stream workflows. The focus of existing research works is on the streaming operator graph, which differs from stream workflow application as it involves heterogeneity, multiple data sources and multiple outputs. Considering the complexity and dynamism of stream workflow, meeting real-time data analysis requirements at deployment time is not the whole story as the velocity of data changes over time. This change is the most dynamic form of stream workflow that occurs frequently during the execution of this application. In this article, we propose a new dynamic scheduling technique that manages cloud resources over time to handle data velocity changes in stream workflow while maintaining user-defined real-time data analysis requirements and minimising execution cost. The efficiency of the proposed technique is evaluated, and experimental results showed that this technique outperformed its competitors and is close to the lower bound."",""1558-2183"","""",""10.1109/TPDS.2021.3059480"",""Australian Government Research Training Program"; UK Research and Innovation(grant numbers:SUPER (EP/T021985/1),,PACE (EP/R033293/1)); Centre for Digital Citizens(grant numbers:EP/T022582/1);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354934"",""IoT";Stream workflow;dynamic scheduling;alpha-beta pruning;GA with random immigrants;"cloud environments"",""Real-time systems";Big Data;Processor scheduling;Dynamic scheduling;Runtime;Computational modeling;"Data models"","""",""4"","""",""31"",""IEEE"",""16 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Optimised Lambda Architecture for Monitoring Scientific Infrastructure,""U. Suthakar"; L. Magnoni; D. R. Smith;" A. Khan"",""Department of Electronic and Computer Engineering, College of Engineering, Design and Physical Sciences, Brunel University London, Middlesex, U.K."; CERN, European Organization for Nuclear Research (CERN), Geneva, Switzerland; Department of Electronic and Computer Engineering, College of Engineering, Design and Physical Sciences, Brunel University London, Middlesex, U.K.;" Department of Electronic and Computer Engineering, College of Engineering, Design and Physical Sciences, Brunel University London, Middlesex, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""1 Feb 2021"",""2021"",""32"",""6"",""1395"",""1408"",""Within scientific infrastructuscientists execute millions of computational jobs daily, resulting in the movement of petabytes of data over the heterogeneous infrastructure. Monitoring the computing and user activities over such a complex infrastructure is incredibly demanding. Whereas present solutions are traditionally based on a Relational Database Management System (RDBMS) for data storage and processing, recent developments evaluate the Lambda Architecture (LA). In particular these studies have evaluated data storage and batch processing for processing large-scale monitoring datasets using Hadoop and its MapReduce framework. Although LA performed better than the RDBMS following evaluation, it was fairly complex to implement and maintain. This paper presents an Optimised Lambda Architecture (OLA) using the Apache Spark ecosystem, which involves modelling an efficient way of joining batch computation and real-time computation transparently without the need to add complexity. A few models were explored: pure streaming, pure batch computation, and the combination of both batch and streaming. An evaluation of the OLA on the CERN IT on-premises Hadoop cluster and the public Amazon cloud infrastructure for the monitoring WLCG Data acTivities (WDT) use case are both presented, demonstrating how the new architecture can offer benefits by combining both batch and real-time processing to compensate for batch-processing latency."",""1558-2183"","""",""10.1109/TPDS.2017.2772241"",""Brunel University London";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8336995"",""Big data";distributed systems;lambda architecture;low-latency computation;"parallel computing"",""Computer architecture";Monitoring;Real-time systems;Sparks;Distributed databases;Computational modeling;"Complexity theory"","""",""8"","""",""23"",""CCBY"",""12 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Optimistic Causal Consistency for Geo-Replicated Key-Value Stores,""K. Spirovska"; D. Didona;" W. Zwaenepoel"",""École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland"; IBM Research Europe, Rüschlikon, Switzerland;" University of Sydney, Camperdown, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Oct 2020"",""2021"",""32"",""3"",""527"",""542"",""Causal consistency (CC) is an attractive consistency model for geo-replicated data stores because it hits a sweet spot in the ease-of-programming versus performance trade-off. We present a new approach for implementing CC in geo-replicated data stores, which we call Optimistic Causal Consistency (OCC). OCC's main design goal is to maximize data freshness. The optimism in our approach lies in the fact that the updates replicated to a remote data center are made visible immediately, without checking if their causal dependencies have been received. Servers perform the dependency check needed to enforce CC only upon serving a client operation, rather than on receipt of a replicated data item as in existing systems. OCC offers a significant gain in data freshness, which is of crucial importance for various types of applications, such as real-time systems. OCC's potentially blocking behavior makes it vulnerable to network partitions. We therefore propose a recovery mechanism that allows an OCC system to fall back on a pessimistic protocol to continue operating during network partitions. We implement POCC, the first causally consistent geo-replicated multi-master key-value data store designed to maximize data freshness. We show that POCC improves data freshness, while offering comparable or better performance than its pessimistic counterparts."",""1558-2183"","""",""10.1109/TPDS.2020.3026778"",""The Swiss National Science Foundation(grant numbers:166306)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9206094"",""Optimistic causal consistency";causal consistency;geo-replication;key-value data stores;read-only transactions;"data freshness"",""Protocols";Data integrity;Servers;Data centers;Data models;Clocks;"Distributed databases"","""",""6"","""",""59"",""IEEE"",""25 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Resource Allocation for Data-Parallel Jobs Via GCN-Based Prediction,""Z. Hu"; D. Li; D. Zhang; Y. Zhang;" B. Peng"",""National University of Defense Technology, Changsha, P.R. China"; National University of Defense Technology, Changsha, P.R. China; Zhejiang University, Hangzhou, P.R. China; National University of Defense Technology, Changsha, P.R. China;" National University of Defense Technology, Changsha, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2021"",""2021"",""32"",""9"",""2188"",""2201"",""Under-allocating or over-allocating computation resources (e.g., CPU cores) can prolong the completion time of data-parallel jobs in a distributed system. We present a predictor, ReLocag, to find the near-optimal number of CPU cores to minimize job completion time (JCT). ReLocag includes a graph convolutional network (GCN) and a fully-connected network (FCNN). The GCN learns the dependency between operations from the workflow of a job, and then the FCNN takes the workflow dependency together with other features (e.g., the input size, the number of CPU cores, the amount of memory, and the number of computation tasks) as input for JCT prediction. The prediction result can guide the user to determine the near-optimal number of CPU cores. Besides, we propose two effective strategies to overcome the time-consuming issue of training sample collection in big data applications. First, we develop an adaptive sampling method to collect essential samples judiciously. Second, we further design a cross-application transfer learning model to exploit the training samples collected from other applications. We conduct extensive experiments in a Spark cluster for 7 types of exemplary Spark applications. Results show that ReLocag improves the JCT prediction accuracy by 4-14 percent. Moreover, the CPU core consumption decreases by 58.2 percent."",""1558-2183"","""",""10.1109/TPDS.2021.3055019"",""National Key Research and Development Program of China(grant numbers:2018YFB2101100)"; National Natural Science Foundation of China(grant numbers:61932001,61872376);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9337194"",""Data-parallel job";resource allocation;performance prediction;"sampling overhead"",""Sparks";Resource management;Predictive models;Training;Task analysis;Transfer learning;"Adaptation models"","""","""","""",""39"",""IEEE"",""27 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Optimizing the LINPACK Algorithm for Large-Scale PCIe-Based CPU-GPU Heterogeneous Systems,""G. Tan"; C. Shui; Y. Wang; X. Yu;" Y. Yan"",""State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China;" State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Apr 2021"",""2021"",""32"",""9"",""2367"",""2380"",""There is a widening gap between GPU and other components (CPU, PCIe bus and communication network) in heterogeneous parallel system. The gap forces us to orchestrate cooperative execution among these components much more carefully than ever before. By taking the LINPACK benchmark as a case study, this article proposes a fine-grained pipelining algorithm on large-scale CPU-GPU heterogeneous cluster systems. First, we build an algorithmic model that reveals a new approach to GPU-centric and fine-grained pipelining algorithm design. Then, we present four model-driven pipelining algorithms that incrementally squeeze bubbles in the pipeline so that it is occupied by more useful floating-point calculations. The algorithms are implemented on both the AMD and NVIDIA GPU platforms. The finally optimized LINPACK program achieves 107 PFlops on 25, 600 GPUs (70 percent floating-point efficiency). Several insights have been drawn to suggest tradeoff of algorithm design, programming support, and architecture design."",""1558-2183"","""",""10.1109/TPDS.2021.3067731"",""The National Key Research and Development Program of China(grant numbers:2018YFB0204400,XDC01030000,XDC05010100)"; National Natural Science Foundation of China(grant numbers:62032023,61972377,61702483); CAS(grant numbers:QYZDJ-SSW-JSC035);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382911"",""LINPACK algorithm";software pipeline;performance model;heterogeneous computing;"cluster"",""Pipeline processing";Graphics processing units;Computer architecture;Supercomputers;Clustering algorithms;Programming;"Optimization"","""",""5"","""",""50"",""IEEE"",""22 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Overlapping Communication With Computation in Parameter Server for Scalable DL Training,""S. Wang"; A. Pi; X. Zhou; J. Wang;" C. -Z. Xu"",""Department of Computer Science, University of Colorado, Colorado Springs, CO, USA"; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, USA;" Faculty of Science and Technology, University of Macau, Taipa, Macau, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Mar 2021"",""2021"",""32"",""9"",""2144"",""2159"",""Scalability of distributed deep learning (DL) training with parameter server (PS) architecture is often communication constrained in large clusters. There are recent efforts that use a layer by layer strategy to overlap gradient communication with backward computation so as to reduce the impact of communication constraint on the scalability. However, the approaches could bring significant overhead in gradient communication. Meanwhile, they cannot be effectively applied to the overlap between parameter communication and forward computation. In this article, we propose and develop iPart, a novel approach that partitions communication and computation in various partition sizes to overlap gradient communication with backward computation and parameter communication with forward computation. iPart formulates the partitioning decision as an optimization problem and solves it based on a greedy algorithm to derive communication and computation partitions. We implement iPart in the open-source DL framework BigDL and perform evaluations with various DL workloads. Experimental results show that iPart improves the scalability of a cluster of 72 nodes by up to 94 percent over the default PS and 52 percent over the layer by layer strategy."",""1558-2183"","""",""10.1109/TPDS.2021.3062721"",""National Science Foundation(grant numbers:CNS-1422119,SHF-1816850)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366342"",""Parameter server";parameter communication;forward computation;gradient communication;"backward computation"",""Servers";Training;Computer architecture;Computational modeling;Neural networks;Synchronization;"Scalability"","""",""13"","""",""59"",""IEEE"",""1 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"OWebSync: Seamless Synchronization of Distributed Web Clients,""K. Jannes"; B. Lagaisse;" W. Joosen"",""imec-DistriNet, KU Leuven, Leuven, Belgium"; imec-DistriNet, KU Leuven, Leuven, Belgium;" imec-DistriNet, KU Leuven, Leuven, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Apr 2021"",""2021"",""32"",""9"",""2338"",""2351"",""Many enterprise software services are adopting a fully web-based architecture for both internal line-of-business applications and for online customer-facing applications. Although wireless connections are becoming more ubiquitous and faster, mobile employees and customers are often offline due to expected or unexpected network disruptions. Nevertheless, continuous operation of the software is expected. This article presents OWebSync: a web-based middleware for data synchronization in interactive groupware with fast resynchronization of offline clients and continuous, interactive synchronization of online clients. To automatically resolve conflicts, OWebSync implements a fine-grained data synchronization model and leverages state-based Conflict-free Replicated Data Types. This middleware uses Merkle-trees embedded in the tree-structured data and virtual Merkle-tree levels to achieve the required interactive performance. Our comparative evaluation with available operation-based and delta-state-based middleware solutions shows that OWebSync is especially better in operating in and recovering from offline settings and network disruptions. In addition, OWebSync scales more efficiently over time, as it does not store version vectors or other meta-data for all past clients."",""1558-2183"","""",""10.1109/TPDS.2021.3066276"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380486"",""CRDTs";groupware;web browsers;"eventual consistency"",""Synchronization";Servers;Metadata;Collaborative software;Middleware;Companies;"Clocks"","""",""5"","""",""60"",""IEEE"",""17 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PaKman: A Scalable Algorithm for Generating Genomic Contigs on Distributed Memory Machines,""P. Ghosh"; S. Krishnamoorthy;" A. Kalyanaraman"",""Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA"; Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA;" School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Jan 2021"",""2021"",""32"",""5"",""1191"",""1209"",""De novo genome assembly is a fundamental problem in the field of bioinformatics, that aims to assemble the DNA sequence of an unknown genome from numerous short DNA fragments (aka reads) obtained from it. With the advent of high-throughput sequencing technologies, billions of reads can be generated in a matter of hours, necessitating efficient parallelization of the assembly process. While multiple parallel solutions have been proposed in the past, conducting a large-scale assembly at scale remains a challenging problem because of the inherent complexities associated with data movement, and irregular access footprints of memory and I/O operations. In this article, we present a novel algorithm, called PaKman, to address the problem of performing large-scale genome assemblies on a distributed memory parallel computer. Our approach focuses on improving performance through a combination of novel data structures and algorithmic strategies for reducing the communication and I/O footprint during the assembly process. PaKman presents a solution for the two most time-consuming phases in the full genome assembly pipeline, namely, k-mer counting and contig generation. A key aspect of our algorithm is its graph data structure (PaK-Graph), which comprises fat nodes (or what we call “macro-nodes”) that reduce the communication burden during contig generation. We present an extensive performance and qualitative evaluation of our algorithm across a wide range of genomes (varying in both size and species group), including comparisons to other state-of-the-art parallel assemblers. Our results demonstrate the ability to achieve near-linear speedups on up to 16K cores (tested) on the NERSC Cori supercomputer"; perform better than or comparable to other state-of-the-art distributed memory and shared memory tools in terms of performance while delivering comparable (if not better) quality;" and reduce time to solution significantly. For instance, PaKman is able to generate a high-quality set of assembled contigs for complex genomes such as the human and bread wheat genomes in under a minute on 16K cores. In addition, PaKman was able to successfully process a 3.1 TB simulated dataset of one of the largest known genomes (to date)-Ambystoma mexicanum (the axolotl), in just over 200 seconds on 16K cores."",""1558-2183"","""",""10.1109/TPDS.2020.3043241"",""U.S. Department of Energy(grant numbers:DE-AC02-05CH11231)"; U.S. Department of Energy(grant numbers:63823); National Science Foundation(grant numbers:CCF 1815467,OAC 1910213,CCF 1919122);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286880"",""Genome assembly";distributed memory;de bruijn graphs;"k-mer counting"",""Bioinformatics";Genomics;Data structures;Sequential analysis;DNA;Compaction;"Optimization"","""",""1"","""",""33"",""CCBY"",""8 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Parallel Blockwise Knowledge Distillation for Deep Neural Network Compression,""C. Blakeney"; X. Li; Y. Yan;" Z. Zong"",""Department of Computer Science, Texas State University, San Marcos, TX, USA"; Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computer Science, Texas State University, San Marcos, TX, USA;" Department of Computer Science, Texas State University, San Marcos, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1765"",""1776"",""Deep neural networks (DNNs) have been extremely successful in solving many challenging AI tasks in natural language processing, speech recognition, and computer vision nowadays. However, DNNs are typically computation intensive, memory demanding, and power hungry, which significantly limits their usage on platforms with constrained resources. Therefore, a variety of compression techniques (e.g., quantization, pruning, and knowledge distillation) have been proposed to reduce the size and power consumption of DNNs. Blockwise knowledge distillation is one of the compression techniques that can effectively reduce the size of a highly complex DNN. However, it is not widely adopted due to its long training time. In this article, we propose a novel parallel blockwise distillation algorithm to accelerate the distillation process of sophisticated DNNs. Our algorithm leverages local information to conduct independent blockwise distillation, utilizes depthwise separable layers as the efficient replacement block architecture, and properly addresses limiting factors (e.g., dependency, synchronization, and load balancing) that affect parallelism. The experimental results running on an AMD server with four Geforce RTX 2080Ti GPUs show that our algorithm can achieve 3x speedup plus 19 percent energy savings on VGG distillation, and 3.5x speedup plus 29 percent energy savings on ResNet distillation, both with negligible accuracy loss. The speedup of ResNet distillation can be further improved to 3.87 when using four RTX6000 GPUs in a distributed cluster."",""1558-2183"","""",""10.1109/TPDS.2020.3047003"",""National Science Foundation(grant numbers:CNS-1908658)"; National Science Foundation; Texas State University Research Enhancement Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9305986"",""Deep neural networks";model compression;knowledge distillation;"parallel training"",""Computational modeling";Training;Task analysis;Quantization (signal);Neural networks;Deep learning;"Hardware"","""",""15"","""",""47"",""IEEE"",""23 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Parallel Fine-Grained Comparison of Long DNA Sequences in Homogeneous and Heterogeneous GPU Platforms With Pruning,""M. Figueiredo"; J. P. Navarro; E. F. O. Sandes; G. Teodoro;" A. C. M. A. Melo"",""University of Brasilia, Brasilia, Brazil"; NVidia, Sao Paulo, Brazil; University of Brasilia, Brasilia, Brazil; Universidade Federal de Minas Gerais, Belo Horizonte, Brazil;" University of Brasilia, Brasilia, Brazil"",""IEEE Transactions on Parallel and Distributed Systems"",""21 Jun 2021"",""2021"",""32"",""12"",""3053"",""3065"",""The parallelization of Smith-Waterman (SW) sequence comparison tools for long DNA sequences has been a big challenge over the years, requesting the use of several devices and sophisticated optimizations. Pruning is one of these optimizations, which can reduce considerably the amount of computation. This article proposes MultiBP, a sequence comparison solution in multiple GPUs with block pruning. Two MultiBP strategies are proposed. In static score-sharing, workload is statically distributed to the GPUs, and the best score is sent to neighbor GPUs to simulate a global view. In the dynamic strategy, execution is divided into cycles and workload is dynamically assigned, according to the GPUs processing rate. MultiBP was integrated to MASA-CUDAlign and tested in homogeneous and heterogeneous platforms, with different NVidia GPU architectures. The best results in our homogeneous and heterogeneous platforms were mostly obtained by the static and dynamic approaches, respectively. We also show that our decision module is able to select the best strategy in most cases. Finally, the comparison of the human and chimpanzee chromosomes 1 in a cluster with 512 V100 NVidia GPUs took 11 minutes and obtained the impressive rate of 82,822 GCUPS (Billions of Cells Updated per Second) which is, to our knowledge, the best performance for SW tools in GPUs."",""1558-2183"","""",""10.1109/TPDS.2021.3084069"",""CNPq/Brazil(grant numbers:305196/2018-9)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442294"",""GPU";sequence comparison;pruning;workload assignment;"heterogeneous platforms"",""Graphics processing units";Sequential analysis;Heuristic algorithms;Dynamic programming;Optimization;"Evolution (biology)"","""",""1"","""",""41"",""IEEE"",""26 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Parallelization and Optimization of NSGA-II on Sunway TaihuLight System,""X. Liu"; J. Sun; L. Zheng; S. Wang; Y. Liu;" T. Wei"",""Faculty of Information, East China Normal University, Shanghai, China"; Faculty of Information, East China Normal University, Shanghai, China; Faculty of Information, East China Normal University, Shanghai, China; Faculty of Information, East China Normal University, Shanghai, China; Faculty of Information, East China Normal University, Shanghai, China;" Faculty of Information, East China Normal University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Dec 2020"",""2021"",""32"",""4"",""975"",""987"",""Sunway TaihuLight system is the first supercomputer offering a peak performance over 100 PFlops, which can be utilized to parallelize Non-dominated Sorting Genetic Algorithm II (NSGA-II), a standard approach to multi-objective optimization. However, insufficient off-chip memory bandwidth and limited scratchpad memory capacity of the supercomputer hinder the performance improvement of parallellizing NSGA-II. In this article, we propose an optimized parallel NSGA-II on Sunway TaihuLight system, called swNSGA-II, by utilizing process- and thread-level parallelism of the system based on an improved island/master-slave model. To overcome the hurdles of low memory bandwidth and capacity, we propose a data sharing scheme based on register-level communication that can efficiently parallelize non-dominated sorting and crowding-distance computation of NSGA-II. Several optimization techniques including vectorization, direct memory accessing, and double buffering are also adopted to further accelerate swNSGA-II. Experiment results show that the proposed swNSGA-II can achieve a speedup of 41284 on a use case of path planning, and a speedup of 62692 on ZDT1 as compared to conventional NSGA-II."",""1558-2183"","""",""10.1109/TPDS.2020.3037082"",""National Key Research and Development Program of China(grant numbers:2020YFA0607902)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9253690"",""Sunway TaihuLight";many-core processor;NSGA-II;"multi-objective optimization"",""Computational modeling";Instruction sets;Bandwidth;Path planning;Supercomputers;Optimization;"Sorting"","""",""18"","""",""30"",""IEEE"",""10 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Partitioning Models for General Medium-Grain Parallel Sparse Tensor Decomposition,""M. O. Karsavuran"; S. Acer;" C. Aykanat"",""Computer Engineering Department, Bilkent University, Turkey"; Center for Computing Research, Sandia National Laboratories, Albuquerque, USA;" Computer Engineering Department, Bilkent University, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Aug 2020"",""2021"",""32"",""1"",""147"",""159"",""The focus of this article is efficient parallelization of the canonical polyadic decomposition algorithm utilizing the alternating least squares method for sparse tensors on distributed-memory architectures. We propose a hypergraph model for general medium-grain partitioning which does not enforce any topological constraint on the partitioning. The proposed model is based on splitting the given tensor into nonzero-disjoint component tensors. Then a mode-dependent coarse-grain hypergraph is constructed for each component tensor. A net amalgamation operation is proposed to form a composite medium-grain hypergraph from these mode-dependent coarse-grain hypergraphs to correctly encapsulate the minimization of the communication volume. We propose a heuristic which splits the nonzeros of dense slices to obtain sparse slices in component tensors. So we partially attain slice coherency at (sub)slice level since partitioning is performed on (sub)slices instead of individual nonzeros. We also utilize the well-known recursive-bipartitioning framework to improve the quality of the splitting heuristic. Finally, we propose a medium-grain tripartite graph model with the aim of a faster partitioning at the expense of increasing the total communication volume. Parallel experiments conducted on 10 real-world tensors on up to 1024 processors confirm the validity of the proposed hypergraph and graph models."",""1558-2183"","""",""10.1109/TPDS.2020.3012624"",""Türkiye Bilimsel ve Teknolojik Araştirma Kurumu(grant numbers:EEEAG-116E043)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158556"",""sparse tensor";tensor decomposition;canonical polyadic decomposition;communication cost;communication volume,medium-grain partitioning,recursive bipartitioning;"hypergraph partitioning,graph partitioning"",""Tensile stress";Program processors;Partitioning algorithms;Computational modeling;Sparse matrices;Load modeling;"Minimization"","""",""9"","""",""33"",""IEEE"",""4 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Partitioning-Based Scheduling of OpenMP Task Systems With Tied Tasks,""Y. Wang"; X. Jiang; N. Guan; Z. Guo; X. Liu;" W. Yi"",""Northeastern University, Shenyang, China"; Northeastern University, Shenyang, China; Hong Kong Polytechnic University, Hong Kong; University of Central Florida, Orlando, FL, USA; McGill University, Montreal, QC, Canada;" Northeastern University, Shenyang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Feb 2021"",""2021"",""32"",""6"",""1322"",""1339"",""OpenMP is a popular programming framework in both general and high-performance computing and has recently drawn much interest in embedded and real-time computing. Although the execution semantics of OpenMP are similar to the DAG task model, the constraints posed by the OpenMP specification make them significantly more challenging to analyze. A tied task is an important feature in OpenMP that must execute on the same thread throughout its entire life cycle. A previous work [1] succeeded in analyzing the real-time scheduling of tied tasks by modifying the Task Scheduling Constraints (TSCs) in OpenMP specification. In this article, we also study the real-time scheduling of OpenMP task systems with tied tasks but without changing the original TSCs. In particular, we propose a partitioning-based algorithm, P-EDF-omp, by which the tied constraint can be automatically guaranteed as long as an OpenMP task system can be successfully partitioned to a multiprocessor platform. Furthermore, we conduct comprehensive experiments with both synthetic workloads and established OpenMP benchmarks to show that our approach consistently outperforms the work in [1] -even without modifying the TSCs."",""1558-2183"","""",""10.1109/TPDS.2020.3048373"",""National Natural Science Foundation of China(grant numbers:61772123)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311807"",""Multicore";parallel tasks;real-time scheduling;partitioning;OpenMP;"tied tasks"",""Task analysis";Program processors;Real-time systems;Time factors;Synchronization;Sun;"Semantics"","""",""4"","""",""44"",""IEEE"",""31 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Pebbles: Leveraging Sketches for Processing Voluminous, High Velocity Data Streams,""T. Buddhika"; S. L. Pallickara;" S. Pallickara"",""Department of Computer Science, Colorado State University, Fort Collins, CO, USA"; Department of Computer Science, Colorado State University, Fort Collins, CO, USA;" Department of Computer Science, Colorado State University, Fort Collins, CO, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Feb 2021"",""2021"",""32"",""8"",""2005"",""2020"",""Voluminous, time-series data streams originating in continuous sensing environments pose data ingestion and processing challenges. We present a holistic methodology centered around data sketching to address both challenges. We introduce an order-preserving sketching algorithm that we have designed for space-efficient representation of multi-feature streams with native support for stream processing related operations. Observational streams are preprocessed at the edges of the network generating sketched streams to reduce data transfer costs and energy consumption. Ingested sketched streams are then processed using sketch-aware extensions to existing stream processing APIs delivering improved performance. Our benchmarks with real-world datasets show up to a ~8× reduction in data volumes transferred and a ~27× improvement in throughput."",""1558-2183"","""",""10.1109/TPDS.2021.3055265"",""National Science Foundation(grant numbers:OAC-1931363,ACI-1553685)"; National Institute of Food and Agriculture(grant numbers:COL0-FACT-2019); Cochran Family Professorship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9339879"",""Data sketches";stream processing systems;edge computing;"Internet-of-Things"",""Sensors";Cloud computing;Data transfer;Monitoring;Logic gates;Distributed databases;"Throughput"","""",""4"","""",""69"",""IEEE"",""28 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Petrel: Heterogeneity-Aware Distributed Deep Learning Via Hybrid Synchronization,""Q. Zhou"; S. Guo; Z. Qu; P. Li; L. Li; M. Guo;" K. Wang"",""Department of Computing, The Hong Kong Polytechnic University, Hong Kong"; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Computer and Information, Hohai University, Nanjing, China; School of Computer Science and Engineering, The University of Aizu, Fukushima-ken, Japan; School of Software, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China;" Department of Electrical and Computer Engineering, University of California, Los Angeles, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2020"",""2021"",""32"",""5"",""1030"",""1043"",""The parameter server (PS) paradigm has achieved great success in deploying large-scale distributed Deep Learning (DL) systems. However, these systems implicitly assume that the cluster is homogeneous and this assumption does not hold in many realworld cases. Although the previous efforts are paid to address heterogeneity, they mainly prioritize the contribution of fast workers and reduce the involvement of slow workers, resulting in the limitations of workload imbalance and computation inefficiency. We reveal that grouping workers into communities, an abstraction proposed by us, and handling parameter synchronization at the community level can conquer these limitations and accelerate the training convergence progress. The inspiration of community comes from our exploration of prior knowledge about the similarity between workers, which is often neglected by previous work. These observations motivate us to propose a new synchronization mechanism named Community-aware Synchronous Parallel (CASP), which uses the Asynchronous Advantage Actor-Critic (A3C)-based algorithm to intelligently determine community configuration and fully improve the synchronization performance. The whole idea has been implemented in a prototype system called Petrel that achieves a good balance between convergence efficiency and communication overhead. The evaluation under various benchmarks with multiple metrics and baseline comparison demonstrates the effectiveness of Petrel. Specifically, Petrel accelerates the training convergence speed by up to 1.87 x faster and reduces communication traffic by up to 26.85 percent, on average, over the non-community synchronization mechanisms."",""1558-2183"","""",""10.1109/TPDS.2020.3040601"",""Hong Kong RGC Research Impact Fund(grant numbers:R5060-19,R5034-18)"; General Research Fund(grant numbers:152221/19E); Collaborative Research Fund(grant numbers:C5026-18G); National Natural Science Foundation of China(grant numbers:61872310); China Postdoctoral Science Foundation(grant numbers:2019M661709); Shanghai Trusted Industrial Control Platform;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9271915"",""Distributed systems";deep learning;parameter server;"heterogeneous environment"",""Synchronization";Training;Convergence;Servers;Task analysis;Acceleration;"Measurement"","""",""14"","""",""52"",""IEEE"",""25 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"PISTIS: An Event-Triggered Real-Time Byzantine-Resilient Protocol Suite,""D. Kozhaya"; J. Decouchant; V. Rahli;" P. Esteves-Verissimo"",""ABB Research, Baden, Switzerland"; TU Delft, Delft, CD, Netherlands; University of Birmingham, Birmingham, U.K.;" King Abdullah University of Science and Technology - RC3, Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Mar 2021"",""2021"",""32"",""9"",""2277"",""2290"",""The accelerated digitalisation of society along with technological evolution have extended the geographical span of cyber-physical systems. Two main threats have made the reliable and real-time control of these systems challenging: (i) uncertainty in the communication infrastructure induced by scale, and heterogeneity of the environment and devices";" and (ii) targeted attacks maliciously worsening the impact of the above-mentioned communication uncertainties, disrupting the correctness of real-time applications. This article addresses those challenges by showing how to build distributed protocols that provide both real-time with practical performance, and scalability in the presence of network faults and attacks, in probabilistic synchronous environments. We provide a suite of real-time Byzantine protocols, which we prove correct, starting from a reliable broadcast protocol, called PISTIS, up to atomic broadcast and consensus. This suite simplifies the construction of powerful distributed and decentralized monitoring and control applications, including state-machine replication. Extensive empirical simulations showcase PISTIS's robustness, latency, and scalability. For example, PISTIS can withstand message loss (and delay) rates up to 50 percent in systems with 49 nodes and provides bounded delivery latencies in the order of a few milliseconds."",""1558-2183"","""",""10.1109/TPDS.2021.3056718"",""National Cyber Security Centre (NCSC)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9347806"",""Real-time distributed systems";probabilistic losses;consensus;atomic broadcast;Byzantine resilience;"intrusion tolerance"",""Protocols";Uncertainty;Scalability;Process control;Bandwidth;Real-time systems;"Monitoring"","""",""8"","""",""37"",""IEEE"",""4 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"PQC Acceleration Using GPUs: FrodoKEM, NewHope, and Kyber,""N. Gupta"; A. Jati; A. K. Chauhan;" A. Chattopadhyay"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore"; Department of Computer Science and Engineering, Indraprastha Institute of Information Technology, New Delhi, Delhi, India; Department of Computer Science and Engineering, IIT, Ropar, Rupnagar, Punjab, India;" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Oct 2020"",""2021"",""32"",""3"",""575"",""586"",""In this article, we present the first GPU implementation for FrodoKEM-976, NewHope-1024, and Kyber-1024. These algorithms belong to three different classes of post-quantum algorithms: Learning with errors (LWE), Ring-LWE, and Module-LWE. We show the practical applicability of the algorithms in different scenarios using two different implementation approaches. Moreover, we achieve highly efficient realization of computationally expensive operations such as NTT (Number Theoretic Transform), matrix multiplication, and Keccak. Since, these are the most common operations in lattice-based cryptographic algorithms, the techniques presented in this article will likely benefit other similar algorithms. Using a NVIDIA QUADRO GV100 graphics card, we undertook a detailed experimental study. For NewHope and Kyber we were able to perform approximately 504K and 473K key exchanges per second, demonstrating a speedup of almost 53.1× and 51.05× compared to the reference C implementation. Compared to the optimized AVX2 versions we obtain speedups of 25.7× and 14.6×, respectively. Further, implementation of FrodoKEM resulted in a speedup of 50.6×, 44.2×, and 36.9× for KeyGen, Encaps and Decaps operations. Compared to its AVX2 counterpart, we achieved a speedup of about 7.3×, 4.7× and 4.9×, respectively. We also show that using multiple streams resulted in further speedup of about 28-38 percent."",""1558-2183"","""",""10.1109/TPDS.2020.3025691"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9201530"",""Cryptography";post-quantum;key exchange;PQC;NewHope;Kyber;FrodoKEM;GPU;CUDA;accelerator;NTT;"SHAKE"",""Graphics processing units";Lattices;Encapsulation;Acceleration;Cryptography;Zinc;"Quantum computing"","""",""26"","""",""18"",""IEEE"",""21 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"PredCom: A Predictive Approach to Collecting Approximated Communication Traces,""S. Miwa"; I. Laguna;" M. Schulz"",""Department of Computer and Network Engineering, University of Electro-Communications, Tokyo, Japan"; Lawrence Livermore National Laboratory, Livermore, USA;" Technical University of Munich, München, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Jul 2020"",""2021"",""32"",""1"",""45"",""58"",""Communication traces collected from MPI applications are an important source of information for performance optimization as they can help analysts determine communication patterns and identify inefficiencies. However, their collection, especially at scale, is time consuming, since it usually requires running the complete target application on a large number of nodes. In this work, we present PredCom, a tool-chain to generate a predictive communication proxy based on information gathered from a few small scale runs, which allows us to extract approximate communication traces with an accuracy high enough for most analysis goals. For this, we combine LLVM passes on the original source code (to capture static program structure) with parameter prediction (to capture dynamic and scaling behavior). This approach drastically reduces the time needed for collecting the communication traces, even for traces on large numbers of MPI processes. We demonstrate that PredCom generates communication traces of various applications up to 1612x faster with an accuracy loss of 0.11 on average compared to the original large-scale traces, and we show that the generated traces can be used to optimize process placement."",""1558-2183"","""",""10.1109/TPDS.2020.3011121"",""Japan Science and Technology Agency"; Kayamori Foundation of Informational Science Advancement(grant numbers:K30-XXIII-524); Japan Society for the Promotion of Science(grant numbers:JP20H04193);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9146385"",""Communication traces";MPI;"LLVM"",""Predictive models";Optimization;Instruments;Transforms;Libraries;Static analysis;"Reactive power"","""",""1"","""",""45"",""IEEE"",""22 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Privacy-Preserving Computation Offloading for Parallel Deep Neural Networks Training,""Y. Mao"; W. Hong; H. Wang; Q. Li;" S. Zhong"",""Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science, College of William & Mary, Williamsburg, VA, USA;" Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1777"",""1788"",""Deep neural networks (DNNs) have brought significant performance improvements to various real-life applications. However, a DNN training task commonly requires intensive computing resources and a huge data collection, which makes it hard for personal devices to carry out the entire training, especially for mobile devices. The federated learning concept has eased this situation. However, it is still an open problem for individuals to train their own DNN models at an affordable price. In this article, we propose an alternative DNN training strategy for resource-limited users. With the help of an untrusted server, end users can offload their DNN training tasks to the server in a privacy-preserving manner. To this end, we study the possibility of the separation of a DNN. Then we design a differentially private activation algorithm for end users to ensure the privacy of the offloading after model separation. Furthermore, to meet the rising demand for federated learning, we extend the offloading solution to parallel DNN models training with a secure model weights aggregation scheme for the privacy concern. Experimental results prove the feasibility of computation offloading solutions for DNN models in both solo and parallel modes."",""1558-2183"","""",""10.1109/TPDS.2020.3040734"",""National Key R&D Program of China(grant numbers:2018YFB1004301,BK20190294,NSFC-61902176,NSFC-61872176)"; Fundamental Research Funds for the Central Universities(grant numbers:14380069); National Science Foundation(grant numbers:CNS-1816399); Commonwealth Cyber Initiative;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272685"",""Deep neural network";federated learning;computation offloading;data privacy;"model parallelism"",""Servers";Training;Computational modeling;Privacy;Data models;Task analysis;"Cryptography"","""",""14"","""",""33"",""IEEE"",""26 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Privacy-Preserving Multi-Keyword Searchable Encryption for Distributed Systems,""X. Liu"; G. Yang; W. Susilo; J. Tonien; X. Liu;" J. Shen"",""Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia"; Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia; Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia; Institute of Cybersecurity and Cryptology, University of Wollongong, Wollongong, NSW, Australia; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China;" School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Oct 2020"",""2021"",""32"",""3"",""561"",""574"",""As cloud storage has been widely adopted in various applications, how to protect data privacy while allowing efficient data search and retrieval in a distributed environment remains a challenging research problem. Existing searchable encryption schemes are still inadequate on desired functionality and security/privacy perspectives. Specifically, supporting multi-keyword search under the multi-user setting, hiding search pattern and access pattern, and resisting keyword guessing attacks (KGA) are the most challenging tasks. In this article, we present a new searchable encryption scheme that addresses the above problems simultaneously, which makes it practical to be adopted in distributed systems. It not only enables multi-keyword search over encrypted data under a multi-writer/multi-reader setting but also guarantees the data and search pattern privacy. To prevent KGA, our scheme adopts a multi-server architecture, which accelerates search response, shares the workload, and lowers the key leakage risk by allowing only authorized servers to jointly test whether a search token matches a stored ciphertext. A novel subset decision mechanism is also designed as the core technique underlying our scheme and can be further used in applications other than keyword search. Finally, we prove the security and evaluate the computational and communication efficiency of our scheme to demonstrate its practicality."",""1558-2183"","""",""10.1109/TPDS.2020.3027003"",""National Natural Science Foundation of China(grant numbers:U1804263,62072109)"; National Natural Science Foundation of China(grant numbers:61922045,U1836115,61672295);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9207857"",""Searchable encryption";multi-keyword search;multi-user access;search pattern;"access pattern"",""Encryption";Servers;Cloud computing;Keyword search;Public key;"Data privacy"","""",""45"","""",""30"",""IEEE"",""28 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Privacy-Preserving Similarity Search With Efficient Updates in Distributed Key-Value Stores,""W. Lin"; H. Cui; B. Li;" C. Wang"",""Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada"; School of Computer Science, Northwestern Polytechnical University, Xi'an, Shaanxi, China; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada;" Department of Computer Science, City University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Dec 2020"",""2021"",""32"",""5"",""1072"",""1084"",""Privacy-preserving similarity search plays an essential role in data analytics, especially when very large encrypted datasets are stored in the cloud. Existing mechanisms on privacy-preserving similarity search were not able to support secure updates (addition and deletion) efficiently when frequent updates are needed. In this article, we propose a new mechanism to support parallel privacypreserving similarity search in a distributed key-value store in the cloud, with a focus on efficient addition and deletion operations, both executed with sublinear time complexity. If search accuracy is the top priority, we further leverage Yao's garbled circuits and the homomorphic property of Hash-ElGamal encryption to build a secure evaluation protocol, which can obtain the top-R most accurate results without extensive client-side post-processing. We have formally analyzed the security strength of our proposed approach, and performed an extensive array of experiments to show its superior performance as compared to existing mechanisms in the literature. In particular, we evaluate the performance of our proposed protocol with respect to the time it takes to build the index and perform similarity queries. Extensive experimental results demonstrated that our protocol can speedup the index building process by up to 800x with 2 threads and the similarity queries by up to -7x with comparable accuracy, as compared to the state-of-the-art in the literature."",""1558-2183"","""",""10.1109/TPDS.2020.3042695"",""Natural Science Basic Research Program of Shaanxi(grant numbers:2020JQ-215)"; Research Grants Council of Hong Kong(grant numbers:CityU 11217819,CityU 11217620); Fundamental Research Funds for the Central Universities(grant numbers:3102019QD1001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9288779"",""Searchable symmetric encryption";key-value stores;data privacy;similarity search;cloud storage;"efficient updates"",""Cryptography";Servers;Indexes;Protocols;Cloud computing;Encryption;"Search problems"","""",""7"","""",""36"",""IEEE"",""9 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Profiles of Upcoming HPC Applications and Their Impact on Reservation Strategies,""A. Gainaru"; B. Goglin; V. Honoré;" G. Pallez"",""Vanderbilt University, Nashville, TN, USA"; Inria & Université de Bordeaux, Talence, France; Inria & Université de Bordeaux, Talence, France;" Inria & Université de Bordeaux, Talence, France"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Dec 2020"",""2021"",""32"",""5"",""1178"",""1190"",""With the expected convergence between HPC, BigData and AI, new applications with different profiles are coming to HPC infrastructures. We aim at better understanding the features and needs of these applications in order to be able to run them efficiently on HPC platforms. The approach followed is bottom-up: we study thoroughly an emerging application, Spatially Localized Atlas Network Tiles (SLANT, originating from the neuroscience community) to understand its behavior. Based on these observations, we derive a generic, yet simple, application model (namely, a linear sequence of stochastic jobs). We expect this model to be representative for a large set of upcoming applications from emerging fields that start to require the computational power of HPC clusters without fitting the typical behavior of large-scale traditional applications. In a second step, we show how one can use this generic model in a scheduling framework. Specifically we consider the problem of making reservations (both time and memory) for an execution on an HPC platform based on the application expected resource requirements. We derive solutions using the model provided by the first step of this work. We experimentally show the robustness of the model, even with very few data points or using another application, to generate the model, and provide performance gains with regards to standard and more recent approaches used in the neuroscience community."",""1558-2183"","""",""10.1109/TPDS.2020.3039728"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9266751"",""Stochastic application";execution time;memory footprint;scheduling;"checkpointing"",""Task analysis";Memory management;Neuroscience;Computational modeling;Magnetic resonance imaging;Correlation;"Three-dimensional displays"","""",""4"","""",""50"",""IEEE"",""23 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Proof of Federated Learning: A Novel Energy-Recycling Consensus Algorithm,""X. Qu"; S. Wang; Q. Hu;" X. Cheng"",""School of Artificial Intelligence, Beijing Normal University, Beijing, China"; School of Artificial Intelligence, Beijing Normal University, Beijing, China; Department of Computer and Information Science, Indiana University-Purdue University Indianapolis, Indianapolis, IN, USA;" School of Computer Science and Technology, Shandong University, Jinan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Mar 2021"",""2021"",""32"",""8"",""2074"",""2085"",""Proof of work (PoW), the most popular consensus mechanism for blockchain, requires ridiculously large amounts of energy but without any useful outcome beyond determining accounting rights among miners. To tackle the drawback of PoW, we propose a novel energy-recycling consensus algorithm, namely proof of federated learning (PoFL), where the energy originally wasted to solve difficult but meaningless puzzles in PoW is reinvested to federated learning. Federated learning and pooled-mining, a trend of PoW, have a natural fit in terms of organization structure. However, the separation between the data usufruct and ownership in blockchain lead to data privacy leakage in model training and verification, deviating from the original intention of federal learning. To address the challenge, a reverse game-based data trading mechanism and a privacy-preserving model verification mechanism are proposed. The former can guard against training data leakage while the latter verifies the accuracy of a trained model with privacy preservation of the task requester's test data as well as the pool's submitted model. To the best of our knowledge, our article is the first work to employ federal learning as the proof of work for blockchain. Extensive simulations based on synthetic and real-world data demonstrate the effectiveness and efficiency of our proposed mechanisms."",""1558-2183"","""",""10.1109/TPDS.2021.3056773"",""National Key Research and Development Program of China(grant numbers:2019YFB2102600)"; National Natural Science Foundation of China(grant numbers:61772080,62072044); Ministry of Education of the People's Republic of China(grant numbers:2020KJ010301); Engineering Research Center of Intelligent Technology and Educational Application;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9347812"",""Blockchain";federated learning;consensus algorithm;"incentive mechanism"",""Data models";Blockchain;Collaborative work;Training;Computational modeling;Training data;"Task analysis"","""",""48"","""",""36"",""IEEE"",""4 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"QShield: Protecting Outsourced Cloud Data Queries With Multi-User Access Control Based on SGX,""Y. Chen"; Q. Zheng; Z. Yan;" D. Liu"",""School of Computer Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China"; School of Computer Science and Engineering, Xi'an Jiaotong University, Xi'an, Shaanxi, China; State Key Lab on Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, Shaanxi, China;" State Key Lab on Integrated Services Networks, School of Cyber Engineering, Xidian University, Xi'an, Shaanxi, China"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Sep 2020"",""2021"",""32"",""2"",""485"",""499"",""Due to the concern on cloud security, digital encryption is applied before outsourcing data to the cloud for utilization. This introduces a challenge about how to efficiently perform queries over ciphertexts. Crypto-based solutions currently suffer from limited operation support, high computational complexity, weak generality, and poor verifiability. An alternative method that utilizes hardware-assisted Trusted Execution Environment (TEE), i.e., Intel SGX, has emerged to offer high computational efficiency, generality and flexibility. However, SGX-based solutions lack support on multi-user query control and suffer from security compromises caused by untrustworthy TEE function invocation, e.g., key revocation failure, incorrect query results, and sensitive information leakage. In this article, we leverage SGX and propose a secure and efficient SQL-style query framework named QShield. Notably, we propose a novel lightweight secret sharing scheme in QShield to enable multi-user query control"; it effectively circumvents key revocation and avoids cumbersome remote attestation for authentication. We further embed a trust-proof mechanism into QShield to guarantee the trustworthiness of TEE function invocation;" it ensures the correctness of query results and alleviates side-channel attacks. Through formal security analysis, proof-of-concept implementation and performance evaluation, we show that QShield can securely query over outsourced data with high efficiency and scalable multi-user support."",""1558-2183"","""",""10.1109/TPDS.2020.3024880"",""National Key Research and Development Program of China(grant numbers:2018YFB1004500,2016YFB1000903)"; National Natural Science Foundation of China(grant numbers:61721002); Innovation Research Team of Ministry of Education(grant numbers:IRT_17R86); National Natural Science Foundation of China(grant numbers:61502379,61532015,61672410,61672420); China Knowledge Center for Engineering Science and Technology; Academy of Finland(grant numbers:308087,314203);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9200772"",""Secure query";outsourced data;secure hardware;Intel SGX;cloud computing;"multi-user query control"",""Encryption";Indexes;Cloud computing;Data models;"Authentication"","""",""14"","""",""42"",""IEEE"",""18 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Realizing Best Checkpointing Control in Computing Systems,""P. Sigdel"; X. Yuan;" N. -F. Tzeng"",""School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA"; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA;" School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""31 Aug 2020"",""2021"",""32"",""2"",""315"",""329"",""This article considers best checkpointing control realizable in real-world systems, whose mean time between failures (MTBFs) often fluctuate. The considered control scheme is based on equating aggregate checkpointing overhead over an activity sequence of interest (θ) and the expected rework amount after a failure recovery for best checkpointing, called “CHORE” (i.e., checkpointing overhead and rework equated), where θ starts from execution resumption after failure recovery and ends after restore from the following failure. CHORE lets its inter-checkpoint intervals in θ follow a pre-determined sequence independent of MTBF to aim at performance optimality and is shown analytically to keep overall execution time overhead upper bounded. When failure occurrences are tracked during job execution for real-time MTBF estimation, an enhanced CHORE (dubbed En-CHORE) is obtained to lower checkpointing overhead by skipping certain checkpoints at the beginning of each θ before taking checkpoints with the most desirable inter-checkpoint intervals determined on-the-fly for best checkpointing control. En-CHORE can outperform optimal checkpointing (which follows a fixed inter-checkpoint interval optimized for one constant global MTBF known a prior) both under synthetic random failures with local MTBF fluctuating markedly and under real failure traces of 22 real HPC systems (whose failure rates actually fluctuate over their trace time spans)."",""1558-2183"","""",""10.1109/TPDS.2020.3015805"",""National Science Foundation(grant numbers:CNS-1527051,III-1652107)"; Louisiana Board of Regents(grant numbers:LEQSF(2018-21)-RD-A-24);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9164989"",""Absorbing Markov chains";checkpointing control;execution time overhead;mean time between failures (MTBFs);optimal checkpointing;"rework after failure recovery"",""Checkpointing";Optimized production technology;Aggregates;Fluctuations;Estimation;Time measurement;"Control systems"","""",""4"","""",""41"",""IEEE"",""11 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Recent Advances of Resource Allocation in Network Function Virtualization,""S. Yang"; F. Li; S. Trajanovski; R. Yahyapour;" X. Fu"",""School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China"; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Microsoft, London, United Kingdom; Gesellschaft für Wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG) and Institute of Computer Science, University of Göttingen, Göttingen, Germany;" Institute of Computer Science, University of Göttingen, Göttingen, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""25 Aug 2020"",""2021"",""32"",""2"",""295"",""314"",""Network Function Virtualization (NFV) has been emerging as an appealing solution that transforms complex network functions from dedicated hardware implementations to software instances running in a virtualized environment. Due to the numerous advantages such as flexibility, efficiency, scalability, short deployment cycles, and service upgrade, NFV has been widely recognized as the next-generation network service provisioning paradigm. In NFV, the requested service is implemented by a sequence of Virtual Network Functions (VNF) that can run on generic servers by leveraging the virtualization technology. These VNFs are pitched with a predefined order through which data flows traverse, and it is also known as the Service Function Chaining (SFC). In this article, we provide an overview of recent advances of resource allocation in NFV. We generalize and analyze four representative resource allocation problems, namely, (1) the VNF Placement and Traffic Routing problem, (2) VNF Placement problem, (3) Traffic Routing problem in NFV, and (4) the VNF Redeployment and Consolidation problem. After that, we study the delay calculation models and VNF protection (availability) models in NFV resource allocation, which are two important Quality of Service (QoS) parameters. Subsequently, we classify and summarize the representative work for solving the generalized problems by considering various QoS parameters (e.g., cost, delay, reliability, and energy) and different scenarios (e.g., edge cloud, online provisioning, and distributed provisioning). Finally, we conclude our article with a short discussion on the state-of-the-art and emerging topics in the related fields, and highlight areas where we expect high potential for future research."",""1558-2183"","""",""10.1109/TPDS.2020.3017001"",""National Natural Science Foundation of China(grant numbers:61802018)"; Beijing Institute of Technology Research Fund Program for Young Scholars; National Natural Science Foundation of China(grant numbers:61772077); Natural Science Foundation of Beijing Municipality(grant numbers:4192051); EU H2020 RISE COSAFE project(grant numbers:824019);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9169857"",""Network function virtualization";service function chaining;resource allocation;QoS;placement;"routing"",""Resource management";Quality of service;Routing;Delays;Virtualization;Network function virtualization;"Hardware"","""",""89"","""",""142"",""IEEE"",""17 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Reliability and Confidentiality Co-Verification for Parallel Applications in Distributed Systems,""G. Xie"; K. Yang; H. Luo; R. Li;" S. Hu"",""Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"; Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; College of Computer and Control Engineering, Minjiang University, Fujian, China; Key Laboratory for Embedded and Cyber-Physical Systems of Hunan Province, College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China;" School of Electronics and Computer Science, University of Southampton, Southampton, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""2 Feb 2021"",""2021"",""32"",""6"",""1353"",""1368"",""Co-verification of reliability and confidentiality is a necessary process for safety- and security-critical applications. While these two objectives are conflicting, preassignment has emerged as an effective and efficient verification solution. In this article, we propose two preassignment-based co-verification techniques, namely, Blocks-based Vulnerability Preassignment (BVP) and Reversed Blocks-based Time Preassignment (RBTP) for a parallel application in distributed CAN FD systems. BVP can significantly improve reliability under a vulnerability bound, while RBTP can reduce vulnerability over a reliability goal. Real case study with the parallel automotive application and parallelism study with two structures of high-parallelism and low-parallelism applications are demonstrated";" the proposed BVP and RBTP can improve the verification acceptance ratio by 19 and 10 percent compared to the state-of-the-art Average Vulnerability Preassignment (AVP) and Average Time Preassignment (ATP) techniques, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3049780"",""National Natural Science Foundation of China(grant numbers:61972139,61932010,61702172,61672217)"; Electronic Information and Control of Fujian University Engineering Research Center, Minjiang University(grant numbers:MJXY-KF-EIC1902); Fundamental Research Funds for the Central Universities; Hunan University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316956"",""Co-verification";safety;"security"",""Task analysis";Reliability;Security;Safety;Reliability engineering;Process control;"Time factors"","""",""11"","""",""39"",""IEEE"",""8 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"ReliableBox: Secure and Verifiable Cloud Storage With Location-Aware Backup,""T. Jiang"; W. Meng; X. Yuan; L. Wang; J. Ge;" J. Ma"",""State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China"; College of Information Engineering, Northwest A&F University, Yangling, Shaanxi, China; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China; State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China;" State Key Laboratory of Integrated Service Networks, Xidian University, Xi’an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jun 2021"",""2021"",""32"",""12"",""2996"",""3010"",""While the prevalent cloud storage platforms are offering convenient services in support of diverse data-driven applications for clients, various security concerns raise in terms of data confidentiality, availability, and retrievability. Among them, servers' dishonesty on the location-specific data backup becomes a serious concern when the data stands out clients' control, considering the strict regulations imposed by many governments and organizations on data storage location. This article studies location-aware data backup verification for the data stored in clouds and aims to design a secure framework, named as ReliableBox, enabling the clients to verify if their data have been backed up on the remote servers with specific geolocation. In the design of ReliableBox, we leverage the prominent proof-of-storage techniques for data possession proof, and take advantage of multilateration geolocation and Intel SGX for the precise communication delay measurement and trust computing delay measurement, respectively. In ReliableBox, a client first computes integrity tags for the files and then outsources both the files and tags to the cloud storage server. In the later attestation, with the precise network delay and distance measurement from location-known verifiers, the client verifies that the outsourced files are intact and backed-up to hosts at the specific geolocation. With the customized design, ReliableBox can support the security needs in terms of both data integrity and backup location verification for clients, even when there exists potential dishonest cloud service providers who may manipulate the network delays or forge verification proofs. We provide security analysis to show the security property of ReliableBox in terms of data access, confidentiality, and verifications. In the end, we implement the system prototype and deploy it into several prevalent and commercial cloud platforms for performance evaluation. The experimental results demonstrate that ReliableBox is secure in support of data integrity checking and location-aware backup auditing, while it is robust to the data possession and location spoofing attacks."",""1558-2183"","""",""10.1109/TPDS.2021.3080594"",""National Key Research and Development Program of China(grant numbers:2018YFB0804103)"; National Natural Science Foundation of China(grant numbers:61702402,U1736216,61902291); China Postdoctoral Science Foundation(grant numbers:2017M613079); Fundamental Research Funds for the Central Universities(grant numbers:XJS211502,XJS191501); University Innovation Platform(grant numbers:2019921815KYPT009JC011); Guangxi Key Laboratory of Cryptography and Information Security(grant numbers:GCIS201716); Louisiana Board of Regents(grant numbers:LEQSF(2018-21)-RD-A-24);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431668"",""Cloud storage";data possession;data backup;"geolocation verification"",""Geology";Cloud computing;Servers;Reliability engineering;Security;"Data integrity"","""",""5"","""",""60"",""IEEE"",""14 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"RENDA: Resource and Network Aware Data Placement Algorithm for Periodic Workloads in Cloud,""H. K. Thakkar"; P. K. Sahoo;" B. Veeravalli"",""Department of Computer Science and Engineering, SRM University, Andhra Pradesh, India"; Department of Computer Science and Information Engineering, Chang Gung University, Guishan, Taiwan;" Department of Electrical and Computer Engineering, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Jun 2021"",""2021"",""32"",""12"",""2906"",""2920"",""The Hadoop enabled cloud platforms are gradually becoming preferred computational environment to execute scientific big data workloads in a periodic manner. However, it is observed that the default data placement approach of such cloud platforms is not the efficient one and often ends up with significant data transfer overhead leading to degradation of the overall job completion time. In this article, a Resource and Network-aware Data Placement Algorithm (RENDA) is proposed to reduce the non-local executions and thereby reduce the overall job completion time for periodic workloads in the cloud environment. The entire job execution is modeled as a two-stage execution characterized as data distribution and data processing. The RENDA reduces the time of the stages as mentioned above by estimating the heterogeneous performance of the nodes on a real-time basis followed by careful allocation of data in several installments to participating nodes. The experimental results show that the proposed RENDA algorithm consistently outperforms over the recent state-of-the-art alternatives with as much as 28 percent reduction in data transfer overhead leading to 16 percent reduction in average job completion time with 27 percent average speedup on average job execution."",""1558-2183"","""",""10.1109/TPDS.2021.3080582"",""Ministry of Science and Technology, Taiwan(grant numbers:109-2221-E-182-014)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431731"",""MapReduce";cloud computing;data placement;"periodic workloads"",""Cloud computing";Data models;Distributed databases;Data centers;Data transfer;"Switches"","""",""14"","""",""31"",""IEEE"",""14 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Resettable Encoded Vector Clock for Causality Analysis With an Application to Dynamic Race Detection,""T. Pozzetti";" A. D. Kshemkalyani"",""Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA";" Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Nov 2020"",""2021"",""32"",""4"",""772"",""785"",""Causality tracking among events is a fundamental challenge in distributed environments. Much previous work on this subject has focused on designing an efficient and scalable protocol to represent logical time. Several implementations of logical clocks have been proposed, most recently the Encoded Vector Clock (EVC), a protocol to encode Vector Clocks (VC) in scalar numbers through the use of prime numbers, to improve performance and scalability. We propose and formalize the concept of Resettable Encoded Vector Clock (REVC), a new logical clock implementation, which builds on the EVC to tackle its very high growth rate issue. We show how our REVC can be applied in both shared memory systems and message passing systems to achieve a consistent logical clock. We show, through practical examples, the advantage of REVC's growth rate with respect to EVC's growth rate. Finally, we show a practical application of the REVC to the dynamic race detection problem in multi-threaded environments. We compare our tool to the currently existing VC-based tool DJIT+ to show how the REVC can help in achieving higher performance with respect to the Vector Clock."",""1558-2183"","""",""10.1109/TPDS.2020.3032293"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9234035"",""Causality";vector clock;prime numbers;encoding;dynamic race detection;clock reset protocol;"performance"",""Clocks";Protocols;Scalability;Tools;Message passing;Encoding;"Message systems"","""",""4"","""",""38"",""CCBY"",""20 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Retargeting Tensor Accelerators for Epistasis Detection,""R. Nobre"; A. Ilic; S. Santander-Jiménez;" L. Sousa"",""INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisboa, Portugal"; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisboa, Portugal; Department of Computer and Communications Technologies, University of Extremadura, Badajoz, Spain;" INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisboa, Portugal"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Mar 2021"",""2021"",""32"",""9"",""2160"",""2174"",""The substitution of nucleotides at specific positions in the genome of a population, known as single-nucleotide polymorphisms (SNPs), has been correlated with a number of important diseases. Complex conditions such as Alzheimer's disease or Crohn's disease are significantly linked to genetics when the impact of multiple SNPs is considered. SNPs often interact in an epistatic manner, where the joint effect of multiple SNPs may not be simply mapped to a linear additive combination of individual effects. Genome-wide association studies considering epistasis are computationally challenging, especially when performing triplet searches is required. Some contemporary computer architectures support fused XOR and population count as the highest throughput operations as part of tensor operations. This article presents a new approach for efficiently repurposing this capability to accelerate 2-way (pairs) and 3-way (triplets) epistasis detection searches. Experimental evaluation targeting the Turing GPU architecture resulted in previously unattainable levels of performance, with the proposal being able to evaluate up to 108.1 and 54.5 tera unique sets of SNPs per second, scaled to the sample size, in 2-way and 3-way searches, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3060322"",""Fundação para a Ciência e a Tecnologia"; European Regional Development Fund(grant numbers:UIDB/50021/2020,LISBOA-01-0145-FEDER-031901,PTDC/CCI-COM/31901/2017, HiPErBio);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9357942"",""GWAS";two- and three-way epistasis;performance evaluation;"parallel architectures"",""Statistics";Sociology;Tensors;Proposals;Hardware;Throughput;"Diseases"","""",""7"","""",""46"",""CCBY"",""18 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Reversible CSP Computations,""C. Galindo"; N. Nishida; J. Silva;" S. Tamarit"",""Departamento de Sistemas Informáticos y Computadores, Universitat Politècnica de València, Valencia, Spain"; Graduate School of Informatics, Nagoya University, Nagoya, Japan; Departamento de Sistemas Informáticos y Computadores, Universitat Politècnica de València, Valencia, Spain;" PFS Tech, Valencia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Feb 2021"",""2021"",""32"",""6"",""1425"",""1436"",""Reversibility enables a program to be executed both forwards and backwards. This ability allows programmers to backtrack the execution to a previous state. This is essential if the computation is not deterministic because re-running the program forwards may not lead to that state of interest. Reversibility of sequential programs has been well studied and a strong theoretical basis exists. Contrarily, reversibility of concurrent programs is still very young, especially in the practical side. For instance, in the particular case of the Communicating Sequential Processes (CSP) language, reversibility is practically missing. In this article, we present a new technique, including its formal definition and its implementation, to reverse CSP computations. Most of the ideas presented can be directly applied to other concurrent specification languages such as Promela or CCS, but we center the discussion and the implementation on CSP. The technique proposes different forms of reversibility, including strict reversibility and causal-consistent reversibility. On the practical side, we provide an implementation of a system to reverse CSP computations that is able to highlight the source code that is being executed in each forwards/backwards computation step, and that has been optimized to be scalable to real systems."",""1558-2183"","""",""10.1109/TPDS.2021.3051747"",""EU (FEDER)"; Spanish MCI/AEI(grant numbers:TIN2016-76843-C4-1-R,PID2019-104735RB-C41); Generalitat Valenciana(grant numbers:Prometeo/2019/098 (DeepTrust)); JSPS KAKENHI(grant numbers:JP17H01722); TAILOR; EU Horizon 2020 research and innovation programme(grant numbers:GA 952215);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9324973"",""Concurrent programming";tracing;debugging aids;"code inspections and walkthroughs"",""Synchronization";Syntactics;Semantics;History;Standards;Data structures;"Debugging"","""","""","""",""31"",""IEEE"",""14 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Rings for Privacy: An Architecture for Large Scale Privacy-Preserving Data Mining,""M. L. Merani"; D. Croce;" I. Tinnirello"",""Dipartimento di Ingegneria “Enzo Ferrari”, University of Modena and Reggio Emilia, Modena, Italy"; Engineering Dept., University of Palermo, Palermo, Italy;" Engineering Dept., University of Palermo, Palermo, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Jan 2021"",""2021"",""32"",""6"",""1340"",""1352"",""This article proposes a new architecture for privacy-preserving data mining based on Multi Party Computation (MPC) and secure sums. While traditional MPC approaches rely on a small number of aggregation peers replacing a centralized trusted entity, the current study puts forth a distributed solution that involves all data sources in the aggregation process, with the help of a single server for storing intermediate results. A large-scale scenario is examined and the possibility that data become inaccessible during the aggregation process is considered, a possibility that traditional schemes often neglect. Here, it is explicitly examined, as it might be provoked by intermittent network connectivity or sudden user departures. For increasing system reliability, data sources are organized in multiple sets, called rings, which independently work on the aggregation process. Two different protocol schemes are proposed and their failure probability, i.e., the probability that the data mining output cannot guarantee the desired level of accuracy, is analytically modeled. The privacy degree, the communication cost and the computational complexity that the schemes exhibit are also characterized. Finally, the new protocols are applied to some specific use cases, demonstrating their feasibility and attractiveness."",""1558-2183"","""",""10.1109/TPDS.2021.3049286"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9314057"",""Privacy";secret sharing;data mining;secure multi-party computation;"C-means"",""Peer-to-peer computing";Protocols;Servers;Privacy;Distributed databases;Computer architecture;"Data privacy"","""",""4"","""",""30"",""IEEE"",""5 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Rusty: Runtime Interference-Aware Predictive Monitoring for Modern Multi-Tenant Systems,""D. Masouros"; S. Xydis;" D. Soudris"",""Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece"; Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece;" Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Aug 2020"",""2021"",""32"",""1"",""184"",""198"",""Modern micro-service and container-based cloud-native applications have leveraged multi-tenancy as a first class system design concern. The increasing number of co-located services/workloads into server facilities stresses resource availability and system capability in an unconventional and unpredictable manner. To efficiently manage resources in such dynamic environments, run-time observability and forecasting are required to capture workload sensitivities under differing interference effects, according to applied co-location scenarios. While several research efforts have emerged on interference-aware performance modelling, they are usually applied at a very coarse-grained manner e.g., estimating the overall performance degradation of an application, thus failing to effectively quantify, predict or provide educated insights on the impact of continuous runtime interference on per-resource allocations. In this paper, we present Rusty, a predictive monitoring system that leverages the power of Long Short-Term Memory networks to enable fast and accurate runtime forecasting of key performance metrics and resource stresses of cloud-native applications under interference. We evaluate Rusty under a diverse set of interference scenarios for a plethora of representative cloud workloads, showing that Rusty i) achieves extremely high prediction accuracy, average R2 value of 0.98, ii) enables very deep prediction horizons retaining high accuracy, e.g., R2 of around 0.99 for a horizon of 1 sec ahead and around 0.94 for an horizon of 5 sec ahead, while iii) satisfying, at the same time, the strict latency constraints required to make Rusty practical for continuous predictive monitoring at runtime."",""1558-2183"","""",""10.1109/TPDS.2020.3013948"",""European Union's Horizon 2020 Research and Innovation programme(grant numbers:825061)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9158547"",""predictive monitoring";system predictability;LSTM networks;interference aware;"multi-tenant systems"",""Monitoring";Interference;Runtime;Resource management;Measurement;Degradation;"Servers"","""",""23"","""",""88"",""IEEE"",""4 Aug 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Scalable Energy Games Solvers on GPUs,""A. Formisano"; R. Gentilini;" F. Vella"",""Università di Udine, Udine, Italy"; Università di Perugia, Perugia, Italy;" Free University of Bozen, Bolzano, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2021"",""2021"",""32"",""12"",""2970"",""2982"",""Modeling the consumption of limited resources, e.g., time or energy, plays a central role on the design of reactive systems such as embedded controllers. To this aim, quantitative objectives are defined on game arenas that can be easily modeled as weighted graphs. Instances of these games, called energy games, can be solved in ${\mathcal {O}(\vert {E}\vert {\cdot }\vert {V}\vert {\cdot }W)}$   O ( | E | · | V | · W )    where $W$  W   is the maximum weight. Recent work has demonstrated that sequential implementations hardly solve practical instances due to their size and the number of interactions required to converge to a solution. Recent work has demonstrated that sequential implementations hardly solve practical instances. Furthermore, emerging approaches, that have investigated the parallelism of CPUs multi-core and GPU for solving the initial credit problem for energy games, still perform poorly due to the non-trivial characteristics of these graphs. In this article we first describe a revised version of the algorithm on multi-core CPU that obtains a faster convergence time on real-world graphs with up to 30x against the serial implementation by showing good scalability overall. Second, we provide a new GPU-based parallel implementation based on warp-level primitives that allows to reduce the time-to-solution on several instances with up to 3.6x of speed-up against traditional parallel vertex-based approaches. We also discuss a methodology to build synthetic energy games to validate the scalability of parallel algorithms on two totally different settings."",""1558-2183"","""",""10.1109/TPDS.2021.3080925"",""Libera Universit di Bolzano(grant numbers:CRC2019-IN2091)"; Istituto Nazionale di Alta Matematica Francesco Severi(grant numbers:Project INdAM-GNCS-2019);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432723"",""Graph analytics";GPU computing;formal verification;"energy games"",""Formal verification";Robots;Energy states;Parallel architectures;Legged locomotion;Game theory;"Economics"","""",""4"","""",""29"",""IEEE"",""17 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"SEIZE: Runtime Inspection for Parallel Dataflow Systems,""Y. Li"; M. Interlandi; F. Psallidas; W. Wang;" C. Zaniolo"",""Department of Computer Science, University of California, Los Angeles, CA, USA"; Gray Systems Lab, Microsoft, Redmond, WA, USA; Gray Systems Lab, Microsoft, Redmond, WA, USA; Department of Computer Science, University of California, Los Angeles, CA, USA;" Department of Computer Science, University of California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Nov 2020"",""2021"",""32"",""4"",""842"",""854"",""Many Data-Intensive Scalable Computing (DISC) Systems provide easy-to-use functional APIs, and efficient scheduling and execution strategies allowing users to build concise data-parallel programs. In these systems, data transformations are concealed by exposed APIs, and intermediate execution states are masked under dataflow transitions. Consequently, many crucial features and optimizations (e.g., debugging, data provenance, runtime skew detection), which require runtime datafow states, are not well-supported. Inspired by our experience in implementing features and optimizations over DISC systems, we present SEIZE, a unified framework that enables dataflow inspection-wiretapping the data-path with listening logic-in MapReduce-style programming model. We generalize our lessons learned by providing a set of primitives defining dataflow inspection, orchestration options for different inspection granularities, and operator decomposition and dataflow punctuation strategy for dataflow intervention. We demonstrate the generality and flexibility of the approach by deploying SEIZE in both Apache Spark and Apache Flink, and by implementing a prototype runtime query optimizer for Spark. Our experiments show that, the overhead introduced by the inspection logic is most of the time negligible (less than 5 percent in Spark and 10 percent in Flink)."",""1558-2183"","""",""10.1109/TPDS.2020.3035170"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9246701"",""Parallel dataflow";runtime inspection;"user desired moments"",""Inspection";Sparks;Runtime;Programming;Data models;Task analysis;"Cluster computing"","""",""1"","""",""53"",""IEEE"",""2 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Self-Balancing Federated Learning With Global Imbalanced Data in Mobile Systems,""M. Duan"; D. Liu; X. Chen; R. Liu; Y. Tan;" L. Liang"",""Key Laboratory of Dependable Service Computing in Cyber Physical Society, the Ministry of Education, Chongqing University, Chongqing, China"; Key Laboratory of Dependable Service Computing in Cyber Physical Society, the Ministry of Education, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, the Ministry of Education, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, the Ministry of Education, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, the Ministry of Education, Chongqing University, Chongqing, China;" School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Aug 2020"",""2021"",""32"",""1"",""59"",""71"",""Federated learning (FL) is a distributed deep learning method that enables multiple participants, such as mobile and IoT devices, to contribute a neural network while their private training data remains in local devices. This distributed approach is promising in the mobile systems where have a large corpus of decentralized data and require high privacy. However, unlike the common datasets, the data distribution of the mobile systems is imbalanced which will increase the bias of model. In this article, we demonstrate that the imbalanced distributed training data will cause an accuracy degradation of FL applications. To counter this problem, we build a self-balancing FL framework named Astraea, which alleviates the imbalances by 1) Z-score-based data augmentation, and 2) Mediator-based multi-client rescheduling. The proposed framework relieves global imbalance by adaptive data augmentation and downsampling, and for averaging the local imbalance, it creates the mediator to reschedule the training of clients based on Kullback-Leibler divergence (KLD) of their data distribution. Compared with FedAvg, the vanilla FL algorithm, Astraea shows +4.39 and +6.51 percent improvement of top-1 accuracy on the imbalanced EMNIST and imbalanced CINIC-10 datasets, respectively. Meanwhile, the communication traffic of Astraea is reduced by 75 percent compared to FedAvg."",""1558-2183"","""",""10.1109/TPDS.2020.3009406"",""National Natural Science Foundation of China(grant numbers:61672116,61601067,61802038,61672115)"; Chongqing High-Tech Research Key Program(grant numbers:cstc2019jscx-mbdx0063); Fundamental Research Funds for the Central Universities(grant numbers:0214005207005,2019CDJGFJSJ001); Chongqing Youth Talent Support Program; China Postdoctoral Science Foundation(grant numbers:2017M620412);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9141436"",""Federated learning";distributed machine learning;"neural networks"",""Distributed databases";Training;Machine learning;Mobile handsets;Data models;Servers;"Neural networks"","""",""146"","""",""49"",""IEEE"",""15 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Self-Stabilizing Population Protocols With Global Knowledge,""Y. Sudo"; M. Shibata; J. Nakamura; Y. Kim;" T. Masuzawa"",""Hosei University, Tokyo, Japan"; Kyushu Institute of Technology, Kitakyushu, Japan; Toyohashi University of Technology, Toyohashi, Japan; Nagoya Institute of Technology, Nagoya, Japan;" Osaka University, Osaka, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Jun 2021"",""2021"",""32"",""12"",""3011"",""3023"",""In the population protocol model, many problems cannot be solved in a self-stabilizing manner. However, global knowledge, such as the number of nodes in a network, sometimes enables the design of a self-stabilizing protocol for such problems. For example, it is known that we can solve the self-stabilizing leader election in complete graphs if and only if every node knows the exact number of nodes. In this article, we investigate the effect of global knowledge on the possibility of self-stabilizing population protocols in arbitrary graphs. Specifically, we clarify the solvability of the leader election problem, the ranking problem, the degree recognition problem, and the neighbor recognition problem by self-stabilizing population protocols with knowledge of the number of nodes and/or the number of edges in a network."",""1558-2183"","""",""10.1109/TPDS.2021.3076769"",""JSPS KAKENHI(grant numbers:17K19977,18K18000,18K18029,18K18031,19H04085,20H04140,20KK0232)"; JST SICORP(grant numbers:JPMJSC1606);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9420769"",""Population protocols";leader election;"self-stabilization"",""Social factors";Protocols;Statistics;Globalization;"Network systems"","""",""1"","""",""27"",""CCBY"",""3 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"SGD$\_$_Tucker: A Novel Stochastic Optimization Strategy for Parallel Sparse Tucker Decomposition,""H. Li"; Z. Li; K. Li; J. S. Rellermeyer; L. Chen;" K. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; TU Delft, Delft, Netherlands; TU Delft, Delft, Netherlands;" College of Computer Science and Electronic Engineering, Hunan University, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1828"",""1841"",""Sparse Tucker Decomposition (STD) algorithms learn a core tensor and a group of factor matrices to obtain an optimal low-rank representation feature for the High-Order, High-Dimension, and Sparse Tensor (HOHDST). However, existing STD algorithms face the problem of intermediate variables explosion which results from the fact that the formation of those variables, i.e., matrices Khatri-Rao product, Kronecker product, and matrix-matrix multiplication, follows the whole elements in sparse tensor. The above problems prevent deep fusion of efficient computation and big data platforms. To overcome the bottleneck, a novel stochastic optimization strategy (SGD Tucker) is proposed for STD which can automatically divide the high-dimension intermediate variables into small batches of intermediate matrices. Specifically, SGD Tucker only follows the randomly selected small samples rather than the whole elements, while maintaining the overall accuracy and convergence rate. In practice, SGD Tucker features the two distinct advancements over the state of the art. First, SGD Tucker can prune the communication overhead for the core tensor in distributed settings. Second, the low data-dependence of SGD Tucker enables fine-grained parallelization, which makes SGD Tucker obtaining lower computational overheads with the same accuracy. Experimental results show that SGD Tucker runs at least 2X faster than the state of the art."",""1558-2183"","""",""10.1109/TPDS.2020.3047460"",""Swiss National Science Foundation(grant numbers:407540_167266)"; China Scholarship Council(grant numbers:CSC201906130109); National Natural Science Foundation of China(grant numbers:61751204); National Natural Science Foundation of China(grant numbers:61625202); National Natural Science Foundation of China(grant numbers:61860206011);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9309187"",""High-order, high-dimension and sparse tensor";low-rank representation learning;machine learning algorithm;sparse tucker decomposition;stochastic optimization;"parallel strategy"",""Tensors";Sparse matrices;Optimization;Stochastic processes;Matrix decomposition;Indexes;"Data models"","""",""3"","""",""55"",""IEEE"",""25 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Silhouette: Efficient Cloud Configuration Exploration for Large-Scale Analytics,""Y. Chen"; L. Lin; B. Li; Q. Wang;" Q. Zhang"",""College of Electrical Engineering, Zhejiang University, Hangzhou, Zhejiang, China"; School of Computer Science, Wuhan University, Wuhan, Hubei, China; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China;" Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Mar 2021"",""2021"",""32"",""8"",""2049"",""2061"",""Choosing the best cloud configuration for large-scale data analytics jobs deployed in the cloud can substantially improve their performance and reduce costs. However, current cloud providers offer a wide variety of instance types and customized cluster sizes, making it both time-consuming and costly to pinpoint the optimal cloud configuration. This article presents the design, implementation, and evaluation of Silhouette, a cloud configuration selection framework based on performance models for various large-scale analytics jobs with minimal training overhead. The essence of Silhouette is to build performance prediction models with carefully selected small-scale experiments on small subsets of input data to estimate the performance with entire input data on larger cluster sizes. To reduce the training time and cost, Silhouette incorporates new statistical techniques to select those experiments that yield the best possible information for performance prediction. Moreover, we develop a novel model transformer to convert a prediction model built on one instance type to a different instance type with only one extra experiment, which significantly reduces the training overhead. We evaluate Silhouette with an extensive array of large-scale data analytics jobs on Amazon EC2. Our experimental results have shown convincing evidence that Silhouette is effective in optimizing cloud configuration while saving both training time and costs compared with existing solutions."",""1558-2183"","""",""10.1109/TPDS.2021.3058165"",""National Natural Science Foundation of China(grant numbers:61972296)"; Wuhan Advanced Application Project(grant numbers:2019010701011419); National Key Research and Development Program of China(grant numbers:2020AAA0107700); National Natural Science Foundation of China(grant numbers:61822207,U20B2049); Fundamental Research Funds for the Central Universities(grant numbers:2042019kf0210); RGC(grant numbers:CERG 16204418,16203719,R8015); Natural Science Foundation of Guangdong Province(grant numbers:2017A030312008);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9351648"",""Cloud configuration";large-scale data analytics;performance prediction;"training overhead"",""Cloud computing";Predictive models;Computational modeling;Training;Data models;Analytical models;"Runtime"","""",""6"","""",""51"",""IEEE"",""9 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"SmartTuning: Selecting Hyper-Parameters of a ConvNet System for Fast Training and Small Working Memory,""X. Li"; G. Zhang;" W. Zheng"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1690"",""1701"",""It is desirable to deploy a ConvNet system with high inference accuracy, as well as fast training and small inference memory. However, existing approaches to hyper-parameter tuning only focus on high accuracy. Although achieving high accuracy, tuning poorly can significantly increase the performance burden, and thus degrade the overall performance of a ConvNet system. In this article, we propose SmartTuning, an approach to identifying the hyper-parameters of a ConvNet system for high training speed and small working memory, with the restriction of high inference accuracy. The key idea of SmartTuning is to build a new performance model for a ConvNet system, and to integrate Bayesian Optimization to learn the relationship between the overall performance and the hyper-parameters of a ConvNet system. In this way, SmartTuning can balance inference accuracy, training speed and inference memory usage during the tuning process, and thus maximizes the overall performance of a ConvNet system. Our experiments show that SmartTuning can stably identify the hyper-parameter sets that offer very close accuracy with faster training speed (i.e., 7×-11× over MNIST and 2×-3× over CIFAR-10) and much less inference memory usage (i.e., 17×-23× over MNIST and 4×-9× over CIFAR-10), compared with existing tuning approaches."",""1558-2183"","""",""10.1109/TPDS.2020.3040723"",""National key R&D Program of China(grant numbers:2018YFB0203902)"; National Natural Science Foundation of China(grant numbers:61672315);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9271899"",""Convolutional neural network";high performance computing;hyper-parameter tuning;training speed;"working memory"",""Tuning";Training;Bayes methods;Optimization;Neural networks;Performance evaluation;"Memory management"","""",""2"","""",""60"",""IEEE"",""25 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Sova: A Software-Defined Autonomic Framework for Virtual Network Allocations,""Z. Ye"; Y. Wang; S. He; C. Xu;" X. -H. Sun"",""Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China"; Chinese Academy of Sciences, Shenzhen Institutes of Advanced Technology, Shenzhen, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; State Key Laboratory of IoT for Smart City, Faculty of Science and Technology, University of Macau, Macau, China;" Department of Computer Science, Illinois Institute of Technology, Chicago, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2020"",""2021"",""32"",""1"",""116"",""130"",""With the rise of network virtualization, the workloads deployed on data center are dramatically changed to support diverse service-oriented applications, which are in general characterized by the time-bounded service response that in turn puts great burden on the data-center networks. Although there have been numerous techniques proposed to optimize the virtual network allocation in data center, the research on coordinating them in a flexible and effective way to autonomically adapt to the workloads for service time reduction is few and far between. To address these issues, in this article we propose Sova, an autonomic framework that can combine the virtual dynamic SR-IOV (DSR-IOV) and the virtual machine live migration (VLM) for virtual network allocations in data centers. DSR-IOV is a SR-IOV-based virtual network allocation technology, but its operation scope is very limited to a single physical machine, which could lead to the local hotspot issue in the course of computation and communication, likely increasing the service response time. In contrast, VLM is an often-used virtualization technique to optimize global network traffic via VM migration. Sova exploits the software-defined approach to combine these two technologies with reducing the service response time as a goal. To realize the autonomic coordination, the architecture of Sova is designed based on the MAPE-K loop in autonomic computing. With this design, Sova can adaptively optimize the network allocation between different services by coordinating DSR-IOV and VLM in autonomic way, depending on the resource usages of physical servers and the network characteristics of VMs. To this end, Sova needs to monitor the network traffic as well as the workload characteristics in the cluster, whereby the network properties are derived on the fly to direct the coordination between these two technologies. Our experiments show that Sova can exploit the advantages of both techniques to match and even beat the better performance of each individual technology by adapting to the VM workload changes."",""1558-2183"","""",""10.1109/TPDS.2020.3012146"",""National Key R&D Program of China(grant numbers:2018YFB1004804)"; National Natural Science Foundation of China(grant numbers:61672513); Science and Technology Planning Project of Guangdong Province(grant numbers:2019B010137002); Shenzhen Oversea High-Caliber Personnel Innovation Funds(grant numbers:KQCX20170331161854); Shenzhen Basic Research Program(grant numbers:JCYJ20170818153016513);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9151358"",""Virtual machine migration";dynamic SR-IOV;software-defined approach;autonomic computing;MAPE-K loop;"network allocation"",""Resource management";Bandwidth;Data centers;Servers;Virtualization;Time factors;"Quality of service"","""",""4"","""",""54"",""IEEE"",""28 Jul 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Spartan: A Sparsity-Adaptive Framework to Accelerate Deep Neural Network Training on GPUs,""S. Dong"; Y. Sun; N. B. Agostini; E. Karimi; D. Lowell; J. Zhou; J. Cano; J. L. Abellán;" D. Kaeli"",""Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA"; William & Mary, Sadler Center, Williamsburg, VA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA; Advanced Micro Devices Inc., Santa Clara, CA, USA; Advanced Micro Devices Inc., Santa Clara, CA, USA; University of Glasgow, Glasgow, U.K.; UCAM Universidad Católica de Murcia, Murcia, Spain;" Department of Electrical and Computer Engineering, Northeastern University, Boston, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Apr 2021"",""2021"",""32"",""10"",""2448"",""2463"",""Deep Neural Networks (DNNs) have emerged as an important class of machine learning algorithms, providing accurate solutions to a broad range of applications. Sparsity in activation maps in DNN training presents an opportunity to reduce computations. However, exploiting activation sparsity presents two major challenges: i) profiling activation sparsity during training comes with significant overhead due to computing the degree of sparsity and the data movement";" ii) the dynamic nature of activation maps requires dynamic dense-to-sparse conversion during training, leading to significant overhead. In this article, we present Spartan, a lightweight hardware/software framework to accelerate DNN training on a GPU. Spartan provides a cost-effective and programmer-transparent microarchitectural solution to exploit activation sparsity detected during training. Spartan provides an efficient sparsity monitor, a tile-based sparse GEMM algorithm, and a novel compaction engine designed for GPU workloads. Spartan can reduce sparsity profiling overhead by 52.5× on average. For the most compute-intensive layers, i.e., convolutional layers, we can speedup AlexNet by 3.4×, VGGNet-16 by 2.14×, and ResNet-18 by 2.02×, when training on the ImageNet dataset."",""1558-2183"","""",""10.1109/TPDS.2021.3067825"",""AMD";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9382871"",""DNN";sparsity;"GPU"",""Training";Monitoring;Sparse matrices;Graphics processing units;Acceleration;Market research;"Engines"","""",""3"","""",""53"",""IEEE"",""22 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Structured Allocation-Based Consistent Hashing With Improved Balancing for Cloud Infrastructure,""Y. Nakatani"",""NTT Network Service Systems Laboratories, Tokyo, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Mar 2021"",""2021"",""32"",""9"",""2248"",""2261"",""Consistent hashing has played an indispensable role in cloud infrastructure, although its load balancing performance is not necessarily perfect. Consistent hashing has long remained the most widely used method despite many methods being proposed to improve load balancing because these methods trade off load balancing against consistency, memory usage, lookup performance, and/or fault-tolerance. This article presents Structured Allocation-based Consistent Hashing (SACH), a cloud-optimized consistent hashing algorithm that overcomes the trade-offs by taking advantage of the characteristics of cloud environments: scaling management and auto-healing. Since scaling can be distinguished from failures, SACH applies two different algorithms to update hashing functions: a fast-update algorithm for unmanaged backend failures to satisfy fault-tolerance with quick response and a slow-update algorithm for managed scaling. Hashing functions are initialized or slow-updated considering the characteristics of the fast-update algorithm to satisfy load balancing and the other properties as far as the number of failed backends is kept small by auto-healing. The experimental results show that SACH outperforms existing algorithms in each aspect. SACH will improve the load balancing of cloud infrastructure components, where the trade-offs have prevented the renewal of hashing functions."",""1558-2183"","""",""10.1109/TPDS.2021.3058963"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9354010"",""Consistent hashing";load balancing;network load balancer;distributed key-value store;"cloud infrastructure"",""Load management";Resource management;Fault tolerant systems;Fault tolerance;Cloud computing;Memory management;"Hash functions"","""",""6"","""",""27"",""CCBY"",""12 Feb 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Subutai: Speeding Up Legacy Parallel Applications Through Data Synchronization,""R. Cataldo"; R. Fernandes; K. J. M. Martin; J. Silveira; G. Sanchez; J. Sepúlveda; C. Marcon;" J. -P. Diguet"",""School of Technology, Pontificia Universidade Catolica do Rio Grande do Sul (PUCRS), Porto Alegre, RS, Brazil"; Faculty of Informatics, Pontificia Universidade Catolica do Rio Grande do Sul, Porto Alegre, RS, Brazil; Lab-STICC, Université Bretagne Sud (UBS), Lorient, France; Department of Computer Science, Universidade Federal do Ceará, Fortaleza, CE, Brazil; School of Technology, Pontificia Universidade Catolica do Rio Grande do Sul (PUCRS), Porto Alegre, RS, Brazil; Lehrstuhl für Sicherheit in der Informationstechnik, Technical University of Munich, München, Germany; School of Technology, Pontificia Universidade Catolica do Rio Grande do Sul (PUCRS), Porto Alegre, RS, Brazil;" Lab-STICC, CNRS / UBS University, Lorient, France"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Dec 2020"",""2021"",""32"",""5"",""1102"",""1116"",""The decrease of the performance gain dictated by Moore's Law boosted the development of manycore architectures to replace single-core architectures. These new architectures must employ parallel applications and distribute its workload over a multitude of cores to reach the desired performance. Parallel applications are harder to develop than sequential ones since the developer must guarantee data integrity using synchronization primitives. While multiple novel solutions have been proposed to speed up parallel applications through handling one type of data synchronization primitive, exceptionally few works support multiple types of synchronization primitives and legacy code. This article proposes Subutai, a hardware/software co-design solution for accelerating multiple synchronization primitives without modifying the application source code. By providing a new user library, while retaining an existing synchronization API, legacy and novel applications can benefit from our solution. Our experimental evaluation, which provides a POSIX Threads implementation, demonstrates Subutai speeds up to 2.71× and 4.61× the execution of single- and multiple-application executions, respectively."",""1558-2183"","""",""10.1109/TPDS.2020.3040066"",""Coordenação de Aperfeiçoamento de Pessoal de Nível Superior";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9268118"",""Legacy parallel applications";PThreads;network-on-chip;"distributed scheduler"",""Synchronization";Software;Libraries;Hardware;Programming;Optimization;"Computational modeling"","""","""","""",""40"",""IEEE"",""24 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Systematically Landing Machine Learning onto Market-Scale Mobile Malware Detection,""L. Gong"; H. Lin; Z. Li; F. Qian; Y. Li; X. Ma;" Y. Liu"",""School of Software and BNRist, Tsinghua University, Beijing, China"; School of Software and BNRist, Tsinghua University, Beijing, China; School of Software and BNRist, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN, USA; School of Software and BNRist, Tsinghua University, Beijing, China; MOE Key Lab for Intelligent Networks and Network Security, School of Electronic and Information Engineering, Xi’an Jiaotong University, Xi’an, China;" Global Innovation Exchange, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1615"",""1628"",""Despite being crucial to today's mobile ecosystem, app markets have meanwhile become a natural, convenient malware delivery channel as they actually “lend credibility” to malicious apps. In the past few years, machine learning (ML) techniques have been widely explored for automated, robust malware detection, but till now we have not seen an ML-based malware detection solution applied at market scales. To systematically understand the real-world challenges, we conduct a collaborative study with T-Market, a popular Android app market that offers us large-scale ground-truth data. Our study illustrates that the key to successfully developing such systems is multifold, including feature selection and encoding, feature engineering and exposure, app analysis speed and efficacy, developer and user engagement, as well as ML model evolution. Failure in any of the above aspects could lead to the “wooden barrel effect” of the whole system. This article presents our judicious design choices and first-hand deployment experiences in building a practical ML-powered malware detection system. It has been operational at T-Market, using a single commodity server to check ~12K apps every day, and has achieved an overall precision of 98.9 percent and recall of 98.1 percent with an average per-app scan time of 0.9 minutes."",""1558-2183"","""",""10.1109/TPDS.2020.3046092"",""National Key R&D Program of China(grant numbers:2018YFB1004700)"; NSF of China(grant numbers:61902211,61972313,61822205,61632020,61632013); NSF of Tianjin(grant numbers:18JCQNJC69900); Postdoctoral Science Fund of China(grant numbers:2019M663725); BNRist;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9301262"",""Machine learning";mobile malware detection;app market;dynamic analysis;"Android emulation"",""Malware";Feature extraction;Encoding;Emulation;Servers;Security;"Metadata"","""",""7"","""",""61"",""IEEE"",""21 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Tardiness Bounds for Sporadic Gang Tasks Under Preemptive Global EDF Scheduling,""Z. Dong"; K. Yang; N. Fisher;" C. Liu"",""Department of Computer Science, Wayne State University, Detroit, MI, USA"; Department of Computer Science, Texas State University, San Marcos, TX, USA; Department of Computer Science, Wayne State University, Detroit, MI, USA;" Department of Computer Science, The University of Texas at Dallas, Dallas, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Jun 2021"",""2021"",""32"",""12"",""2867"",""2879"",""Following the trend of increasing autonomy in cyber-physical systems, parallel embedded architectures have enabled devices to better handle the large streams of data and intensive computation required by such autonomous systems. However, while the explosion of highly-parallel platforms has seen a proportional growth in the number of applications/devices that utilize these platforms, the embedded systems community's understanding of how to build time-predictable, safety-critical systems with parallel platforms has not kept pace. As a well-motivated but challenging parallel scheduling model, gang scheduling requires all parallel threads of each parallel task to simultaneously execute in unison, which is in contrast to traditional, multi-threaded parallel scheduling, where a parallel task may spawn multiple threads, and each thread will be scheduled independently of other threads of the same task. While increasing research efforts on hard real-time (HRT) gang scheduling have recently been seen, the problem of gang scheduling in the context of soft real-time (SRT) systems, where provably bounded deadline tardiness can be tolerated, has hardly been studied yet. In this article, we derive and prove the first tardiness bounds for sporadic gang task systems under preemptive GEDF scheduling. A total utilization bound for SRT-schedulability is required for ensuring such tardiness bounds but it is shown to be tight with respect to the platform capacity and maximum parallelism-induced idleness. Furthermore, we also empirically evaluate the effects of different degrees of task parallelism upon the SRT-schedulability."",""1558-2183"","""",""10.1109/TPDS.2021.3081019"",""National Science Foundation(grant numbers:CNS-2038727,CNS-1750263,CNS-1618185,IIS-1724227)"; Texas State University; Wayne State University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9432744"",""Real-time scheduling";gang tasks;schedulability test;tardiness bound;"Global-Earliest-Deadline"",""Task analysis";Real-time systems;Parallel processing;Message systems;Scheduling;Multicore processing;"Instruction sets"","""",""5"","""",""20"",""IEEE"",""17 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"The Case for Cross-Component Power Coordination on Power Bounded Systems,""R. Ge"; X. Feng; T. Allen;" P. Zou"",""School of Computing, Clemson University, Clemson, SC, USA"; School of Computing, Clemson University, Clemson, SC, USA; School of Computing, Clemson University, Clemson, SC, USA;" School of Computing, Clemson University, Clemson, SC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Apr 2021"",""2021"",""32"",""10"",""2464"",""2476"",""Modern computer systems are increasingly bounded by the available or permissible power at multiple layers from components to systems. To cope with this reality, it is necessary to understand how power bounds impact the design and performance of emergent computer systems. Prior work mainly focuses on power capping and budgeting on individual components without coordinating them to achieve the best possible performance. In this article, we study the problem of power bounded computing and power allocation across computer components on CPU and GPU-accelerated systems. We investigate the dynamics between cross-component power allocation and generalize the performance impacts, and propose lightweight heuristics to maximize performance. We draw multiple insights: (1) for a given application and power bound, there exists a maximum achievable performance which requires coordinated power allocation among components for balanced computation and memory access"; (2) the max performance increases with the total power bound but only in a definite range specific to applications; (3) the dynamics of power allocations has categorical patterns with regard to performance trends and actual power use;" and (4) the categorical patterns can be leveraged to design coordinated power allocations. These findings suggest the promises of cross-component coordination in forthcoming power bounded high performance computing."",""1558-2183"","""",""10.1109/TPDS.2021.3068235"",""National Science Foundation(grant numbers:CCF-1551511,CNS-1551262)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384293"",""Power-bounded computing";cross-component power coordination;power capping;performance analysis;"power management"",""Graphics processing units";Resource management;Central Processing Unit;Memory modules;Memory management;Hardware;"Power demand"","""",""1"","""",""36"",""IEEE"",""23 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"The Case for Strong Scaling in Deep Learning: Training Large 3D CNNs With Hybrid Parallelism,""Y. Oyama"; N. Maruyama; N. Dryden; E. McCarthy; P. Harrington; J. Balewski; S. Matsuoka; P. Nugent;" B. Van Essen"",""Tokyo Institute of Technology, Tokyo, Japan"; Lawrence Livermore National Laboratory, Livermore, CA, USA; ETH Zürich, Zurich, Switzerland; University of Oregon, Eugene, OR, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; RIKEN Center for Computational Science, Hyogo, Japan; Lawrence Berkeley National Laboratory, Berkeley, CA, USA;" Lawrence Livermore National Laboratory, Livermore, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Feb 2021"",""2021"",""32"",""7"",""1641"",""1652"",""We present scalable hybrid-parallel algorithms for training large-scale 3D convolutional neural networks. Deep learning-based emerging scientific workflows often require model training with large, high-dimensional samples, which can make training much more costly and even infeasible due to excessive memory usage. We solve these challenges by extensively applying hybrid parallelism throughout the end-to-end training pipeline, including both computations and I/O. Our hybrid-parallel algorithm extends the standard data parallelism with spatial parallelism, which partitions a single sample in the spatial domain, realizing strong scaling beyond the mini-batch dimension with a larger aggregated memory capacity. We evaluate our proposed training algorithms with two challenging 3D CNNs, CosmoFlow and 3D U-Net. Our comprehensive performance studies show that good weak and strong scaling can be achieved for both networks using up to 2K GPUs. More importantly, we enable training of CosmoFlow with much larger samples than previously possible, realizing an order-of-magnitude improvement in prediction accuracy."",""1558-2183"","""",""10.1109/TPDS.2020.3047974"",""JSPS KAKENHI(grant numbers:JP18J22858)"; Exascale Computing(grant numbers:17-SC-20-SC);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9311438"",""Deep learning";convolutional neural network;model-parallel training;"hybrid-parallel training"",""Training";Three-dimensional displays;Computational modeling;Parallel processing;Solid modeling;Memory management;"Image segmentation"","""",""11"","""",""64"",""IEEE"",""30 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"The Deep Learning Compiler: A Comprehensive Survey,""M. Li"; Y. Liu; X. Liu; Q. Sun; X. You; H. Yang; Z. Luan; L. Gan; G. Yang;" D. Qian"",""School of Computer Science and Engineering, Beihang University, Beijing, China"; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" School of Computer Science and Engineering, Beihang University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Oct 2020"",""2021"",""32"",""3"",""708"",""727"",""The difficulty of deploying various deep learning (DL) models on diverse DL hardware has boosted the research and development of DL compilers in the community. Several DL compilers have been proposed from both industry and academia such as Tensorflow XLA and TVM. Similarly, the DL compilers take the DL models described in different DL frameworks as input, and then generate optimized codes for diverse DL hardware as output. However, none of the existing survey has analyzed the unique design architecture of the DL compilers comprehensively. In this article, we perform a comprehensive survey of existing DL compilers by dissecting the commonly adopted design in details, with emphasis on the DL oriented multi-level IRs, and frontend/backend optimizations. We present detailed analysis on the design of multi-level IRs and illustrate the commonly adopted optimization techniques. Finally, several insights are highlighted as the potential research directions of DL compiler. This is the first survey article focusing on the design architecture of DL compilers, which we hope can pave the road for future research towards DL compiler."",""1558-2183"","""",""10.1109/TPDS.2020.3030548"",""National Key R&D Program of China(grant numbers:2020YFB150001)"; National Natural Science Foundation of China(grant numbers:62072018,61502019,61732002); SenseTime Research Fund for Young Scholars;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9222299"",""Neural networks";deep learning;compiler;intermediate representation;"optimization"",""Optimization";Hardware;Computational modeling;Libraries;Computer architecture;Deep learning;"Integrated circuit modeling"","""",""63"","""",""75"",""IEEE"",""13 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Thermal Prediction for Efficient Energy Management of Clouds Using Machine Learning,""S. Ilager"; K. Ramamohanarao;" R. Buyya"",""Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia"; Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia;" Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2020"",""2021"",""32"",""5"",""1044"",""1056"",""Thermal management in the hyper-scale cloud data centers is a critical problem. Increased host temperature creates hotspots which significantly increases cooling cost and affects reliability. Accurate prediction of host temperature is crucial for managing the resources effectively. Temperature estimation is a non-trivial problem due to thermal variations in the data center. Existing solutions for temperature estimation are inefficient due to their computational complexity and lack of accurate prediction. However, data-driven machine learning methods for temperature prediction is a promising approach. In this regard, we collect and study data from a private cloud and show the presence of thermal variations. We investigate several machine learning models to accurately predict the host temperature. Specifically, we propose a gradient boosting machine learning model for temperature prediction. The experiment results show that our model accurately predicts the temperature with the average RMSE value of 0.05 or an average prediction error of 2.38 °C, which is 6 °C less as compared to an existing theoretical model. In addition, we propose a dynamic scheduling algorithm to minimize the peak temperature of hosts. The results show that our algorithm reduces the peak temperature by 6.5 °C and consumes 34.5 percent less energy as compared to the baseline algorithm."",""1558-2183"","""",""10.1109/TPDS.2020.3040800"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9272657"",""Cloud computing";machine learning;energy efficiency in a data center;datacenter cooling;"hotspots"",""Temperature distribution";Data centers;Predictive models;Cloud computing;Data models;Temperature sensors;"Cooling"","""",""33"","""",""42"",""IEEE"",""26 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Timestamped State Sharing for Stream Analytics,""Y. Zhao"; Z. Liu; Y. Wu; G. Jiang; J. Cheng; K. Liu;" X. Yan"",""Chinese University of Hong Kong, Hong Kong"; Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong; Chinese University of Hong Kong, Hong Kong;" Chinese University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2021"",""2021"",""32"",""11"",""2691"",""2704"",""State access in existing distributed stream processing systems is restricted locally within each operator. However, in advanced stream analytics such as online learning and dynamic graph analytics, enabling state sharing across different operators makes application development easier and stream processing more efficient. In addition, when stream records are timestamped, proper time semantics should be defined for both state updates and fetches. We propose a new state abstraction to address the limitations of existing systems and develop a distributed stream processing system, Nova, with native support for timestamped state sharing. We validate the expressiveness and efficiency of Nova with extensive experiments."",""1558-2183"","""",""10.1109/TPDS.2021.3073253"",""GRF(grant numbers:14208318)"; HKSAR;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404871"",""State sharing";distributed stream processing;online learning;"dynamic graph analytics"",""Semantics";Pattern matching;Throughput;Sparks;Real-time systems;"Industries"","""",""2"","""",""36"",""IEEE"",""14 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Towards Efficient Distributed Subgraph Enumeration Via Backtracking-Based Framework,""Z. Wang"; W. Hu; G. Chen; C. Yuan; R. Gu;" Y. Huang"",""Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China;" Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Jun 2021"",""2021"",""32"",""12"",""2953"",""2969"",""Finding or monitoring subgraph instances that are isomorphic to a given pattern graph in a data graph is a fundamental query operation in many graph analytic applications, such as network motif mining and fraud detection. Existing distributed methods are inefficient in communication. They have to shuffle partial matching results during the distributed multiway join. The partial matching results may be much larger than the data graph itself. To overcome the drawback, we develop the Batch-BENU framework for distributed subgraph enumeration on static data graphs. Batch-BENU executes a group of local search tasks in parallel. Each task enumerates subgraphs around a vertex in the data graph, guided by a backtracking-based execution plan. To handle large-scale data graphs that may exceed the memory capacity of a single machine, Batch-BENU stores the data graph in a distributed database. Each task queries adjacency sets of the data graph on demand, shuffling the data graph instead of partial matching results. To support incremental subgraph enumeration on dynamic data graphs, we propose the Streaming-BENU framework. Streaming-BENU turns the problem of enumerating incremental matching results into enumerating all matching results of incremental pattern graphs at each time step. We implement Batch-BENU and Streaming-BENU with the local database cache and the load balance optimization to improve their efficiency. Extensive experiments show that Batch-BENU and Streaming-BENU can scale to big graphs and complex pattern graphs. They outperform the state-of-the-art distributed methods by up to one and two orders of magnitude, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3076246"",""National Key Research and Development Program of China(grant numbers:2019YFC1711000)"; National Natural Science Foundation of China(grant numbers:U1811461,62072230); Jiangsu Province Science and Technology Research Program(grant numbers:BE2017155); Collaborative Innovation Center of Novel Software Technology and Industrialization, Jiangsu; Alibaba Innovative Resarch Project; Nanjing University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9417730"",""Backtracking-based framework";distributed graph querying;incremental subgraph matching;subgraph isomorphism;"subgraph matching"",""Distributed databases";Pattern matching;Graph theory;Big Data;Software algorithms;Optimization;"Distributed computing"","""","""","""",""39"",""IEEE"",""28 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Towards Efficient Large-Scale Interprocedural Program Static Analysis on Distributed Data-Parallel Computation,""R. Gu"; Z. Zuo; X. Jiang; H. Yin; Z. Wang; L. Wang; X. Li;" Y. Huang"",""State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China"; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, and Department of Computer Science and Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2020"",""2021"",""32"",""4"",""867"",""883"",""Static program analysis has been widely applied along the whole process of the program development for bug detection, code optimization, testing, etc. Although researchers have made significant work in static program analysis, it is still challenging to perform sophisticated interprocedural analysis on large-scale modern software. The underlying reason is that interprocedural analysis for large-scale modern software is highly computation- and memory-intensive, leading to poor efficiency and scalability. In this article, we introduce an efficient distributed and scalable solution for sophisticated static analysis. Specifically, we propose a data-parallel algorithm and a join-process-filter computation model for the CFL-reachability-based interprocedural analysis. Based on that, an efficient distributed static analysis engine called BigSpa is developed, which is composed of an offline batch static program analysis system and an online incremental static program analysis system. The BigSpa system has high generality and can support all kinds of static analysis tasks that can be expressed as CFL reachability problems. The performance of BigSpa is evaluated on real-world large-scale software datasets. Our experiments show that the offline batch system can exceed an order of magnitude compared with the most advanced analysis tools available on performance, and for incremental analysis with small batch updates on the same data sets, the online analysis system can achieve near real-time response, which is very fast and flexible."",""1558-2183"","""",""10.1109/TPDS.2020.3036190"",""National Key R&D Program of China(grant numbers:2019YFC1711000,2017YFA0700604)"; China NSF(grant numbers:61802168,61932021,61702254,62072230,U1811461); Natural Science Foundation of Jiangsu Province(grant numbers:BK20170651,BK20191247); Fundamental Research Funds for the Central Universities(grant numbers:14380065); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252147"",""Interprocedural static analysis";distributed systems;"data-parallel computation"",""Static analysis";Software;Scalability;Task analysis;Optimization;Big Data;"Computational modeling"","""",""6"","""",""52"",""IEEE"",""9 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Towards Efficient Scheduling of Federated Mobile Devices Under Computational and Statistical Heterogeneity,""C. Wang"; Y. Yang;" P. Zhou"",""Department of Computer Science, Old Dominion University, Norfolk, USA"; Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, USA;" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 Sep 2020"",""2021"",""32"",""2"",""394"",""410"",""Originated from distributed learning, federated learning enables privacy-preserved collaboration on a new abstracted level by sharing the model parameters only. While the current research mainly focuses on optimizing learning algorithms and minimizing communication overhead left by distributed learning, there is still a considerable gap when it comes to the real implementation on mobile devices. In this article, we start with an empirical experiment to demonstrate computation heterogeneity is a more pronounced bottleneck than communication on the current generation of battery-powered mobile devices, and the existing methods are haunted by mobile stragglers. Further, non-identically distributed data across the mobile users makes the selection of participants critical to the accuracy and convergence. To tackle the computational and statistical heterogeneity, we utilize data as a tuning knob and propose two efficient polynomial-time algorithms to schedule different workloads on various mobile devices, when data is identically or non-identically distributed. For identically distributed data, we combine partitioning and linear bottleneck assignment to achieve near-optimal training time without accuracy loss. For non-identically distributed data, we convert it into an average cost minimization problem and propose a greedy algorithm to find a reasonable balance between computation time and accuracy. We also establish an offline profiler to quantify the runtime behavior of different devices, which serves as the input to the scheduling algorithms. We conduct extensive experiments on a mobile testbed with two datasets and up to 20 devices. Compared with the common benchmarks, the proposed algorithms achieve 2-100× speedup epoch-wise, 2–7 percent accuracy gain and boost the convergence rate by more than 100 percent on CIFAR10."",""1558-2183"","""",""10.1109/TPDS.2020.3023905"",""National Science Foundation(grant numbers:CCF-1850045,IIS-2007386)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9195793"",""Federated learning";on-device deep learning;scheduling optimization;"non-IID data"",""Mobile handsets";Computational modeling;Training;Task analysis;Distributed databases;Convergence;"Servers"","""",""38"","""",""51"",""IEEE"",""14 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Towards Greening MapReduce Clusters Considering Both Computation Energy and Cooling Energy,""T. R. Toha"; A. S. M. Rizvi; J. Noor; M. A. Adnan;" A. B. M. A. Al Islam"",""Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh"; Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh; Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh;" Department of Computer Science and Engineering, Bangladesh University of Engineering and Technology, Dhaka, Bangladesh"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Dec 2020"",""2021"",""32"",""4"",""931"",""942"",""Increased processing power of MapReduce clusters generally enhances performance and availability at the cost of substantial energy consumption that often incurs higher operational costs (e.g., electricity bills) and negative environmental impacts (e.g., carbon dioxide emissions). There exist a few greening methods for computing clusters in the literature that focus mainly on computational energy consumption leaving cooling energy, which occupies a significant portion of the total energy consumed by the clusters. To this extent, in this article, we propose a machine learning-based approach that reduces the total energy consumption of a MapReduce cluster considering both computational energy and cooling energy. Our approach predicts the number of machines that results in minimum total energy consumption. We perform the prediction through applying different machine learning techniques over year-long data collected from a real setup. We evaluate performance of our approach through both real test-bed experimentation and simulation. Our evaluation reveals that our approach achieves substantial reduction in total energy consumption compared to other state-of-the-art alternatives while experiencing marginal performance degradation in a few cases."",""1558-2183"","""",""10.1109/TPDS.2020.3029724"",""ICT Division, Government of the People";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9217959"",""Green parallel computing";MapReduce;hadoop;"machine learning"",""Cooling";Energy consumption;Time factors;Task analysis;Machine learning algorithms;Cloud computing;"Temperature distribution"","""",""2"","""",""37"",""IEEE"",""8 Oct 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Towards Minimizing Resource Usage With QoS Guarantee in Cloud Gaming,""Y. Li"; C. Zhao; X. Tang; W. Cai; X. Liu; G. Wang;" X. Gong"",""Department of Computer Science, Nankai University, Tianjin, China"; Department of Computer Science, Nankai University, Tianjin, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Department of Computer Science, Nankai University, Tianjin, China; Department of Computer Science, Nankai University, Tianjin, China;" Department of Computer Science, Nankai University, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Sep 2020"",""2021"",""32"",""2"",""426"",""440"",""Cloud gaming has been very popular recently, but providing satisfactory gaming experiences to players at a modest cost is still challenging. Colocating several games onto one server could improve server utilization. However, prior work regarding colocating games either ignores the performance interference between games or uses simple performance model to charaterize it, which may make inefficient game colocation decisions and cause QoS violations. In this article, we address the resource allocation issues for colocating games in cloud gaming. We first propose a novel machine learning-based performance model, which is able to capture the complex relationship among the performance interference, the contention features of colocated games and resource partition. Guided by the performance model, we then propose efficient and effective algorithms for two resource allocation scenarios in cloud gaming. We evaluate the proposed solutions through extensive experiments using a large number of real popular games. The results show that our performance model is able to identify whether a colocated game satisfies QoS requirement within an average error of 5 percent, which significantly outperforms the alternatives. Our resource allocation algorithms are able to increase the resource utilization by up to 60 percent compared to the state-of-the-art solutions."",""1558-2183"","""",""10.1109/TPDS.2020.3024068"",""Science and Technology Development Plan of Tianjin(grant numbers:17JCYBJC15300,18ZXZNGX00140,18ZXZNGX00200)"; National Natural Science Foundation of China(grant numbers:61602266,61702521,61872201,U1833114);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9197611"",""Cloud gaming";game colocation;performance interference;performance prediction;"machine learning"",""Servers";Resource management;Cloud gaming;Quality of service;Interference;"Graphics processing units"","""",""8"","""",""47"",""IEEE"",""15 Sep 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Transformations of High-Level Synthesis Codes for High-Performance Computing,""J. de Fine Licht"; M. Besta; S. Meierhans;" T. Hoefler"",""Department of Computer Science, ETH Zurich, Universita, Zürich, Switzerland"; Department of Computer Science, ETH Zurich, Universita, Zürich, Switzerland; Department of Computer Science, ETH Zurich, Universita, Zürich, Switzerland;" Department of Computer Science, ETH Zurich, Universita, Zürich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Dec 2020"",""2021"",""32"",""5"",""1014"",""1029"",""Spatial computing architectures promise a major stride in performance and energy efficiency over the traditional load/store devices currently employed in large scale computing systems. The adoption of high-level synthesis (HLS) from languages such as C++ and OpenCL has greatly increased programmer productivity when designing for such platforms. While this has enabled a wider audience to target spatial computing architectures, the optimization principles known from traditional software design are no longer sufficient to implement high-performance codes, due to fundamentally distinct aspects of hardware design, such as programming for deep pipelines, distributed memory resources, and scalable routing. To alleviate this, we present a collection of optimizing transformations for HLS, targeting scalable and efficient architectures for high-performance computing (HPC) applications. We systematically identify classes of transformations (pipelining, scalability, and memory), the characteristics of their effect on the HLS code and the resulting hardware (e.g., increasing data reuse or resource consumption), and the objectives that each transformation can target (e.g., resolve interface contention, or increase parallelism). We show how these can be used to efficiently exploit pipelining, on-chip distributed fast memory, and on-chip dataflow, allowing for massively parallel architectures. To quantify the effect of various transformations, we cover the optimization process of a sample set of HPC kernels, provided as open source reference codes. We aim to establish a common toolbox to guide both performance engineers and compiler engineers in tapping into the performance potential offered by spatial computing architectures using HLS."",""1558-2183"","""",""10.1109/TPDS.2020.3039409"",""European Research Council(grant numbers:678880)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9264692"",""Parallel architectures";parallel programming;"high performance computing"",""Pipeline processing";Hardware;Computer architecture;Optimization;Software;Performance evaluation;"Registers"","""",""40"","""",""103"",""IEEE"",""19 Nov 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Transparency and Reproducibility Practice in Large-Scale Computational Science: A Preface to the Special Section,""B. Plale";" S. L. Harrell"",""Intelligent Systems Engineering Department, Indiana University Bloomington, Bloomington, IN, USA";" Texas Advanced Computing Center, University of Texas at Austin, Austin, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2021"",""2021"",""32"",""11"",""2607"",""2608"",""With this special section we bring you a practice and experience effort in transparency and reproducibility for large-scale computational science. A unique section, it consists of a research work plus six critques, each by a student team that reproduced the work. The original research work has been expanded in its science and also in its contribution to open science with a discussion of the student effort. Our letter contemplates implications as well."",""1558-2183"","""",""10.1109/TPDS.2021.3058393"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9431328"",""Open science";computational science;reproducibility;"practice and experience"",""Special issues and sections";Reproducibility of results;"Research and development"","""",""3"","""",""1"",""IEEE"",""14 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"True Load Balancing for Matricized Tensor Times Khatri-Rao Product,""N. Abubaker"; S. Acer;" C. Aykanat"",""Department of Computer Engineering, Bilkent University, Ankara, Turkey"; Sandia National Labs, Albuquerque, NM, USA;" Department of Computer Engineering, Bilkent University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Feb 2021"",""2021"",""32"",""8"",""1974"",""1986"",""MTTKRP is the bottleneck operation in algorithms used to compute the CP tensor decomposition. For sparse tensors, utilizing the compressed sparse fibers (CSF) storage format and the CSF-oriented MTTKRP algorithms is important for both memory and computational efficiency on distributed-memory architectures. Existing intelligent tensor partitioning models assume the computational cost of MTTKRP to be proportional to the total number of nonzeros in the tensor. However, this is not the case for the CSF-oriented MTTKRP on distributed-memory architectures. We outline two deficiencies of nonzero-based intelligent partitioning models when CSF-oriented MTTKRP operations are performed locally: failure to encode processors' computational loads and increase in total computation due to fiber fragmentation. We focus on existing fine-grain hypergraph model and propose a novel vertex weighting scheme that enables this model encode correct computational loads of processors. We also propose to augment the fine-grain model by fiber nets for reducing the increase in total computational load via minimizing fiber fragmentation. In this way, the proposed model encodes minimizing the load of the bottleneck processor. Parallel experiments with real-world sparse tensors on up to 1024 processors prove the validity of the outlined deficiencies and demonstrate the merit of our proposed improvements in terms of parallel runtimes."",""1558-2183"","""",""10.1109/TPDS.2021.3053836"",""Türkiye Bilimsel ve Teknolojik Araştirma Kurumu(grant numbers:EEEAG-116E043)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9334414"",""Load balancing";sparse tensors;MTTKRP;CP decomposition;"fine-grain hypergraph partitioning"",""Tensors";Computational modeling;Program processors;Load modeling;Sparse matrices;Partitioning algorithms;"Computational efficiency"","""",""1"","""",""37"",""IEEE"",""22 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Trust: Triangle Counting Reloaded on GPUs,""S. Pandey"; Z. Wang; S. Zhong; C. Tian; B. Zheng; X. Li; L. Li; A. Hoisie; C. Ding; D. Li;" H. Liu"",""Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Huazhong University of Science and Technology, Wuhan, Hubei, China; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Brookhaven National Laboratory, Upton, NY, USA; Brookhaven National Laboratory, Upton, NY, USA; Department of Computer Science & Engineering, University of Connecticut, Storrs, CT, USA; Department of Electrical Engineering and Computer Science, University of California, Merced, CA, USA;" Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2021"",""2021"",""32"",""11"",""2646"",""2660"",""Triangle counting is a building block for a wide range of graph applications. Traditional wisdom suggests that i) hashing is not suitable for triangle counting, ii) edge-centric triangle counting beats vertex-centric design, and iii) communication-free and workload balanced graph partitioning is a grand challenge for triangle counting. On the contrary, we advocate that i) hashing can help the key operations for scalable triangle counting on Graphics Processing Units (GPUs), i.e., list intersection and graph partitioning, ii) vertex-centric design reduces both hash table construction cost and memory consumption, which is limited on GPUs. In addition, iii) we exploit graph and workload collaborative, and hashing-based 2D partitioning to scale vertex-centric triangle counting over 1000 GPUs with sustained scalability. In this article, we present Trust which performs triangle counting with the hash operation and vertex-centric mechanism at the core. To the best of our knowledge, Trust is the first work that achieves over one trillion Traversed Edges Per Second (TEPS) rate for triangle counting."",""1558-2183"","""",""10.1109/TPDS.2021.3064892"",""National Science Foundation CRII(grant numbers:2000722,2046102)"; Exascale Computing Project(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; National Key Research and Development Program of China(grant numbers:2018YFB1003505); National Natural Science Foundation of China(grant numbers:61772265,61802172,62072228); NSFC-61872176; Leading-edge Technology Program of Jiangsu Natural Science Foundation(grant numbers:BK20202001); National Key Research and Development Program of China(grant numbers:2020YFB1005900); U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); U.S. Department of Energy; Brookhaven Science Associates(grant numbers:DE-SC0012704);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373989"",""GPGPU";triangle counting;graph algorithms;"parallel processing"",""Graphics processing units";Two dimensional displays;Partitioning algorithms;Message systems;Instruction sets;Arrays;"Time complexity"","""",""13"","""",""92"",""IEEE"",""9 Mar 2021"","""","""",""IEEE"",""IEEE Journals"""
"VeriML: Enabling Integrity Assurances and Fair Payments for Machine Learning as a Service,""L. Zhao"; Q. Wang; C. Wang; Q. Li; C. Shen;" B. Feng"",""Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China"; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Institute for Network Sciences and Cyberspace and Beijing National Research Centre for Information Science and Technology (BNRist), Tsinghua University, Beijing, China; MOE Key Laboratory for Intelligent Networks and Network Security, Xi'an, Shaanxi, China;" Khoury College of Computer Sciences, Northeastern University, Boston, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""4 May 2021"",""2021"",""32"",""10"",""2524"",""2540"",""Machine Learning as a Service (MLaaS) allows clients with limited resources to outsource their expensive ML tasks to powerful servers. Despite the huge benefits, current MLaaS solutions still lack strong assurances on: 1) service correctness (i.e., whether the MLaaS works as expected)"; 2) trustworthy accounting (i.e., whether the bill for the MLaaS resource consumption is correctly accounted);" 3) fair payment (i.e., whether a client gets the entire MLaaS result before making the payment). Without these assurances, unfaithful service providers can return improperly-executed ML task results or partially-trained ML models while asking for over-claimed rewards. Moreover, it is hard to argue for wide adoption of MLaaS to both the client and the service provider, especially in the open market without a trusted third party. In this article, we present VeriML, a novel and efficient framework to bring integrity assurances and fair payments to MLaaS. With VeriML, clients can be assured that ML tasks are correctly executed on an untrusted server, and the resource consumption claimed by the service provider equals to the actual workload. We strategically use succinct non-interactive arguments of knowledge (SNARK) on randomly-selected iterations during the ML training phase for efficiency with tunable probabilistic assurance. We also develop multiple ML-specific optimizations to the arithmetic circuit required by SNARK. Our system implements six common algorithms: linear regression, logistic regression, neural network, support vector machine, K-means and decision tree. The experimental results have validated the practical performance of VeriML."",""1558-2183"","""",""10.1109/TPDS.2021.3068195"",""National Key Research and Development Program of China(grant numbers:2020AAA0107700)"; National Natural Science Foundation of China(grant numbers:U20B2049,61822207,61572412,61572278,61822309,61773310,U1736205); Research Grants Council of Hong Kong(grant numbers:11217819,11217620); Innovation and Technology Commission of Hong Kong(grant numbers:ITS/145/19); BNRist(grant numbers:BNR2020RC01013);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9384314"",""Verifiable computation";machine learning;"secure outsourcing"",""Training";Task analysis;Computational modeling;Servers;Machine learning;Predictive models;"Optimization"","""",""14"","""",""64"",""IEEE"",""23 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Virtualization Overhead of Multithreading in X86 State-of-the-Art & Remaining Challenges,""S. Schildermans"; J. Shan; K. Aerts; J. Jackrel;" X. Ding"",""Department of Computer Science, KU Leuven, Diepenbeek, Limburg, Belgium"; Department of Computer Science, Hofstra University, Heampstead, NY, USA; Department of Computer Science, KU Leuven, Diepenbeek, Limburg, Belgium; Department of Computer Science, Hofstra University, Heampstead, NY, USA;" Department of Computer Science, New Jersey Institute of Technology, Newark, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""5 May 2021"",""2021"",""32"",""10"",""2557"",""2570"",""Despite great advancements in hardware-assisted virtualization of the x86 architecture, certain workloads still suffer significant overhead. This article dissects said overhead in the context of multi-threading. We describe the state-of-the-art, pinpoint challenges, and suggest improvements, aiming to provide a valuable reference to developers and users of virtualization systems alike. We study the virtualization overhead of the PARSEC and SPLASH2X multithreaded benchmarks in a variety of scenarios using a state-of-the-art system. Through controlled experiments, source code analysis and literature review, we quantify the virtualization overhead multithreading still induces and link it to its root causes, after which we suggest possible mitigation strategies. Multithreading still induces high virtualization overhead, mainly caused by synchronization, spinning at user level and NUMA management. The overhead is diverse in nature and embodiment as it is a function of many system and workload properties. System-level solutions are feasible, but often imply difficult trade-offs. Systematic workload optimization is a promising alternative."",""1558-2183"","""",""10.1109/TPDS.2021.3064709"",""National Science Foundation(grant numbers:CCF 1617749)"; Flemish FWO(grant numbers:V433819N); KU Leuven(grant numbers:19005);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373922"",""Multi-threading";virtualization;overhead;performance;guidelines;"classification"",""Virtualization";Virtual machine monitors;Hardware;Synchronization;Multithreading;Spinning;"Degradation"","""",""5"","""",""66"",""IEEE"",""9 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Why Dataset Properties Bound the Scalability of Parallel Machine Learning Training Algorithms,""D. Cheng"; S. Li; H. Zhang; F. Xia;" Y. Zhang"",""SKL, Institute of Computing Technology, Chinese Academy of Science, Beijing, China"; Department of Computer Science, ETH, Zurich, ZH, Switzerland; Algorithm Department, Beijing Wisdom Uranium Technology Co., Ltd., Beijing, China; Algorithm Department, Beijing Wisdom Uranium Technology Co., Ltd., Beijing, China;" SKL of Computer Architecture, Institute of Computing Technology, Chinese Academy of Science, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Feb 2021"",""2021"",""32"",""7"",""1702"",""1712"",""As the training dataset size and the model size of machine learning increase rapidly, more computing resources are consumed to speedup the training process. However, the scalability and performance reproducibility of parallel machine learning training, which mainly uses stochastic optimization algorithms, are limited. In this paper, we demonstrate that the sample difference in the dataset plays a prominent role in the scalability of parallel machine learning algorithms. We propose to use statistical properties of dataset to measure sample differences. These properties include the variance of sample features, sample sparsity, sample diversity, and similarity in sampling sequences. We choose four types of parallel training algorithms as our research objects: (1) the asynchronous parallel SGD algorithm (Hogwild! algorithm), (2) the parallel model average SGD algorithm (minibatch SGD algorithm), (3) the decentralization optimization algorithm, and (4) the dual coordinate optimization (DADM algorithm). Our results show that the statistical properties of training datasets determine the scalability upper bound of these parallel training algorithms."",""1558-2183"","""",""10.1109/TPDS.2020.3048836"",""National Natural Science Foundation of China(grant numbers:61972376,61502450,61432018,61521092)"; National Key Research and Development Program of China(grant numbers:2016YFB0200800,2016YFB0200803,2017YFB0202302,2017YFB0202105); State Key Laboratory of Computer Architecture Foundation(grant numbers:CARCH3504); Natural Science Foundation of Beijing Municipality(grant numbers:L182053);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9316159"",""Parallel training algorithms";training dataset;scalability;"stochastic optimization methods"",""Training";Scalability;Machine learning;Machine learning algorithms;Stochastic processes;Task analysis;"Upper bound"","""",""10"","""",""44"",""IEEE"",""6 Jan 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"WindFlow: High-Speed Continuous Stream Processing With Parallel Building Blocks,""G. Mencagli"; M. Torquati; A. Cardaci; A. Fais; L. Rinaldi;" M. Danelutto"",""Department of Computer Science, University of Pisa, Pisa, Italy"; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Information Engineering, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy;" Department of Computer Science, University of Pisa, Pisa, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""26 May 2021"",""2021"",""32"",""11"",""2748"",""2763"",""Nowadays, we are witnessing the diffusion of Stream Processing Systems (SPSs) able to analyze data streams in near realtime. Traditional SPSs like Storm and Flink target distributed clusters and adopt the continuous streaming model, where inputs are processed as soon as they are available while outputs are continuously emitted. Recently, there has been a great focus on SPSs for scale-up machines. Some of them (e.g., BriskStream) still use the continuous model to achieve low latency. Others optimize throughput with batching approaches that are, however, often inadequate to minimize latency for live-streaming applications. Our contribution is to show a novel software engineering approach to design the runtime system of SPSs targeting multicores, with the aim of providing a uniform solution able to optimize throughput and latency. The approach has a formal nature based on the assembly of components called building blocks, whose composition allows optimizations to be easily expressed in a compositional manner. We use this methodology to build a new SPS called WindFlow. Our evaluation showcases the benefits of WindFlow: it provides lower latency than SPSs for continuous streaming, and can be configured to optimize throughput, to perform similarly and even better than batch-based scale-up SPSs."",""1558-2183"","""",""10.1109/TPDS.2021.3073970"",""European H2020 Project TEACHING(grant numbers:871385)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9408386"",""Data stream processing";multicore programming;"parallel computing"",""Runtime";Throughput;Libraries;Multicore processing;Algebra;"Semantics"","""",""12"","""",""35"",""IEEE"",""19 Apr 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"YuenyeungSpTRSV: A Thread-Level and Warp-Level Fusion Synchronization-Free Sparse Triangular Solve,""F. Zhang"; J. Su; W. Liu; B. He; R. Wu; X. Du;" R. Wang"",""Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"; Computer Science Department, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science and Technology, China University of Petroleum, Beijing, China; School of Computing, National University of Singapore, Singapore; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China;" Computer Science Department, Illinois Institute of Technology, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Apr 2021"",""2021"",""32"",""9"",""2321"",""2337"",""Sparse triangular solves (SpTRSVs) are widely used in linear algebra domains, and several GPU-based SpTRSV algorithms have been developed. Synchronization-free SpTRSVs, due to their short preprocessing time and high performance, are currently the most popular SpTRSV algorithms. However, we observe that the performance of those SpTRSV algorithms on different matrices can vary greatly by 845 times. Our further studies show that when the average number of components per level is high and the average number of nonzero elements per row is low, those SpTRSVs exhibit extremely low performance. The reason is that, they use a warp on the GPU to process a row in sparse matrices, and such warp-level designs have severe underutilization of the GPU. To solve this problem, we propose YuenyeungSpTRSV, a thread-level and wrap-level fusion synchronization-free SpTRSV algorithm, which handles the rows with a large number of nonzero elements at warp-level while the rows with a low number of nonzero elements at thread-level. Particularly, YuenyeungSpTRSV has three novel features. First, unlike the previous studies, YuenyeungSpTRSV does not need long preprocessing time to calculate levels. Second, YuenyeungSpTRSV exhibits high performance on matrices that previous SpTRSVs cannot handle efficiently. Third, YuenyeungSpTRSV's optimization does not rely on the specific sparse matrix storage format. Instead, it can achieve very good performance on the most popular sparse matrix storage, compressed sparse row (CSR) format, and thus users do not need to conduct format conversion. We evaluate YuenyeungSpTRSV with 245 matrices from the Florida Sparse Matrix Collection on four GPU platforms, and experiments show that our YuenyeungSpTRSV exhibits 7.14 GFLOPS/s, which is 5.98x speedup over the state-of-the-art synchronization-free SpTRSV algorithm, and 4.83x speedup over the SpTRSV in cuSPARSE."",""1558-2183"","""",""10.1109/TPDS.2021.3066635"",""National Natural Science Foundation of China(grant numbers:61802412)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380961"",""Thread-level";warp-level;synchronization-free;SpTRSV;"GPU"",""Sparse matrices";Graphics processing units;Synchronization;Optimization;Linear algebra;Instruction sets;"Arrays"","""",""3"","""",""41"",""IEEE"",""17 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;