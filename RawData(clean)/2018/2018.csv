"$k$ NN-DP: Handling Data Skewness in $kNN$ Joins Using MapReduce,""X. Zhao"; J. Zhang;" X. Qin"",""Taiyuan University of Science and Technology (TYUST), Taiyuan, Shanxi, China"; Taiyuan University of Science and Technology (TYUST), Taiyuan, Shanxi, China;" Department of Computer Science and Software Engineering, Auburn University, Auburn, AL"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""600"",""613"",""In this study, we discover that the data skewness problem imposes adverse impacts on MapReduce-based parallel kNN-join operations running clusters. We propose a data partitioning approach-called kNN-DP-to alleviate load imbalance incurred by data skewness. The overarching goal of kNN-DP is to equally divide data objects into a large number of partitions, which are processed by mappers and reducers in parallel. At the heart of kNN-DP is a data partitioning module, which dynamically and judiciously partitions data to optimize kNN-join performance by suppressing data skewness on Hadoop clusters. Data partitioning decisions largely depends on data properties (e.g., distributions), the analysis of which is highly expensive for a massive amount of data. To speed up the data-property analysis, we incorporate a sampling technique to profile the data distribution of a small sample dataset representing big datasets. After building a data-partitioning cost model for parallel kNN-joins, we derive the time-complexity upper and lower bounds of parallel kNN-join algorithms. The cost model offers us a guidance to systematically investigate kNN-DP's performance. kNN-DP obtains global nearest neighbors using local nearest neighbors. To improve the accuracy of such an approximation solution, we augment each node's local data by a small amount of redundant data. We develop two kNN-DP-based schemes called LSH+ and z-value+, which seamlessly integrate kNN-DP with the existing LSH and z-value algorithms for kNN-join computing. We implement and evaluate LSH+ and z-value+ on a 24-node Hadoop cluster driven by both synthetic and real-world high-dimensional datasets. The experimental results show that kNN-DP significantly improves the performance of LSH and z-value while offering high extensibility and scalability on Hadoop clusters."",""1558-2183"","""",""10.1109/TPDS.2017.2767596"",""National Natural Science Foundation of China(grant numbers:61572343)";" Shanxi ""1331 Project"" Key Innovative Research Team"; U.S. National Science Foundation(grant numbers:CCF-0845257(CAREER));" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8089758"",""Data skewness";kNN-joins;data partitioning;"MapReduce"",""Time complexity";Distributed databases;Silicon;Algorithm design and analysis;Optimization;"Partitioning algorithms"","""",""24"","""",""32"",""IEEE"",""30 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Code Generator for Energy-Efficient Wavefront Parallelization of Uniform Dependence Computations,""Y. Zou";" S. Rajopadhye"",""Computer Science Department, State University, Fort Collins, CO";" Computer Science Department, State University, Fort Collins, CO"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""1923"",""1936"",""Energy is now critical in all aspects of computing. We address a class of programs that includes so-called “stencil computations.” We address energy optimization of such programs. Since optimizing for speed alone already minimizes energy for most components, we seek to further improve the energy consumption by reducing the total number of off-chip memory accesses without sacrificing execution time. Our strategy uses two-level tiling: we first partition the iteration space into “passes,” each of which is tiled and parallelized. Here, the schedules that map the original program to the multi-pass, parametrically tiled code are specified by polynomials. They are more general than affine multidimensional schedules used by state of the art polyhedral compilers, so generating such codes automatically is an important open problem, and goes beyond the motivation of energy efficiency. We develop a parametric tiled code generator supporting our energy-efficient parallelization strategy. We give a simple linear regression model for energy as a function of performance counters. Our experimental validation on three platforms shows a reduction by about 74 percent (resp. 75 and 67 percent) of the dynamic memory energy consumption on an 8-core Xeon E5-2650 v2 (resp. 6-core Xeon E5-2620 v2 and 6-core Xeon E52602 v3). This leads to a reduction in the total energy of the program by 2 to 14 percent."",""1558-2183"","""",""10.1109/TPDS.2017.2709748"",""US National Science Foundation(grant numbers:CCF-0917319,CNS-1240991)"; AFOSR(grant numbers:FA9550-13-1-0064); DoE(grant numbers:DE-SC0014495);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7935535"",""Energy consumption";automatic parallelization;hierarchical tiling;off-chip memory access;"polyhedral model"",""Two dimensional displays";Standards;Heating systems;Jacobian matrices;Generators;Energy consumption;"Schedules"","""",""2"","""",""39"",""IEEE"",""30 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Dataflow Processor as the Basis of a Tiled Polymorphic Computing Architecture with Fine-Grain Instruction Migration,""D. Hentrich"; E. Oruklu;" J. Saniie"",""Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, IL"; Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, IL;" Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2164"",""2175"",""A dataflow processor is presented that is to be used as the basis of a tiled polymorphic computing architecture. The key contribution is a new scheme that allows a program's instructions to be migrated before and during runtime in a fine-grained manner across the collection of processors. The primary reason to perform this migration is to execute programs faster. Other reasons to perform instruction migration are to prioritize computational resources and to achieve thermal balancing. The act of performing instruction migration across of a collection of processors is logically equivalent to rearranging the computer architecture under the program (i.e., polymorphic computing). Additionally, a new dataflow instruction set which enables the migration is presented. This instruction set is built upon the concept of a single RISC-like dataflow instruction that can atomically execute, make decisions, and independently route results. Furthermore, the novel concept of an operation cell is presented. An operation cell holds a single instruction and its data. It also contains logic to independently determine when an instruction is executed and when to forward data to other operation cells in the collection of processors. In addition, the internal architecture of this processor is presented. This includes the arithmetic logic unit (ALU) that is used to execute the instructions and a series of buses that allow data movement to occur in parallel to instruction execution. Finally, an inter-processor migration strategy is defined."",""1558-2183"","""",""10.1109/TPDS.2018.2819670"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8325523"",""Polymorphic computing";dataflow instruction sets;computer architecture;microprocessors;parallel machines;parallel architectures;reconfigurable architectures;"high performance computing (HPC)"",""Computer architecture";Microprocessors;Fabrics;Instruction sets;Technological innovation;Runtime;"Monitoring"","""",""4"","""",""31"",""IEEE"",""26 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Design Space Exploration Methodology for Parameter Optimization in Multicore Processors,""P. Kansakar";" A. Munir"",""Department of Computer Science, Kansas State University, Manhattan, KS";" Department of Computer Science, Kansas State University, Manhattan, KS"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""2"",""15"",""The need for application-specific design of multicore/manycore processing platforms is evident with computing systems finding use in diverse application domains. In order to tailor multicore/manycore processors for application specific requirements, a multitude of processor design parameters have to be tuned accordingly which involves rigorous and extensive design space exploration over large search spaces. In this paper, we propose an efficient methodology for design space exploration. We evaluate our methodology over two search spaces small and large, using a cycle-accurate simulator (ESESC) and a standard set of PARSEC and SPLASH-2 benchmarks. For the smaller design space, we compare results obtained from our design space exploration methodology with results obtained from fully exhaustive search. The results show that solution quality obtained from our methodology are within 1.35 - 3.69 percent of the results obtained from fully exhaustive search while only exploring 2.74 - 3 percent of the design space. For larger design space, we compare solution quality of different results obtained by varying the number of tunable processor design parameters included in the exhaustive search phase of our methodology. The results show that including more number of tunable parameters in the exhaustive search phase of our methodology greatly improves solution quality."",""1558-2183"","""",""10.1109/TPDS.2017.2745580"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8017478"",""Multicore/manycore processors";processor design parameters;design space exploration;cycle-accurate simulator (ESESC);PARSEC and SPLASH-2 benchmarks;"parameter optimization"",""Space exploration";Algorithm design and analysis;Optimization;Program processors;Measurement;Multicore processing;"Clustering algorithms"","""",""8"","""",""23"",""IEEE"",""29 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Differentiated Caching Mechanism to Enable Primary Storage Deduplication in Clouds,""H. Wu"; C. Wang; Y. Fu; S. Sakr; K. Lu;" L. Zhu"",""University of New South Wales, Sydney, NSW, Australia"; Data61, CSIRO, Eveleigh NSW, Australia; Army Engineering University, Nanjing, China; University of New South Wales, Sydney, NSW, Australia; National University of Defense Technology, Changsha, China;" University of New South Wales, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1202"",""1216"",""Existing primary deduplication techniques either use inline caching to exploit locality in primary workloads or use post-processing deduplication to avoid the negative impact on I/O performance. However, neither of them works well in the cloud servers running multiple services for the following two reasons: First, the temporal locality of duplicate data writes varies among primary storage workloads, which makes it challenging to efficiently allocate the inline cache space and achieve a good deduplication ratio. Second, the post-processing deduplication does not eliminate duplicate I/O operations that write to the same logical block address as it is performed after duplicate blocks have been written. A hybrid deduplication mechanism is promising to deal with these problems. Inline fingerprint caching is essential to achieving efficient hybrid deduplication. In this paper, we present a detailed analysis of the limitations of using existing caching algorithms in primary deduplication in the cloud. We reveal that existing caching algorithms either perform poorly or incur significant memory overhead in fingerprint cache management. To address this, we propose a novel fingerprint caching mechanism that estimates the temporal locality of duplicates in different data streams and prioritizes the cache allocation based on the estimation. We integrate the caching mechanism and build a hybrid deduplication system. Our experimental results show that the proposed mechanism provides significant improvement for both deduplication ratio and overhead reduction."",""1558-2183"","""",""10.1109/TPDS.2018.2790946"",""The National Key Research and Development Program of China(grant numbers:2016YFB0200401)"; program for New Century Excellent Talents in University; National Science Foundation (NSF) China(grant numbers:61402492,61402486,61402518,61379146); laboratory pre-research fund(grant numbers:9140C810106150C81001); HUNAN Province Science Foundation(grant numbers:2017RS3045);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8249847"",""Data deduplication";cache management;ghost cache;primary storage;"cloud services"",""Cloud computing";Estimation;Servers;Algorithm design and analysis;Electronic mail;Resource management;"Containers"","""",""20"","""",""45"",""IEEE"",""8 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Double Auction Mechanism to Bridge Users’ Task Requirements and Providers’ Resources in Two-Sided Cloud Markets,""L. Lu"; J. Yu; Y. Zhu;" M. Li"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, P.R.China"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, P.R.China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, P.R.China;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, P.R.China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""720"",""733"",""Double auction-based pricing model is an efficient pricing model to balance users' and providers' benefits. Existing double auction mechanisms usually require both users and providers to bid with the unit price and the number of VMs. However, in practice users seldom know the exact number of VMs that meets their task requirements, which leads to users' task requirements inconsistent with providers' resource. In this paper, we propose a truthful double auction mechanism, including a matching process as well as a pricing and VM allocation scheme, to bridge users' task requirements and providers' resources in two-sided cloud markets. In the matching process, we design a cost-aware resource algorithm based on Lyapunov optimization techniques to precisely obtain the number of VMs that meets users' task requirements. In the pricing and VM allocation scheme, we apply the idea of second-price auction to determine the final price and the number of provisioned VMs in the double auction. We theoretically prove our proposed mechanism is individual-rational, truthful and budget-balanced, and analyze the optimality of proposed algorithm. Through simulation experiments, the results show that the individual profits achieved by our algorithm are 12.35 and 11.02 percent larger than that of scaleout and greedy scale-up algorithms respectively for 90 percent of users, and the social welfare of our mechanism is only 7.01 percent smaller than that of the optimum mechanism in the worst case."",""1558-2183"","""",""10.1109/TPDS.2017.2781236"",""National Key R&D Program of China(grant numbers:2017YFC0803700)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170253"",""Double auction mechanism";two-sided cloud markets;VM trading;"Lyapunov optimization"",""Pricing";Cloud computing;Bridges;Algorithm design and analysis;Resource management;"Optimization"","""",""45"","""",""48"",""IEEE"",""8 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Fault-Tolerant Framework for Asynchronous Iterative Computations in Cloud Environments,""Z. Wang"; L. Gao; Y. Gu; Y. Bao;" G. Yu"",""School of Computer Science and Engineering, Northeastern University, Shenyang, Liaoning, China"; Department of Electrical and Computer Engineering, University of Massachusetts Amherst, Amherst, MA; School of Computer Science and Engineering, Northeastern University, Shenyang, Liaoning, China; School of Computer Science and Engineering, Northeastern University, Shenyang, Liaoning, China;" College of Information, Liaoning University, Shenyang, Liaoning, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1678"",""1692"",""Most graph algorithms are iterative in nature. They can be processed by distributed systems in memory in an efficient asynchronous manner. However, it is challenging to recover from failures in such systems. This is because traditional checkpoint fault-tolerant frameworks incur expensive barrier costs that usually offset the gains brought by asynchronous computations. Worse, surviving data are rolled back, leading to costly re-computations. This paper first proposes to leverage surviving data for failure recovery in an asynchronous system. Our framework guarantees the correctness of algorithms and avoids rolling back surviving data. Additionally, a novel asynchronous checkpointing solution is introduced to accelerate recovery at the price of nearly zero overheads. Some optimization strategies like message pruning, non-blocking recovery and load balancing are also designed to further boost the performance. We have conducted extensive experiments to show the effectiveness of our proposals using real-world graphs."",""1558-2183"","""",""10.1109/TPDS.2018.2808519"",""National Natural Science Foundation of China(grant numbers:61433008,U143520006,61472071,61528203)"; U.S. National Natural Science Foundation(grant numbers:CNS-1217284); UMass Amherst; China Scholarship Council;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8300647"",""Fault-tolerance";asynchronous model;iterative graph algorithm;"distributed memory-based systems"",""Fault tolerance";Fault tolerant systems;Checkpointing;Computational modeling;Engines;Iterative algorithms;"Synchronization"","""",""10"","""",""35"",""IEEE"",""22 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Flattened Metadata Service for Distributed File Systems,""S. Li"; F. Liu; J. Shu; Y. Lu; T. Li;" Y. Hu"",""State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, Henan, China"; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, Henan, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; University of Florida, Gainesville, FL;" University of Florida, Gainesville, FL"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2641"",""2657"",""Key-Value stores provide scalable metadata service for distributed file systems. However, the metadata's organization itself, which is organized using a directory tree structure, does not fit the key-value access pattern, thereby limiting the performance. To address this issues, we propose a distributed file system with a flattened and fine-grained division metadata service, LocoMeta, to bridge the performance gap between file system metadata and key-value stores. LocoMeta is designed to bridge the gap between file metadata to key-value store with two techniques. First, LocoMeta flattens the directory content and structure, which organizes file and directory index nodes in a flat space while reversely indexing the directory entries. Second, it exploits a fine-grained division method to improve the key-value access performance. Evaluations show that LocoMeta with eight nodes boosts the metadata throughput by five times, which approaches 93 percent throughput of a single-node key-value store, compared to 18 percent in the state-of-the-art IndexFS."",""1558-2183"","""",""10.1109/TPDS.2018.2842183"",""National Key R&D Program of China(grant numbers:2016YFB0801303,2016QY01W0105)"; National Natural Science Foundation of China(grant numbers:61772300,61433008); Young Elite Scientists Sponsorship Program of China Association for Science and Technology (CAST);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370078"",""Metadata server";distributed file system;"storage system"",""Metadata";Servers;Data storage systems;Scalability;File systems;"Throughput"","""",""7"","""",""66"",""IEEE"",""31 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Framework for the Automatic Vectorization of Parallel Sort on x86-Based Processors,""K. Hou"; H. Wang;" W. -C. Feng"",""Department of Computer Science, Virginia Tech, Blacksburg, VA"; Department of Computer Science, Virginia Tech, Blacksburg, VA;" Department of Computer Science, Virginia Tech, Blacksburg, VA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""958"",""972"",""The continued growth in the width of vector registers and the evolving library of intrinsics on the modern x86 processors make manual optimizations for data-level parallelism tedious and error-prone. In this paper, we focus on parallel sorting, a building block for many higher-level applications, and propose a framework for the Automatic SIMDization of Parallel Sorting (ASPaS) on x86-based multiand many-core processors. That is, ASPaS takes any sorting network and a given instruction set architecture (ISA) as inputs and automatically generates vector code for that sorting network. After formalizing the sort function as a sequence of comparators and the transpose and merge functions as sequences of vector-matrix multiplications, ASPaS can map these functions to operations from a selected “pattern pool” that is based on the characteristics of parallel sorting, and then generate the vector code with the real ISA intrinsics. The performance evaluation on the Intel Ivy Bridge and Haswell CPUs, and Knights Corner MIC illustrates that automatically generated sorting codes from ASPaS can outperform the widely used sorting tools, achieving up to 5.2x speedup over the single-threaded implementations from STL and Boost and up to 6.7x speedup over the multi-threaded parallel sort from Intel TBB."",""1558-2183"","""",""10.1109/TPDS.2018.2789903"",""NSF-BIGDATA(grant numbers:IIS-1247693)"; NSF-XPS(grant numbers:CCF-1337131);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8247204"",""SIMD";IMCI;AVX2;Xeon Phi;sorting networks;merging networks;"code-generation"",""Sorting";Microwave integrated circuits;Registers;Merging;Distributed databases;"Instruction sets"","""",""8"","""",""38"",""IEEE"",""5 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Guide for Achieving High Performance with Very Small Matrices on GPU: A Case Study of Batched LU and Cholesky Factorizations,""A. Haidar"; A. Abdelfattah; M. Zounon; S. Tomov;" J. Dongarra"",""Innovative Computing Laboratory, University of Tennessee, Knoxville, TN"; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN; University of Manchester, Manchester, United Kingdom; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN;" University of Manchester, Manchester, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""973"",""984"",""We present a high-performance GPU kernel with a substantial speedup over vendor libraries for very small matrix computations. In addition, we discuss most of the challenges that hinder the design of efficient GPU kernels for small matrix algorithms. We propose relevant algorithm analysis to harness the full power of a GPU, and strategies for predicting the performance, before introducing a proper implementation. We develop a theoretical analysis and a methodology for high-performance linear solvers for very small matrices. As test cases, we take the Cholesky and LU factorizations and show how the proposed methodology enables us to achieve a performance close to the theoretical upper bound of the hardware. This work investigates and proposes novel algorithms for designing highly optimized GPU kernels for solving batches of hundreds of thousands of small-size Cholesky and LU factorizations. Our focus on efficient batched Cholesky and batched LU kernels is motivated by the increasing need for these kernels in scientific simulations (e.g., astrophysics applications). Techniques for optimal memory traffic, register blocking, and tunable concurrency are incorporated in our proposed design. The proposed GPU kernels achieve performance speedups versus CUBLAS of up to 6× for the factorizations, using double precision arithmetic on an NVIDIA Pascal P100 GPU."",""1558-2183"","""",""10.1109/TPDS.2017.2783929"",""National Science Foundation(grant numbers:CSR 1514286,OAC 1740250)"; NVIDIA; Department of Energy;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8214236"",""Batched computation";GPUs;"variable small sizes"",""Kernel";Graphics processing units;Libraries;Algorithm design and analysis;Linear algebra;Computational modeling;"Computer architecture"","""",""15"","""",""42"",""IEEE"",""15 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Hardware Architecture for Radial Basis Function Neural Network Classifier,""M. Mohammadi"; A. Krishna; N. S.;" S. K. Nandy"",""Computer Aided Design Laboratory, Indian Institute of Science, Bangalore, Karnataka, India"; Computer Aided Design Laboratory, Indian Institute of Science, Bangalore, Karnataka, India; Indian Institute of Science, Bangalore, Karnataka, IN;" Computer Aided Design Laboratory, Indian Institute of Science, Bangalore, Karnataka, India"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""481"",""495"",""In this paper we present design and analysis of scalable hardware architectures for training learning parameters of RBFNN to classify large data sets. We design scalable hardware architectures for K-means clustering algorithm to training the position of hidden nodes at hidden layer of RBFNN and pseudoinverse algorithm for weight adjustments at output layer. These scalable parallel pipelined architectures are capable of implementing data sets with no restriction on their dimensions. This paper also presents a flexible and scalable hardware accelerator for realization of classification using RBFNN, which puts no limitation on the dimension of the input data is developed. We report FPGA synthesis results of our implementations. We compare results of our hardware accelerator with CPU, GPU and implementations of the same algorithms and with other existing algorithms. Analysis of these results show that scalability of our hardware architecture makes it favorable solution for classification of very large data sets."",""1558-2183"","""",""10.1109/TPDS.2017.2768366"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094305"",""Data classification";RBFNN classifier;scalable architectures;reconfigurable computing;"parallel and pipelined architectures"",""Hardware";Kernel;Computer architecture;Copper;Training;Clustering algorithms;"Algorithm design and analysis"","""",""15"","""",""22"",""IEEE"",""2 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Hierarchical RAID Architecture Towards Fast Recovery and High Reliability,""Y. Li"; N. Wang; C. Tian; S. Wu; Y. Zhang;" Y. Xu"",""Collaborative Innovation Center of High Performance Computing, National University of Defense Technology, Changsha, Hunan, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""734"",""747"",""Disk failures are very common in modern storage systems due to the large number of inexpensive disks. As a result, it takes a long time to recover a failed disk due to its large capacity and limited I/O. To speed up the recovery process and maintain a high system reliability, we propose a hierarchical code architecture with erasure codes, OI-RAID, which consists of two layers of codes, outer layer code and inner layer code. Specifically, the outer layer code is deployed with disk grouping technique based on Balanced Incomplete Block Design (BIBD) or complete graph with skewed data layout to provide efficient parallel I/O of all disks for fast failure recovery, and the inner layer code is deployed within each group of disks to provide high reliability. As an example, we deploy RAID5 in both layers to achieve fault tolerance of at least three disk failures, which meets the requirement of data availability in practical systems, as well as much higher speed up ratio for disk failure recovery than existing approaches. Besides, OI-RAID also keeps the optimal data update complexity and incurs low storage overhead in practice."",""1558-2183"","""",""10.1109/TPDS.2017.2775231"",""National Natural Science Foundation of China(grant numbers:61772484,61379038)"; Natural Science Foundation of Anhui Province(grant numbers:1508085SQF214);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115198"",""RAID";fast recovery;data reliability;BIBD;"complete graph"",""Layout";Bandwidth;Fault tolerant systems;Redundancy;"Reed-Solomon codes"","""",""6"","""",""32"",""IEEE"",""20 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Job Sizing Strategy for High-Throughput Scientific Workflows,""B. Tovar"; R. F. da Silva; G. Juve; E. Deelman; W. Allcock; D. Thain;" M. Livny"",""University of Notre Dame, Notre Dame, IN"; Information Sciences Institute, University of Southern California, Marina Del Rey, CA; Information Sciences Institute, University of Southern California, Marina Del Rey, CA; Information Sciences Institute, University of Southern California, Marina Del Rey, CA; Argonne National Laboratory, Lemont, IL; University of Notre Dame, Notre Dame, IN;" University of Wisconsin, Madison, WI"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""240"",""253"",""The user of a computing facility must make a critical decision when submitting jobs for execution: how many resources (such as cores, memory, and disk) should be requested for each job? If the request is too small, the job may fail due to resource exhaustion"; if the request is too large, the job may succeed, but resources will be wasted. This decision is especially important when running hundreds of thousands of jobs in a high throughput workflow, which may exhibit complex, long tailed distributions of resource consumption. In this paper, we present a strategy for solving the job sizing problem: (1) applications are monitored and measured in user-space as they run; (2) the resource usage is collected into an online archive;" and (3) jobs are automatically sized according to historical data in order to maximize throughput or minimize waste. We evaluate the solution analytically, and present case studies of applying the technique to high throughput physics and bioinformatics workflows consisting of hundreds of thousands of jobs, demonstrating an increase in throughput of 10-400 percent compared to naive approaches."",""1558-2183"","""",""10.1109/TPDS.2017.2762310"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8066333"",""High throughput computing (HTC)";resource monitoring and enforcement;automatic provision of resources;automatic job sizing;"throughput and waste optimization"",""Throughput";Resource management;Random access memory;Physics;Monitoring;Tools;"Feedback loop"","""",""12"","""",""45"",""IEEE"",""12 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Model Predictive Controller for Managing QoS Enforcements and Microarchitecture-Level Interferences in a Lambda Platform,""M. R. HoseinyFarahabady"; A. Y. Zomaya;" Z. Tari"",""Center for Distributed & High Performance Computing, School of IT, The University of Sydney, Camperdown, New South Wales, Australia"; Center for Distributed & High Performance Computing, School of IT, The University of Sydney, Camperdown, New South Wales, Australia;" School of Science, RMIT University, Melbourne, Victoria, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1442"",""1455"",""Lambda paradigm, also known as Function as a Service (FaaS), is a novel event-driven concept that allows companies to build scalable and reliable enterprise applications in an off-premise computing data-center as a serverless solution. In practice, however, an important goal for the service provider of a Lambda platform is to devise an efficient way to consolidate multiple Lambda functions in a single host. While the majority of existing resource management solutions use only operating-system level metrics (e.g., average utilization of computing and I/O resources) to allocate the available resources among the submitted workloads in a balanced way, a resource allocation schema that is oblivious to the issue of shared-resource contention can result in a significant performance variability and degradation within the entire platform. This paper proposes a predictive controller scheme that dynamically allocates resources in a Lambda platform. This scheme uses a prediction tool to estimate the future rate of every event stream and takes into account the quality of service enforcements requested by the owner of each Lambda function. This is formulated as an optimization problem where a set of cost functions are introduced (i) to reduce the total QoS violation incidents"; (ii) to keep the CPU utilization level within an accepted range;" and (iii) to avoid the fierce contention among collocated applications for obtaining shared resources. Performance evaluation is carried out by comparing the proposed solution with an enhanced interference-aware version of three well-known heuristics, namely spread, binpack (the two native clustering solutions employed by Docker Swarm) and best-effort resource allocation schema. Experimental results show that the proposed controller improves the overall performance (in terms of reducing the end-to-end response time) by 14.9 percent on average compared to the best result of the other heuristics. The proposed solution also increases the overall CPU utilization by 18 percent on average (for lightweight workloads), while achieves an average 87 percent (maximum 146 percent) improvement in preventing QoS violation incidents."",""1558-2183"","""",""10.1109/TPDS.2017.2779502"",""Australian Research Council(grant numbers:LP160100406)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8126823"",""Serverless lambda platform";function as a service (FaaS);model predictive control;dynamic resource allocation/scheduling;"performance degradation"",""Resource management";Quality of service;Interference;Bandwidth;Measurement;Servers;"Cloud computing"","""",""14"","""",""78"",""IEEE"",""4 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A New Algorithm for Parallel Connected-Component Labelling on GPUs,""D. P. Playne";" K. Hawick"",""Institute of Natural and Mathematical Sciences, Massey University, Auckland, Wellington, New Zealand";" The Digital Centre, University of Hull, Hull, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1217"",""1230"",""Connected-component labelling remains an important and widely-used technique for processing and analysing images and other forms of data in various application areas. Different data sources produce components with different structural features and may be more or less suited to certain connected-component labelling algorithms. Although many efficient serial algorithms exist, determining connected-components on Graphical Processing Units (GPUs) is of interest as many applications use GPUs for processing other parts of the application and labelling on the GPU can avoid expensive memory transfers. The general problem of connected-component labelling is discussed and two existing GPU-based algorithms are discussed-label-equivalence and Komura-equivalence. A new GPU-based, parallel component-labelling algorithm is presented that identifies and eliminates redundant operations in the previous methods for rectilinear two- and three-dimensional datasets. A set of test-cases with a range of structural features and systems sizes is presented and used to evaluate the new labelling algorithm on modern NVIDIA GPU devices and compare it to existing algorithms. The results of the performance evaluation are presented and show that the new algorithm can provide a meaningful performance improvement over previous methods across a range of test cases."",""1558-2183"","""",""10.1109/TPDS.2018.2799216"",""The Neuroinformatics Research Group (NRG) at Washington University School of Medicine"; Biomedical Informatics Research Network (BIRN)(grant numbers:NIHP50 AG05681,P01 AG03991,P20 MH071616,RR14075,RR 16594,U24 RR21382); the Alzheimer's Association; James S. McDonnell Foundation; Mental Illness and Neuroscience Discovery Institute; HHMI;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8274991"",""Connected-component labelling";GPGPU;"parallel computing"",""Kernel";Labeling;Algorithm design and analysis;Graphics processing units;Clustering algorithms;Performance evaluation;"Computer architecture"","""",""25"","""",""51"",""IEEE"",""30 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Novel Data-Partitioning Algorithm for Performance Optimization of Data-Parallel Applications on Heterogeneous HPC Platforms,""H. Khaleghzadeh"; R. R. Manumachu;" A. Lastovetsky"",""School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland"; School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland;" School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2176"",""2190"",""Modern HPC platforms have become highly heterogeneous owing to tight integration of multicore CPUs and accelerators (such as Graphics Processing Units, Intel Xeon Phis, or Field-Programmable Gate Arrays) empowering them to maximize the dominant objectives of performance and energy efficiency. Due to this inherent characteristic, processing elements contend for shared on-chip resources such as Last Level Cache (LLC), interconnect, etc. and shared nodal resources such as DRAM, PCI-E links, etc. This has resulted in severe resource contention and Non-Uniform Memory Access (NUMA) that have posed serious challenges to model and algorithm developers. Moreover, the accelerators feature limited main memory compared to the multicore CPU host and are connected to it via limited bandwidth PCI-E links thereby requiring support for efficient out-of-card execution. To summarize, the complexities (resource contention, NUMA, accelerator-specific limitations, etc.) have introduced new challenges to optimization of data-parallel applications on these platforms for performance. Due to these complexities, the performance profiles of data-parallel applications executing on these platforms are not smooth and deviate significantly from the shapes that allowed state-of-the-art load-balancing algorithms to find optimal solutions. In this paper, we formulate the problem of optimization of data-parallel applications on modern heterogeneous HPC platforms for performance. We then propose a new model-based data partitioning algorithm, which minimizes the execution time of computations in the parallel execution of the application. This algorithm takes as input a set of p discrete speed functions corresponding top available heterogeneous processors. It does not make any assumptions about the shapes of these functions. We prove the correctness of the algorithm and its complexity of O(m3 × p3), where m is the cardinality of the input discrete speed functions. We experimentally demonstrate the optimality and efficiency of our algorithm using two data-parallel applications, matrix multiplication and fast Fourier transform, on a heterogeneous cluster of nodes where each node contains an Intel multicore Haswell CPU, an Nvidia K40c GPU, and an Intel Xeon Phi co-processor."",""1558-2183"","""",""10.1109/TPDS.2018.2827055"",""Science Foundation Ireland(grant numbers:14/IA/2474)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338082"",""Heterogeneous HPC platforms";data-parallel applications;data partitioning;load balancing;multicore CPU;GPU;Intel Xeon Phi;"performance optimization"",""Multicore processing";Graphics processing units;Computational modeling;Kernel;Central Processing Unit;"Load modeling"","""",""34"","""",""48"",""IEEE"",""16 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Novel Network Structure with Power Efficiency and High Availability for Data Centers,""Z. Li";" Y. Yang"",""Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY";" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""254"",""268"",""Designing a cost-effective network for data centers that can deliver sufficient bandwidth and provide high availability has drawn tremendous attentions recently. In this paper, we propose a novel server-centric network structure called RCube, which is energy efficient and can deploy a redundancy scheme to improve the availability of data centers. Moreover, RCube shares many good properties with BCube, a well known server-centric network structure, yet its network size can be adjusted more conveniently. We also present a routing algorithm to find paths in RCube and an algorithm to find multiple parallel paths between any pair of source and destination servers. In addition, we theoretically analyze the power efficiency of the network and availability of RCube under server failure. Our comprehensive simulations demonstrate that RCube provides higher availability and flexibility to make trade-off among many factors, such as power consumption and aggregate throughput, than BCube, while delivering similar performance to BCube in many critical metrics, such as average path length, path distribution and graceful degradation, which makes RCube a very promising empirical structure for an enterprise data center network product."",""1558-2183"","""",""10.1109/TPDS.2017.2762297"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8066383"",""Data center networks";diameter;parallel paths;server-centric;power efficiency;"high availability"",""Servers";Routing;Power demand;Throughput;Redundancy;"Switches"","""",""14"","""",""27"",""IEEE"",""12 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Parallel Complex Coloring Algorithm for Scheduling of Input-Queued Switches,""L. Wang"; T. Ye; T. T. Lee;" W. Hu"",""State Key Laboratory of Advanced Optical Communication Systems and Networks, Shanghai Jiao Tong University, Shanghai, China"; State Key Laboratory of Advanced Optical Communication Systems and Networks, Shanghai Jiao Tong University, Shanghai, China; Chinese University of Hong Kong (Shenzhen), Shenzhen, China;" State Key Laboratory of Advanced Optical Communication Systems and Networks, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1456"",""1468"",""This paper explores the scheduling problem of input-queued switches, based on a new algebraic method of edge coloring called complex coloring. The proposed scheduling algorithm possesses three important features inherent from complex coloring: parallelizability, optimality and rearrangeability. Parallelizability makes the algorithm running very fast in a distributed manner, optimality ensures that the algorithm always returns a proper connection pattern with the minimum number of required colors, and rearrangeability allows partially re-scheduling the existing connection patterns if the traffic patterns only changes slightly. The amortized time complexity of the proposed parallel scheduling algorithm, in terms of the time to compute a matching in a timeslot, is O(log N), where N is the switch size. As for the scalability of input-queued switches, due to its low complexity, our algorithm can achieve nearly 100 percent throughput and provide acceptable queuing delay when N is large. Furthermore, the complex coloring method naturally provides an adaptive solution to non-uniform input traffic pattern. Thus, the proposed parallel scheduling algorithm is highly robust in the face of traffic fluctuations."",""1558-2183"","""",""10.1109/TPDS.2018.2802523"",""National Natural Science Foundation of China(grant numbers:61571288,61671286,61433009)"; Open Research Fund of Key Laboratory of Optical Fiber Communications (Ministry of Education of China);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281531"",""Packet switching";scheduling;matching;edge coloring;"complex coloring"",""Scheduling";Switches;Packet switching;Edge computing;"Image color analysis"","""",""11"","""",""40"",""IEEE"",""5 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Relaxation-Based Network Decomposition Algorithm for Parallel Transient Stability Simulation with Improved Convergence,""J. Shi"; B. Sullivan; M. Mazzola; B. Saravi; U. Adhikari;" T. Haupt"",""High Performance Computing Collabotory at Mississippi State University, Starkviile, MS"; High Performance Computing Collabotory at Mississippi State University, Starkviile, MS; High Performance Computing Collabotory at Mississippi State University, Starkviile, MS; High Performance Computing Collabotory at Mississippi State University, Starkviile, MS; PEAK Reliability, Loveland, CO;" High Performance Computing Collabotory at Mississippi State University, Starkviile, MS"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""496"",""511"",""Transient stability simulation of a large-scale and interconnected electric power system involves solving a large set of differential algebraic equations (DAEs) at every simulation time-step. With the ever-growing size and complexity of power grids, dynamic simulation becomes more time-consuming and computationally difficult using conventional sequential simulation techniques. To cope with this challenge, this paper aims to develop a fully distributed approach intended for implementation on High Performance Computer (HPC) clusters. A novel, relaxation-based domain decomposition algorithm known as Parallel-General-Norton with Multiple-port Equivalent (PGNME) is proposed as the core technique of a two-stage decomposition approach to divide the overall dynamic simulation problem into a set of subproblems that can be solved concurrently to exploit parallelism and scalability. While the convergence property has traditionally been a concern for relaxation-based decomposition, an estimation mechanism based on multiple-port network equivalent is adopted as the preconditioner to enhance the convergence of the proposed algorithm. The proposed algorithm is illustrated using rigorous mathematics and validated both in terms of speed-up and capability. Moreover, a complexity analysis is performed to support the observation that PGNME scales well when the size of the subproblems are sufficiently large."",""1558-2183"","""",""10.1109/TPDS.2017.2769649"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094963"",""High-performance computing";parallel simulation;power system;"dynamic simulation"",""Computational modeling";Power system dynamics;Convergence;Parallel processing;Power system stability;"Mathematical model"","""",""12"","""",""42"",""IEEE"",""3 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Self-Adaptive Network for HPC Clouds: Architecture, Framework, and Implementation,""F. Zahid"; A. Taherkordi; E. G. Gran; T. Skeie;" B. D. Johnsen"",""Department of Advanced Computing and System Performance, Simula Research Laboratory, Fornebu, Norway"; Department of Informatics, University of Oslo, Oslo, Norway; Norwegian University of Science and Technology, Trondheim, Norway; Department of Advanced Computing and System Performance, Simula Research Laboratory, Fornebu, Norway;" Oracle Corporation, Lysaker, Norway"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2658"",""2671"",""Clouds offer flexible and economically attractive compute and storage solutions for enterprises. However, the effectiveness of cloud computing for high-performance computing (HPC) systems still remains questionable. When clouds are deployed on lossless interconnection networks, like InfiniBand (IB), challenges related to load-balancing, low-overhead virtualization, and performance isolation hinder full potential utilization of the underlying interconnect. Moreover, cloud data centers incorporate a highly dynamic environment rendering static network reconfigurations, typically used in IB systems, infeasible. In this paper, we present a framework for a self-adaptive network architecture for HPC clouds based on lossless interconnection networks, demonstrated by means of our implemented IB prototype. Our solution, based on a feedback control and optimization loop, enables the lossless HPC network to dynamically adapt to the varying traffic patterns, current resource availability, workload distributions, and also in accordance with the service provider-defined policies. Furthermore, we present IBAdapt, a simplified ruled-based language for the service providers to specify adaptation strategies used by the framework. Our developed self-adaptive IB network prototype is demonstrated using state-of-the-art industry software. The results obtained on a test cluster demonstrate the feasibility and effectiveness of the framework when it comes to improving Quality-of-Service compliance in HPC clouds."",""1558-2183"","""",""10.1109/TPDS.2018.2842224"",""Norges Forskningsråd(grant numbers:213283/O70)"; DILUTE(grant numbers:262854/F20); European Commission's H2020 research programme(grant numbers:731664 (MELODIC));" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8385210"",""Self-adaptive systems";interconection networks;HPC clouds;infiniband;"network reconfiguration"",""Cloud computing";Routing;Computer architecture;Network topology;"High performance computing"","""",""7"","""",""48"",""IEEE"",""14 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Study of Systems with Multiple Operating Levels, Probabilistic Thresholds and Hysteresis,""A. Brandwajn"; T. Begin; H. Castel-Taleb;" T. Atmaca"",""Baskin School of Engineering, University of California, Santa Cruz, CA"; Université Lyon 1 / LIP (UMR InriaENS Lyon CNRSUCBL), Villeurbanne, France; SAMOVARTélécom SudParis, Université Paris-Saclay, 9 rue Charles Fourier, Évry, France;" SAMOVARTélécom SudParis, Université Paris-Saclay, 9 rue Charles Fourier, Évry, France"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""748"",""757"",""Current architecture of many computer systems relies on dynamic allocation of a pool of resources according to workload conditions to meet specific performance objectives while minimizing cost (e.g., energy or billing). In such systems, different levels of operation may be defined, and switching between operating levels occurs at certain thresholds of system congestion. To avoid rapid oscillations between levels of service, “hysteresis” is introduced by using different thresholds for increasing and decreasing workload levels, respectively. We propose a model of such systems with general arrivals, arbitrary number of servers and operating levels where each higher operating level may correspond to an arbitrary number of additional servers and soft (i.e., non-deterministic) thresholds to account for “inertia” in switching between operating levels. In our model, request service times are assumed to be memoryless and server processing rates may be a function of the current operating level and of the number of requests (users) in the system. Additionally, we allow for delays in the activation of additional operating levels. We use simple mathematics to obtain a semi-numerical solution of our model. We illustrate the versatility of our model using several case study examples inspired by features of real systems. In particular, we explore optimal thresholds as a tradeoff between performance and energy consumption."",""1558-2183"","""",""10.1109/TPDS.2017.2773496"",""LABEX MILYON(grant numbers:ANR-10-LABX-0070)"; “Investissements d'Avenir”(grant numbers:ANR-11-IDEX-0007); French National Research Agency (ANR); ANR MARMOTE(grant numbers:ANR-12-MONU-0019); Télécom SudParis;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107533"",""Multi-server systems";multiple operating levels;hysteresis;probabilistic thresholds;general arrivals;"activation delays"",""Servers";Mathematical model;Hysteresis;Delays;Steady-state;Resource management;"Switches"","""",""2"","""",""26"",""IEEE"",""14 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Survey of Desktop Grid Scheduling,""E. Ivashko"; I. Chernov;" N. Nikitina"",""Institute of Applied Mathematical Research, Russian Academy of Sciences, Petrozavodsk, Russia"; Institute of Applied Mathematical Research, Russian Academy of Sciences, Petrozavodsk, Russia;" Institute of Applied Mathematical Research, Russian Academy of Sciences, Petrozavodsk, Russia"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2882"",""2895"",""The paper surveys the state of the art of task scheduling in Desktop Grid computing systems. We describe the general architecture of a Desktop Grid system and the computing model adopted by the BOINC middleware. We summarize research papers to bring together and examine the optimization criteria and methods proposed by researchers so far for improving Desktop Grid task scheduling (assigning tasks to computing nodes by a server). In addition, we review related papers, which address non-regular architectures, like hierarchical and peer-to-peer Desktop Grids, as well as Desktop Grids designed for solving interdependent tasks."",""1558-2183"","""",""10.1109/TPDS.2018.2850004"",""Российский Фонд Фундаментальных Исследований (РФФИ)(grant numbers:16-07-00622)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8395065"",""Scheduling and task partitioning";"scheduling"",""Task analysis";Processor scheduling;Peer-to-peer computing;Scheduling;Servers;Middleware;"Computer architecture"","""",""11"","""",""107"",""IEEE"",""25 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Survey on Recent OS-Level Energy Management Techniques for Mobile Processing Units,""Y. G. Kim"; J. Kong;" S. W. Chung"",""Department of Computer Science, Korea University, Seoul, Korea"; School of Electronics Engineering, Kyungpook National University, Daegu, Korea;" Department of Computer Science, Korea University, Seoul, Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2388"",""2401"",""To improve mobile experience of users, recent mobile devices have adopted powerful processing units (CPUs and GPUs). Unfortunately, the processing units often consume a considerable amount of energy, which in turn shortens battery life of mobile devices. For energy reduction of the processing units, mobile devices adopt energy management techniques based on software, especially OS (Operating Systems), as well as hardware. In this survey paper, we summarize recent OS-level energy management techniques for mobile processing units. We categorize the energy management techniques into three parts, according to main operations of the summarized techniques: 1) techniques adjusting power states of processing units, 2) techniques exploiting other computing resources, and 3) techniques considering interactions between displays and processing units. We believe this comprehensive survey paper will be a useful guideline for understanding recent OS-level energy management techniques and developing more advanced OS-level techniques for energy-efficient mobile processing units."",""1558-2183"","""",""10.1109/TPDS.2018.2822683"",""National Research Foundation of Korea"; Korea government (MSIP)(grant numbers:2017M3C4A7080243); Samsung Electronics; Korea University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330039"",""Energy management";mobile device;"operating systems"",""Central Processing Unit";Energy management;Mobile handsets;Quality of service;Hardware;Performance evaluation;"Software"","""",""21"","""",""71"",""IEEE"",""3 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Write-Friendly and Cache-Optimized Hashing Scheme for Non-Volatile Memory Systems,""P. Zuo";" Y. Hua"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China";" Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""985"",""998"",""Non-volatile memory technologies (NVMs) are promising candidates for building future memory systems, due to their advantages of high density, high scalability, and requiring near-zero standby power, while suffering from the limited endurance and asymmetric properties of reads and writes, compared with traditional memory technologies including DRAM and SRAM. The significant changes of low-level memory devices cause nontrivial challenges to high-level in-memory and in-cache data structure design due to overlooking the NVM device properties. In this paper, we study an important and common data structure, hash table, which is ubiquitous and widely used to construct the index and lookup table in main memory and caches. Based on the observations that existing hashing schemes cause many extra writes to NVMs, we propose a cost-efficient write-friendly hashing scheme, called path hashing, which incurs no extra writes to NVMs while delivers high performance. The basic idea of path hashing is to leverage a novel hash-collision resolution method, i.e., position sharing, which meets the needs of insertion and deletion requests without extra writes to NVMs. By further exploiting double-path hashing and path shortening techniques, path hashing delivers high performance of hash tables in terms of space utilization and request latency. Nevertheless, the original path hashing has low utilization of each cache line for small items, causing low cache efficiency. We hence propose a cache-optimized path hashing to pack multiple cells in the same path together and store them into one cache line, thus improving the cache line utilization to obtain higher performance. We have implemented path hashing and used a gem5 full system simulator with NVMain to evaluate its performance in the context of NVMs. Extensive experimental results demonstrate that path hashing incurs no extra writes to NVMs, and achieves up to 95 percent space utilization ratio as well as low request latency, compared with existing state-of-the-art hashing schemes. We have released the source code of path hashing for public use at github."",""1558-2183"","""",""10.1109/TPDS.2017.2782251"",""National Key Research and Development Program of China(grant numbers:2016YFB1000202)"; National Natural Science Foundation of China (NSFC)(grant numbers:61772212); State Key Laboratory of Computer Architecture(grant numbers:CARCH201505);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8186236"",""Non-volatile memory";hashing scheme;write endurance;"collision resolution"",""Nonvolatile memory";Random access memory;Data structures;Phase change materials;Probes;Memory management;"Indexes"","""",""31"","""",""60"",""IEEE"",""11 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating Persistent Scatterer Pixel Selection for InSAR Processing,""T. Reza"; A. Zimmer; J. M. D. Blasco; P. Ghuman; T. K. Aasawat;" M. Ripeanu"",""Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada"; 3vGeomatics, Vancouver, BC, Canada; Grupo de Investigación Microgeodesia Jaén, Universidad de Jaén, Jaén, Spain; 3vGeomatics, Vancouver, BC, Canada; Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada;" Electrical and Computer Engineering, University of British Columbia, Vancouver, BC, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""16"",""30"",""Interferometric Synthetic Aperture Radar (InSAR) is a remote sensing technology used for estimating the displacement of an object on the ground or the earth's surface itself. Persistent Scatterer-InSAR (PS-InSAR) is a category of time series algorithms enabling high resolution monitoring. PS-InSAR relies on successful selection of points that appear stable across a set of satellite images taken overtime. This paper presents PtSel, a new algorithm for selecting these points, a problem known as Persistent Scatterer Selection. The key advantage of PtSel over the key existing techniques is that it does not require model assumptions, yet preserves solution accuracy. Motivated by the abundance of parallelism the algorithm exposes, we have implemented it for GPUs. Our evaluation using real-world data shows that the GPU implementation not only offers superior performance but also scales linearly with GPU count and workload size. We compare the GPU implementation and a parallel CPU implementation: a consumer grade GPU offers 18x speedup over a 16-core Ivy Bridge Xeon System, while four GPUs offer 65x speedup. The GPU solution consumes 28x less energy than the CPU-only solution. Additionally, we present a comparison with the most widely used PS-interferometry software package StaMPS, in terms of point selection coverage and precision."",""1558-2183"","""",""10.1109/TPDS.2017.2706291"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932146"",""GPU acceleration";image processing;time series analysis;remote sensing;"InSAR"",""Graphics processing units";Coherence;Synthetic aperture radar;Image resolution;Satellites;"Acceleration"","""",""7"","""",""30"",""IEEE"",""19 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Adaptive Resource Allocation and Provisioning in Multi-Service Cloud Environments,""A. Alsarhan"; A. Itradat; A. Y. Al-Dubai; A. Y. Zomaya;" G. Min"",""Department Computer Information System, Hashemite University, Zarqa, Jordan"; Department Computer Information System, Hashemite University, Zarqa, Jordan; School of Computing, Edinburgh Napier University, Edinburgh, United Kingdom; School of Information Technologies, University of Sydney, Camperdown, Australia;" Department Mathematics and Computer Science, University of Exeter, Exeter, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""31"",""42"",""In the current cloud business environment, the cloud provider (CP) can provide a means for offering the required quality of service (QoS) for multiple classes of clients. We consider the cloud market where various resources such as CPUs, memory, and storage in the form of Virtual Machine (VM) instances can be provisioned and then leased to clients with QoS guarantees. Unlike existing works, we propose a novel Service Level Agreement (SLA) framework for cloud computing, in which a price control parameter is used to meet QoS demands for all classes in the market. The framework uses reinforcement learning (RL) to derive a VM hiring policy that can adapt to changes in the system to guarantee the QoS for all client classes. These changes include: service cost, system capacity, and the demand for service. In exhibiting solutions, when the CP leases more VMs to a class of clients, the QoS is degraded for other classes due to an inadequate number of VMs. However, our approach integrates computing resources adaptation with service admission control based on the RL model. To the best of our knowledge, this study is the first attempt that facilitates this integration to enhance the CP's profit and avoid SLA violation. Numerical analysis stresses the ability of our approach to avoid SLA violation while maximizing the CP's profit under varying cloud environment conditions."",""1558-2183"","""",""10.1109/TPDS.2017.2748578"",""Edinburgh Napier University(grant numbers:415847)"; Hashemite University; University of Sydney and University of Exeter;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8027098"",""Resource management";cloud computing;quality of service;cloud service trading;"economic model"",""Cloud computing";Quality of service;Computational modeling;Resource management;Adaptation models;"Electronic mail"","""",""49"","""",""36"",""IEEE"",""7 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Adaptive Scheduling Parallel Jobs with Dynamic Batching in Spark Streaming,""D. Cheng"; X. Zhou; Y. Wang;" C. Jiang"",""Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA"; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA;" Key Laboratory of Embedded System and Service Computing, Tongji University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2672"",""2685"",""Today enterprises have massive stream data that require to be processed in real time due to data explosion in recent years. Spark Streaming as an emerging system is developed to process real time stream data analytics by using micro-batch approach. The unified programming model of Spark Steaming leads to some unique benefits over other traditional streaming systems, such as fast recovery from failures, better load balancing and resource usage. It treats the continuous stream as a series of micro-batches of data and continuously process these micro-batch jobs. However, efficient scheduling of micro-batch jobs to achieve high throughput and low latency is very challenging due to the complex data dependency and dynamism inherent in streaming workloads. In this paper, we propose A-scheduler, an adaptive scheduling approach that dynamically schedules parallel micro-batch jobs in Spark Streaming and automatically adjusts scheduling parameters to improve performance and resource efficiency. Specifically, A-scheduler dynamically schedules multiple jobs concurrently using different policies based on their data dependencies and automatically adjusts the level of job parallelism and resource shares among jobs based on workload properties. Furthermore, we integrate dynamic batching technique with A-Scheduler to further improve the overall performance of the customized Spark Streaming system. It relies on an expert fuzzy control mechanism to dynamically adjust the length of each batch interval in response to time-varying streaming workload and system processing rate. We implemented A-scheduler and evaluated it with a real-time security event processing workload. Our experimental results show that A-scheduler with dynamic batching can reduce end-to-end latency by 38 percent and meanwhile improve workload throughput and energy efficiency by 23 and 15 percent, respectively, compared to the default Spark Streaming scheduler."",""1558-2183"","""",""10.1109/TPDS.2018.2846234"",""University of North Carolina at Charlotte";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8382327"",""Adaptive scheduling";spark streaming;parallel job;data dependency;resource allocation;"dynamic batching"",""Throughput";Parallel processing;Dynamic scheduling;Real-time systems;Task analysis;"Streamimg media"","""",""27"","""",""30"",""USGov"",""12 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"AIRA: A Framework for Flexible Compute Kernel Execution in Heterogeneous Platforms,""R. Lyerly"; A. Murray; A. Barbalace;" B. Ravindran"",""Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA"; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA; Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA;" Bradley Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""269"",""282"",""Heterogeneous-ISA computing platforms have become ubiquitous, and will be used for diverse workloads which render static mappings of computation to processors inadequate. Dynamic mappings which adjust an application's usage in consideration of platform workload can reduce application latency and increase throughput for heterogeneous platforms. We introduce AIRA, a compiler and runtime for flexible execution of applications in CPU-GPU platforms. Using AIRA, we demonstrate up to a 3.78× speedup in benchmarks from Rodinia and Parboil, run with various workloads on a server-class platform. Additionally, AIRA is able to extract up to an 87 percent increase in platform throughput over a static mapping."",""1558-2183"","""",""10.1109/TPDS.2017.2761748"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8063925"",""Heterogeneous architectures";compilers;runtimes;"programming models"",""Computer architecture";Kernel;Runtime;Graphics processing units;Throughput;"Atmospheric modeling"","""",""2"","""",""48"",""IEEE"",""10 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Alleviating Memory Refresh Overhead via Data Compression for High Performance and Energy Efficiency,""K. Zhou"; W. Liu; K. Tang; P. Huang;" X. He"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science & Technology, Wuhan, China"; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science & Technology, Wuhan, China; Temple University, Philadelphia, PA; Temple University, Philadelphia, PA;" Temple University, Philadelphia, PA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1469"",""1483"",""DRAM memory suffers from increasingly aggravating refresh penalty, which causes significant performance degradation and power consumption. As memory capacity increases, refresh penalty has become increasingly worse as more rows have to be refreshed. In this work, we propose an effective refresh approach called Compression-Aware Refresh (CAR) to efficiently mitigate refresh overheads. We apply a data compression technique to store data in a compressed format so that the resultant sparse banks need only be partially refreshed. Because of compression, data blocks which are originally distributed across all the constituent chips of a rank only need to be stored in a subset of those chips, leaving banks in the remaining chips not fully occupied. As a result, the memory controller can safely skip refreshing memory rows which contain no useful data without compromising data integrity. Such a compression-aware refresh scheme significantly reduces the refresh and thus improves overall memory performance and energy efficiency. To further take advantage of data compression, we adopt a rank subsetting technique to enable accesses to only those occupied chips for memory requests accessing compressed data blocks. Evaluations using benchmarks from SPEC CPU 2006 and the PARSEC 3.0 on recent DDR4 memory systems have shown that CAR achieves up to 1.66× performance improvement (11.7 percent on average) and up to 45.9 percent energy reduction (27.3 percent on average), and reduce memory traffic by up to 99.9 percent for zero cache lines intensive workloads with an average of 66.3 percent across all benchmarks."",""1558-2183"","""",""10.1109/TPDS.2017.2763141"",""National Natural Science Foundation of China(grant numbers:61232004,61502189)"; National Basic Research Program of China (973 Program)(grant numbers:2016YFB0800402); US National Science Foundation (NSF)(grant numbers:CNS-1702474,CNS-1700719,CCF-1547804);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8068250"",""DRAM refresh";energy;performance;"compression"",""Memory management";Random access memory;Automobiles;Data compression;Benchmark testing;Energy consumption;"Performance evaluation"","""",""4"","""",""51"",""IEEE"",""16 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Auto-Tuner for OpenCL Work-Group Size on GPUs,""T. T. Dao";" J. Lee"",""School of Computer Science and Engineering, Seoul National Univeristy, Seoul, Korea";" School of Computer Science and Engineering, Seoul National Univeristy, Seoul, Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""283"",""296"",""Tuning the kernel work-group size for GPUs is a challenging problem. In this paper, using the performance counters provided by GPUs, we characterize a large body of OpenCL kernels to identify the performance factors that affect the choice of a good work-group size. Based on the characterization, we realize that the most influential performance factors with regard to the work-group size include occupancy, coalesced global memory accesses, cache contention, and variation in the amount of workload in the kernel. By tackling the performance factors one by one, we propose auto-tuning techniques that selects the best work-group size and shape for GPU kernels. We show the effectiveness of our auto-tuner by evaluating it with a set of 54 OpenCL kernels on three different NVIDIA GPUs and one AMD GPU. On average, the auto-tuner needs to spend no more than 8 percent of the time required by an exhaustive search to find an optimal work-group size. The execution time of the selected sub-optimal work-group size is at most 1.14x slower than that of the optimal work-group size found by the exhaustive search, on average."",""1558-2183"","""",""10.1109/TPDS.2017.2755657"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8048544"",""GPU";auto-tuning;workload characterization;workgroup size;OpenCL;NVIDIA;"AMD"",""Kernel";Graphics processing units;Tuners;Indexes;Memory management;"Hardware"","""",""6"","""",""35"",""IEEE"",""22 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Efficient and Fair Multi-Resource Allocation Mechanism for Heterogeneous Servers,""J. Khamse-Ashari"; I. Lambadaris; G. Kesidis; B. Urgaonkar;" Y. Zhao"",""Department of SCE, Carleton University, Ottawa, ON, Canada"; Department of SCE, Carleton University, Ottawa, ON, Canada; Department of EECS, Pennsylvania State University, State College, PA; Department of EECS, Pennsylvania State University, State College, PA;" School of Math and Statistics, Carleton University, Ottawa, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2686"",""2699"",""Efficient and fair allocation of multiple types of resources is a crucial objective in a cloud/distributed computing cluster. Users may have diverse resource needs. Furthermore, diversity in server properties/capabilities may mean that only a subset of servers may be usable by a given user. In platforms with such heterogeneity, we identify important limitations in existing multi-resource fair allocation mechanisms, notably Dominant Resource Fairness and its follow-up work. To overcome such limitations, we propose a new server-based approach";" each server allocates resources by maximizing a per-server utility function. We propose a specific class of utility functions which, when appropriately parameterized, adjusts the trade-off between efficiency and fairness, and captures a variety of fairness measures (such as our recently proposed Per-Server Dominant Share Fairness ). We establish conditions for the proposed mechanism to satisfy certain properties that are generally deemed desirable, e.g., envy-freeness, sharing incentive, bottleneck fairness, and Pareto optimality. To implement our resource allocation mechanism, we develop an iterative algorithm which is shown to be globally convergent. Subsequently, we show how the proposed mechanism could be implemented in a distributed fashion. Finally, we carry out extensive trace-driven simulations to show the enhanced performance of our proposed mechanism over the existing ones."",""1558-2183"","""",""10.1109/TPDS.2018.2841915"",""Bitbrains IT Services Inc.";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368291"",""Multi-resource fair allocation";heterogeneous servers;cloud computing;concave game;"distributed algorithm"",""Servers";Resource management;Task analysis;Cloud computing;Games;Measurement;"Distributed computing"","""",""35"","""",""35"",""IEEE"",""29 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Efficient In-Memory Checkpoint Method and its Practice on Fault-Tolerant HPL,""X. Tang"; J. Zhai; B. Yu; W. Chen; W. Zheng;" K. Li"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science, State University of New York, New Paltz, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""758"",""771"",""Fault tolerance is increasingly important in high-performance computing due to the substantial growth of system scale and decreasing system reliability. In-memory/diskless checkpoint has gained extensive attention as a solution to avoid the IO bottleneck of traditional disk-based checkpoint methods. However, applications using previous in-memory checkpoint suffer from little available memory space. To provide high reliability, previous in-memory checkpoint methods either need to keep two copies of checkpoints to tolerate failures while updating old checkpoints or trade performance for space by flushing in-memory checkpoints into disk. In this paper, we propose a novel in-memory checkpoint method, called self-checkpoint, which can not only achieve the same reliability of previous in-memory checkpoint methods, but also increase the available memory space for applications by almost 50 percent. To validate our method, we apply self-checkpoint method to an important problem: High-Performance Linpack (HPL) with fault tolerance. We implement a scalable and fault tolerant HPL based on this new method, called SKT-HPL, and validate it on two large-scale systems. Experimental results with 24,576 processes show that SKT-HPL achieves over 95 percent of the performance of the original HPL. Compared to the state-of-the-art in-memory checkpoint method, it improves the available memory size by 47 percent and the performance by 5 percent."",""1558-2183"","""",""10.1109/TPDS.2017.2781257"",""National Key Research and Development Program of China(grant numbers:2017YFB1003103)"; NSFC(grant numbers:61232008,61722208); Tsinghua University Initiative Scientific Research Program; Microsoft Research Asia Collaborative Research Program(grant numbers:FY16-RES-THEME-095);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170311"",""Fault tolerance";fault-tolerant HPL;in-memory checkpoint;"memory consumption"",""Fault tolerance";Fault tolerant systems;Encoding;Memory management;Random access memory;"Large-scale systems"","""",""8"","""",""43"",""IEEE"",""8 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Energy Efficient VM Management Scheme with Power-Law Characteristic in Video Streaming Data Centers,""H. -W. Tseng"; T. -T. Yang; K. -C. Yang;" P. -S. Chen"",""Department of Computer Science and Engineering, National Chung-Hsing University, Taiwan, R.O.C"; Department of Computer Science and Engineering, National Chung-Hsing University, Taiwan, R.O.C; Department of Computer Science and Engineering, National Chung-Hsing University, Taiwan, R.O.C;" Department of Computer Science and Engineering, National Chung-Hsing University, Taiwan, R.O.C"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""297"",""311"",""As cloud computing services have gained popularity, users view videos on websites (e.g., YouTube) to generate high CPU resource utilization and bandwidth for video streaming data centers. However, popular videos result in power-law features to cause imbalanced resource utilization. In addition, hotspot and idle servers generate extra power consumption in data centers. Previous studies considered to satisfy the requirements of users, provide faster access rates and save power consumption. However, fewer studies considered resource utilization with different popularity videos. Therefore, this paper proposes an energy efficient virtual machine (VM) management scheme with power-law features (VMPL). VMPL predicts the resource utilization of the video in the future based on the popularity, ensures enough resources for upcoming videos, and turns off idle servers for power saving. Simulation results validated by mathematical analysis show that VMPL has the best resource utilization and the lowest power consumption compared with Nash and Best-Fit algorithms."",""1558-2183"","""",""10.1109/TPDS.2017.2753229"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8039288"",""Video streaming data center";power-law;VM migration;"YouTube"",""Streaming media";Servers;Reactive power;Resource management;YouTube;"Energy consumption"","""",""7"","""",""36"",""IEEE"",""18 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Improved Approximation for Scheduling Malleable Tasks with Precedence Constraints via Iterative Method,""C. -Y. Chen"",""Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan, ROC"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""1937"",""1946"",""The problem of scheduling malleable tasks with precedence constraints is one of the most important strongly NP-hard problems, given m identical processors and n tasks. A malleable task is one that runs in parallel on a varying number of processors. In addition, the processing sequences of tasks are constrained by the precedence constraints. The goal is to find a feasible schedule that minimizes the makespan (maximum completion time). This article presents an iterative method for improving the performance ratio of scheduling malleable tasks. The proposed algorithm achieves an approximation ratio of 4.4841 after 2 iterations. This improves the so far best-known factor of 4.7306 due to Jansen and Zhang. For a large number of iterations (> 100), the approximation ratio of the proposed algorithm is tends toward 2 + √2 ≈ 3.4143."",""1558-2183"","""",""10.1109/TPDS.2018.2813387"",""Ministry of Science and Technology, Taiwan, R.O.C.(grant numbers:MOST 106-2221-E-006-006)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8315149"",""Approximation algorithms";scheduling;malleable tasks;"precedence constraints"",""Task analysis";Program processors;Approximation algorithms;Processor scheduling;Scheduling;Schedules;"Heuristic algorithms"","""",""8"","""",""30"",""OAPA"",""13 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Analysis and Design Techniques towards High-Performance and Energy-Efficient Dense Linear Solvers on GPUs,""s. Abdelfattah"; A. Haidar; S. Tomov;" J. Dongarra"",""Innovative Computing Laboratory, University of Tennessee, Knoxville, TN"; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN;" Innovative Computing Laboratory, University of Tennessee, Knoxville, TN"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2700"",""2712"",""Graphics Processing Units (GPUs) are widely used in accelerating dense linear solvers. The matrix factorizations, which dominate the runtime for these solvers, are often designed using a hybrid scheme, where GPUs perform trailing matrix updates, while the CPUs perform the panel factorizations. Consequently, hybrid solutions require high-end CPUs and optimized CPU software in order to deliver high performance. Furthermore, they lack the energy efficiency inherent for GPUs due to the use of less energy-efficient CPUs, as well as CPU-GPU communications. This paper presents analysis and design techniques that overcome the shortcomings of the hybrid algorithms, and allow the design of high-performance and energy-efficient dense LU and Cholesky factorizations that use GPUs only. The full GPU solution eliminates the need for a high-end CPU and optimized CPU software, which leads to a better energy efficiency. We discuss different design choices, and introduce optimized GPU kernels for panel factorizations. The developed solutions achieve $90+$  percent of the performance of optimized hybrid solutions, while improving the energy efficiency by 50 percent. They outperform the vendor library by 30-50 percent in single precision, and 15-50 percent in double precision. We also show that hybrid designs trail the proposed solutions in performance when optimized CPU software is not available."",""1558-2183"","""",""10.1109/TPDS.2018.2842785"",""NSF(grant numbers:CSR 1514286)"; NVIDIA; Exascale Computing Project(grant numbers:17-SC-20-SC);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370686"",""Dense linear solvers";GPU computing;"energy efficiency"",""Graphics processing units";Energy efficiency;Task analysis;"Multicore processing"","""",""6"","""",""37"",""USGov"",""1 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Analysis of Bounds on Hybrid Vector Clocks,""S. Yingchareonthawornchai"; D. N. Nguyen; S. S. Kulkarni;" M. Demirbas"",""Department of Computer Science and Engineering, Michigan State University, East Lansing, MI"; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI;" Department of Computer Science and SUNY, Buffalo, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""1947"",""1960"",""Hybrid vector clock(s) (HVC) provide a mechanism to combine the theory and practice of distributed systems. Improving on traditional vector clock(s) (VC), HVC utilizes synchronized physical clocks to reduce the size by focusing only on causality where the physical time associated with two events is within a given uncertainty window ε and letting physical clock alone determine the order of events that are outside the uncertainty window. In this paper, we develop a model for determining the bounds on the size of HVC. Our model uses four parameters, ε: uncertainty window, 8: message delay, a: communication frequency and n: number of nodes in the system. We derive the size of HVC in terms of a delay differential equation, and show that the size predicted by our model is almost identical to the results obtained by simulation. We also identify closed form solutions that provide tight lower and upper bounds for useful special cases. We show that for many practical applications and deployment environments in Amazon EC2, the size of HVC remains only as a couple entries and substantially less than n. Finally, although the analytical results rely on a specific communication pattern they are useful in evaluating size of HVC in different communication scenarios."",""1558-2183"","""",""10.1109/TPDS.2018.2818700"",""US National Science Foundation(grant numbers:CNS 1329807)"; US National Science Foundation(grant numbers:CNS 1318678); US National Science Foundation(grant numbers:XPS 1533870,XPS 1533802);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8323242"",""Vector clocks";physical clocks;logical clocks;"distributed systems"",""Clocks";Synchronization;Analytical models;Uncertainty;Mathematical model;Delays;"Differential equations"","""",""1"","""",""19"",""IEEE"",""23 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Architectural Synthesis of Multi-SIMD Dataflow Accelerators for FPGA,""Y. Wu";" J. McAllister"",""Institute of Electronics, Communications and Information Technology (ECIT), Queen’s University Belfast, Belfast, United Kingdom";" Institute of Electronics, Communications and Information Technology (ECIT), Queen’s University Belfast, Belfast, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""43"",""55"",""Field Programmable Gate Array (FPGA) boast abundant resources with which to realise high-performance accelerators for computationally demanding operations. Highly efficient accelerators may be automatically derived from Signal Flow Graph (SFG) models by using architectural synthesis techniques, but in practical design scenarios, these currently operate under two important limitations - they cannot efficiently harness the programmable datapath components which make up an increasing proportion of the computational capacity of modern FPGA and they are unable to automatically derive accelerators to meet a prescribed throughput or latency requirement. This paper addresses these limitations. SFG synthesis is enabled which derives software-programmable multicore single-instruction, multiple-data (SIMD) accelerators which, via combined offline characterisation of multicore performance and compile-time program analysis, meet prescribed throughput requirements. The effectiveness of these techniques is demonstrated on tree-search and linear algebraic accelerators for 802.11n WiFi transceivers, an application for which satisfying real-time performance requirements has, to this point, proven challenging for even manually-derived architectures."",""1558-2183"","""",""10.1109/TPDS.2017.2746081"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8017591"",""Field programmable gate array (FPGA)";dataflow;signal flow;architectural synthesis;"single-instruction multiple-data (SIMD)"",""Field programmable gate arrays";Program processors;Multicore processing;Registers;Throughput;"IEEE 802.11n Standard"","""",""3"","""",""45"",""CCBY"",""29 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Argobots: A Lightweight Low-Level Threading and Tasking Framework,""S. Seo"; A. Amer; P. Balaji; C. Bordage; G. Bosilca; A. Brooks; P. Carns; A. Castelló; D. Genet; T. Herault; S. Iwasaki; P. Jindal; L. V. Kalé; S. Krishnamoorthy; J. Lifflander; H. Lu; E. Meneses; M. Snir; Y. Sun; K. Taura;" P. Beckman"",""Argonne National Laboratory, Lemont, IL"; Argonne National Laboratory, Lemont, IL; Argonne National Laboratory, Lemont, IL; Inria Bordeaux, Talence, France; University of Tennessee, Knoxville, TN; University of Illinois at Urbana-Champaign, Champaign, IL; Argonne National Laboratory, Lemont, IL; Universitat Jaume I, Castellón de la Plana, Castellón, Spain; University of Tennessee, Knoxville, TN; University of Tennessee, Knoxville, TN; University of Tokyo, Bunkyo, Tokyo, Japan; University of Illinois at Urbana-Champaign, Champaign, IL; University of Illinois at Urbana-Champaign, Champaign, IL; Pacific Northwest National Laboratory, Richland, WA; Sandia National Laboratories, Livermore, CA; Tencent, Shenzhen, China; Costa Rica Institute of Technology, Cartago, Costa Rica; University of Illinois at Urbana-Champaign, Champaign, IL; Google, Mountain View, CA; University of Tokyo, Bunkyo, Tokyo, Japan;" Argonne National Laboratory, Lemont, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""512"",""526"",""In the past few decades, a number of user-level threading and tasking models have been proposed in the literature to address the shortcomings of OS-level threads, primarily with respect to cost and flexibility. Current state-of-the-art user-level threading and tasking models, however, either are too specific to applications or architectures or are not as powerful or flexible. In this paper, we present Argobots, a lightweight, low-level threading and tasking framework that is designed as a portable and performant substrate for high-level programming models or runtime systems. Argobots offers a carefully designed execution model that balances generality of functionality with providing a rich set of controls to allow specialization by end users or high-level programming models. We describe the design, implementation, and performance characterization of Argobots and present integrations with three high-level models: OpenMP, MPI, and colocated I/O services. Evaluations show that (1) Argobots, while providing richer capabilities, is competitive with existing simpler generic threading runtimes"; (2) our OpenMP runtime offers more efficient interoperability capabilities than production OpenMP runtimes do; (3) when MPI interoperates with Argobots instead of Pthreads, it enjoys reduced synchronization costs and better latency-hiding capabilities;" and (4) I/O services with Argobots reduce interference with colocated applications while achieving performance competitive with that of a Pthreads approach."",""1558-2183"","""",""10.1109/TPDS.2017.2766062"",""U.S. Department of Energy, Office of Science(grant numbers:DE-AC02-06CH11357)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8082139"",""Argobots";user-level thread;tasklet;OpenMP;MPI;I/O;interoperability;lightweight;context switch;"stackable scheduler"",""Runtime";Message systems;Interoperability;Context;Libraries;"Synchronization"","""",""65"","""",""58"",""IEEE"",""24 Oct 2017"","""","""",""IEEE"",""IEEE Journals"""
"AROMa: Aging-Aware Deadlock-Free Adaptive Routing Algorithm and Online Monitoring in 3D NoCs,""Z. Ghaderi"; A. Alqahtani;" N. Bagherzadeh"",""Computer Science Department, University of California, Irvine, CA"; Department of Electrical Engineering and Computer Science, University of California, Irvine, CA;" Department of Electrical Engineering and Computer Science, University of California, Irvine, CA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""772"",""788"",""The movement toward 3D fabrication coupled with Network-on-Chip (NoC) aims to improve area, performance, power, and scalability of many-core systems. However, reliability issue as a perpetual challenge in advanced silicon technology imperils it. Aging is an emerging reliability concern, which degrades the system's performance and causes timing failure eventually. Bias-Temperature-Instability (BTI) and Hot-Carrier-Injection (HCI) are the dominant aging mechanisms, which escalate in high temperature and stress (i.e., usage). In addition to the intra-layer temperature variations, 3D NoCs experience inter-layer temperature variations, which demand necessary investigations for aging as compared to 2D NoC. In this paper, we propose AROMa, an aging-aware deadlock-free adaptive routing algorithm integrated with a novel online aging monitoring system for 3D NoCs. The monitoring system in AROMa exploits Distributed-Centralized-Aging-Table (D-CAT) to obtain routers' aging rates for each layer of 3D NoCs periodically. Consequently, AROMa swaps between different k-best source-destination shortest paths periodically to avoid highly aged routers, force them in recovery phase of BTI, and accordingly balance aging in the network. We prove that AROMa is deadlock free. Our extensive experimental analysis using gem5 full system mode for PARSEC and SPLASH-2 benchmark suites concludes that AROMa outperforms state-of-the-art works while improving age imbalance by 70 percent and maximum age by 35 percent in 3D NoC with negligible overheads."",""1558-2183"","""",""10.1109/TPDS.2017.2780173"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8166793"",""Aging";deadlock freedom;adaptive routing algorithm;online monitoring;"3D network-on-chips (NoC)"",""Aging";Three-dimensional displays;Stress;Routing;Monitoring;Two dimensional displays;"Transistors"","""",""5"","""",""68"",""IEEE"",""6 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Asynchronous and Exact Forward Recovery for Detected Errors in Iterative Solvers,""L. Jaulmes"; M. Moretó; E. Ayguadé; J. Labarta; M. Valero;" M. Casas"",""Barcelona Supercomputing Center (BSC), Barcelona, Spain"; Barcelona Supercomputing Center (BSC), Barcelona, Spain; Barcelona Supercomputing Center (BSC), Barcelona, Spain; Barcelona Supercomputing Center (BSC), Barcelona, Spain; Barcelona Supercomputing Center (BSC), Barcelona, Spain;" Barcelona Supercomputing Center, Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""1961"",""1974"",""Current trends and projections show that faults in computer systems become increasingly common. Such errors may be detected, and possibly corrected transparently, e.g., by Error Correcting Codes (ECC). For a program to be fault-tolerant, it needs to also handle the Errors that are Detected and Uncorrected (DUE), such as an ECC encountering too many bit flips in a codeword. While correcting an error has an overhead in itself, it can also affect the progress of a program. The most generic technique, rolling back the program state to a previously taken checkpoint, sets back any progress done since then. Alternately, application specific techniques exist, such as restarting an iterative program with its latest iteration's values as initial guess. We introduce a novel error correction technique for iterative linear solvers, designed to preserve both the progress made and the solver's future convergence by recovering the program's state exactly. Leveraging the asynchrony of task-based programming models, we mask our technique's overhead by overlapping error correction with the solver's normal workload. Our technique relies on analysing solvers to find redundancy in the form of relations between data. We are then able to restore discarded or corrupted data by recomputing or inverting the appropriate relations. We demonstrate that this approach allows to recover any part of three widely used Krylov subspace methods: CG, GMRES and BiCGStab, and their pre-conditioned versions. We implement our technique for CG and recover lost data at the scale of a memory page, which is the granularity at which Operating Systems (OS) report memory errors on commodity hardware, and study the effect of varying the memory page size to address non-standard sizes and the possible use of huge pages in High Performance Computing (HPC). When compared to checkpointing and to the state-of-the-art algorithmic restart technique, on small (8 cores) to large scale (1024 cores), our methods show less overhead. A trade-off arises between our straightforward and asynchronous approaches, based on the rate at which faults happen. At the lowest considered rate and page size, overlapping recoveries decreases their average cost from 5.40 to 2.24 percent of the ideal faultless execution time. Our methods generally outperform the state-of-the-art even with increased overheads on big page sizes, and perform similarly on edge cases. These results also indicate that our techniques are increasingly efficient as the matrix size increases."",""1558-2183"","""",""10.1109/TPDS.2018.2817524"",""European Research Council(grant numbers:321253)"; Spanish Ministry of Science and Innovation(grant numbers:TIN2015-65316-P); Spanish Ministry of Education, Culture and Sports(grant numbers:FPU2013/06982); Spanish Ministry of Economy and Competitiveness(grant numbers:JCI-2012-15047); Secretary for Universities and Research of the Ministry of Economy and Knowledge of the Government of Catalonia and the Co-fund programme of the Marie Curie Actions of the European Union's 7th FP(grant numbers:2013 BP_B 00243);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320336"",""Resilience";detected uncorrected errors;exact recovery;Krylov-subspace methods;"HPC"",""Error correction codes";Hardware;Redundancy;Programming;Program processors;"Registers"","""",""3"","""",""42"",""IEEE"",""20 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Asynchronous Task-Based Polar Decomposition on Single Node Manycore Architectures,""D. Sukkari"; H. Ltaief; M. Faverge;" D. Keyes"",""Division of Computer, Electrical, and Mathematical Sciences and Engineering, Extreme Computing Research Center, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"; Division of Computer, Electrical, and Mathematical Sciences and Engineering, Extreme Computing Research Center, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Bordeaux INP, CNRS, INRIA et Université de Bordeaux, Talence, France;" Division of Computer, Electrical, and Mathematical Sciences and Engineering, Extreme Computing Research Center, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""312"",""323"",""This paper introduces the first asynchronous, task-based formulation of the polar decomposition and its corresponding implementation on manycore architectures. Based on a new formulation of the iterative QR dynamically-weighted Halley algorithm (QDWH) for the calculation of the polar decomposition, the proposed implementation replaces the original and hostile LU factorization for the condition number estimator by the more adequate QR factorization to enable software portability across various architectures. Relying on fine-grained computations, the novel task-based implementation is also capable of taking advantage of the identity structure of the matrix involved during the QDWH iterations, which decreases the overall algorithmic complexity. Furthermore, the artifactual synchronization points have been weakened compared to previous implementations, unveiling look-ahead opportunities for better hardware occupancy. The overall QDWH-based polar decomposition can then be represented as a directed acyclic graph (DAG), where nodes represent computational tasks and edges define the inter-task data dependencies. The StarPU dynamic runtime system is employed to traverse the DAG, to track the various data dependencies and to asynchronously schedule the computational tasks on the underlying hardware resources, resulting in an out-of-order task scheduling. Benchmarking experiments show significant improvements against existing state-of-the-art high performance implementations (i.e., Intel MKL and Elemental) for the polar decomposition on latest shared-memory vendors' systems (i.e., Intel Haswell/Broadwell/Knights Landing, NVIDIA K80/P100 GPUs and IBM Power8), while maintaining high numerical accuracy."",""1558-2183"","""",""10.1109/TPDS.2017.2755655"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8053812"",""Polar decomposition";asynchronous execution;dynamic runtime system;fine-grained execution;directed acyclic graph;"high performance computing"",""Heuristic algorithms";Hardware;Software algorithms;Matrix decomposition;Complexity theory;"Computer architecture"","""",""8"","""",""50"",""IEEE"",""29 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Auditing Big Data Storage in Cloud Computing Using Divide and Conquer Tables,""M. Sookhak"; F. R. Yu;" A. Y. Zomaya"",""Department of System and Computer Engineering, Carleton University, Ottawa, ON, Canada"; Department of System and Computer Engineering, Ottawa, ON, Canada;" School of Information Technologies, Building J12, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""999"",""1012"",""Cloud computing has arisen as the mainstream platform of utility computing paradigm that offers reliable and robust infrastructure for storing data remotely, and provides on demand applications and services. Currently, establishments that produce huge volume of sensitive data, leverage data outsourcing to reduce the burden of local data storage and maintenance. The outsourced data, however, in the cloud are not always trustworthy because of the inadequacy of physical control over the data for data owners. To better streamline this issue, scientists have now focused on relieving the security threats by designing remote data checking (RDC) techniques. However, the majority of these techniques are inapplicable to big data storage due to incurring huge computation cost on the user and cloud sides. Such schemes in existence suffer from data dynamicity problem from two sides. First, they are only applicable for static archive data and are not subject to audit the dynamic outsourced data. Second, although, some of the existence methods are able to support dynamic data update, increasing the number of update operations impose high computation and communication cost on the auditor due to maintenance of data structure, i.e., merkle hash tree. This paper presents an efficient RDC method on the basis of algebraic properties of the outsourced files in cloud computing, which inflicts the least computation and communication cost. The main contribution of this paper is to present a new data structure, called Divide and Conquer Table (D&CT), which proficiently supports dynamic data for normal file sizes. Moreover, this data structure empowers our method to be applicable for large-scale data storage with minimum computation cost. The one-way analysis of variance shows that there are significant differences between the proposed method and the existing methods in terms of the computation and communication cost on the auditor and cloud."",""1558-2183"","""",""10.1109/TPDS.2017.2784423"",""Natural Sciences and Engineering Research Council (NSERC) of Canada";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8219724"",""Cloud computing";algebraic signature;remote data checking;dynamic data update;data integrity;"big data storage"",""Cloud computing";Data structures;Servers;Security;Organizations;Big Data;"Distributed databases"","""",""50"","""",""32"",""IEEE"",""18 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Automatic Detection of Large Extended Data-Race-Free Regions with Conflict Isolation,""A. Jimborean"; P. Ekemark; J. Waern; S. Kaxiras;" A. Ros"",""Department of Information Technology, Uppsala University, Uppsala, Sweden"; Department of Information Technology, Uppsala University, Uppsala, Sweden; Department of Information Technology, Uppsala University, Uppsala, Sweden; Department of Information Technology, Uppsala University, Uppsala, Sweden;" Computer Engineering Department, University of Murcia, Murcia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""527"",""541"",""Data-race-free (DRF) parallel programming becomes a standard as newly adopted memory models of mainstream programming languages such as C++ or Java impose data-race-freedom as a requirement. We propose compiler techniques that automatically delineate extended data-race-free (xDRF) regions, namely regions of code that provide the same guarantees as the synchronization-free regions (in the context of DRF codes). xDRF regions stretch across synchronization boundaries, function calls and loop back-edges and preserve the data-race-free semantics, thus increasing the optimization opportunities exposed to the compiler and to the underlying architecture. We further enlarge xDRF regions with a conflict isolation (CI) technique, delineating what we call xDRF-CI regions while preserving the same properties as xDRF regions. Our compiler (1) precisely analyzes the threads' memory accessing behavior and data sharing in shared-memory, general-purpose parallel applications, (2) isolates data-sharing and (3) marks the limits of xDRF-CI code regions. The contribution of this work consists in a simple but effective method to alleviate the drawbacks of the compiler's conservative nature in order to be competitive with (and even surpass) an expert in delineating xDRF regions manually. We evaluate the potential of our technique by employing xDRF and xDRF-CI region classification in a state-of-the-art, dual-mode cache coherence protocol. We show that xDRF regions reduce the coherence bookkeeping and enable optimizations for performance (6.4 percent) and energy efficiency (12.2 percent) compared to a standard directory-based coherence protocol. Enhancing the xDRF analysis with the conflict isolation technique improves performance by 7.1 percent and energy efficiency by 15.9 percent."",""1558-2183"","""",""10.1109/TPDS.2017.2771509"",""Swedish Research Council(grant numbers:2016-05086)"; Spanish MINECO; European Commission FEDER funds(grant numbers:TIN2015-66972-C5-3-R); Fundación Séneca; “Jóvenes Líderes en Investigación”(grant numbers:18956/JLI/13);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8100964"",""Compile-time analysis";inter-procedural analysis;inter-thread analysis;data sharing;data races;"cache coherence"",""Synchronization";Coherence;Optimization;Instruction sets;Protocols;Standards;"Semantics"","""",""1"","""",""42"",""IEEE"",""9 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"BenchBox: A User-Driven Benchmarking Framework for Fat-Client Storage Systems,""R. Gracia-Tinedo"; C. Zou; M. Sánchez-Artigas;" P. Garcıa-Lopez"",""Department of Computer Engineering and Maths, Universitat Rovira i Virgili, Tarragona, Spain"; Department of Computer Engineering and Maths, Universitat Rovira i Virgili, Tarragona, Spain; Department of Computer Engineering and Maths, Universitat Rovira i Virgili, Tarragona, Spain;" Department of Computer Engineering and Maths, Universitat Rovira i Virgili, Tarragona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2191"",""2205"",""In many online storage services, end-users mainly interact with the system via “fat” storage clients that integrate complex functionality. This means that to obtain a complete performance evaluation of one of such systems we may need to generate workloads on the client side that reproduce the behavior of real users. Unfortunately, this remains as an open research challenge today. We present BenchBox: A distributed performance evaluation framework for fat-client storage systems. On the one hand, BenchBox can generate workloads directly in storage clients that mimic users exhibiting a certain behavior, namely, user stereotypes. To this end, the framework enables to plug-in workload models and feed them with compact recipes that capture the behavior of user stereotypes (e.g., storage activity, type of file contents, data sharing links). On the other hand, BenchBox provides researchers with management and monitoringfacilities to deploy experiments and analyze the performance of groups of storage clients. To demonstrate our framework, we equipped BenchBox with a 2-layer workload modelthat reproduces both the activity-e.g., types of operations, frequency- and data-e.g., file sizes, data types-of users in a Personal Cloud. We used this model to generate workloads based on user stereotypes that we identified in real traces (UbuntuOne). Our experiments with public providers show how distinct types of users impact on the performance and efficiency of Personal Clouds, which may guide their optimization."",""1558-2183"","""",""10.1109/TPDS.2018.2819657"",""EU projects CloudSpaces(grant numbers:317555)"; IOStack(grant numbers:644182); Spanish Ministry of Science and Innovation research projects; Cloud Services and Community Clouds(grant numbers:TIN2013-47245-C2-2-R); Software-Defined Edge Clouds(grant numbers:TIN2016-77836-C2-1-R);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8325508"",""Performance evaluation";user modeling;client-centric benchmarking;"personal clouds"",""Benchmark testing";Software;Fats;Tools;Monitoring;Data models;"Performance evaluation"","""",""1"","""",""62"",""IEEE"",""26 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"BiGNoC: Accelerating Big Data Computing with Application-Specific Photonic Network-on-Chip Architectures,""S. V. R. Chittamuru"; D. Dang; S. Pasricha;" R. Mahapatra"",""Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, Colorado"; Department of Computer Science and Engineering, Texas A&M University, College Station, Texas; Department of Electrical and Computer Engineering, Colorado State University, Fort Collins, Colorado;" Department of Computer Science and Engineering, Texas A&M University, College Station, Texas"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2402"",""2415"",""In the era of big data, high performance data analytics applications are frequently executed on large-scale cluster architectures to accomplish massive data-parallel computations. Often, these applications involve iterative machine learning algorithms to extract information and make predictions from large data sets. Multicast data dissemination is one of the major performance bottlenecks for such data analytics applications in cluster computing, as terabytes of data need to be distributed frequently from a single data source to hundreds of computing nodes. To overcome this bottleneck for big data applications, we propose BiGNoC, a manycore chip platform with a novel application-specific photonic network-on-chip (PNoC) fabric. BiGNoC is designed for big data computing and exploits multicasting in photonic waveguides. For high performance data analytics applications,  BiGNoC improves throughput by up to ${{9.9}}\times$  while reducing latency by up to 88 percent and energy-per-bit by up to 98 percent over two state-of-the-art PNoC architectures as well as a broadcast-optimized electrical mesh NoC architecture, and a traditional electrical mesh NoC architecture."",""1558-2183"","""",""10.1109/TPDS.2018.2833876"",""SRC"; NSF(grant numbers:CCF-1252500,CCF-1302693); AFOSR(grant numbers:FA9550-13-1-0110);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8356088"",""Photonic networks-on-chip";photonic channel sharing;big data computing;"multicasting"",""Photonics";Computer architecture;Big Data;Data analysis;Optical waveguides;Multicast communication;"Manganese"","""",""18"","""",""43"",""IEEE"",""8 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Blocking Analysis for Spin Locks in Real-Time Parallel Tasks,""S. Dinh"; J. Li; K. Agrawal; C. Gill;" C. Lu"",""Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO"; Department of Computer Science, New Jersey Institute of Technology, Newark, NJ; Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO; Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO;" Department of Computer Science and Engineering, Washington University in St. Louis, St. Louis, MO"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""789"",""802"",""In recent years, there has been significant interest in developing real-time schedulers for parallel tasks. Most of that research has concentrated on idealized task models where tasks do not access any shared resources protected with locks. In this paper, we consider the problem of scheduling parallel tasks which experience contention due to shared resources. In particular, we provide a schedulability test for federated scheduling by deriving blocking time analyses for parallel tasks that access shared resources protected by FIFO-ordered and priority-ordered spin locks. Our numerical evaluation on randomly generated task sets indicates that priority-ordered locks generally provide better schedulability results than FIFO-ordered locks. We also incorporated both FIFO-ordered and priority-ordered spin lock implementations into a federated scheduling platform, which is able to schedule parallel tasks written with OpenMP. Via empirical evaluations, we found that priority-ordered locks also have better performance than FIFO-ordered locks in practice."",""1558-2183"","""",""10.1109/TPDS.2017.2777454"",""NSF(grant numbers:CCF-1337218,CNS-1329861)"; ONR(grant numbers:N000141612108);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8122044"",""Real-time synchronization";spin locks;parallel scheduling;"blocking analysis"",""Real-time systems";Schedules;Synchronization;Computational modeling;Numerical models;"Processor scheduling"","""",""21"","""",""40"",""IEEE"",""28 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cache-Oblivious MPI All-to-All Communications Based on Morton Order,""S. Li"; Y. Zhang;" T. Hoefler"",""State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Beijing, China"; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Beijing, China;" Department of Computer Science, ETH Zurich, Zurich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""542"",""555"",""Many-core systems with a rapidly increasing number of cores pose a significant challenge to parallel applications to use their complex memory hierarchies efficiently. Many such applications rely on collective communications in performance-critical phases, which become a bottleneck if they are not optimized. We address this issue by proposing cache-oblivious algorithms for MPI_Alltoall, MPI_Allgather, and the MPI neighborhood collectives to exploit the data locality. To implement the cache-oblivious algorithms, we allocate the send and receive buffers on a shared heap and use Morton order to guide the memory copies. Our analysis shows that our algorithm for MPI_Alltoall is asymptotically optimal. We show an extension to our algorithms to minimize the communication distance on NUMA systems while maintaining optimality within each socket. We further demonstrate how the cache-oblivious algorithms can be applied to multi-node machines. Experiments are conducted on different many-core architectures. For MPI_Alltoall, our implementation achieves on average 1.40X speedup over the naive implementation based on shared heap for small and medium block sizes (less than 16 KB) on a Xeon Phi KNC, achieves on average 3.03X speedup over MVAPICH2 on a Xeon E7-8890, and achieves on average 2.23X speedup over MVAPICH2 on a 256-node Xeon E5-2680 cluster for block sizes less than 1 KB."",""1558-2183"","""",""10.1109/TPDS.2017.2768413"",""National Natural Science Foundation of China(grant numbers:61502450,61432018,61521092)"; National Key R&D Program of China(grant numbers:2016YFB0200800,2017YFB0202302);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8091010"",""cache-oblivious algorithms";collective communication;NUMA;MPI_Alltoall;MPI_Allgather;"neighborhood collectives"",""Algorithm design and analysis";Arrays;Electronic mail;Heuristic algorithms;Random access memory;"Complexity theory"","""",""19"","""",""39"",""IEEE"",""31 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Capacity Optimization for Resource Pooling in Virtualized Data Centers with Composable Systems,""A. -D. Lin"; C. -S. Li; W. Liao;" H. Franke"",""Graduate Institute of Communication Engineering, National Taiwan University, Taipei, Taiwan"; Accenture, San Jose, CA; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan;" IBM Thomas J. Watson Research Center, Yorktown Heights, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""324"",""337"",""Recent research trends exhibit a growing imbalance between the demands of tenants' software applications and the provisioning of hardware resources. Misalignment of demand and supply gradually hinders workloads from being efficiently mapped to fixed-sized server nodes in traditional data centers. The incurred resource holes not only lower infrastructure utilization but also cripple the capability of a data center for hosting large-sized workloads. This deficiency motivates the development of a new rack-wide architecture referred to as the composable system. The composable system transforms traditional server racks of static capacity into a dynamic compute platform. Specifically, this novel architecture aims to link up all compute components that are traditionally distributed on traditional server boards, such as central processing unit (CPU), random access memory (RAM), storage devices, and other application-specific processors. By doing so, a logically giant compute platform is created and this platform is more resistant against the variety of workload demands by breaking the resource boundaries among traditional server boards. In this paper, we introduce the concepts of this reconfigurable architecture and design a framework of the composable system for cloud data centers. We then develop mathematical models to describe the resource usage patterns on this platform and enumerate some types of workloads that commonly appear in data centers. From the simulations, we show that the composable system sustains nearly up to 1.6 times stronger workload intensity than that of traditional systems and it is insensitive to the distribution of workload demands. This demonstrates that this composable system is indeed an effective solution to support cloud data center services."",""1558-2183"","""",""10.1109/TPDS.2017.2757479"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8052507"",""Performance analysis";disaggregated data center architecture;"composable data center"",""Computer architecture";Servers;Cloud computing;Data models;Mathematical model;"Bandwidth"","""",""11"","""",""57"",""IEEE"",""28 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Coalescing and Deduplicating Incremental Checkpoint Files for Restore-Express Multi-Level Checkpointing,""P. Sigdel";" N. -F. Tzeng"",""School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA";" School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2713"",""2727"",""In multicore systems, a large portion of checkpoint time overhead can be hidden from the execution critical path by resorting to a dedicated checkpointing thread run concurrently with regular execution threads for compressing checkpoint files to lower checkpointing overhead. On the other hand, the restore time is on the critical path that cannot be hidden, making it most important to accelerate execution restore upon failures. This work pursues a restore-express (REX) strategy for multi-level checkpointing (MLC), applicable to any incremental checkpointing (IC). Oblivious to application codes, REX employs adaptive IC (AIC) for local (L1) checkpointing and follows our runtime control for second-level (L2) checkpointing, with its aim at express restore from failures while holding down the overall execution time. It takes advantage of two unique insights for overhead reduction: (1) the modified pages of an incremental checkpoint file are likely to exist in a subsequent checkpoint file, and (2) many data patterns (on an average, some 40 percent of them) stay unchanged from one L2 checkpoint file to the next. These insights enable REX to (1) coalesce IC files (by involving only the last copy of every dirty page among files) and (2) boost file compression across multiple L2 checkpoints. Time and storage overhead results of REX during normal job execution are gathered for 16 benchmarks from SPEC, PARSEC, and NPB suites. The evaluation outcomes of the execution restore time confirm that REX is fast and able to quicken restore by a factor of 4.5× when compared with its IC counterpart (without utilizing the unique insights), while incurring same execution time overhead."",""1558-2183"","""",""10.1109/TPDS.2018.2844210"",""National Science Foundation(grant numbers:CNS-1527051)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8372629"",""Compression and deduplication";execution restore;hash functions;incremental checkpointing;"multi-level checkpointing"",""Checkpointing";Integrated circuits;Benchmark testing;Acceleration;Multicore processing;"Runtime"","""",""7"","""",""40"",""IEEE"",""5 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CoCloud: Enabling Efficient Cross-Cloud File Collaboration Based on Inefficient Web APIs,""J. E."; Y. Cui; P. Wang; Z. Li;" C. Zhang"",""Tsinghua University, Beijing, Beijing, CN"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; School of Software, TNLIST;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""56"",""69"",""Cloud storage services such as Dropbox have been widely used for file collaboration among multiple users. However, this desirable functionality is yet restricted to the “walled-garden” of each service. At present, the only feasible approach to cross-cloud file collaboration seems to be using web APIs, whose performance is known to be highly unstable and unpredictable. Now that using inefficient web APIs is inevitable, in this paper we attempt to achieve sound user-perceived performance for cross-cloud file collaboration. This attempt is enabled by two key observations from real-world measurements. First, for each cloud, we are always able to deploy one or several nearby (client) proxies which can efficiently access the web APIs. Second, during file collaboration, significant similarity exists among different versions of a file. This can be exploited to substantially reduce inter-proxy traffic and thus shorten the data sync time. Guided by the observations, we design and implement an open-source prototype system called CoCloud. Currently, it supports file collaboration among four popular cloud storage services in the US and China. Its performance is well acceptable to users under representative workloads, even approaching or exceeding that of intra-cloud collaboration in many cases."",""1558-2183"","""",""10.1109/TPDS.2017.2750161"",""National High-Technology Research and Development Program(grant numbers:2015AA016101,2015AA01A201)"; National Natural Science Foundation of China(grant numbers:61422206); Tsinghua University Initiative Scientific Research Program(grant numbers:2014Z09103); CCF-Tencent Open Fund(grant numbers:AGR20160105);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8030092"",""Cloud storage";cloud computing;cross-cloud file collaboration;"transfer efficiency"",""Collaboration";Cloud computing;Synchronization;Servers;Google;Protocols;"Uniform resource locators"","""",""8"","""",""39"",""IEEE"",""8 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CoMan: Managing Bandwidth Across Computing Frameworks in Multiplexed Datacenters,""W. Li"; D. Guo; A. X. Liu; K. Li; H. Qi; S. Guo; A. Munir;" X. Tao"",""School of Computer Science and Technology, Dalian University of Technology, Dalian, China"; College of System Engineering, National University of Defense Technology, Changsha, P. R. China; Department of Computer Science, Michigan State University, East Lansing, MI; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; Department of Computing, The Hong Kong Polytecnic, Hung Hom, Kowloon, Hong Kong SAR; Department of Computer Science, Michigan State University, East Lansing, MI;" School of Computer Science and Technology, Dalian University of Technology, Dalian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1013"",""1029"",""Inefficient bandwidth sharing in a datacenter network, between different application frameworks, e.g., MapReduce and Spark, can lead to inelastic and skewed usage of link bandwidth and increased completion times for the applications. Existing work, however, either solely focuses on managing computation and storage resources or controlling only sending/receiving rate at hosts. In this paper, we present CoMan, a solution that provides global in-network bandwidth management in multiplexed data centers, with two goals: improving bandwidth utilization and reducing application completion time. CoMan first designs a novel abstraction of virtual link groups (VLGs) to establish a shared bandwidth resource pool. Based on this pool, CoMan implements a three-level bandwidth allocation model, which enables elastic bandwidth sharing among computing frameworks as well as guarantees network performance for the applications. CoMan further improves the bandwidth utilization by devising a VLG dependency graph and solves an optimization problem to guide the path selection using a 32-approximation algorithm. We conduct comprehensive trace-driven simulations as well as small-scale testbed experiments to evaluate the performance of CoMan. Extensive simulation results show that CoMan improves the bandwidth utilization and speeds up the application completion time by up to 2.83× and 6.68×, respectively, compared to the ECMP + ElasticSwitch solution. Our implementation also verifies that CoMan can realistically speed up the application completion times by 2.32× on average."",""1558-2183"","""",""10.1109/TPDS.2017.2788003"",""National Key Research and Development Program of China(grant numbers:2016YFB1000205)"; State Key Program of National Natural Science of China(grant numbers:61432002); NSFC(grant numbers:61672379,61772112,61370199); Dalian High-level Talent Innovation Program(grant numbers:2015R049); National Natural Science Foundation for Outstanding Excellent young scholars of China(grant numbers:61422214); National Natural Science Foundation of China(grant numbers:61772544); National Basic Research Program (973 program)(grant numbers:2014CB347800);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8241720"",""Data-parallel computing frameworks";multiplexed datacenter;"bandwidth management"",""Bandwidth";Computational modeling;Resource management;Channel allocation;Multiplexing;Processor scheduling;"Data transfer"","""",""7"","""",""57"",""IEEE"",""29 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Combining Static and Dynamic Storage Management for Data Intensive Scientific Workflows,""N. Hazekamp"; N. Kremer-Herman; B. Tovar; H. Meng; O. Choudhury; S. Emrich;" D. Thain"",""Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN"; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN;" Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""338"",""350"",""Workflow management systems are widely used to express and execute highly parallel applications. For data-intensive workflows, storage can be the constraining resource: The number of tasks running at once must be artificially limited to not overflow the space available in the filesystem. It is all too easy for a user to dispatch a workflow which consumes all available storage and disrupts all system users. To address these issues, we present a three-tiered approach to workflow storage management: (1) A static analysis algorithm which analyzes the storage needs of a workflow before execution, giving a realistic prediction of success or failure. (2) An online storage management algorithm which accounts for the storage needed by future tasks to avoid deadlock at runtime. (3) A task containment system which limits storage consumption of individual tasks, enabling the strong guarantees of the static analysis and dynamic management algorithms. We demonstrate the application of these techniques on three complex workflows."",""1558-2183"","""",""10.1109/TPDS.2017.2764897"",""Division of Advanced Cyberinfrastructure(grant numbers:OCI-1148330)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8078264"",""Workflow management";storage;workflow analysis;"storage management"",""System recovery";Storage management;Algorithm design and analysis;Heuristic algorithms;Runtime;"Static analysis"","""",""17"","""",""31"",""IEEE"",""23 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Competitiveness of a Non-Linear Block-Space GPU Thread Map for Simplex Domains,""C. A. Navarro"; M. Vernier; B. Bustos;" N. Hitschfeld"",""Institute of Informatics, Austral University of Chile, Valdivia, Región de los Ríos, Chile"; Institute of Informatics, Austral University of Chile, Valdivia, Región de los Ríos, Chile; Millenium Institute for Foundational Research on Data, University of Chile, Santiago, Chile;" Computer Science Department, University of Chile, Santiago, Chile"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2728"",""2741"",""This work presents and studies the efficiency problem of mapping GPU threads onto simplex domains. A non-linear map $\lambda (\omega)$  is formulated based on a block-space enumeration principle that reduces the number of thread-blocks by a factor of approximately $2\times$  and  $6\times$  for 2-simplex and 3-simplex domains, respectively, when compared to the standard approach. Performance results show that $\lambda (\omega)$  is competitive and even the fastest map when ran in recent GPU architectures such as the Tesla V100, where it reaches up to $1.5\times$  of speedup in 2-simplex tests. In 3-simplex tests, it reaches up to $2.3\times$  of speedup for small workloads and up to $1.25\times$  for larger ones. The results obtained make $\lambda (\omega)$  a useful GPU optimization technique with applications on parallel problems that define all-pairs, all-triplets or nearest neighbors interactions in a 2-simplex or 3-simplex domain."",""1558-2183"","""",""10.1109/TPDS.2018.2849705"",""Fondo Nacional de Desarrollo Científico y Tecnológico(grant numbers:3160182,1181506)"; Nvidia GPU Research Center from University of Chile; Millennium Institute for Foundational Research on Data (IMFD);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8392762"",""GPU thread mapping";block-space;simplex domains;"GPU optimization"",""Graphics processing units";Instruction sets;Symmetric matrices;Computer architecture;Optimization;"Programming"","""",""8"","""",""34"",""IEEE"",""22 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Computing Hierarchical Summary from Two-Dimensional Big Data Streams,""Z. Shah"; A. N. Mahmood; M. Barlow; Z. Tari; X. Yi;" A. Y. Zomaya"",""Centre for Health Informatics at the Australian Institute of Health Innovation, Macquarie University, New South Wales, Australia"; Department of Computer Science, La Trobe University, Melbourne, Victoria, Australia; School of Engineering and IT, University of New South Wales Canberra, Bruce, ACT, Australia; School of Computer Science and Information Technology, Royal Melbourne Institute of Technology, Melbourne, Victoria, Australia; School of Computer Science and Information Technology, Royal Melbourne Institute of Technology, Melbourne, Victoria, Australia;" Centre for Distributed and High Performance Computing, the University of Sydney, Camperdown, New South Wales, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""803"",""818"",""There are many application domains, where hierarchical data is inherent, but surprisingly, there are few techniques for mining patterns from such important data. Hierarchical Heavy Hitters (HHH) and multilevel and Cross-Level Association Rules (CLAR) mining are well-known hierarchical pattern mining techniques. The problem in these techniques"; however, is that they focus on capturing only global patterns from data but cannot identify local contextual patterns. Another problem in these techniques is that they treat all data items in the transaction equally and do not consider the sequential nature of the relationship among items within a transaction;" hence, they cannot capture the correlation semantic within the transactions of the data items. There are many applications such as clickstream mining, healthcare data mining, network monitoring, and recommender systems, which require to identify local contextual patterns and correlation semantics. In this work, we introduce a new concept, which can capture the sequential nature of the relationship between pairs of hierarchical items at multiple concept levels and can capture local contextual patterns within the context of the global patterns. We call this notion Hierarchically Correlated Heavy Hitters (HCHH). Specifically, the proposed approach finds the correlation between items corresponding to hierarchically discounted frequency counts. We have provided formal definitions of the proposed concept and developed algorithmic approaches for computing HCHH in data streams efficiently. The proposed HCHH algorithm have deterministic error guarantees, and space bounds. It requires O(η/ϵpϵs) memory, where h is a small constant, and ϵp ∈ [0,1], ϵs ∈ [0,1] are user defined parameters on upper bounds of estimation error. We have compared the proposed HCHH concept with existing hierarchical pattern mining approaches both theoretically as well as experimentally."",""1558-2183"","""",""10.1109/TPDS.2017.2778734"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8125170"",""Hierarchical patterns";association rules;"frequent patterns"",""Data mining";Correlation;Footwear;Algorithm design and analysis;IP networks;Monitoring;"Semantics"","""",""6"","""",""34"",""IEEE"",""30 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Confluence: Speeding Up Iterative Distributed Operations by Key-Dependency-Aware Partitioning,""F. Liang"; F. C. M. Lau; H. Cui;" C. -L. Wang"",""Department of Computer Science, The University of Hong Kong, Hong Kong, China"; Department of Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Science, The University of Hong Kong, Hong Kong, China;" Department of Computer Science, The University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""351"",""364"",""A typical shuffle operation randomly partitions data on many computers, generating possibly a significant amount of network traffic which often dominates a job's completion time. This traffic is particularly pronounced in iterative distributed operations where each iteration invokes a shuffle operation. We observe that data of different iterations are related according to the transformation logic of distributed operations. If data generated by the current iteration are partitioned to the computers where they will be processed in the next iteration, unnecessary shuffle network traffic between the two iterations can be prevented. We model general iterative distributed operations as the transform-and-shuffle primitive and define a powerful notion named Confluence key dependency to precisely capture the data relations in the primitive. We further find that by binding key partitions between different iterations based on the Confluence key dependency, the shuffle network traffic can always be reduced by a predictable percentage. We implemented the Confluence system. Confluence provides a simple interface for programmers to express the Confluence key dependency, based on which Confluence automatically generates efficient key partitioning schemes. Evaluation results on diverse real-life applications show that Confluence greatly reduces the shuffle network traffic, resulting in as much as 23 percent job completion time reduction."",""1558-2183"","""",""10.1109/TPDS.2017.2756054"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8049395"",""Spark";shuffle;key dependency;iterative distributed operation;"partitioning"",""Distributed databases";Transforms;Computers;Sparks;Computational modeling;Data models;"Indexes"","""",""1"","""",""43"",""IEEE"",""25 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Context-Aware Task Migration for HART-Centric Collaboration over FiWi Based Tactile Internet Infrastructures,""M. Chowdhury"; E. Steinbach; W. Kellerer;" M. Maier"",""Optical Zeitgeist Laboratory, Institut National de la Recherche Scientifique (INRS), Montréal, QC, Canada"; Technical University of Munich, Munich, Germany; Technical University of Munich, Munich, Germany;" Optical Zeitgeist Laboratory, Institut National de la Recherche Scientifique (INRS), Montréal, QC, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1231"",""1246"",""Low task execution time and low energy consumption of collaborating mobile human users and robots are important requirements of emerging human-agent-robot teamwork (HART)-centric Tactile Internet applications. In particular, task migration among mobile HART members has emerged as an important research topic, taking different task types, task deadlines, collaborative node capabilities, and mobility patterns into account. We propose a context-aware task migration scheme for efficiently orchestrating the real-time collaboration among human mobile users, central and decentralized computational agents (cloud/cloudlets), and collaborative robots (cobots) across converged fiber-wireless (FiWi) communications infrastructures. We investigate the problem of whether and, if so, when and where a HART-centric task should be best migrated to. For resource-efficient task execution, the migration decision is made according to given task processing capabilities of cloud/cloudlet agents and cobots, task execution deadline, energy consumption of involved cobots and mobile devices, and task migration latency. We evaluate the performance of our context-aware HART-centric task migration scheme and compare it to conventional task execution without migration. Towards this end, we develop an analytical framework for quantifying its performance in terms of a variety of task migration key performance metrics, including task migration gain-overhead ratio, deadline-miss ratio, task response time, and energy consumption efficiency."",""1558-2183"","""",""10.1109/TPDS.2018.2791406"",""Alexander von Humboldt (AvH) Foundation"; NSERC Discovery(grant numbers:2016-04521);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8252736"",""Cloudlet";collaborative computing;FiWi enhanced networks;human-agent-robot teamwork (HART);Tactile Internet;"task migration"",""Cloud computing";Collaboration;Mobile communication;Energy consumption;"Robots"","""",""23"","""",""22"",""IEEE"",""9 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Core Maintenance in Dynamic Graphs: A Parallel Approach Based on Matching,""H. Jin"; N. Wang; D. Yu; Q. -S. Hua; X. Shi;" X. Xie"",""Services Computing Technology and System Lab, Huazhong University of Science and Technology, 1037 Luoyu Road, Wuhan, P.R. China"; Services Computing Technology and System Lab, Huazhong University of Science and Technology, 1037 Luoyu Road, Wuhan, P.R. China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, 1037 Luoyu Road, Wuhan, P.R. China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, 1037 Luoyu Road, Wuhan, P.R. China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, 1037 Luoyu Road, Wuhan, P.R. China;" Services Computing Technology and System Lab, Huazhong University of Science and Technology, 1037 Luoyu Road, Wuhan, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2416"",""2428"",""The core number of vertices is a basic index depicting cohesiveness of a graph, and has been widely used in large-scale graph analytics. In this paper, we study the update of core numbers of vertices in dynamic graphs with edge insertions/deletions, which is known as the core maintenance problem. Different from previous approaches that just focus on the case of single-edge insertion/deletion and sequentially handle the edges when multiple edges are inserted/deleted, we investigate the parallelism in the core maintenance procedure. Specifically, we show that if the inserted/deleted edges constitute a matching, the core number update with respect to each inserted/deleted edge can be handled in parallel. Based on this key observation, we propose parallel algorithms for core maintenance in both cases of edge insertions and deletions. Extensive experiments are conducted to evaluate the efficiency, stability, parallelism and scalability of our algorithms on different types of real-world, synthetic graphs and temporal networks. Comparing with former approaches, our algorithms can improve the core maintenance efficiency significantly."",""1558-2183"","""",""10.1109/TPDS.2018.2835441"",""National Key R&D Program of China(grant numbers:2018YFB1003203)"; National Natural Science Foundation of China(grant numbers:61602195,61572216,61433019,U1435217); Natural Science Foundation of Hubei Province(grant numbers:2017CFB301); Outstanding Youth Foundation of Hubei Province(grant numbers:2016CFA032); Fundamental Research Funds; HUST;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8357916"",""Graph analysis";dynamic graph;core number maintenance;"parallel algorithm"",""Maintenance engineering";Parallel algorithms;Heuristic algorithms;Indexes;Stability analysis;"Scalability"","""",""29"","""",""30"",""IEEE"",""11 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CoreVA-MPSoC: A Many-Core Architecture with Tightly Coupled Shared and Local Data Memories,""J. Ax"; G. Sievers; J. Daberkow; M. Flasskamp; M. Vohrmann; T. Jungeblut; W. Kelly; M. Porrmann;" U. Rückert"",""Center of Excellence Cognitive Interaction Technology (CITEC), Cognitronics and Sensor Systems, Bielefeld University, Bielefeld, Germany"; dSPACE GmbH, Paderborn, Germany; Center of Excellence Cognitive Interaction Technology (CITEC), Cognitronics and Sensor Systems, Bielefeld University, Bielefeld, Germany; Center of Excellence Cognitive Interaction Technology (CITEC), Cognitronics and Sensor Systems, Bielefeld University, Bielefeld, Germany; Center of Excellence Cognitive Interaction Technology (CITEC), Cognitronics and Sensor Systems, Bielefeld University, Bielefeld, Germany; Center of Excellence Cognitive Interaction Technology (CITEC), Cognitronics and Sensor Systems, Bielefeld University, Bielefeld, Germany; Science and Engineering Faculty, Queensland University of Technology, Brisbane, QLD, Australia; Center of Excellence Cognitive Interaction Technology (CITEC), Cognitronics and Sensor Systems, Bielefeld University, Bielefeld, Germany;" Center of Excellence Cognitive Interaction Technology (CITEC), Cognitronics and Sensor Systems, Bielefeld University, Bielefeld, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1030"",""1043"",""MPSoCs with hierarchical communication infrastructures are promising architectures for low power embedded systems. Multiple CPU clusters are coupled using an Network-on-Chip (NoC). Our CoreVA-MPSoC targets streaming applications in embedded systems, like signal and video processing. In this work we introduce a tightly coupled shared data memory to each CPU cluster, which can be accessed by all CPUs of a cluster and the NoC with low latency. The main focus is the comparison of different memory architectures and their connection to the NoC. We analyze memory architectures with local data memory only, shared data memory only, and a hybrid architecture integrating both. Implementation results are presented for a 28 nm FD-SOI standard cell technology. A CPU cluster with shared memory shows similar area requirements compared to the local memory architecture. We use post place and route simulations for precise analysis of energy consumption on both cluster and NoC level using the different memory architectures. An architecture with shared data memory shows best performance results in combination with a high resource efficiency. On average, the use of shared memory shows a 17.2 percent higher throughput for a benchmark suite of 10 applications compared to the use of local memory only."",""1558-2183"","""",""10.1109/TPDS.2017.2785799"",""DFG Cluster of Excellence Cognitive Interaction Technology ‘CITEC’ (EXC 277)"; Bielefeld University; the BMBF Leading-Edge Cluster “Intelligent Technical Systems OstWestfalenLippe”;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8240949"",""Parallel architectures";multi-core/single-chip multiprocessors;on-chip interconnection networks;"memory hierarchy"",""Memory architecture";Memory management;Clocks;System-on-chip;Programming;"VLIW"","""",""17"","""",""33"",""IEEE"",""27 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cost-Efficient and Robust On-Demand Video Transcoding Using Heterogeneous Cloud Services,""X. Li"; M. A. Salehi; M. Bayoumi; N. -F. Tzeng;" R. Buyya"",""Brightcove Inc, Boston, MA"; HPCC lab., University of Louisiana at Lafayette, LA; Center for Advanced Computer Studies, University of Louisiana at Lafayette, LA; Center for Advanced Computer Studies, University of Louisiana at Lafayette, LA;" Department of Computing and Information Systems, The University of Melbourne, Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""556"",""571"",""Video streams, either in the form of Video On-Demand (VOD) or live streaming, usually have to be converted (i.e., transcoded) to match the characteristics of viewers' devices (e.g., in terms of spatial resolution or supported formats). Transcoding is a computationally expensive and time-consuming operation. Therefore, streaming service providers have to store numerous transcoded versions of a given video to serve various display devices. With the sharp increase in video streaming, however, this approach is becoming cost-prohibitive. Given the fact that viewers' access pattern to video streams follows a long tail distribution, for the video streams with low access rate, we propose to transcode them in an on-demand (i.e., lazy) manner using cloud computing services. The challenge in utilizing cloud services for on-demand video transcoding, however, is to maintain a robust QoS for viewers and cost-efficiency for streaming service providers. To address this challenge, in this paper, we present the Cloud-based Video Streaming Services (CVS2) architecture. It includes a QoS-aware scheduling component that maps transcoding tasks to the Virtual Machines (VMs) by considering the affinity of the transcoding tasks with the allocated heterogeneous VMs. To maintain robustness in the presence of varying streaming requests, the architecture includes a cost-efficient VM Provisioner component. The component provides a self-configurable cluster of heterogeneous VMs. The cluster is reconfigured dynamically to maintain the maximum affinity with the arriving workload. Simulation results obtained under diverse workload conditions demonstrate that CVS2 architecture can maintain a robust QoS for viewers while reducing the incurred cost of the streaming service provider by up to 85 percent."",""1558-2183"","""",""10.1109/TPDS.2017.2766069"",""Louisiana Board of Regents(grant numbers:LEQSF(2016-19)-RD-A-25)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8081853"",""Cloud services";heterogeneous VM provisioning;QoS-aware scheduling;"On-demand video transcoding"",""Streaming media";Transcoding;Quality of service;Robustness;Cloud computing;Spatial resolution;"Delays"","""",""55"","""",""35"",""IEEE"",""24 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CUDAMPF++: A Proactive Resource Exhaustion Scheme for Accelerating Homologous Sequence Search on CUDA-Enabled GPU,""H. Jiang"; N. Ganesan;" Y. -D. Yao"",""Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ"; Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ;" Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2206"",""2222"",""Biological sequence alignment is an important research topic in bioinformatics and continues to attract significant efforts. As biological data grow exponentially, however, most of alignment methods face challenges due to their huge computational costs. HMMER, a suite of bioinformatics tools, is widely used for the analysis of homologous protein and nucleotide sequences with high sensitivity, based on profile hidden Markov models (HMMs). Its latest version, HMMER3, introduces a heuristic pipeline to accelerate the alignment process, which is carried out on central processing units (CPUs) and highly optimized. Only a few acceleration results are reported on the basis of HMMER3. In this paper, we propose a five-tiered parallel framework, CUDAMPF++, to accelerate the most computationally intensive stages in HMMER3's pipeline, multiple/single segment Viterbi (MSV/SSV), on a single graphics processing unit (GPU) without any loss of accuracy. As an architecture-aware design, the proposed framework aims to fully utilize hardware resources via exploiting finer-grained parallelism (multi-sequence alignment) compared with its predecessor (CUDAMPF). In addition, we propose a novel method that proactively sacrifices L1 Cache Hit Ratio (CHR) to get improved performance and scalability in return. A comprehensive evaluation shows that the proposed framework outperforms all existing work and exhibits good consistency in performance regardless of the variation of query models or sequence datasets. For MSV (SSV) kernels, the peak performance of CUDAMPF++ is 283.9 (471.7) GCUPS on a single K40 GPU, and impressive speedups ranging from 1.8x (1.7×) to 168.3× (160.7×) are achieved over the CPU-based implementation (16 cores, 32 threads)."",""1558-2183"","""",""10.1109/TPDS.2018.2830393"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8350332"",""GPU";CUDA;SIMD;L1 cache;hidden Markov model;HMMER;MSV;SSV;"viterbi algorithm"",""Hidden Markov models";Graphics processing units;Pipelines;Acceleration;Instruction sets;Viterbi algorithm;"Computer architecture"","""",""2"","""",""33"",""IEEE"",""26 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Developing User Perceived Value Based Pricing Models for Cloud Markets,""P. Cong"; L. Li; J. Zhou; K. Cao; T. Wei; M. Chen;" S. Hu"",""Department of Computer Science and Technology, East China Normal University, Shanghai, China"; Department of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Department of Computer Science and Technology, East China Normal University, Shanghai, China; Department of Computer Science and Technology, East China Normal University, Shanghai, China; Shanghai Key Laboratory of Trustworthy Computing, East China Normal University, Shanghai, China;" Department of Electrical and Computer Engineering, Michigan Technological University, Houghton, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2742"",""2756"",""With the rapid deployment of cloud computing infrastructures, understanding the economics of cloud computing has become a pressing issue for cloud service providers. However, existing pricing models rarely consider the dynamic interactions between user requests and the cloud service provider. Thus, the law of supply and demand in marketing is not fully explored in these pricing models. In this paper, we propose a dynamic pricing model based on the concept of user perceived value that accurately captures the real supply and demand relationship in the cloud service market. Subsequently, a profit maximization scheme is designed based on the dynamic pricing model that optimizes profit of the cloud service provider without violating service-level agreement. Finally, a dynamic closed loop control scheme is developed to adjust the cloud service price and multiserver configurations according to the dynamics of the cloud computing environment such as fluctuating electricity and rental fees. Extensive simulations using the data extracted from real-world applications validate the effectiveness of the proposed user perceived value-based pricing model and the dynamic profit maximization scheme. Our algorithm can achieve up to 31.32 percent profit improvement compared to a state-of-the-art approach."",""1558-2183"","""",""10.1109/TPDS.2018.2843343"",""Natural Science Foundation of Shanghai(grant numbers:16ZR1409000)"; National Natural Science Foundation of China(grant numbers:61672230);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370902"",""Cloud computing";dynamic pricing model;user perceived value;profit maximization;"augmented Lagrange function"",""Cloud computing";Pricing;Computational modeling;Biological system modeling;Supply and demand;"Random variables"","""",""45"","""",""45"",""IEEE"",""4 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Convergence Detection Based on Global Residual Error Under Asynchronous Iterations,""F. Magoulès";" G. Gbikpi-Benissan"",""CentraleSupélec, Université Paris-Saclay, Gif-sur-Yvette, France";" IRT SystemX, Paris-Saclay, France"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""819"",""829"",""Convergence of classical parallel iterations is detected by performing a reduction operation at each iteration in order to compute a residual error relative to a potential solution vector. To efficiently run asynchronous iterations, blocking communication requests are avoided, which makes it hard to isolate and handle any global vector. While some termination protocols were proposed for asynchronous iterations, only very few of them are based on global residual computation and guarantee effective convergence. But the most effective and efficient existing solutions feature two reduction operations, which constitutes an important factor of termination delay. In this paper, we present new, non-intrusive, protocols to compute a residual error under asynchronous iterations, requiring only one reduction operation. Various communication models show that some heuristics can even be introduced and formally evaluated. Extensive experiments with up to 5,600 processor cores confirm the practical effectiveness and efficiency of our approach."",""1558-2183"","""",""10.1109/TPDS.2017.2780856"",""GENCI-TGCC(grant numbers:2014-t2014069065)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8169062"",""Asynchronous iterations";convergence detection;global residual;distributed snapshot;"parallel computing"",""Convergence";Protocols;Iterative methods;Computational modeling;Delays;Parallel processing;"Linear systems"","""",""19"","""",""31"",""IEEE"",""7 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Privacy-Aware Fast Selection Algorithm for Large-Scale Data,""H. Liu";" J. Chen"",""State Key Lab. of Industrial Control Technology, Zhejiang University, Hangzhou, China";" State Key Lab. of Industrial Control Technology, Zhejiang University, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""365"",""376"",""Finding the k smallest/largest element of a large array, i.e., k-selection is a fundamental supporting algorithm in data analysis. Due to the fact that big data born in geo-distributed environments, it especially requires communication-efficient distributed k-selection, besides typical computation and memory efficiency. Moreover, sensitive organizations make data privacy a rigorous precondition for their participation in such distributed statistical analysis for common profit. To this end, we propose a Distributed Privacy-Aware Median (DPAM) selection algorithm for median selection in distributed large-scale data while preserving local statistics privacy, and extend it to arbitrary k-selection. DPAM utilizes mean to approximate median, via contraction of the standard deviation. It is the theoretical fastest with a worst computation complexity of O(N), and also highly efficient in communication overhead (in logarithm of data range). To preserve ε-differential privacy of local statistics, DPAM randomly adds dummy elements (the number follows a rounded Laplacian distribution) to local data. The noise does not degrade the estimation precision or convergence rate. Performance of DPAM is compared with centralized/distributed quick select and optimization, in terms of complexity and privacy preserving ability. Extensive simulation and experiment results show the higher efficiency of DPAM."",""1558-2183"","""",""10.1109/TPDS.2017.2761344"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8063950"",""Privacy";distributed computing;selection algorithm;"distributed optimization"",""Big Data";Privacy;Algorithm design and analysis;Arrays;Standards;Data privacy;"Servers"","""",""6"","""",""35"",""IEEE"",""10 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Randomized $k$ -Clustering Based PCID Assignment for Ultra-Dense Femtocellular Networks,""A. Pratap"; R. Singhal; R. Misra;" S. K. Das"",""Department of Computer Science and Engineering, Indian Institute of Technology, Patna, Bihta, Bihar, India"; Department of Computer Science and Engineering, Indian Institute of Technology, Patna, Bihta, Bihar, India; Department of Computer Science and Engineering, Indian Institute of Technology, Patna, Bihta, Bihar, India;" Department of Computer Science, Missouri University of Science and Technology, Rolla, MO"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2018"",""2018"",""29"",""6"",""1247"",""1260"",""Next-generation wireless networks are going to have highly dense, small cell structure with a large number of femtocells. The dense deployment of the femtocell network architecture is expected to meet the growing data demand by leveraging millimeter-wave structure of 5G wireless networks. However, arbitrary deployment of large number of femtocells underlying a macrocell will pose a challenge for collision and confusion-free Physical Cell ID (PCID) assignments as the total number of available PCIDs is limited to 504. In this paper we propose a distributed, randomized k-clustering algorithm for collision and confusion-free PCID assignment problem, which is known to be NP-complete. To reduce the total control message flow, we create overlapping clusters in ultra-dense femtocellular networks, where each cluster head runs the distributed randomized PCID allocation algorithm and locally monitors the conflicts to avoid the collision and confusion constraints. We prove the correctness of our proposed algorithm and analyze its time and message complexity. Through simulation experiments, we also show the effect of different parameters on the PCID allocation objectives."",""1558-2183"","""",""10.1109/TPDS.2018.2800050"",""Council of Scientific and Industrial Research, India";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8276600"",""LTE-A";5G wireless;PCID;coloring;randomized;"clustering"",""Femtocells";Resource management;Computer architecture;Microprocessors;Femtocell networks;"Long Term Evolution"","""",""14"","""",""50"",""IEEE"",""31 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Stream Rebalance for Stateful Operator Under Workload Variance,""J. Fang"; R. Zhang; T. Z. J. Fu; Z. Zhang; A. Zhou;" X. Zhou"",""School of Computer Science and Technology, Soochow University, Suzhou Shi, Jiangsu Sheng, China"; School of Computer Science and Software Engineering, East China Normal University, Shanghai, China; Advanced Digital Sciences Center, Illinois at Singapore Pte. Ltd., Singapore; Singapore R&D, Yitu Technology Co. Ltd., Singapore; School of Data Science and Engineering, East China Normal University, Shanghai, China;" School of Computer Science and Technology, Soochow University, Suzhou Shi, Jiangsu Sheng, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2223"",""2240"",""Key-based workload partitioning is now commonly used in parallel stream processing, enabling effective key-value tuple distribution over worker threads in a logical operator. While randomized hashing on the keys is capable of balancing the workload for key-based partitioning when the keys generally follow a static distribution, it is likely to generate poor balancing performance when workload variance occurs on the incoming data stream. This paper presents a new key-based workload partitioning framework, with practical algorithms to support dynamic workload assignment for stateful operators. The framework combines hash-based and explicit key-based routing strategies for workload distribution, which specifies the destination worker threads for a handful of keys and assigns the other keys with the hashing function. We formulate the rebalance operation as an optimization problem, with multiple objectives on minimizing state migration costs, controlling the size of the routing table and breaking workload imbalance among the worker threads. Despite of the NP-hardness nature behind the optimization formulation, we carefully investigate and justify the heuristics behind key (re)routing and state migration, to facilitate fast response to workload variance with ignorable cost to the normal processing in the distributed system. Empirical studies on synthetic data and real-world stream applications validate the usefulness of our proposals and prove the huge advantage of our approaches over state-of-the-art solutions in the literature."",""1558-2183"","""",""10.1109/TPDS.2018.2827380"",""National Science Foundation of China (NSFC)(grant numbers:61332006,61672233)"; China Postdoctoral Science Foundation (CPSF)(grant numbers:2017M621813); National Research Foundation, Prime Minister’s Office, Singapore; NSFC(grant numbers:61702113); CPSF(grant numbers:2017M612613); NSFC(grant numbers:61772356);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8340854"",""Distributed stream processing";stateful operation;workload variance;load balance;"adjustment"",""Task analysis";Routing;Engines;Optimization;Instruction sets;Proposals;"Distributed databases"","""",""10"","""",""31"",""IEEE"",""18 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Dynamic Adaptable Asynchronous Progress Model for MPI RMA Multiphase Applications,""M. Si"; A. J. Peña; J. Hammond; P. Balaji; M. Takagi;" Y. Ishikawa"",""Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL"; Barcelona Supercomputing Center (BSC), Barcelona, Spain; Intel Corporation, Santa Clara, CA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL; Riken Advanced Institute for Computational Science, Kobe, Japan;" Riken Advanced Institute for Computational Science, Kobe, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""1975"",""1989"",""Casper is a process-based asynchronous progress model for MPI one-sided communication on multi- and many-core architectures. The one-sided communication is not truly one-sided in most MPI implementations: the target process still relies on software progress to complete incoming operations. Casper allows the user to specify an arbitrary number of cores dedicated to background ghost processes and transparently redirects the RMA operations to ghost processes by utilizing the PMPI redirection and MPI-3 shared-memory technologies. Although Casper benefits applications that suffer from lack of asynchronous progress, the operation redirection design might not support complex multiphase applications effectively, which often involve dynamically changing communication density and computing workloads. In this paper, we present an adaptive mechanism in Casper to address the limitation of static asynchronous progress in multiphase applications. We exploit two adaptive strategies, a user-guided strategy and a fully transparent and automatic strategy based on self-profiling and prediction, to dynamically reconfigure the asynchronous progress in Casper according to real-time performance characteristics during multiphase execution. We evaluate the adaptive approaches in both microbenchmarks and a real quantum chemistry application suite, NWChem, on the Cray XC30 supercomputer and an Intel Omni-Path cluster."",""1558-2183"","""",""10.1109/TPDS.2018.2815568"",""U.S. Dept. of Energy, Office of Science, Advanced Scientific Computing Research (SC-21)(grant numbers:DE-AC02-06CH11357)"; National Energy Research Scientific Computing Center (NERSC); Laboratory Computing Resource Center on the Bebop cluster at Argonne National Laboratory; Spanish Ministry of Economy and Competitiveness(grant numbers:IJCI-2015-23266);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8315136"",""MPI";multiphase;one-sided;RMA;adaptation;"asynchronous progress"",""Computational modeling";Runtime;Hardware;Semantics;Message systems;Software;"Task analysis"","""",""5"","""",""32"",""IEEE"",""13 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Dynamic Resource Scheduling in Mobile Edge Cloud with Cloud Radio Access Network,""X. Wang"; K. Wang; S. Wu; S. Di; H. Jin; K. Yang;" S. Ou"",""Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China"; Department of Computer and Information Sciences, Northumbria University, Newcastle upon Tyne, United Kingdom; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; Argonne National Laboratory, Lemont, IL; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; University of Essex, Colchester, United Kingdom;" Oxford Brookes University, Oxford, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2429"",""2445"",""Nowadays, by integrating the cloud radio access network (C-RAN) with the mobile edge cloud computing (MEC) technology, mobile service provider (MSP) can efficiently handle the increasing mobile traffic and enhance the capabilities of mobile devices. But the power consumption has become skyrocketing for MSP and it gravely affects the profit of MSP. Previous work often studied the power consumption in C-RAN and MEC separately while less work had considered the integration of C-RAN with MEC. In this paper, we present an unifying framework for the power-performance tradeoff of MSP by jointly scheduling network resources in C-RAN and computation resources in MEC to maximize the profit of MSP. To achieve this objective, we formulate the resource scheduling issue as a stochastic problem and design a new optimization framework by using an extended Lyapunov technique. Specially, because the standard Lyapunov technique critically assumes that job requests have fixed lengths and can be finished within each decision making interval, it is not suitable for the dynamic situation where the mobile job requests have variable lengths. To solve this problem, we extend the standard Lyapunov technique and design the VariedLen algorithm to make online decisions in consecutive time for job requests with variable lengths. Our proposed algorithm can reach time average profit that is close to the optimum with a diminishing gap (1/V) for the MSP while still maintaining strong system stability and low congestion. With extensive simulations based on a real world trace, we demonstrate the efficacy and optimality of our proposed algorithm."",""1558-2183"","""",""10.1109/TPDS.2018.2832124"",""National Key Research and Development Program(grant numbers:2018YFB1004800)"; National Science Foundation of China(grant numbers:61620106011); U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research Program(grant numbers:DE-AC02-06CH11357); UK EPSRC Project; NIRVANA(grant numbers:EP/L026031/1); EU FP7 Project; MONICA(grant numbers:GA-2011-295222);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8353131"",""Cloud radio access network";mobile edge computing;power-performance tradeoff;Lyapunov optimization;"scheduling"",""Cloud computing";Processor scheduling;Mobile handsets;Optimization;Standards;Containers;"Power demand"","""",""73"","""",""51"",""IEEE"",""1 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Early Identification of Critical Blocks: Making Replicated Distributed Storage Systems Reliable Against Node Failures,""J. Fang"; S. Wan; P. Huang; C. Xie;" X. He"",""Wuhan National Laboratory for Optoelectronics, Ministry of Education of China, Wuhan, China"; Key Laboratory of Information Storage System, Ministry of Education of China, Wuhan, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA; Wuhan National Laboratory for Optoelectronics, Ministry of Education of China, Wuhan, China;" Department of Computer and Information Sciences, Temple University, Philadelphia, PA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2446"",""2459"",""In large-scale replicated distributed storage systems consisting of hundreds to thousands of nodes, node failures are not rare and can cause data blocks to lose their replicas and become faulty. A simple but effective approach to prevent data loss from the node failures, i.e., ensuring reliability, is to shorten the identification time of the node failures and faulty blocks, which is determined by both timeouts and check intervals for node states. However, to maintain low repair network traffic, the identification time is actually relatively long and even dominates repair processes of critical blocks. In this paper, we propose a novel scheme, named RICK, to explore the potential in the identification time, and thus improve data reliability of replicated distributed storage systems while maintaining a low repair cost. First, by introducing an additional replica state, critical blocks (with two or more lost replicas) have individual short timeouts while sick blocks (with only one lost replica) preserve the long timeouts. Second, by replacing the static check intervals for node states with adaptive ones, the check intervals and the identification time of critical blocks are further shortened, which improves data reliability. Meanwhile, due to the low ratio of critical blocks in all faulty blocks, the repair network traffic remains low. The results from our simulation and prototype implementation show that RICK improves data reliability of replicated distributed storage systems by a factor of up to 14 in terms of mean time to data loss. Meanwhile, the extra repair network traffic caused by RICK is less than 1.5 percent of the total network traffic for data repairs."",""1558-2183"","""",""10.1109/TPDS.2018.2833457"",""National Natural Science Foundation of China(grant numbers:61300046,61331010)"; National Science Foundation(grant numbers:CCF-1717660,CNS-1702474,CCF-1547804);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8355707"",""Reliability";replication;"distributed storage system"",""Reliability";Maintenance engineering;Distributed databases;Transient analysis;Heart beat;Computer network reliability;"Google"","""",""5"","""",""49"",""IEEE"",""7 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Easy PRAM-Based High-Performance Parallel Programming with ICE,""F. Ghanim"; U. Vishkin;" R. Barua"",""Electrical and Computer Engineering Department, University of Maryland, College Park, MD"; Electrical and Computer Engineering Department, University of Maryland, College Park, MD;" Electrical and Computer Engineering Department, University of Maryland, College Park, MD"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""377"",""390"",""Parallel machines have become more widely used. Unfortunately parallel programming technologies have advanced at a much slower pace except for regular programs. For irregular programs, this advancement is inhibited by high synchronization costs, non-loop parallelism, non-array data structures, recursively expressed parallelism and parallelism that is too fine-grained to be exploitable. We present ICE, a new parallel programming language that is easy-to-program, since: (i) ICE is a synchronous, lock-step language so there is no need for programmer-specified synchronization"; (ii) for a PRAM algorithm its ICE program amounts to directly transcribing it; and (iii) the PRAM algorithmic theory offers unique wealth of parallel algorithms and techniques. We propose ICE to be a part of an ecosystem consisting of the XMT architecture, the PRAM algorithmic model, and ICE itself, that together deliver on the twin goal of easy programming and efficient parallelization of irregular programs. The XMT architecture, developed at UMD, can exploit fine-grained parallelism in irregular programs. We have built the ICE compiler which translates the ICE language into the multithreaded XMTC language;" the significance of this is that multi-threading is a feature shared by practically all current scalable parallel programming languages thus providing a method to compile ICE code. As one indication of ease of programming, we observed a reduction in code size in 11 out of 16 benchmarks as compared to hand-optimized XMTC. For these programs, the average reduction in number of lines of code was 35.5 percent. The remaining 5 benchmarks had almost the same code size for both ICE and hand-optimized XMTC. Our main result is perhaps surprising: The run-time was comparable to XMTC with a 0.53 percent average gain for ICE across all benchmarks."",""1558-2183"","""",""10.1109/TPDS.2017.2754376"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8046100"",""ICE";ease of programming;irregular programs;PRAM;fine-grained parallelism;XMT;nested ICE;"nested parallelism"",""Ice";Phase change random access memory;Instruction sets;Parallel processing;Synchronization;"Parallel programming"","""",""11"","""",""38"",""IEEE"",""19 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"EDC: Improving the Performance and Space Efficiency of Flash-Based Storage Systems with Elastic Data Compression,""B. Mao"; S. Wu; H. Jiang; Y. Yang;" Z. Xi"",""Software School of Xiamen University, Xiamen, Fujian, China"; Computer Science Department of Xiamen University, Xiamen, Fujian, China; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, USA; Apple in San Francisco, CA, USA;" Computer Science Department of Xiamen University, Xiamen, Fujian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1261"",""1274"",""By leveraging data reduction technologies, such as data compression, all flash-based storage systems can have the same total cost of ownership (TCO) as traditional HDD-based storage systems. Thus, data compression has become a commodity feature for space efficiency and reliability in flash-based storage systems by reducing write traffic and space capacity demand. However, it introduces noticeable processing overheads on the critical I/O path, which degrades the system performance significantly. Existing data compression schemes for flash-based storage systems use fixed compression algorithms for all the incoming write data, failing to recognize and exploit the significant diversity in compressibility and access patterns of data and missing an opportunity to improve the system performance, the space efficiency or both. To achieve a reasonable trade-off between these two important design objectives, in this paper we introduce an Elastic Data Compression scheme, called EDC, which exploits the data compressibility and access intensity characteristics by judiciously matching data of different compressibility with different compression algorithms while leveraging the access idleness. Specifically, for compressible data blocks EDC exploits the compression diversity of the workload, and employs algorithms of higher compression rate in periods of lower system utilization and algorithms of lower compression rate in periods of higher system utilization. For non-compressible (or very lowly compressible) data blocks, it will write them through to the flash storage directly without any compression. The experiments conducted on our lightweight prototype implementation of the EDC system show that EDC saves storage space by up to 38.7 percent, with an average of 33.7 percent . In addition, it significantly outperforms the fixed compression schemes in the I/O performance measure by up to 61.4 percent, with an average of 36.7 percent."",""1558-2183"","""",""10.1109/TPDS.2018.2794966"",""National Natural Science Foundation of China(grant numbers:61772439,U1705261,61472336,61402385)"; US National Science Foundation(grant numbers:CCF-1704504,CCF-1629625);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8263239"",""Elastic data compression";flash-based storage;data compressibility;"I/O intensity"",""Data compression";System performance;Compression algorithms;Time factors;Distributed databases;Market research;"Reliability"","""",""7"","""",""45"",""IEEE"",""18 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Disk-Based Directed Graph Processing: A Strongly Connected Component Approach,""Y. Zhang"; X. Liao; X. Shi; H. Jin;" B. He"",""Service Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China"; Service Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; Service Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; Service Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China;" Department of Computer Science, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""830"",""842"",""Recently, there have been many disk-based systems proposed for iterative graph processing. In the popular vertex/edge-centric systems, an iterative directed graph algorithm needs to reprocess many partitions so as to update their vertices' states according to other non-convergent vertices for the unawareness of their dependencies. As a result, it induces high data access cost and a long time to converge. To tackle this problem, we propose a novel system for iterative directed graph processing with taking advantage of the strongly connected component (SCC) structure. It stores a directed graph into a directed acyclic graph (DAG) sketch, with each node representing a SCC in the original data graph. During execution, the SCCs are loaded into memory for processing in a parallel way according to the topological order of the DAG sketch, and the vertices in each SCC are tried to be handled along the directed paths. In this way, each SCC is able to reach convergence in order and needs to be loaded into the main memory for exactly once, getting much lower data access cost and faster convergence. Besides, the vertices of each SCC need fewer updates for convergence. We further develop a lightweight approach to maintain the DAG sketch and handle SCCs in an incremental way for evolving graphs. Compared with the state-of-the-art methods, experimental results show that our approach achieves a performance improvements of 1.46-8.37 times for static graphs, and can reduce the execution time by 61.4-72.7 percent for evolving graphs."",""1558-2183"","""",""10.1109/TPDS.2017.2776115"",""National Natural Science Foundation of China(grant numbers:61702202,61628204,61732010)"; China Postdoctoral Science Foundation Funded Project(grant numbers:2017M610477,2017T100555);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8118091"",""Iterative direct graph processing";strongly connected component;convergence;"I/O cost"",""Convergence";Partitioning algorithms;Optimization;Load modeling;Iterative algorithms;"Venus"","""",""15"","""",""48"",""IEEE"",""22 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Distributed All-Pairs Algorithms: Management Using Optimal Cyclic Quorums,""C. J. Kleinheksel";" A. K. Somani"",""Department of Electrical and Computer Engineering, Iowa State University, Ames, IA";" Department of Electrical and Computer Engineering, Iowa State University, Ames, IA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""391"",""404"",""All Pairs problems occur in many research fields. The all-pairs problem requires all data elements to be paired with all other data elements. With the advent of new data intensive big data applications and increase in data size, methods to reduce memory foot print and distribute to work equally across compute nodes are needed. In this paper, we propose cyclic quorum sets for all-pairs algorithm computations to reduce memory foot print. We show that the cyclic quorum sets have a unique all-pairs property that allows for minimal data replication. The cyclic quorums set based computing requires only N/√P size memory, up to 50 percent smaller than the dual N/√P array implementations proposed earlier, and significantly smaller than solutions requiring all data in each node. Computation can be distributed efficiently and more importantly and are communication-less after initial data distribution, which is a huge advantage in minimizing computation time. Scaling from 16 to 512 cores (1 to 32 compute nodes), our application experiments on a real dataset demonstrated scalability with greater than 150x (super-linear) speedup with less than 1/4th the memory usage per node in our experiments."",""1558-2183"","""",""10.1109/TPDS.2017.2707417"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932888"",""Distributed all-pair computations";cyclic quorum set and all pair property;communication-less computation;"memory foot print efficiency"",""Memory management";Arrays;Distributed databases;Big Data;Bioinformatics;Proteins;"Algorithm design and analysis"","""",""2"","""",""21"",""IEEE"",""23 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Performance-Centric Bandwidth Allocation with Fairness Tradeoff,""L. Chen"; Y. Feng; B. Li;" B. Li"",""Department of Electrical and Computer Engineering, University of Toronto, ON, Canada"; Department of Electrical and Computer Engineering, University of Toronto, ON, Canada; Department of Electrical and Computer Engineering, University of Toronto, ON, Canada;" Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1693"",""1706"",""Fair bandwidth allocation in datacenter networks has received a substantial amount of research attention, as multiple tenants are hosted by virtual machines in a public cloud. In the context of private datacenters, link bandwidth is shared among applications running data parallel frameworks, such as MapReduce, instead. In this paper, we introduce the rigorous definition of performance-centric fairness, with the guiding principle that the performance that data parallel applications will enjoy should be proportional to their weights. We first investigate the problem of maximizing application performance while maintaining strict performance-centric fairness. We then present an inherent tradeoff between fairness and efficiency, which is interpreted from the perspectives of bandwidth utilization and social welfare, respectively. From the first perspective, we propose an algorithm to improve bandwidth utilization by introducing an extended version of fairness. From the second perspective, we formulate an optimization problem of bandwidth allocation that maximizes the social welfare across all the applications, allowing a tunable degree of relaxation on performance-centric fairness. A distributed algorithm is then presented to solve the problem, based on dual based decomposition. With extensive simulations, we demonstrate the effectiveness of our algorithms in improving efficiency and application performance (by up to 1.4X), with flexible degree of relaxation on the performance-centric fairness."",""1558-2183"","""",""10.1109/TPDS.2018.2808202"",""NSERC Collaborative Research and Development Grant"; RGC GRF(grant numbers:16211715,16206417); RGC CRF(grant numbers:C7036-15G);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8295233"",""Datacenter networks";bandwidth allocation;"fairness"",""Bandwidth";Task analysis;Channel allocation;Optimization;Virtual machining;"Cloud computing"","""",""6"","""",""29"",""IEEE"",""20 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Realization of Householder Transform Through Algorithm-Architecture Co-Design for Acceleration of QR Factorization,""F. Merchant"; T. Vatwani; A. Chattopadhyay; S. Raha; S. K. Nandy;" R. Narayan"",""Institute for Communication Technologies and Embedded Systems, RWTH Aachen University, Aachen, Germany"; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Indian Institute of Science, Bangalore, Karnataka, India; Indian Institute of Science, Bangalore, Karnataka, India;" Morphing Machines Pvt. LTd., Bengaluru, Karnataka, India"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1707"",""1720"",""QR factorization is a ubiquitous operation in many engineering and scientific applications. In this paper, we present efficient realization of Householder Transform (HT) based QR factorization through algorithm-architecture co-design where we achieve performance improvement of 3-90x in-terms of Gflops/watt over state-of-the-art multicore, General Purpose Graphics Processing Units (GPGPUs), Field Programmable Gate Arrays (FPGAs), and ClearSpeed CSX700. Theoretical and experimental analysis of classical HT is performed for opportunities to exhibit higher degree of parallelism where parallelism is quantified as a number of parallel operations per level in the Directed Acyclic Graph (DAG) of the transform. Based on theoretical analysis of classical HT, an opportunity to re-arrange computations in the classical HT is identified that results in Modified HT (MHT) where it is shown that MHT exhibits 1.33x times higher parallelism than classical HT. Experiments in off-the-shelf multicore and General Purpose Graphics Processing Units (GPGPUs) for HT and MHT suggest that MHT is capable of achieving slightly better or equal performance compared to classical HT based QR factorization realizations in the optimized software packages for Dense Linear Algebra (DLA). We implement MHT on a customized platform for Dense Linear Algebra (DLA) and show that MHT achieves 1.3x better performance than native implementation of classical HT on the same accelerator. For custom realization of HT and MHT based QR factorization, we also identify macro operations in the DAGs of HT and MHT that are realized on a Reconfigurable Data-path (RDP). We also observe that due to re-arrangement in the computations in MHT, custom realization of MHT is capable of achieving 12 percent better performance improvement over multicore and GPGPUs than the performance improvement reported by General Matrix Multiplication (GEMM) over highly tuned DLA software packages for multicore and GPGPUs which is counter-intuitive."",""1558-2183"","""",""10.1109/TPDS.2018.2803820"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283815"",""Parallel computing";dense linear algebra;multiprocessor system-on-chip;"instruction level parallelism"",""Parallel processing";Multicore processing;Linear algebra;Transforms;Eigenvalues and eigenfunctions;"Algorithm design and analysis"","""",""8"","""",""36"",""IEEE"",""7 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Timing Channel Protection for Hybrid (Packet/Circuit-Switched) Network-on-Chip,""A. K. Biswas"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1044"",""1057"",""Continuous development of Network-on-Chip (NoC) enables different types of applications to run efficiently in a Multiprocessor System-on-Chip (MP-SoC). Guaranteed service (GS) can be provided by circuit switching NoC and Best effort service (BES) can be provided by packet switching NoC. A hybrid NoC containing both packet and circuit switching, can provide both types of services to these different applications. But these different applications can be of different security levels and one application can interfere another application's timing characteristics during network transmission. Using this interference, a malicious application can extract secret information from higher security level flows (timing side channel) or two applications can communicate covertly violating the system's security policy (covert timing channel). We propose different mechanisms to protect hybrid routers from timing channel attacks. For design space exploration, we propose three timing channel secure hybrid routers viz. Separate Hybrid (SH), Combined with Separate interface Hybrid (CSH), and Combined Hybrid (CH) routers. Simulation results show that all three routers are secure from timing channel when compared to a conventional hybrid router. Synthesis results show that the area increments compared to a conventional hybrid router are only 7.63, 11.8, and 19.69 percent for SH, CSH, and CH routers respectively. Thus simulation and synthesis results prove the effectiveness of our proposed mechanisms with acceptable area overheads."",""1558-2183"","""",""10.1109/TPDS.2017.2783337"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8207636"",""Hybrid NoC";timing channel;security in NoC;timing side channel;covert timing channel;TDM;packet switching;"circuit switching"",""Timing";Switching circuits;Packet switching;Throughput;Integrated circuit modeling;"Side-channel attacks"","""",""11"","""",""35"",""IEEE"",""14 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Elastic Parity Logging for SSD RAID Arrays: Design, Analysis, and Implementation,""H. H. W. Chan"; Y. Li; P. P. C. Lee;" Y. Xu"",""Department of Computer Science and Engineering, Chinese University of Hong Kong, Sha Tin, Hong Kong"; School of Computer Science and Technology, University of Science and Technology of China, Hefei Shi, Anhui Sheng, China; Department of Computer Science and Engineering, Chinese University of Hong Kong, Sha Tin, Hong Kong;" School of Computer Science and Technology, University of Science and Technology of China, Hefei Shi, Anhui Sheng, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2241"",""2253"",""Parity-based RAID poses a design trade-off issue for large-scale SSD storage systems: it improves reliability against SSD failures through redundancy, yet its parity updates incur extra I/Os and garbage collection operations, thereby degrading the endurance and performance of SSDs. We propose EPLOG, a storage layer that reduces parity traffic to SSDs, so as to provide endurance, reliability, and performance guarantees for SSD RAID arrays. EPLOG mitigates parity update overhead via elastic parity logging, which redirects parity traffic to separate log devices (to improve endurance and reliability) and eliminates the need of pre-reading data in parity computations (to improve performance). We design EPLOG as a user-level implementation that is fully compatible with commodity hardware and general erasure coding schemes. We evaluate EPLOG through reliability analysis and trace-driven testbed experiments. Compared to the Linux software RAID implementation, our experimental results show that our EPLOG prototype reduces the total write traffic to SSDs, reduces the number of garbage collection operations, and increases the I/O throughput. In addition, EPLOG significantly improves the I/O performance over the original parity logging design, and incurs low metadata overhead."",""1558-2183"","""",""10.1109/TPDS.2018.2818171"",""National Natural Science Foundation of China(grant numbers:61772484)"; Natural Science Foundation of Anhui Province(grant numbers:1508085SQF214);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8322269"",""SSD";RAID;"parity logging"",""Performance evaluation";Arrays;Prototypes;Reliability engineering;Encoding;"Software"","""",""8"","""",""58"",""IEEE"",""22 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Elastic Symbiotic Scaling of Operators and Resources in Stream Processing Systems,""F. Lombardi"; L. Aniello; S. Bonomi;" L. Querzoni"",""Department of Computer, Control, and Management Engineering Antonio Ruberti, Sapienza University of Rome, Roma, Italy"; Department of Computer, Control, and Management Engineering Antonio Ruberti, Sapienza University of Rome, Roma, Italy; Department of Computer, Control, and Management Engineering Antonio Ruberti, Sapienza University of Rome, Roma, Italy;" Department of Computer, Control, and Management Engineering Antonio Ruberti, Sapienza University of Rome, Roma, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""572"",""585"",""Distributed stream processing frameworks are designed to perform continuous computation on possibly unbounded data streams whose rates can change over time. Devising solutions to make such systems elastically scale is a fundamental goal to achieve desired performance and cut costs caused by resource over-provisioning. These systems can be scaled along two dimensions: the operator parallelism and the number of resources. In this paper, we show how these two dimensions, as two symbiotic entities, are independent but must mutually interact for the global benefit of the system. On the basis of this observation, we propose a fine-grained model for estimating the resource utilization of a stream processing application that enables the independent scaling of operators and resources. A simple, yet effective, combined management of the two dimensions allows us to propose ELYSIUM, a novel elastic scaling approach that provides efficient resource utilization. We implemented the proposed approach within Apache Storm and tested it by running two real-world applications with different input load curves. The outcomes backup our claims showing that the proposed symbiotic management outperforms elastic scaling strategies where operators and resources are jointly scaled."",""1558-2183"","""",""10.1109/TPDS.2017.2762683"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8067517"",""Cloud";elasticity;elastic scaling;stream processing;"storm"",""Symbiosis";Parallel processing;Runtime;Computational modeling;Stress;Storms;"Process control"","""",""39"","""",""31"",""IEEE"",""13 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Enabling Generic, Verifiable, and Secure Data Search in Cloud Services,""J. Zhu"; Q. Li; C. Wang; X. Yuan; Q. Wang;" K. Ren"",""Department of Computer Science, Tsinghua University, Beijing, China"; Department of Computer Science, Tsinghua University, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Cyber Science and Engineering, Wuhan University, Wuhan, Hubei, China;" Department of Computer Science and Engineering, University at Buffalo, State University of New York"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1721"",""1735"",""Searchable Symmetric Encryption (SSE) has been widely studied in cloud storage, which allows cloud services to directly search over encrypted data. Most SSE schemes only work with honest-but-curious cloud services that do not deviate from the prescribed protocols. However, this assumption does not always hold in practice due to the untrusted nature in storage outsourcing. To alleviate the issue, there have been studies on Verifiable Searchable Symmetric Encryption (VSSE), which functions against malicious cloud services by enabling results verification. But to our best knowledge, existing VSSE schemes exhibit very limited applicability, such as only supporting static database, demanding specific SSE constructions, or only working in the single-user model. In this paper, we propose GSSE, the first generic verifiable SSE scheme in the single-owner multiple-user model, which provides verifiability for any SSE schemes and further supports data updates. To generically support result verification, we first decouple the proof index in GSSE from SSE. We then leverage Merkle Patricia Tree (MPT) and Incremental Hash to build the proof index with data update support. We also develop a timestamp-chain for data freshness maintenance across multiple users. Rigorous analysis and experimental evaluations show that GSSE is secure and introduces small overhead for result verification."",""1558-2183"","""",""10.1109/TPDS.2018.2808283"",""National Key R&D Program of China(grant numbers:2016YFB0800102)"; National Natural Science Foundation of China (NSFC)(grant numbers:61572278,U1736209,U1636219,61572412,61373167,61772236); Research Grants Council of Hong Kong(grant numbers:CityU 11212717,CityU C1008-16G); Innovation and Technology Commission of Hong Kong; ITF Project(grant numbers:ITS/168/17); Key Program of Natural Science Foundation of Hubei Province(grant numbers:2017CFA047,2017CFA007);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8295253"",""Cloud";secure data search;"verifiable data search"",""Cloud computing";Encryption;Servers;Data integrity;Data models;"Indexes"","""",""61"","""",""45"",""IEEE"",""20 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Encoding-Aware Data Placement for Efficient Degraded Reads in XOR-Coded Storage Systems: Algorithms and Evaluation,""z. shen"; P. P. C. Lee; J. Shu;" W. Guo"",""Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong"; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Fujian Provincial Key Laboratory of Network Computing and Intelligent Information Processing, Fuzhou University, Fuzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2757"",""2770"",""Modern storage systems adopt erasure coding to maintain fault tolerance with low storage redundancy. However, how to improve the performance of degraded reads in erasure-coded storage has been a critical issue. We revisit this problem from two different perspectives that are neglected by existing studies: data placement and encoding rules. To this end, we propose encoding-aware data placement (EDP), which mitigates the number of I/Os in degraded reads during a single failure for general XOR-based erasure codes. EDP carefully selects appropriate parity units to be generated by sequential data based on the encoding rules and establishes their generation orders. We further refine the data placement for optimizing the degraded reads to any two sequential data units. Trace-driven evaluation results show that EDP significantly reduces I/Os in degraded reads and hence shortens the read time."",""1558-2183"","""",""10.1109/TPDS.2018.2842210"",""National Natural Science Foundation of China(grant numbers:61602120,61327902,61433008,U1435216,61672159,U1705262)"; Technology Innovation Platform Project of Fujian Province(grant numbers:2014H2005); Fujian Collaborative Innovation Center for Big Data Application in Governments; Fujian Engineering Research Center of Big Data Analysis and Processing; Fujian Provincial Natural Science Foundation(grant numbers:2017J05102); Research Grants Council of Hong Kong(grant numbers:GRF 14216316,CRF C7036-15G);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8370044"",""Encoding-aware data placement, degraded reads, XOR-coded storage systems"",""Encoding";Fault tolerant systems;Layout;Redundancy;Distributed databases;"Decoding"","""",""26"","""",""38"",""IEEE"",""31 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Energy Efficiency Aware Task Assignment with DVFS in Heterogeneous Hadoop Clusters,""D. Cheng"; X. Zhou; P. Lama; M. Ji;" C. Jiang"",""Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC"; Department of Computer Science, University of Colorado, Colorado Springs, CO; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX; chief scientist in Valley Technologies Ltd., Winnipeg, MB, Canada;" Department of Computer Science & Technology, Tongji University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""70"",""82"",""While Hadoop ecosystems become increasingly important for practitioners of large-scale data analysis, they also incur tremendous energy cost. This trend is driving up the need for designing energy-efficient Hadoop clusters in order to reduce the operational costs and the carbon emission associated with its energy consumption. However, despite extensive studies of the problem, existing approaches for energy efficiency have not fully considered the heterogeneity of both workload and machine hardware found in production environments. In this paper, we find that heterogeneity-oblivious task assignment approaches are detrimental to both performance and energy efficiency of Hadoop clusters. Our observation shows that even heterogeneity-aware techniques that aim to reduce the job completion time do not guarantee a reduction in energy consumption of heterogeneous machines. We propose a heterogeneity-aware task assignment approach, E-Ant, that aims to improve the overall energy consumption in a heterogeneous Hadoop cluster without sacrificing job performance. It adaptively schedules heterogeneous workloads on energy-efficient machines, without a priori knowledge of the workload properties. E-Ant employs an ant colony optimization approach that generates task assignment solutions based on the feedback of each task's energy consumption reported by Hadoop TaskTrackers in an agile way. Furthermore, we integrate DVFS technique with E-Ant to further improve the energy efficiency of heterogeneous Hadoop clusters. It relies on a DVFS controller to dynamically scale the CPU frequency of each slave machine in response to time-varying resource demands. Experimental results on a heterogeneous cluster with varying hardware capabilities show that E-Ant with DVFS improves the overall energy savings for a synthetic workload from Microsoft by 23 and 17 percent compared to Fair Scheduler and Tarazu, respectively."",""1558-2183"","""",""10.1109/TPDS.2017.2745571"",""US National Science Foundation(grant numbers:CNS-1422119,CNS-1217979)"; University of North Carolina at Charlotte;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8017447"",""Energy efficiency";task assignment;DVFS;fuzzy control;heterogeneity;hadoop;"ant colony optimization"",""Energy consumption";Hardware;Servers;Benchmark testing;Power demand;Electronic mail;"Ant colony optimization"","""",""40"","""",""23"",""IEEE"",""29 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Energy-Aware Virtual Machine Scheduling on Data Centers with Heterogeneous Bandwidths,""D. G. Lago"; E. R. M. Madeira;" D. Medhi"",""Institute of Computing, University of Campinas, Campinas, Brazil"; Institute of Computing, University of Campinas, Campinas, Brazil;" University of Missouri-Kansas City, Kansas, MO"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""83"",""98"",""Ethernet is the main choice of connectivity in data centers operating in clouds. The Ethernet standards body (IEEE 802.3) has generated Ethernet specifications with increasing transmission rates since its inception. As a result, it is observed that data centers with groups of physical hosts having heterogeneous bandwidths. In this paper, we present an energy-aware virtual machine's scheduling method, taking into account this heterogeneity while aiming at energy efficiency. This method consists of two algorithms: one to determine to which physical hosts virtual machines should be allocated, and the other for the provision of bandwidth on physical hosts to virtual machines. Our study shows that, regarding energy savings for the studied scenarios, the presented method is comparable to other energy-aware methods in data centers with groups of machines with homogeneous settings, surpassing them in groups of machines with heterogeneous configurations, bringing improvements especially to data centers with heterogeneous bandwidths' networks or to where restrictive SLAs are used for the provision of bandwidth to virtual machines."",""1558-2183"","""",""10.1109/TPDS.2017.2753247"",""Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior"; Conselho Nacional de Desenvolvimento Cientifico e Tecnologico(grant numbers:402482/2012-3);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8039210"",""Data centers";ethernet networks;high-speed networks;"scheduling algorithms"",""Bandwidth";Cloud computing;Scheduling algorithms;Scheduling;Topology;"Switches"","""",""16"","""",""27"",""IEEE"",""18 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Errata to “Evaluation of a Heterogeneous Multicore Architecture by Design and Test of an OFDM Receiver”,""S. Nouri"; W. Hussain;" J. Nurmi"",NA"; NA;" NA,""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""719"",""719"",""Presents corrections to the paper, “Evaluation of a heterogeneous multicore architecture by design and test of an OFDM receiver,” (Nouri, S. et al,), IEEE Trans. Parallel Dist. Syst., vol. 28, no. 11, pp. 3171–3187, Nov. 2017."",""1558-2183"","""",""10.1109/TPDS.2017.2788164"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8290633"","""",""Multicore processing";OFDM;"Receivers"","""","""","""",""1"",""IEEE"",""12 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Error Resilient GPU Accelerated Image Processing for Space Applications,""R. L. Davidson";" C. P. Bridges"",""Surrey Space Centre, University of Surrey, Guildford, Surrey, United Kingdom";" Surrey Space Centre, University of Surrey, Guildford, Surrey, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""1990"",""2003"",""Significant advances in spaceborne imaging payloads have resulted in new big data problems in the Earth Observation (EO) field. These challenges are compounded onboard satellites due to a lack of equivalent advancement in onboard data processing and downlink technologies. We have previously proposed a new GPU accelerated onboard data processing architecture and developed parallelised image processing software to demonstrate the achievable data processing throughput and compression performance. However, the environmental characteristics are distinctly different to those on Earth, such as available power and the probability of adverse single event radiation effects. In this paper, we analyse new performance results for a low power embedded GPU platform, investigate the error resilience of our GPU image processing application and offer two new error resilient versions of the application. We utilise software based error injection testing to evaluate data corruption and functional interrupts. These results inform the new error resilient methods that also leverages GPU characteristics to minimise time and memory overheads. The key results show that our targeted redundancy techniques reduce the data corruption from a probability of up to 46 percent to now less than 2 percent for all test cases, with a typical execution time overhead of 130 percent."",""1558-2183"","""",""10.1109/TPDS.2018.2812853"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8307249"",""GPU";parallel processing;error resilience;EO;image processing;image compression;onboard;"satellites"",""Graphics processing units";Hardware;Data processing;Instruction sets;Acceleration;"Resilience"","""",""25"","""",""23"",""IEEE"",""6 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Eunomia: Scaling Concurrent Index Structures Under Contention Using HTM,""W. Zhang"; X. Wang; S. Ji; Z. Wei; Z. Wang;" H. Chen"",""Software School, Fudan University, Shanghai, China"; Software School, Fudan University, Shanghai, China; Software School, Fudan University, Shanghai, China; Software School, Fudan University, Shanghai, China; Department of Computer Sciences, New York University, New York, NY;" Institute of Parallel and Distributed Systems, Shanghai Jiaotong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1837"",""1850"",""Hardware transactional memory (HTM) is an emerging hardware feature. HTM simplifies the programming model of concurrent programs while preserving high and scalable performance. With the commercial availability of HTM-capable processors, HTM has recently been adopted to construct efficient concurrent index structures. However, with the expansion of data volume and user amount, data management systems have to process workloads exhibiting high contention";" meanwhile, according to our experiments, the conventional HTM-base concurrent index structures fail to provide scalable performance under highly-contented workloads. Such performance pathology strictly constrains the usage of HTM on data management systems. In this paper, we first conduct a thorough analysis on HTM-based concurrent index structures, and uncover several reasons for excessive HTM aborts incurred by both false and true conflicts under contention. Based on the analysis, we advocate Eunomia, a design pattern for HTM-based concurrent index structure which contains several principles to improve HTM performance, including splitting HTM regions with version-based concurrency control to reduce HTM working sets, partitioned data layout to reduce false conflicts, proactively detecting and avoiding conflicting requests, and adaptive concurrency control strategy. To validate their effectiveness, we apply such design principles to construct a scalable concurrent B+Tree and a skip list using HTM. Evaluation using key-value store and database benchmarks on a 20-core HTM-capable multi-core machine shows that Eunomia leads to substantial speedup under high contention, while incurring small overhead under low contention."",""1558-2183"","""",""10.1109/TPDS.2017.2729551"",""National Key Research and Development Program of China(grant numbers:2017YFB0202105)"; National Natural Science Foundation of China(grant numbers:61672160,61370081); Shanghai Science and Technology Development Funds(grant numbers:17511102200);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7987039"",""Hardware transactional memory";concurrent index structure;"data conflicts"",""Indexes";Programming;Data structures;Program processors;Pathology;"Synchronization"","""",""2"","""",""48"",""IEEE"",""20 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Evacuate Before Too Late: Distributed Backup in Inter-DC Networks with Progressive Disasters,""X. Xie"; Q. Ling; P. Lu; W. Xu;" Z. Zhu"",""School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; Department of Automation, University of Science and Technology of China, Hefei, Anhui, China; School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Automation, University of Science and Technology of China, Hefei, Anhui, China;" School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1058"",""1074"",""Inter-datacenter (inter-DC) networks are essential for large enterprises to deliver high-quality services to end-users. Since DCs are vulnerable to natural disasters, an inter-DC network operator needs an effective emergency backup plan to evacuate the endangered data out in case of a progressive disaster whose status can be predicted by an early warning system. In this paper, we try to solve the problem of emergency backup in inter-DC networks with progressive disasters. We first utilize the time-expanded network (TEN) approach to model the time-variant inter-DC network during a progressive disaster as a variant TEN (VTEN) and convert the dynamic flow scheduling for emergency backup to a static one. Then, with the VTEN, we formulate an optimization model to maximize the profit from the emergency backup in consideration of data values and resource costs. Although this large-scale optimization can be solved in a distributed way by leveraging the alternation direction method of multipliers (ADMM), we find that one of its subproblems is nontrivial in the distributed setting. We propose a novel inexact ADMM approach to resolve the issue induced by the subproblem, and prove that the proposed algorithm can converge to the optimal solution. The results from extensive simulations confirm that our algorithm is robust and time-efficient, and outperforms several benchmarks in terms of backup profit and running time."",""1558-2183"","""",""10.1109/TPDS.2017.2785385"",""National Natural Science Foundation of China(grant numbers:61371117)"; SPR Program of the CAS(grant numbers:XDA06011202); Key Research Project of the CAS(grant numbers:QYZDY-SSW-JSC003); NGBWMCN(grant numbers:2017ZX03001019-004);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8227003"",""Inter-DC networks";emergency backup;progressive disasters;alternating direction method of multipliers (ADMM);"time-expanded network (TEN)"",""Dynamic scheduling";Network topology;Optimization;Convex functions;Topology;Data transfer;"Distributed databases"","""",""22"","""",""36"",""IEEE"",""19 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"ExaGeoStat: A High Performance Unified Software for Geostatistics on Manycore Systems,""S. Abdulah"; H. Ltaief; Y. Sun; M. G. Genton;" D. E. Keyes"",""Extreme Computing Research Center, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"; Extreme Computing Research Center, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Extreme Computing Research Center, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia;" Extreme Computing Research Center, King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2771"",""2784"",""We present ExaGeoStat, a high performance software for geospatial statistics in climate and environment modeling. In contrast to simulation based on partial differential equations derived from first-principles modeling, ExaGeoStat employs a statistical model based on the evaluation of the Gaussian log-likelihood function, which operates on a large dense covariance matrix. Generated by the parametrizable Matérn covariance function, the resulting matrix is symmetric and positive definite. The computational tasks involved during the evaluation of the Gaussian log-likelihood function become daunting as the number  $n$  of geographical locations grows, as  ${\mathcal O}(n^2)$  storage and ${\mathcal O}(n^3)$  operations are required. While many approximation methods have been devised from the side of statistical modeling to ameliorate these polynomial complexities, we are interested here in the complementary approach of evaluating the exact algebraic result by exploiting advances in solution algorithms and many-core computer architectures. Using state-of-the-art high performance dense linear algebra libraries associated with various leading edge parallel architectures (Intel KNLs, NVIDIA GPUs, and distributed-memory systems), ExaGeoStat raises the game for statistical applications from climate and environmental science. ExaGeoStat provides a reference evaluation of statistical parameters, with which to assess the validity of the various approaches based on approximation. The software takes a first step in the merger of large-scale data analytics and extreme computing for geospatial statistical applications, to be followed by additional complexity reducing improvements from the solver side that can be implemented under the same interface. Thus, a single uncompromised statistical model can ultimately be executed in a wide variety of emerging exascale environments."",""1558-2183"","""",""10.1109/TPDS.2018.2850749"",""King Abdullah University of Science and Technology";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8396303"",""Maximum likelihood optimization";Matérn covariance function;high performance computing;climate/environment applications;"prediction"",""Computational modeling";Mathematical model;Covariance matrices;Geospatial analysis;Task analysis;"Geophysical measurements"","""",""39"","""",""46"",""IEEE"",""26 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Execution-Efficient Response Time Analysis on Global Multiprocessor Platforms,""Q. Zhou"; G. Li; J. Li;" C. Deng"",""School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, P.R. China"; School of Software Engineering, Huazhong University of Science and Technology, Wuhan, Hubei, P.R. China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, P.R. China;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2785"",""2797"",""Response time analysis (RTA) is an important and fundamental tool for analyzing the schedulability of real-time tasks on multiprocessor platforms, and many promising techniques have been developed during the past few years. However, most of the existing researches focus on improving the analysis precision, while less has been done on enhancing the execution efficiency. In this paper, we take, to our best knowledge, the first effort towards improving the efficiency of the state-of-the-art RTA methods for sporadic tasks under both global fixed-priority (G-FP) and global earliest deadline first (G-EDF) scheduling. Specifically, three factors that impact the efficiency of existing global RTA tests are identified: 1) pessimistic initial value when computing the worst-case response time (WCRT) of tasks under both G-FP and G-EDF"; 2) conservative interference upper bound under both G-FP and G-EDF;" and 3) unnecessary recalculation of WCRTs when there is update of any task WCRT under G-EDF. By addressing these three limitations, we propose two efficient RTA methods for G-FP and G-EDF scheduling, respectively, which achieve better run-time performance but without sacrificing any analysis precision. Experimental evaluations with randomly generated task sets show that the proposed methods exhibit remarkable performance improvements and can save on average 60 and 61 percent run time, as compared to the state-of-the-art technologies under G-FP and G-EDF scheduling, respectively."",""1558-2183"","""",""10.1109/TPDS.2018.2843763"",""State Key Program of National Natural Science of China(grant numbers:61332001)"; NSFC(grant numbers:61572215,61672252); Wuhan Youth Science and Technology Plan(grant numbers:2017050304010287); Fundamental Research Funds(grant numbers:HUST-2016YXMS076);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8371627"",""real-time scheduling";response time analysis;multiprocessor;"execution efficiency"",""Task analysis";Processor scheduling;Interference;Time factors;Real-time systems;Upper bound;"Scheduling"","""",""8"","""",""25"",""IEEE"",""4 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Exploring Customizable Heterogeneous Power Distribution and Management for Datacenter,""L. Liu"; H. Sun; C. Li; Y. Hu; T. Li;" N. Zheng"",""School of Electrical and Information Engineering, Xi'an Jiaotong University, Xi'an, China"; School of Electrical and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical and Computer Engineering, University of Florida, Gainesville, Florida; Department of Electrical and Computer Engineering, University of Florida, Gainesville, Florida;" School of Electrical and Information Engineering, Xi'an Jiaotong University, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2798"",""2813"",""Large-scale datacenters are facing increasing pressure of capping their carbon emission and power cost. Many leading-edge studies have started to explore server clusters running on multiple power sources. Existing approaches do not sufficiently consider the fine-grained power delivery to satisfy diverse requirements in datacenter, especially in the multi-tenant/colocation datacenter, which may yield low energy utilization. To address the emerging trend and new requirements, this article proposes a novel Datacenter inner Power Switch Network (DiPSN) to improve datacenter power efficiency and user satisfaction. DiPSN is a reconfigurable and easy-to-scale-out power architecture, which enables datacenter to distribute various power sources in a fine-grained manner. Moreover, a tailored machine learning based power source management framework is proposed for DiPSN to dynamically optimize user customized performance metrics and maximize datacenter revenue. Compared with conventional single-switch power distribution system, our DiPSN can be configured to improve solar energy utilization by 39.6 percent, reduce utility power cost by 11.1 percent and improve workload performance by 33.8 percent. Meanwhile, our design can extend battery lifetime by 9.3 percent. This work could provide valuable guidelines for designing heterogeneous power distribution architecture and management methodology in datacenters for improving user-customizable efficiency, sustainability and economy."",""1558-2183"","""",""10.1109/TPDS.2018.2841405"",""National Key Research and Development Program of China(grant numbers:2016YFB0200501)"; NSF(grant numbers:1423090,1320100,1117261); NSFC(grant numbers:61602368); China Postdoctoral Science Foundation(grant numbers:168236); Fundamental Research Funds for the Central Universities;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368308"",""Datacenters";power distribution architecture;power management;energy utilization;"computer system implementation"",""Power system management";Servers;Switches;Power distribution;Computer architecture;Renewable energy sources;"Green products"","""",""7"","""",""68"",""IEEE"",""29 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Expressive Content-Based Routing in Software-Defined Networks,""S. Bhowmik"; M. A. Tariq; J. Grunert; D. Srinivasan;" K. Rothermel"",""University of Stuttgart, Stuttgart, Germany"; University of Stuttgart, Stuttgart, Germany; University of Stuttgart, Stuttgart, Germany; University of Stuttgart, Stuttgart, Germany;" University of Stuttgart, Stuttgart, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2460"",""2477"",""With the vision of Internet of Things gaining popularity at a global level, efficient publish/subscribe middleware for communication within and across data centers is extremely desirable. In this respect, the very popular Software-Defined Networking, which enables publish/subscribe middleware to perform line-rate filtering of events directly on hardware, can prove to be very useful. While deploying content filters directly on switches of a software-defined network allows optimized paths, high throughput rates, and low end-to-end latency, it suffers from certain inherent limitations with respect to number of bits available on hardware switches to represent these filters. Such a limitation affects expressiveness of filters, resulting in unnecessary traffic in the network. In this paper, we explore various complementary techniques to represent content filters expressively while being limited by hardware. We implement and evaluate techniques that i) use workload, in terms of events and subscriptions, to represent content, and ii) efficiently select attributes to reduce redundancy in content. Our detailed performance evaluations show the potential of these techniques in reducing unnecessary traffic when subjected to different workloads. Furthermore, the techniques proposed in this paper require significant updates to the network, i.e., the data plane, which must be performed in a consistent manner to ensure desired system behavior. As a result, in this paper, we, also, design and evaluate a light-weight approach that ensures data plane consistency in the presence of dynamic network updates."",""1558-2183"","""",""10.1109/TPDS.2018.2840698"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8365752"",""Content-based routing";software-defined networking;bandwidth efficiency;"hardware limitations"",""Hardware";Control systems;Data centers;Bandwidth;"Cloud computing"","""",""5"","""",""37"",""IEEE"",""25 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Extending the Cutting Stock Problem for Consolidating Services with Stochastic Workloads,""M. Hähnel"; J. Martinovic; G. Scheithauer; A. Fischer; A. Schill;" W. Dargie"",""Technische Universität Dresden, Chair of Computer Networks, Dresden, Germany"; Technische Universität Dresden, Institute of Numerical Mathematics, Dresden, Germany; Technische Universität Dresden, Institute of Numerical Mathematics, Dresden, Germany; Technische Universität Dresden, Institute of Numerical Mathematics, Dresden, Germany; Technische Universität Dresden, Chair of Computer Networks, Dresden, Germany;" Technische Universität Dresden, Chair of Computer Networks, Dresden, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2478"",""2488"",""Data centres and similar server clusters consume a large amount of energy. However, not all consumed energy produces useful work. Servers consume a disproportional amount of energy when they are idle, underutilised, or overloaded. The effect of these conditions can be minimised by attempting to balance the demand for and the supply of resources through a careful prediction of future workloads and their efficient consolidation. In this paper we extend the cutting stock problem for consolidating workloads having stochastic characteristics. Hence, we employ the aggregate probability density function of co-located and simultaneously executing services to establish valid patterns. A valid pattern is one yielding an overall resource utilisation below a set threshold. We tested the scope and usefulness of our approach on a 16-core server with 29 different benchmarks. The workloads of these benchmarks have been generated based on the CPU utilisation traces of 100 real-world virtual machines which we obtained from a Google data centre hosting more than 32000 virtual machines. Altogether, we considered 600 different consolidation scenarios during our experiment. We compared the performance of our approach-system overload probability, job completion time, and energy consumption-with four existing/proposed scheduling strategies. In each category, our approach incurred a modest penalty with respect to the best performing approach in that category, but overall resulted in a remarkable performance clearly demonstrating its capacity to achieve the best trade-off between resource consumption and performance."",""1558-2183"","""",""10.1109/TPDS.2018.2819680"",""German Research Foundation (DFG)"; Collaborative Research Center 912 “Highly Adaptive Energy-efficient Computing” (HAEC);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8352052"",""Bin-packing problem";cloud computing;consolidation;cutting stock problem;data center;energy-efficient computing;workload consolidation;"server consolidation"",""Servers";Data centers;Google;Cloud computing;Virtual machining;Resource management;"Benchmark testing"","""",""12"","""",""33"",""OAPA"",""27 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FA-Stack: A Fast Array-Based Stack with Wait-Free Progress Guarantee,""Y. Peng";" Z. Hao"",""Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China";" Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""843"",""857"",""The prevalence of multicore processors necessitates the design of efficient concurrent data structures. Shared concurrent stacks are widely used as inter-thread communication structures in parallel applications. Wait-free stacks can ensure that each thread completes operations on them in a finite number of steps. This characteristic is valuable for parallel applications and operating systems, especially in real-time environments. Unfortunately, because wait-free algorithms are typically hard to design and considered inefficient, practical wait-free stacks are rare. In this paper, we present a practical, fast array-based concurrent stack with wait-free progress guarantee, named FA-Stack. A series of optimizations are proposed to bound the number of steps required to complete every push and pop operation. In addition, FA-Stack adopts a time-stamped scheme to reclaim memory. We use linearizability, a correctness condition for concurrent data structures, to prove that FA-Stack is a wait-free linearizable stack with respect to the Last in First Out (LIFO) semantics. Our evaluation with representative benchmarks shows that FA-Stack is an efficient wait-free stack. For example, compared to Sim-Stack (a state-of-the-art wait-free stack), FA-Stack improves the throughput of halfhalf benchmark by upto 2.4×."",""1558-2183"","""",""10.1109/TPDS.2017.2770121"",""National Natural Science Foundation of China(grant numbers:61702499)"; National Key Research and Development Program of China(grant numbers:2016QY04W0804); Beijing Natural Science Foundation(grant numbers:4172069); Research on Core Technologies of national key infrastructure security supervision platform(grant numbers:Z161100002616032);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8097018"",""Concurrent stack";wait-free;"multicore"",""Indexes";Arrays;Message systems;Synchronization;Semantics;"Multicore processing"","""",""7"","""",""23"",""IEEE"",""7 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Firework: Data Processing and Sharing for Hybrid Cloud-Edge Analytics,""Q. Zhang"; Q. Zhang; W. Shi;" H. Zhong"",""Department of Computer Science, Wayne State University, Detroit, MI"; School of Computer Science and Technology, Anhui University, Hefei, China; Department of Computer Science, Wayne State University, Detroit, MI;" School of Computer Science and Technology, Anhui University, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2004"",""2017"",""Now we are entering the era of the Internet of Everything (IoE) and billions of sensors and actuators are connected to the network. As one of the most sophisticated IoE applications, real-time video analytics is promising to significantly improve public safety, business intelligence, and healthcare & life science, among others. However, cloud-centric video analytics requires that all video data must be preloaded to a centralized cluster or the cloud, which suffers from high response latency and high cost of data transmission, given the scale of zettabytes of video data generated by IoE devices. Moreover, video data is rarely shared among multiple stakeholders due to various concerns, which restricts the practical deployment of video analytics that takes advantages of many data sources to make smart decisions. Furthermore, there is no efficient programming interface for developers and users to easily program and deploy IoE applications across geographically distributed computation resources. In this paper, we present a new computing framework, Firework, which facilitates distributed data processing and sharing for IoE applications via a virtual shared data view and service composition. We designed an easy-to-use programming interface for Firework to allow developers to program on Firework. This paper describes the system design, implementation, and programming interface of Firework. The experimental results of a video analytics application demonstrate that Firework reduces up to 19.52 percent of response latency and at least 72.77 percent of network bandwidth cost, compared to a cloud-centric solution."",""1558-2183"","""",""10.1109/TPDS.2018.2812177"",""US National Science Foundation(grant numbers:CNS-1741635)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8306827"",""Distributed big data processing";edge computing;"internet of everything"",""Cloud computing";Streaming media;Stakeholders;Programming;Edge computing;Data processing;"Distributed databases"","""",""58"","""",""50"",""IEEE"",""5 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"G-CRS: GPU Accelerated Cauchy Reed-Solomon Coding,""C. Liu"; Q. Wang; X. Chu;" Y. -W. Leung"",""Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong"; Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong;" Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1484"",""1498"",""Recently, erasure coding has been extensively deployed in large-scale storage systems to replace data replication. With the increase in disk I/O throughput and network bandwidth, the performance of erasure coding becomes a major bottleneck of erasure-coded storage systems. In this paper, we propose a graphics processing unit (GPU)-based implementation of erasure coding named G-CRS, which employs the Cauchy Reed-Solomon (CRS) code, to overcome the aforementioned bottleneck. To maximize the coding performance of G-CRS, we designed and implemented a set of optimization strategies, such as a compact structure to store the bitmatrix in GPU constant memory, efficient data access through shared memory, and decoding parallelism, to fully utilize the GPU resources. In addition, we derived a simple yet accurate performance model to demonstrate the maximum coding performance of G-CRS on GPU. We evaluated the performance of G-CRS through extensive experiments on modern GPU architectures such as Maxwell and Pascal, and compared with other state-of-the-art coding libraries. The evaluation results revealed that the throughput of G-CRS was 10 times faster than most of the other coding libraries. Moreover, G-CRS outperformed PErasure (a recently developed, well optimized CRS coding library on the GPU) by up to 3 times in the same architecture."",""1558-2183"","""",""10.1109/TPDS.2018.2791438"",""NVIDIA Corporation"; Shenzhen Basic Research(grant numbers:SCI-2015-SZTIC-002); Hong Kong ITF(grant numbers:ITS/443/16FX);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8252749"",""Cauchy reed-solomon code";graphics processing unit;erasure coding;"distributed storage system"",""Encoding";Graphics processing units;Throughput;Libraries;Generators;Decoding;"Instruction sets"","""",""10"","""",""43"",""IEEE"",""9 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"G-ML-Octree: An Update-Efficient Index Structure for Simulating 3D Moving Objects Across GPUs,""Z. Deng"; L. Wang; W. Han; R. Ranjan;" A. Zomaya"",""School of Computer Science, China University of Geosciences, Wuhan, P. R. China"; School of Computer Science, China University of Geosciences, Wuhan, P. R. China; School of Computer Science, China University of Geosciences, Wuhan, P. R. China; School of Computer Science, China University of Geosciences, Wuhan, P. R. China;" School of Information Technologies, University of Sydney, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1075"",""1088"",""In real simulation applications, simulations often involve large volumes of three-dimensinal (3D) moving objects. With the rapid growth of the scale of simulation-problem domains, it has become a key requirement to efficiently manage massive 3D moving objects. Conventional indexing approaches for managing 3D moving objects during simulations generally sufferfrom excessive update costs. Aiming to this problem, this paper first proposes an update-efficient indexing structure by fusing a loose Octree and one update-memo structure, namely ML-Octree. ML-Octree significantly reduces the update costs of one simulation involving massive 3D moving objects. Towards providing a more efficient indexing approach, this paper has explored the feasibility of paralleling ML-Octree by employing Graphic Processing Unit (GPU). A load-balancing scheme is used to further improve the update performance of the GPU-aided ML-Octree. Finally, a distributed GPU-aided ML-Octree is proposed for large-scale simulations. The experimental results indicate that (1) ML-Octree can acquire the update-performance gain of an order of magnitude similar to that of Octree, (2) the GPU-aided ML-Octree can accelerate 5.07χ fasterthan a parallel ML-Octree with 8 CPU threads on average, (3) the load-balance scheme can improve GPU-aided ML-Octree by 2.3χ on average, and (4) the distributed GPU-aided ML-Octree can efficiently support large-scale simulations."",""1558-2183"","""",""10.1109/TPDS.2017.2787747"",""National Science and Technology Major Project of the Ministry of Science and Technology of China(grant numbers:2016ZX05014-003)"; China Postdoctoral Science Foundation(grant numbers:2014M552112);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8241384"",""Parallel processign";streaming data;"big data"",""Indexing";Three-dimensional displays;Octrees;Solid modeling;Graphics processing units;"Load modeling"","""",""2"","""",""43"",""IEEE"",""28 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Game-Based Thermal-Delay-Aware Adaptive Routing (GTDAR) for Temperature-Aware 3D Network-on-Chip Systems,""K. -C. Chen"",""Department of Computer Science and Engineering, National Sun Yat-Sen University, Kaohsiung, Taiwan, Taiwan, R.O.C"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2018"",""2032"",""The thermal problem of three-dimensional Network-on-Chip (3D NoC) is proven to be severer than 2D NoC due to the stacking dies and heterogeneous thermal conduction between each silicon layers. To control the system temperature under a certain thermal limit, the current Dynamic Thermal Managements (DTMs) can be classified into temporal approaches and spatial approaches. The temporal DTM approaches reduce the processing speed of those overheated NoC components. However, for emerging cooling consideration, the full throttling scheme is usually applied as the system temperature reaches the alarming level, which results in significant system performance overhead. On the other hand, the spatial DTM approaches migrate the traffic load away from the overheated components. Although the spatial DTM approaches can mitigate the performance impact during the temperature control, the cooling period is longer than the temporal approaches because of the asynchronous phenomenon of traffic and temperature behavior among the NoC components. To consider the advantages of the temporal and spatial DTM approaches, it is necessary to synchronize the information of traffic and temperature behavior in the NoC systems. In this paper, we apply the Game Theory to propose a Game-based Thermal-Delay-aware Adaptive Routing (GTDAR) scheme. The GTDAR first adopts the Thermal-Delay principle to transfer the long-term temperature information to short-term traffic information by allocating the input buffer length of each NoC routers, which can reduce the thermal problem into the traffic problem. Afterward, the GTDAR involves the Nash Equilibrium property to distribute the packet routing to mitigate the thermal problem by considering the traffic and temperature simultaneously. In our experiments, the proposed Game-based Thermal-Delay-aware Adaptive Routing (GTDAR) scheme can help to improve 8.7 percent to 130 percent system performance with only 2.4 percent area overhead compared with the previous works."",""1558-2183"","""",""10.1109/TPDS.2018.2812164"",""Ministry of Science and Technology, TAIWAN(grant numbers:MOST 104-2218-E-110-011-MY2,MOST 106-2221-E-110-077)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8306659"",""Game theory";buffer allocation;3D NoC;thermal;"nash equilibrium"",""Routing";Three-dimensional displays;Cooling;Adaptive systems;Telecommunication traffic;"Traffic congestion"","""",""5"","""",""44"",""IEEE"",""5 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GFlink: An In-Memory Computing Architecture on Heterogeneous CPU-GPU Clusters for Big Data,""C. Chen"; K. Li; A. Ouyang; Z. Zeng;" K. Li"",""College of Information Science and Engineering, Hunan University, Changsha, Hunan, China"; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; Department of Information Engineering, Zunyi Normal College, Zunyi, Guizhou, China; Institute for Infocomm Research, A*STAR, Singapore;" College of Information Science and Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2018"",""2018"",""29"",""6"",""1275"",""1288"",""The increasing main memory capacity and the explosion of big data have fueled the development of in-memory big data management and processing. By offering an efficient in-memory parallel execution model which can eliminate disk I/O bottleneck, existing in-memory cluster computing platforms (e.g., Flink and Spark) have already been proven to be outstanding platforms for big data processing. However, these platforms are merely CPU-based systems. This paper proposes GFlink, an in-memory computing architecture on heterogeneous CPU-GPU clusters for big data. Our proposed architecture extends the original Flink from CPU clusters to heterogeneous CPU-GPU clusters, greatly improving the computational power of Flink. Furthermore, we have proposed a programming framework based on Flink's abstract model, i.e., DataSet (DST), hiding the programming complexity of GPUs behind the simple and familiar high-level interfaces. To achieve high performance and good load-balance, an efficient JVM-GPU communication strategy, a GPU cache scheme, and an adaptive locality-aware scheduling scheme for three-stage pipelining execution are proposed. Extensive experiment results indicate that the high computational power of GPUs can be efficiently utilized, and the implementation on GFlink outperforms that on the original CPU-based Flink."",""1558-2183"","""",""10.1109/TPDS.2018.2794343"",""Key Program of National Natural Science Foundation of China(grant numbers:61432005)"; National Outstanding Youth Science Program of National Natural Science Foundation of China(grant numbers:61625202); International (Regional) Cooperation and Exchange Program of National Natural Science Foundation of China(grant numbers:61661146006); the Singapore-China NRF-NSFC(grant numbers:NRF2016NRF-NSFC001-111); National Natural Science Foundation of China(grant numbers:61370095,61472124,61662090,61602350); Key Technology Research and Development Programs of Guangdong Province(grant numbers:2015B010108006); National Key R&D Program of China(grant numbers:2016YFB0201303);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8259455"",""Big data";GPGPU;heterogeneous cluster;in-memory computing;"OpenCL"",""Graphics processing units";Sparks;Computational modeling;Big Data;Computer architecture;Data models;"Programming"","""",""73"","""",""29"",""IEEE"",""16 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GrapH: Traffic-Aware Graph Processing,""C. Mayer"; M. A. Tariq; R. Mayer;" K. Rothermel"",""Institute of Parallel and Distributed Systems, University of Stuttgart, Stuttgart, Germany"; Department of Computer Science, FAST - National University of Computer and Emerging Sciences; Institute of Parallel and Distributed Systems, University of Stuttgart, Stuttgart, Germany;" Institute of Parallel and Distributed Systems, University of Stuttgart, Stuttgart, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2018"",""2018"",""29"",""6"",""1289"",""1302"",""Distributed graph processing systems such as Pregel, PowerGraph, or GraphX gained popularity due to their superior performance of data analytics on graph-structured data. These systems employ partitioning algorithms to parallelize graph analytics while minimizing inter-partition communication. Recent partitioning algorithms, however, unrealistically assume a uniform and constant amount of data exchanged between graph vertices (i.e., uniform vertex traffic) and homogeneous network costs between workers hosting the graph partitions. This leads to suboptimal partitioning decisions and inefficient graph processing. To this end, we developed Grapes, the first graph processing system using vertex-cut graph partitioning that considers both, diverse vertex traffic and heterogeneous network costs. The main idea is to avoid frequent communication over expensive network links using an adaptive edge migration strategy. Our evaluations show an improvement of 10 percent in graph processing latency and 60 percent in communication costs compared to state-of-the-art partitioning approaches."",""1558-2183"","""",""10.1109/TPDS.2018.2794989"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8263157"",""Distributed graph processing";partitioning;vertex-cut;heterogeneous;traffic;geo-distributed;adaptive;workload;architecture;network;streaming partitioning;"communication"",""Heuristic algorithms";Automata;Data analysis;Distributed databases;Mirrors;"Partitioning algorithms"","""",""22"","""",""43"",""IEEE"",""18 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GraphD: Distributed Vertex-Centric Graph Processing Beyond the Memory Limit,""D. Yan"; Y. Huang; M. Liu; H. Chen; J. Cheng; H. Wu;" C. Zhang"",""Department of Computer Science, University of Alabama at Birmingham, Birmingham, AL"; Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, Hong Kong;" Department of Computer Science, University of Alabama at Birmingham, Birmingham, AL"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""99"",""114"",""We propose GraphD, an out-of-core Pregel-like system targeting efficient big graph processing with a small cluster of commodity PCs connected by Gigabit Ethernet, an environment affordable to most users. This is in contrast to some recent efforts for out-of-core graph computation with specialized hardware. In our setting, a vertex-centric program is often data-intensive, since the CPU cost of calculating a message value is negligible compared with the network cost of transmitting that message. As a result, network bandwidth is usually the bottleneck, and out-of-core execution would not sacrifice performance if disk IO overhead can be hidden by message transmission, which is achieved by GraphD through the parallelism of computation and communication. GraphD streams edge and message data on local disks, and thus consumes negligible memory space. For a broad class of Pregel algorithms where message combiner is applicable, GraphD completely eliminates the need of any expensive external-memory join or group-by, which is required by existing systems such as Pregelix and Chaos. Extensive experiments show that GraphD beats existing out-of-core systems by orders of magnitude, and achieves comparable performance to in-memory systems running with adequate memory."",""1558-2183"","""",""10.1109/TPDS.2017.2743708"",""CUHK(grant numbers:14206715,14222816)"; Kong RGC, MSRA(grant numbers:6904224); ITF(grant numbers:6904079,3132821); CUHK;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8016377"",""Out-of-core";graph;vertex-centric;"Pregel"",""Bandwidth";Venus;Chaos;Memory management;Business;Aggregates;"Computational modeling"","""",""30"","""",""33"",""IEEE"",""24 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"High-Speed Transfer Optimization Based on Historical Analysis and Real-Time Tuning,""E. Arslan";" T. Kosar"",""University of Nevada, Reno, NV";" University at Buffalo, SUNY, Buffalo, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1303"",""1316"",""Data-intensive scientific and commercial applications increasingly require frequent movement of large datasets from one site to the other(s). Despite growing network capacities, these data movements rarely achieve the promised data transfer rates of the underlying physical network due to poorly tuned data transfer protocols. Accurately and efficiently tuning the data transfer protocol parameters in a dynamically changing network environment is a major challenge and remains as an open research problem. In this paper, we present a novel dynamic parameter tuning algorithm based on historical data analysis and real-time background traffic probing, dubbed HARP. Most of the previous work in this area are solely based on real-time network probing or static parameter tuning, which either result in an excessive sampling overhead or fail to accurately predict the optimal transfer parameters. Combining historical data analysis with real-time sampling lets HARP tune the application-layer data transfer parameters accurately and efficiently to achieve close-to-optimal end-to-end data transfer throughput with very low overhead. Instead of one-time parameter estimation, HARP uses a feedback loop to adjust the parameter values to changing network conditions in real-time. Our experimental analyses over a variety of network settings show that HARP outperforms existing solutions by up to 50 percent in terms of the achieved data transfer throughput."",""1558-2183"","""",""10.1109/TPDS.2018.2790948"",""US National Science Foundation (NSF)(grant numbers:OAC-1724898)"; US National Science Foundation(grant numbers:ACI-1548562);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8249824"",""High performance networks";transfer tuning;application-layer optimization;network modeling;"online tuning"",""Throughput";Tuning;Concurrent computing;Data transfer;Real-time systems;Heuristic algorithms;"Pipeline processing"","""",""15"","""",""41"",""IEEE"",""8 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Holistic Virtual Machine Scheduling in Cloud Datacenters towards Minimizing Total Energy,""X. Li"; P. Garraghan; X. Jiang; Z. Wu;" J. Xu"",""Department of Computer Science and Technology, Zhejiang University, Hangzhou, China"; School of Computing & Communications, Lancaster University, LA, United Kingdom; Department of Computer Science and Technology, Zhejiang University, Hangzhou, China; Department of Computer Science and Technology, Zhejiang University, Hangzhou, China;" School of Computing, University of Leeds, Leeds, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2018"",""2018"",""29"",""6"",""1317"",""1331"",""Energy consumed by Cloud datacenters has dramatically increased, driven by rapid uptake of applications and services globally provisioned through virtualization. By applying energy-aware virtual machine scheduling, Cloud providers are able to achieve enhanced energy efficiency and reduced operation cost. Energy consumption of datacenters consists of computing energy and cooling energy. However, due to the complexity of energy and thermal modeling of realistic Cloud datacenter operation, traditional approaches are unable to provide a comprehensive in-depth solution for virtual machine scheduling which encompasses both computing and cooling energy. This paper addresses this challenge by presenting an elaborate thermal model that analyzes the temperature distribution of airflow and server CPU. We propose GRANITE - a holistic virtual machine scheduling algorithm capable of minimizing total datacenter energy consumption. The algorithm is evaluated against other existing workload scheduling algorithms MaxUtil, TASA, IQR and Random using real Cloud workload characteristics extracted from Google datacenter tracelog. Results demonstrate that GRANITE consumes 4.3-43.6 percent less total energy in comparison to the state-of-the-art, and reduces the probability of critical temperature violation by 99.2 with 0.17 percent SLA violation rate as the performance penalty."",""1558-2183"","""",""10.1109/TPDS.2017.2688445"",""National High Technology Research 863 Major Program of China(grant numbers:2011AA01A207)"; National Science Foundation of China(grant numbers:61272128); EPSRC(grant numbers:EP/P031617/1); program of China Scholarship (SCS);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7888576"",""Cloud computing";energy efficiency;datacenter modeling;workload scheduling;"virtual machine"",""Servers";Cooling;Computational modeling;Cloud computing;Processor scheduling;Virtual machining;"Energy consumption"","""",""75"","""",""42"",""IEEE"",""28 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Hybrid Transactional Replication: State-Machine and Deferred-Update Replication Combined,""T. Kobus"; M. Kokociński;" P. T. Wojciechowski"",""Institute of Computing Science, Poznan University of Technology, Poznań, Poland"; Institute of Computing Science, Poznan University of Technology, Poznań, Poland;" Institute of Computing Science, Poznan University of Technology, Poznań, Poland"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1499"",""1514"",""We propose Hybrid Transactional Replication (HTR), a novel replication scheme for highly dependable services. It combines two schemes: a transaction is executed either optimistically by only one service replica in the deferred update mode (DU), or deterministically by all replicas in the state machine mode (SM)";" the choice is made by an oracle. The DU mode allows for parallelism and thus takes advantage of multicore hardware. In contrast to DU, the SM mode guarantees abort-free execution, soit is suitable for irrevocable operations and transactions generating high contention. For expressiveness, transactions can be discarded or retried on demand. We prove that the higher flexibility of the scheme does not come at the cost of weaker guarantees for clients: HTR satisfies strong consistency guarantees akin to those provided by other popular transactional replication schemes such as Deferred Update Replication. We developed HTR-enabled Paxos STM, an object-based distributed transactional memory system, and evaluated it thoroughly under various workloads. We show the benefits of using a novel oracle that relies on machine learning techniques for automatic adaptation to changing conditions. The ML-based oracle, based on algorithms for the multi-armed bandit problem, provides up to 50 percent improvement in throughput when compared to the system running with DU-only or SM-only oracles."",""1558-2183"","""",""10.1109/TPDS.2018.2796079"",""National Science Centre (NCN)(grant numbers:DEC-2011/01/N/ST6/06762)"; Foundation for Polish Science (FNP)(grant numbers:103/UD/SKILLS/2014);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8265027"",""State machine replication";transactional replication;deferred update;"distributed transactional memory"",""Protocols";Semantics;Synchronization;Scalability;Throughput;Fault tolerance;"Fault tolerant systems"","""",""1"","""",""59"",""IEEE"",""22 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"IBOM: An Integrated and Balanced On-Chip Memory for High Performance GPGPUs,""J. Wang"; Q. Wang; L. Jiang; C. Li; X. Liang;" N. Jing"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China;" Department of Micro-Nano Electronics, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""586"",""599"",""GPGPU accelerated computing has revolutionized a broad range of applications. To serve between the ever-growing computing capability and external memory, the on-chip memory is becoming increasingly important to GPGPU performance for general-purpose computing. Inherited from the traditional CPUs, however, the contemporary GPGPU on-chip memory design is suboptimal to the SIMT (single instruction, multiple threads) execution. In particular, the on-chip first-level data (L1D) cache thrashing, resulting from insufficient capacity and imbalanced usage, leads to a low hit rate and limits the overall performance. In this study, we reform the contemporary on-chip memory design and propose an integrated and balanced on-chip memory (IBOM) architecture for high-performance GPGPUs. It first virtually enlarges the L1D cache size by an integrated architecture that exploits the under-utilized register file (RF) with lightweight ISA, compiler and microarchitecture supports. Then with sufficient capacity, it is able to improve the cache usage by a set balancing technique that exploits the under-utilized set resources. In our proposed IBOM design, the register and cache accesses are amenable to normal pipeline operations with simple changes. It adequately exploits the size inversion in GPGPU on-chip memory, and enables optimized utilization of the precious resources for higher performance and energy efficiency with even smaller on-chip memory size. The experiment results demonstrate that the proposed IBOM design can offer an average of 29.6 percent increase in L1D hit rate and in turn 3X performance improvement for the cache-sensitive applications."",""1558-2183"","""",""10.1109/TPDS.2017.2773516"",""National Natural Science Foundation of China(grant numbers:61772331,61402285,61602300,61202026,61332001)"; Program of China National 1000 Young Talent Plan;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107587"",""GPGPU";cache thrashing;register file;integrated memory;set balancing;compiler;"high performance"",""Radio frequency";System-on-chip;Registers;Instruction sets;Computer architecture;Kernel;"Graphics processing units"","""",""4"","""",""46"",""IEEE"",""14 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"iDaaS: Inter-Datacenter Network as a Service,""W. Li"; D. Guo; K. Li; H. Qi;" J. Zhang"",""School of Computer Science and Technology, Dalian University of Technology, Dalian, China"; College of Information System and Management, National University of Defense Technology, Changsha, P.R. China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China; School of Computer Science and Technology, Dalian University of Technology, Dalian, China;" School of Computer Science and Technology, Dalian University of Technology, Dalian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1515"",""1529"",""Increasing number of Internet-scale applications, such as video streaming, incur huge amount of wide area traffic. Such traffic over the unreliable Internet without bandwidth guarantee suffers unpredictable network performance. This result, however, is unappealing to the application providers. Fortunately, Internet giants like Google and Microsoft are increasingly deploying their private wide area networks (WANs) to connect their global datacenters. Such high-speed private WANs are reliable, and can provide predictable network performance. In this paper, we propose a new type of service-inter-datacenter network as a service (iDaaS), where traditional application providers can reserve bandwidth from those Internet giants to guarantee their wide area traffic. Specifically, we design a bandwidth trading market among multiple iDaaS providers and application providers, and concentrate on the essential bandwidth pricing problem. The involved challenging issue is that the bandwidth price of each iDaaS provider is not only influenced by other iDaaS providers, but also affected by the application providers. To address this issue, we characterize the interaction between iDaaS providers and application providers using a Stackelberg game model, and analyze the existence and uniqueness of the equilibrium. We further present an efficient bandwidth pricing algorithm by blending the advantage of a geometrical Nash bargaining solution and the demand segmentation method. For comparison, we present two bandwidth reservation algorithms, where each iDaaS provider's bandwidth is reserved in a weighted fair manner and a max-min fair manner, respectively. Finally, we conduct comprehensive trace-driven experiments. The evaluation results show that our proposed algorithms not only ensure the revenue of iDaaS providers, but also provide bandwidth guarantee for application providers with lower bandwidth price per unit."",""1558-2183"","""",""10.1109/TPDS.2015.2505731"",""National Key Research and Development Program of China(grant numbers:2016YFB1000205)"; State Key Program of National Natural Science of China(grant numbers:61432002); National Science Foundation for Distinguished Young Scholars of China(grant numbers:61225010); NSFC(grant numbers:61772112,61672379,61272417,61370199); Specialized Research Fund for the Doctoral Program of Higher Education(grant numbers:20130041110019); Fundamental Research Funds for the Central Universities(grant numbers:DUT15QY20); National Natural Science Foundation for Outstanding Excellent young scholars of China(grant numbers:61422214); National Basic Research Program (973 program)(grant numbers:2014CB347800);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7347416"",""Inter-datacenter network";WANs;bandwidth price;"Stackelberg game"",""Bandwidth";Internet;Wide area networks;Pricing;Games;Distributed databases;"Google"","""",""10"","""",""35"",""IEEE"",""4 Dec 2015"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Implementing Snapshot Objects on Top of Crash-Prone Asynchronous Message-Passing Systems,""C. Delporte-Gallet"; H. Fauconnier; S. Rajsbaum;" M. Raynal"",""IRIF/GANG, Université Paris Diderot, Paris, France"; IRIF/GANG, Université Paris Diderot, Paris, France; Instituto de Matemáticas, National Autonomous University of Mexico, Mexico, Mexico;" Hong Kong Polytechnic University, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2033"",""2045"",""In asynchronous crash-prone read/write shared-memory systems there is the notion of a snapshot object, which simulates the behavior of an array of single-writer/multi-reader (SWMR) shared registers that can be read atomically. Processes in the system can access the object invoking (any number of times) two operations, denoted write() and snapshot(). A process invokes write() to update the value of its register in the array. When it invokes snapshot(), the process obtains the values of all registers, as if it read them simultaneously. It is known that a snapshot object can be implemented on top of SWMR registers, tolerating any number of process failures. Snapshot objects provide a level of abstraction higher than individual SWMR registers, and they simplify the design of applications. Building a snapshot object on an asynchronous crash-prone message-passing system has similar benefits. The object can be implemented by using the known simulations of a SWMR shared memory on top of an asynchronous message-passing system (if less than half the processes can crash), and then build a snapshot object on top of the simulated SWMR memory. This paper presents an algorithm that implements a snapshot object directly on top of the message-passing system, without building an intermediate layer of a SWMR shared memory. To the authors knowledge, the proposed algorithm is the first providing such a direct construction. The algorithm is more efficient than the indirect solution, yet relatively simple."",""1558-2183"","""",""10.1109/TPDS.2018.2809551"",""French ANR"; UNAM PAPIIT-DGAPA(grant numbers:IN107714,IN109917); INRIA-UNAM Equipe Associé LiDiCo (at the Limits of Distributed Computability); Fordecyt(grant numbers:265667);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302604"",""Asynchronous message-passing system";atomic read/write register;linearizability;process crash failure;"snapshot object"",""Registers";Computer crashes;Message passing;Atomic layer deposition;Buildings;Concurrent computing;"Computational modeling"","""",""7"","""",""38"",""IEEE"",""26 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving Medium-Grain Partitioning for Scalable Sparse Tensor Decomposition,""S. Acer"; T. Torun;" C. Aykanat"",""Computer Engineering Department, Bilkent University, Ankara, Turkey"; Computer Engineering Department, Bilkent University, Ankara, Turkey;" Computer Engineering Department, Bilkent University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2814"",""2825"",""Tensor decomposition is widely used in the analysis of multi-dimensional data. The canonical polyadic decomposition (CPD) is one of the most popular decomposition methods and commonly found by the CPD-ALS algorithm. High computational and memory costs of CPD-ALS necessitate the use of a distributed-memory-parallel algorithm for efficiency. The medium-grain CPD-ALS algorithm, which adopts multi-dimensional cartesian tensor partitioning, is one of the most successful distributed CPD-ALS algorithms for sparse tensors. This is because cartesian partitioning imposes nice upper bounds on communication overheads. However, this model does not utilize the sparsity pattern of the tensor to reduce the total communication volume. The objective of this work is to fill this literature gap. We propose a novel hypergraph-partitioning model, CartHP, whose partitioning objective correctly encapsulates the minimization of total communication volume of multi-dimensional cartesian tensor partitioning. Experiments on twelve real-world tensors using up to 1024 processors validate the effectiveness of the proposed CartHP model. Compared to the baseline medium-grain model, CartHP achieves average reductions of 52, 43 and 24 percent in total communication volume, communication time and overall runtime of CPD-ALS, respectively."",""1558-2183"","""",""10.1109/TPDS.2018.2841843"",""Scientific and Technological Research Council of Turkey (TUBITAK)(grant numbers:EEEAG-116E043)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8368258"",""Sparse tensor";canonical polyadic decomposition;cartesian partitioning;load balancing;communication volume;"hypergraph partitioning"",""Tensile stress";Program processors;Partitioning algorithms;Solid modeling;Mathematical model;Runtime;"Matrix decomposition"","""",""8"","""",""37"",""IEEE"",""29 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving Restore Performance in Deduplication-Based Backup Systems via a Fine-Grained Defragmentation Approach,""Y. Tan"; B. Wang; J. Wen; Z. Yan; H. Jiang;" W. Srisa-an"",""College of Computer Science, Chongqing University, Chongqing, China"; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; University of Texas Arlington, Arlington, TX; University of Texas Arlington, Arlington, TX;" University of Nebraska Lincoln, Lincoln, NE"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2254"",""2267"",""In deduplication-based backup systems, the removal of redundant data transforms the otherwise logically adjacent data chunks into physically scattered chunks on the disks. This, in effect, changes the retrieval operations from sequential to random and significantly degrades the performance of restoring data. These scattered chunks are called fragmented data and many techniques have been proposed to identify and sequentially rewrite such fragmented data to new address areas, trading off the increased storage space for reduced number of random reads (disk seeks) to improve the restore performance. However, existing solutions for backup workloads share a common assumption that every read operation involves a large fixed-size window of contiguous chunks, which restricts the fragment identification to a fixed-size read window. This can lead to inaccurate identifications due to false positives since the data fragments can vary in size and appear in any different and unpredictable address locations. Based on these observations, we propose FGDEFRAG, a Fine-Grained defragmentation approach that uses variable-sized and adaptively located data groups, instead of using fixed-size read windows, to accurately identify and effectively remove fragmented data. When we compare its performance to those of existing solutions, FGDEFRAG not only reduces the amount of rewritten data but also significantly improves the restore performance. Our experimental results show that FGDEFRAG can improve the restore performance by 14 to 329 percent, while simultaneously reducing the rewritten data by 25 to 87 percent."",""1558-2183"","""",""10.1109/TPDS.2018.2828842"",""National Natural Science Foundation of China(grant numbers:61402061)"; Chongqing Basic and Frontier Research Project of China(grant numbers:cstc2016jcyjA0274); US National Science Foundation(grant numbers:CCF-1704504,CCF-1629625); NetApp Grant;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8344493"",""Data deduplication";defragmentation;deduplication ratio;"restore performance"",""Containers";Bandwidth;Distributed databases;Transforms;Approximation algorithms;Microsoft Windows;"Indexes"","""",""12"","""",""21"",""IEEE"",""20 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Intra-Node Memory Safe GPU Co-Scheduling,""C. Reaño"; F. Silla; D. S. Nikolopoulos;" B. Varghese"",""Universitat Politècnica de València, València, Spain"; Universitat Politècnica de València, València, Spain; Queen’s University Belfast, Belfast, United Kingdom;" Queen’s University Belfast, Belfast, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1089"",""1102"",""GPUs in High-Performance Computing systems remain under-utilised due to the unavailability of schedulers that can safely schedule multiple applications to share the same GPU. The research reported in this paper is motivated to improve the utilisation of GPUs by proposing a framework, we refer to as schedGPU, to facilitate intra-node GPU co-scheduling such that a GPU can be safely shared among multiple applications by taking memory constraints into account. Two approaches, namely a client-server and a shared memory approach are explored. However, the shared memory approach is more suitable due to lower overheads when compared to the former approach. Four policies are proposed in schedGPU to handle applications that are waiting to access the GPU, two of which account for priorities. The feasibility of schedGPU is validated on three real-world applications. The key observation is that a performance gain is achieved. For single applications, a gain of over 10 times, as measured by GPU utilisation and GPU memory utilisation, is obtained. For workloads comprising multiple applications, a speed-up of up to 5x in the total execution time is noted. Moreover, the average GPU utilisation and average GPU memory utilisation is increased by 5 and 12 times, respectively."",""1558-2183"","""",""10.1109/TPDS.2017.2784428"",""Generalitat Valenciana(grant numbers:PROMETEO/2017/77)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8219713"",""GPU co-scheduling";access synchronisation;memory safe;accelerator;under-utilisation;"schedGPU"",""Graphics processing units";Memory management;Libraries;Schedules;Servers;Nonvolatile memory;"Scheduling"","""",""7"","""",""35"",""IEEE"",""18 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Joint DVFS and Parallelism for Energy Efficient and Low Latency Software Video Decoding,""y. Benmoussa"; E. Senn; N. Derouineau; N. Tizon;" J. Boukhobza"",""University M'Hamed Bougara de Boumerdes, LMSS, Boumerdès, Algeria"; University of South Brittany, Lab-STICC, France; Vitec Multimedia, Chatillon, France; Vitec Multimedia, Chatillon, France;" University of Western Brittany, Lab-STICC, Brest, France"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""858"",""872"",""In this paper, we aim to bridge the gap between the energy efficiency of software and hardware video decoders by combining both DVFS and parallelism. For this purpose, we, first, propose an adaptive DVFS algorithm for energy efficient mono-core decoding of H.264 videos. The proposed solution uses metadata (normalized by MPEG) providing information about the upcoming workload. These metadata are processed within an adaptive filter to build dynamically an accurate complexity model used to calculate the minimal processor frequencies for decoding video frames while guaranteeing real time constraints. Then, we generalize the proposed DVFS to slice-based multi-threaded parallel video decoders on multi-core platforms. Our performance evaluations showed that the proposed algorithm for mono-core decoding is able to converge to an accurate complexity model (4 percent) in less than 1 second. Moreover, it is simple to implement, induces very low overhead and achieves up to 46 percent energy saving as compared to the ondemand Linux DVFS governor. On the other hand, joint use of parallelism and DVFS allows 720p software video decoding with only 17 percent more energy consumption as compared to a hardware video decoder."",""1558-2183"","""",""10.1109/TPDS.2017.2779812"",""BPI France, Région Ile-de-France"; Région Bretagne and Rennes Métropole;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8148962"",""DVFS";multi-core;video decoding;green metadata;"energy efficiency"",""Decoding";Streaming media;Hardware;Complexity theory;Software;Transform coding;"Power demand"","""",""3"","""",""53"",""IEEE"",""4 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Joint Scheduling and Source Selection for Background Traffic in Erasure-Coded Storage,""S. Li"; T. Lan; M. -R. Ra;" R. Panta"",""ECE, George Washington University, Washington, DC, USA"; ECE, George Washington University, Washington, DC, USA; AT&T Labs Research, Florham Park, NJ, USA;" AT&T Labs Research, Florham Park, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2826"",""2837"",""Erasure-coded storage systems have gained considerable adoption recently since they can provide the same level of reliability with significantly lower storage overhead compared to replicated systems. However, background traffic of such systems – e.g., repair, rebalance, backup and recovery traffic – often has large volume and consumes significant network resources. Independently scheduling such tasks and selecting their sources can easily create interference among data flows, causing severe deadline violation. We show that the well-known heuristic scheduling algorithms fail to consider important constraints, thus resulting in unsatisfactory performance. In this paper, we claim that an optimal scheduling algorithm, which aims to maximize the number of background tasks completed before deadlines, must simultaneously consider task deadline, network topology, chunk placement, and time-varying resource availability. We first show that the corresponding optimization problem is NP-hard. Then we propose a novel algorithm, called Linear Programming for Selected Tasks (LPST) to maximize the number of successful tasks and improve overall utilization of the datacenter network. It jointly schedules tasks and selects their sources based on a notion of Remaining Time Flexibility, which measures the slackness of the starting time of a task. We evaluated the efficacy of our algorithm using extensive simulations and validate the results with experiments in a real cloud environment. Our results show that, under certain scenarios, LPST can perform 7x$\sim$ 10x better than the heuristics which blindly treat the infrastructure as a collection of homogeneous resources, and 21.7 $\sim$ 65.9 percent better than the algorithms that only take the network topology into account."",""1558-2183"","""",""10.1109/TPDS.2018.2845845"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8378267"",""Traffic scheduling";erasure code;"storage"",""Task analysis";Bandwidth;Servers;Scheduling algorithms;Network topology;"Maintenance engineering"","""",""4"","""",""44"",""IEEE"",""11 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Large-Scale and Extreme-Scale Computing with Stranded Green Power: Opportunities and Costs,""F. Yang";" A. A. Chien"",""Department of Computer Science, The University of Chicago, Chicago, IL";" Department of Computer Science, The University of Chicago, Chicago, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2018"",""2018"",""29"",""5"",""1103"",""1116"",""Power consumption and associated carbon emissions are increasingly critical challenges for large-scale computing. Recent research proposes exploiting stranded power - uneconomic renewable power - for green supercomputing in a system called Zero-Carbon Cloud (ZCCloud) [1], [2], [3]. These efforts studied production supercomputing workloads on stranded-power based computing resources, demonstrating their achievable productivity. We explore economic viability of stranded-power based supercomputing, using three datacenter total-cost-of-ownership (TCO) models to study cost-effectiveness. These studies show that ZCCloud's approach can be cost-effective in the USA today, and is even more attractive in regions with higher power prices (e.g., Japan, Germany), achieving cost advantages as large as 50 percent. Environmental and power-grid benefits are a further advantage. We also explore the sensitivity of these results to changes in hardware TCO";" cheaper hardware or longer lifetimes magnify the attractiveness of stranded-power based approaches, yielding advantages as large as 91 percent. These results are robust across different TCO models. Finally, we study extreme-scale supercomputers (>100 MW), finding stranded-power can increase peak capability per cost by as much as 80 percent."",""1558-2183"","""",""10.1109/TPDS.2017.2782677"",""NSF";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8187696"",""Supercomputing";extreme scale;cost;power grid;"stranded power"",""MISO";Wind power generation;Carbon dioxide;Power grids;Hardware;"Biological system modeling"","""",""10"","""",""92"",""IEEE"",""12 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Lattice-Based Turn Model for Adaptive Routing,""E. Fusella";" A. Cilardo"",""Department of Electrical Engineering and Information Technologies, University of Naples Federico II, Naples, Italy";" Department of Electrical Engineering and Information Technologies, University of Naples Federico II, Naples, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1117"",""1130"",""This paper presents a model for designing partially adaptive logic-based distributed routing algorithms. Unlike the previous methods, the Lattice-based Turn Model associates turn prohibitions with the points of different full-rank integer lattices. Due to the generality of the proposed model, existing approaches can be considered a subset of the solution space identified by this work. Morover, we propose three theorems that are instrumental to the design of lattice-based routing algorithms. In particular, the second theorem gives a necessary and sufficient condition to prove that a lattice-based routing algorithm is deadlock-free as long as the lattice basis meets certain requirements. Based on the proposed model, a novel routing algorithm, called lattice-based routing algorithm (LBRA), is presented. Simulation results exhibit encouraging performance improvements over state-of-the-art approaches when considering real and synthetic benchmarks. For instance, average 71 and 18 percent latency reductions are observed under transpose1 traffic compared to, respectively, Odd-Even and Repetitive Turn Model routing algorithms. Furthermore, LBRA achieves up to 38 and 17 percent performance improvement under real traffic as compared to Odd-Even and Repetitive Turn Model routing algorithms."",""1558-2183"","""",""10.1109/TPDS.2017.2787705"",""H2020 Future and Emerging Technologies(grant numbers:671668)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8240635"",""Network-on-chip";adaptive routing;deadlock;mesh;"turn model"",""Routing";Algorithm design and analysis;Adaptation models;System recovery;Lattices;Two dimensional displays;"Mathematical model"","""",""5"","""",""27"",""IEEE"",""27 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Learning-Based Memory Allocation Optimization for Delay-Sensitive Big Data Processing,""L. Tsai"; H. Franke; C. -S. Li;" W. Liao"",""Graduate Institute of Electrical Engineering, National Taiwan University, Taipei, Taiwan"; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; Accenture, San Jose, CA;" Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2018"",""2018"",""29"",""6"",""1332"",""1341"",""Optimal resource provisioning is essential for scalable big data analytics. However, it has been difficult to accurately forecast the resource requirements before the actual deployment of these applications as their resource requirements are heavily application and data dependent. This paper identifies the existence of effective memory resource requirements for most of the big data analytic applications running inside JVMs in distributed Spark environments. Provisioning memory less than the effective memory requirement may result in rapid deterioration of the application execution in terms of its total execution time. A machine learning-based prediction model is proposed in this paper to forecast the effective memory requirement of an application given its service level agreement. This model captures the memory consumption behavior of big data applications and the dynamics of memory utilization in a distributed cluster environment. With an accurate prediction of the effective memory requirement, it is shown that up to 60 percent savings of the memory resource is feasible if an execution time penalty of 10 percent is acceptable. The accuracy of the model is evaluated on a physical Spark cluster with 128 cores and 1TB of total memory. The experiment results show that the proposed solution can predict the minimum required memory size for given acceptable delays with high accuracy, even if the behavior of target applications is unknown during the training of the model."",""1558-2183"","""",""10.1109/TPDS.2018.2800011"",""Ministry of Science and Technology, Taiwan(grant numbers:MOST 104-2221-E-002-081-MY3,MOST 104-2917-I-002-038)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8276656"",""Big data";spark;memory over-commitment;garbage collection;profiling;modeling;"performance-cost tradeoff"",""Memory management";Sparks;Predictive models;Big Data;Resource management;Task analysis;"Java"","""",""12"","""",""46"",""IEEE"",""31 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Light Weight Write Mechanism for Cloud Data,""M. Jahan"; M. Rezvani; Q. Zhao; P. S. Roy; K. Sakurai; A. Seneviratne;" S. Jha"",""University of New South Wales, Sydney, NSW, Australia"; Faculty of Computer Engineering, Shahrood University of Technology, Shahrood, Semnan, Iran; University of New South Wales, Sydney, NSW, Australia; Information Security Group, KDDI Research Inc., Fujimino-shi, Japan; Faculty of Information Science and Electrical Engineering, Kyushu University, Fukuoka, Japan; University of New South Wales, Sydney, NSW, Australia;" University of New South Wales, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1131"",""1146"",""Outsourcing data to the cloud for computation and storage has been on the rise in recent years. In this paper we investigate the problem of supporting write operation on the outsourced data for clients using mobile devices. We consider the Ciphertext-Policy Attribute-based Encryption (CP-ABE) scheme as it is well suited to support access control in outsourced cloud environments. One shortcoming of CP-ABE is that users can modify the access policy specified by the data owner if write operations are incorporated in the scheme. We propose a protocol for collaborative processing of outsourced data that enables the authorized users to perform write operation without being able to alter the access policy specified by the data owner. Our scheme is accompanied with a light weight signature scheme and simple, inexpensive user revocation mechanism to make it suitable for processing on resource-constrained mobile devices. The implementation and detailed performance analysis of the scheme indicate the suitability of the proposed scheme for real mobile applications. Moreover, the security analysis demonstrates that the security properties of the system are not compromised."",""1558-2183"","""",""10.1109/TPDS.2017.2782253"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8186229"",""Mobile computing";attribute-based encryption;access controls;"security and privacy"",""Encryption";Cloud computing;Access control;Mobile handsets;"Servers"","""",""3"","""",""31"",""IEEE"",""11 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"List-Scheduling versus Cluster-Scheduling,""H. Wang";" O. Sinnen"",""University of Auckland, Auckland, New Zealand";" University of Auckland, Auckland, New Zealand"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1736"",""1749"",""In scheduling theory and parallel computing practice, programs are often represented as directed acyclic graphs. Finding a makespan-minimising schedule for such a graph on a given number of homogenous processors (PIprec";" cijICmax) is an NP-hard optimisation problem. Among the many proposed heuristics, the two dominant approaches are list-scheduling and cluster-scheduling (based on clustering), whereby clustering targets an unlimited number of processors at its core. Given their heuristic nature, many experimental comparisons exist. However, their overwhelming majority compares algorithms within but not across categories. Hence it is not clear how cluster-scheduling, for a limited number of processors, performs relative to list-scheduling or how list-scheduling, for an unlimited number of processors, performs against clustering. This study addresses these open questions by comparing a large set of representative algorithms from the two approaches in an extensive experimental evaluation. The algorithms are discussed and studied in a modular nature, categorizing algorithms into components. Some of the included algorithms are previously unpublished combinations of these techniques. This approach also permits to study the separate merit of techniques like task insertion or lookahead. The results show that simple low-complexity algorithms are surprisingly competitive and that more sophisticated algorithms only exhibit their strengths under certain conditions."",""1558-2183"","""",""10.1109/TPDS.2018.2808959"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301529"",""Task scheduling with communication delay";list-scheduling;clustering;DAG;parallel processing;"experiment"",""Task analysis";Program processors;Clustering algorithms;Processor scheduling;Schedules;Scheduling;"Heuristic algorithms"","""",""34"","""",""41"",""IEEE"",""23 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Long-Term Multi-Resource Fairness for Pay-as-you Use Computing Systems,""S. Tang"; Z. Niu; B. He; B. -S. Lee;" C. Yu"",""School of Computer Science & Technology, Tianjin University, Tianjin, China"; School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore;" School of Computer Science & Technology, Tianjin University, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1147"",""1160"",""Many current computing systems such as clouds and supercomputers charge users for their resource usages. A user's demand is often changing over time, indicating that it is difficult to keep the high resource utilization all the time for cost efficiency. Resource sharing is a classical and effective approach for high resource utilization. In view of the heterogeneous resource demands of users' workloads, multi-resource allocation fairness is a must for resource sharing in such pay-as-you-use computing systems. However, we find that, existing multi-resource fair policies such as Dominant Resource Fairness (DRF), implemented in currently popular resource management systems such as Apache YARN [4] and Mesos [23], are not suitable for the pay-as-you-use computing systems. We show that this is because of their memoryless characteristic that can cause the following problems in the pay-as-you-use computing systems: 1). users can get resource benefits by cheating";" 2). users might not be able to get the total amount of resources that they are entitled to in terms of their resource contributions. In this paper, we propose a new policy called H-MRF, which generalizes DRF and Asset Fairness with the long-term notion. We show that it can address these problems and is suitable for pay-as-you-use computing systems. We have implemented it into YARN by developing a prototype called MRYARN. Finally, we evaluate H-MRF using both testbed and simulated experiments. The experimental results show that there are about 1.1 ~1.5 sharing benefit degrees and 1.2× ~ 1.8× performance improvement for users with H-MRF, better than existing fair schedulers."",""1558-2183"","""",""10.1109/TPDS.2017.2788880"",""National Natural Science Foundation of China(grant numbers:61602336,61772544,U1731125)"; National Natural Science Foundation of China(grant numbers:U1531111,U1731243); MoE AcRF Tier 1(grant numbers:T1 251RES1610); NUS;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8244302"",""Long-term multi-resource fairness";cloud computing;supercomputing;YARN;"MRYARN"",""Resource management";Cloud computing;Pricing;Yarn;Computational modeling;"Aggregates"","""",""14"","""",""42"",""IEEE"",""1 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Loop Tiling in Large-Scale Stencil Codes at Run-Time with OPS,""I. Z. Reguly"; G. R. Mudalige;" M. B. Giles"",""PPCU ITK, Budapest, Hungary"; Department of Computer Science, University of Warwick, Coventry, United Kingdom;" Maths Institute, University of Oxford, Oxford, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""873"",""886"",""The key common bottleneck in most stencil codes is data movement, and prior research has shown that improving data locality through optimisations that optimise across loops do particularly well. However, in many large PDE applications it is not possible to apply such optimisations through compilers because there are many options, execution paths and data per grid point, many dependent on run-time parameters, and the code is distributed across different compilation units. In this paper, we adapt the data locality improving optimisation called tiling for use in large OPS applications both in shared-memory and distributed-memory systems, relying on run-time analysis and delayed execution. We evaluate our approach on a number of applications, observing speedups of 2× on the Cloverleaf 2D/3D proxy applications, which contain 83(2D)/141(3D) loops, 3.5× on the linear solver TeaLeaf, and 1.7× on the compressible Navier-Stokes solver OpenSBLI. We demonstrate strong and weak scalability on up to 4608 cores of CINECA's Marconi supercomputer. We also evaluate our algorithms on Intel's Knights Landing, demonstrating maintained throughput as the problem size grows beyond 16GB, and we do scaling studies up to 8704 cores. The approach is generally applicable to any stencil DSL that provides per loop nest data access information."",""1558-2183"","""",""10.1109/TPDS.2017.2778161"",""János Bólyai Research Scholarship of the Hungarian Academy of Sciences"; UK Engineering and Physical Sciences Research Council(grant numbers:EP/K038494/1,EP/K038486/1,EP/K038451/1,EP/K038567/1); Hungarian Human Resources Development Operational Programme(grant numbers:EFOP-3.6.2-16-2017-00013);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8121995"",""DSL";tiling;cache blocking;memory locality;OPS;stencil;"structured mesh"",""Optimization";Libraries;Algorithm design and analysis;DSL;Computer architecture;Schedules;"Electronic mail"","""",""22"","""",""47"",""IEEE"",""28 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"LVRM: On the Design of Efficient Link Based Virtual Resource Management Algorithm for Cloud Platforms,""P. K. Sahoo"; C. K. Dehury;" B. Veeravalli"",""Department of Computer Science and Information Engineering, Chang Gung University, Guishan, Taiwan"; Department of Computer Science and Information Engineering, Chang Gung University, Guishan, Taiwan;" Department of Electrical and Computer Engineering, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""887"",""900"",""Virtualization technology boosts up traditional computing concept to cloud computing by introducing Virtual Machines (VMs) over the Physical Machines (PMs), which enables the cloud service providers to share the limited computing and network resources among multiple users. Virtual resource mapping can be defined as the process of embedding multiple VMs and their network resource demand onto multiple inter-connected PMs. The existing mechanisms of resource mapping need to be efficient enough to minimize the number of PMs without compromising the deadline of the tasks assigned to the VMs, which is NP-hard. To deal with this problem, a Link based Virtual Resource Management (LVRM) algorithm is designed to map the VMs onto PMs based on the available and required resources of the PMs and VMs, respectively. The designed algorithm exploits the fact that the demanded network bandwidth among VMs should be given higher priority while allocating the physical resources to the inter-connected virtual machines as insufficient network bandwidth may detain the task execution. The proposed algorithm is evaluated by a discrete event simulator and is compared with similar virtual network embedded algorithms. Simulation results show that LVRM can outperform over other network embedded algorithms."",""1558-2183"","""",""10.1109/TPDS.2017.2780844"",""Ministry of Science and Technology, Taiwan(grant numbers:106-2221-E-182-014)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8169111"",""Resource mapping";VM placement;"graph theory"",""Cloud computing";Bandwidth;Resource management;Servers;Virtual machining;"Algorithm design and analysis"","""",""15"","""",""27"",""IEEE"",""7 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"LWPTool: A Lightweight Profiler to Guide Data Layout Optimization,""C. Yu"; P. Roy; Y. Bai; H. Yang;" X. Liu"",""Beihang University, Beijing, CN"; College of William and Mary, Williamsburg, VA, US; Beihang University, Beijing, CN; Beihang University, Beijing, CN;" College of William and Mary, Williamsburg, VA, US"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Oct 2018"",""2018"",""29"",""11"",""2489"",""2502"",""Memory access latency continues to be a dominant bottleneck in a large class of applications on modern architectures. To optimize memory performance, it is important to utilize the locality in the memory hierarchy. Data layout optimization can significantly improve memory locality. However, pinpointing inefficient code and providing insightful guidance for data layout optimization is challenging. Existing tools typically leverage heavyweight memory instrumentations, which hinders the applicability of these tools for real long-running programs. To address this issue, we develop LWPTool, a profiler to pinpoint top candidates that benefit from data layout optimization. LWPTool makes three unique contributions. First, it adopts lightweight address sampling to collect and analyze memory traces. Second, LWPTool employs a set of novel methods to determine memory access patterns to guide data layout optimization. We also formally prove that our method has high accuracy even with sparse memory access samples. Third, LWPTool scales on multithreaded machines. LWPTool works on fully optimized, unmodified binary executables independently from their compiler and language, incurring around 6.2 percent runtime overhead. To evaluate LWPTool, we study ten sequential and parallel benchmarks. With the guidance of LWPTool, we are able to significantly improve all these benchmarks";" the speedup is up to 1.39× on average."",""1558-2183"","""",""10.1109/TPDS.2018.2840992"",""National Key Research and Development Program of China(grant numbers:2016YFB1000503)"; National Science Foundation of China(grant numbers:61572062,61502019); National Science Foundation (NSF)(grant numbers:1464157);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8367889"",""Data locality";address sampling;lightweight profiling;structure splitting;"array regrouping"",""Optimization";Layout;Monitoring;Arrays;Instruments;Program processors;"Benchmark testing"","""",""6"","""",""46"",""IEEE"",""28 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"M-Oscillating: Performance Maximization on Temperature-Constrained Multi-Core Processors,""S. Sha"; W. Wen; S. Ren;" G. Quan"",""Department of Electrical and Computer Engineering, Florida International University, Miami, FL"; Department of Electrical and Computer Engineering, Florida International University, Miami, FL; Department of Electrical and Computer Engineering, University of California at Riverside, Riverside, CA;" Department of Electrical and Computer Engineering, Florida International University, Miami, FL"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2528"",""2539"",""The ever-increasing computational demand drives modern electronic devices to integrate more processing elements for pursuing higher computing performance. However, the resulting soaring power density and potential thermal crisis constrain the system performance under a maximally allowed temperature. This paper analytically studies the throughput maximization problem of multi-core platforms under the peak temperature constraints. To take advantage of thermal heterogeneity of different cores for performance improvement, we propose to run each core with multiple speed levels and develop a schedule based on two novel concepts, i.e., the step-up schedule and the m-Oscillating schedule, for multi-core platforms. The proposed methodology can ensure the peak temperature guarantee with a significant improvement in computing throughput up to 89 percent, with an average improvement of 11 percent. Meanwhile, the computational time reduces orders of magnitude compared to the traditional exhaustive search-based approach."",""1558-2183"","""",""10.1109/TPDS.2018.2835474"",""U.S. NSF(grant numbers:CNS-1551661,ECCS-1610471)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359006"",""Performance maximization";throughput;temperature;thermal-aware;peak temperature minimization;parallel processing;frequency oscillation;"multi-core architecture"",""Multicore processing";Schedules;Throughput;Program processors;Density measurement;Power system measurements;"System performance"","""",""6"","""",""30"",""IEEE"",""15 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Machine Learning-Based Quality-Aware Power and Thermal Management of Multistream HEVC Encoding on Multicore Servers,""A. Iranfar"; M. Zapater;" D. Atienza"",""Electrical Engineering, Ecole Polytechnique Federale de Lausanne Faculte des Sciences et Techniques de l’Ingenieur, Lausanne, Switzerland"; Electrical Engineering, Ecole Polytechnique Federale de Lausanne, Lausanne, Switzerland;" Embedded Systems Laboratory (ESL), Ecole Polytechnique Federale de Lausanne (EPFL), Lausanne, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2268"",""2281"",""The emergence of video streaming applications, together with the users' demand for high-resolution contents, has led to the development of new video coding standards, such as High Efficiency Video Coding (HEVC). HEVC provides high efficiency at the cost of increased complexity. This higher computational burden results in increased power consumption in current multicore servers. To tackle this challenge, algorithmic optimizations need to be accompanied by content-aware application-level strategies, able to reduce power while meeting compression and quality requirements. In this paper, we propose a machine learning-based power and thermal management approach that dynamically learns and selects the best encoding configuration and operating frequency for each of the videos running on multicore servers, by using information from frame compression, quality, encoding time, power, and temperature. In addition, we present a resolution-aware video assignment and migration strategy that reduces the peak and average temperature of the chip while maintaining the desirable encoding time. We implemented our approach in an enterprise multicore server and evaluated it under several common scenarios for video providers. On average, compared to a state-of-the-art technique, for the most realistic scenario, our approach improves BD-PSNR and BD-rate by 0.54 dB, and 8 percent, respectively, and reduces the encoding time, power consumption, and average temperature by 15.3, 13, and 10 percent, respectively. Moreover, our proposed approach enhances BDPSNR and BD-rate compared to the HEVC Test Model (HM), by 1.19 dB and 24 percent, respectively, without any encoding time degradation, when power and temperature constraints are relaxed."",""1558-2183"","""",""10.1109/TPDS.2018.2827381"",""EC H2020 MANGO project(grant numbers:671668)"; ERC Consolidator Grant COMPUSAPIEN(grant numbers:725657);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338403"",""HEVC standard";multicore servers;encoding efficiency;encoding configuration;"reinforcement learning"",""Encoding";Servers;Multicore processing;Streaming media;Thermal management;Complexity theory;"Power demand"","""",""10"","""",""48"",""IEEE"",""16 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Machine Learning-Based Temperature Prediction for Runtime Thermal Management Across System Components,""K. Zhang"; A. Guliani; S. Ogrenci-Memik; G. Memik; K. Yoshii; R. Sankaran;" P. Beckman"",""Department of Electrical Engineering and Computer Science, Northwestern University, 2145 Sheridan Road, Evanston, IL"; Department of Electrical Engineering and Computer Science, Northwestern University, 2145 Sheridan Road, Evanston, IL; Department of Electrical Engineering and Computer Science, Northwestern University, 2145 Sheridan Road, Evanston, IL; Department of Electrical Engineering and Computer Science, Northwestern University, 2145 Sheridan Road, Evanston, IL; Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL; Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL;" Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""405"",""419"",""Elevated temperatures limit the peak performance of systems because of frequent interventions by thermal throttling. Non-uniform thermal states across system nodes also cause performance variation within seemingly equivalent nodes leading to significant degradation of overall performance. In this paper we present a framework for creating a lightweight thermal prediction system suitable for run-time management decisions. We pursue two avenues to explore optimized lightweight thermal predictors. First, we use feature selection algorithms to improve the performance of previously designed machine learning methods. Second, we develop alternative methods using neural network and linear regression-based methods to perform a comprehensive comparative study of prediction methods. We show that our optimized models achieve improved performance with better prediction accuracy and lower overhead as compared with the Gaussian process model proposed previously. Specifically we present a reduced version of the Gaussian process model, a neural network-based model, and a linear regression-based model. Using the optimization methods, we are able to reduce the average prediction errors in the Gaussian process from 4.2°C to 2.9°C. We also show that the newly developed models using neural network and Lasso linear regression have average prediction errors of 2.9°C and 3.8°C respectively. The prediction overheads are 0.22, 0.097, and 0.026 ms per prediction for reduced Gaussian process, neural network, and Lasso linear regression models, respectively, compared with 0.57 ms per prediction for the previous Gaussian process model. We have implemented our proposed thermal prediction models on a two-node system configuration to help identify the optimal task placement. The task placement identified by the models reduces the average system temperature by up to 11.9°C without any performance degradation. Furthermore, these models respectively achieve 75, 82.5, and 74.17 percent success rates in correctly pointing to those task placements with better thermal response, compared with 72.5 percent success for the original model in achieving the same objective. Finally, we extended our analysis to a 16-node system and we were able to train models and execute them in real time to guide task migration and achieve on average 17 percent reduction in the overall system cooling power."",""1558-2183"","""",""10.1109/TPDS.2017.2732951"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7995115"",""Thermal modeling";many-core processors;operating systems;"high performance computing systems"",""Predictive models";Thermal management;Gaussian processes;Computational modeling;Cooling;Analytical models;"Neural networks"","""",""67"","""",""37"",""IEEE"",""28 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Maelstream: Self-Organizing Media Streaming for Many-to-Many Interaction,""L. Provensi"; A. Singh; F. Eliassen;" R. Vitenberg"",""Department of Informatics, University of Oslo, Norway"; Department of Informatics, University of Oslo, Norway; Department of Informatics, University of Oslo, Norway;" Department of Informatics, University of Oslo, Norway"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1342"",""1356"",""A number of emerging multimedia applications, such as webinars, require users to interact by exchanging media streams. In such application there are multiple interacting participants which both produce and consume media content and a set of participants which are only consumers. Keeping the end-to-end latency as low as possible while not violating bandwidth constraints is one of the most important requirements for this type of application. While there exists solutions to this problem for applications such as multi-party video conferencing, they rely on dedicated infrastructures which may be expensive and not available to all users. On the other hand, decentralized P2P solutions have been focusing on single source media streaming, which does not consider multiple interactive participants. In this paper, we propose Maelstream, a self-organizing media streaming solution that supports multiple interacting participants as well as a large number of consumers. Maelstream uses gossip protocols to generate multiple latency-aware streaming trees on top of a P2P overlay. We have evaluated our solution with simulations implemented using Peersim and ns-3 simulators, and compared Maelstream with Chunkyspread, an unstructured protocol capable of fine-tuning latency. We show that Maelstream can achieve low end-to-end latency and scales well with the number of streams."",""1558-2183"","""",""10.1109/TPDS.2018.2791599"",""Research Council of Norway(grant numbers:187828)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253478"",""Self-organizing systems";P2P streaming;"many-to-many low-latency interaction"",""Peer-to-peer computing";Streaming media;Bandwidth;Media;Digital audio broadcasting;Webinars;"Protocols"","""",""4"","""",""36"",""IEEE"",""10 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Malleable Task-Graph Scheduling with a Practical Speed-Up Model,""L. Marchal"; B. Simon; O. Sinnen;" F. Vivien"",""CNRS, LIP, ENS Lyon, 46 allée d'Italie, Lyon, France"; CNRS, LIP, ENS Lyon, 46 allée d'Italie, Lyon, France; Department of Electrical and Computer Engineering, University of Auckland, Auckland, New Zealand;" CNRS, LIP, ENS Lyon, 46 allée d'Italie, Lyon, France"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2018"",""2018"",""29"",""6"",""1357"",""1370"",""Scientific workloads are often described by Directed Acyclic task Graphs. Indeed, DAGs represent both a theoretical model and the structure employed by dynamic runtime schedulers to handle HPC applications. A natural problem is then to compute a makespan-minimizing schedule of a given graph. In this paper, we are motivated by task graphs arising from multifrontal factorizations of sparse matrices and therefore work under the following practical model. Tasks are malleable (i.e., a single task can be allotted a time-varying number of processors) and their speedup behaves perfectly up to a first threshold, then speedup increases linearly, but not perfectly, up to a second threshold where the speedup levels off and remains constant. After proving the NP-hardness of minimizing the makespan of DAGs under this model, we study several heuristics. We propose model-optimized variants for PROPSCHEDULING, widely used in linear algebra application scheduling, and FLOWFLEX. GREEDYFILLING is proposed, a novel heuristic designed for our speedup model, and we demonstrate that PROPSCHEDULING and GREEDYFILLING are 2-approximation algorithms. In the evaluation, employing synthetic data sets and task graphs arising from multifrontal factorization, the proposed optimized variants and GREEDYFILLING significantly outperform the traditional algorithms, whereby GREEDYFILLING demonstrates a particular strength for balanced graphs."",""1558-2183"","""",""10.1109/TPDS.2018.2793886"",""SOLHAR project"; LABEX MILYON(grant numbers:ANR-10-LABX-0070); French National Research Agency (ANR);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8259313"",""Scheduling";task graph;malleable tasks;speedup model;approximation algorithms;"proportional mapping"",""Program processors";Approximation algorithms;Algorithm design and analysis;Processor scheduling;"Schedules"","""",""7"","""",""44"",""IEEE"",""15 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Managing Risk in a Derivative IaaS Cloud,""P. Sharma"; S. Lee; T. Guo; D. Irwin;" P. Shenoy"",""Department of Computer Science, University of Massachusetts Amherst, Amherst, MA"; Department of Computer Science, University of Massachusetts Amherst, Amherst, MA; Department of Computer Science, University of Massachusetts Amherst, Amherst, MA; Department of Electrical and Computer Engineering, University of Massachusetts Amherst, Amherst, MA;" Department of Computer Science, University of Massachusetts Amherst, Amherst, MA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1750"",""1765"",""Infrastructure-as-a-Service (IaaS) cloud platforms rent computing resources with different cost and availability tradeoffs. For example, users may acquire virtual machines (VMs) in the spot market-that are cheap, but can be unilaterally terminated by the cloud operator. Because of this revocation risk, spot servers have been conventionally used for delay and risk tolerant batch jobs. In this paper, we develop risk mitigation policies which allow even interactive applications to run on spot servers. Our System, SpotCheck is a derivative cloud platform, and provides the illusion of an IaaS platform that offers always-available VMs on demand for a cost near that of spot servers, and supports unmodified applications. SpotCheck's design combines virtualization-based mechanisms for fault-tolerance, and bidding and server selection policies for managing the risk and cost. We implement SpotCheck on EC2 and show that it i) provides nested VMs with 99.9989 percent availability, ii) achieves upto 2-5x cost savings compared to using on-demand VMs, and iii) eliminates any risk of losing VM state."",""1558-2183"","""",""10.1109/TPDS.2017.2658622"",""US National Science Foundation(grant numbers:#1422245,#1229059)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833031"",""Distributed computing";platform virtualization;system software;"virtual machine monitors"",""Servers";Cloud computing;Virtual machine monitors;Contracts;Risk management;Virtualization;"Degradation"","""",""10"","""",""41"",""IEEE"",""25 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Massively Parallel Stencil Code Solver with Autonomous Adaptive Block Distribution,""M. Berghoff"; I. Kondov;" J. Hötzer"",""Steinbuch Centre for Computing (SCC), Karlsruhe Institute of Technology (KIT), Eggenstein-Leopoldshafen, Germany"; Steinbuch Centre for Computing (SCC), Karlsruhe Institute of Technology (KIT), Eggenstein-Leopoldshafen, Germany;" Institute for Applied Materials (IAM), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2282"",""2296"",""In the last decades, simulations have been established in several fields of science and industry to study various phenomena by solving, inter alia, partial differential equations. For an efficient use of current and future high performance computing systems, with many thousands of computation ranks, high node-level performance, scalable communication, and the omission of unnecessary calculations are of high priority in the development of new solvers. The challenge of contemporary simulation applications is to bridge the gap between the scales of the various physical processes. We introduce the NAStJA framework, a block-based MPI parallel solver for arbitrary algorithms, based on stencil code or other regular grid methods. NAStJA decomposes the domain of spatially complex structures into small cuboid blocks. A special feature of NAStJA is the dynamic block adaption which modifies the calculation domain around the region where the computation currently takes place, and hence avoids unnecessary calculations. This often occurs, inter alia, in phase-field simulations. Block creation and deletion is managed autonomously within local neighborhoods. A basic load balancing mechanism allows a re-distribution of newly created blocks to the involved computing ranks. The use of a multi-hop network, to distribute information to the entire domain, avoids collective all-gather communications. Thus, we can demonstrate excellent scaling. The present scaling tests substantiate the enormous advantage of this adaptive method. For certain simulation scenarios, we can show that the calculation effort and memory consumption can be reduced to only 3.5 percent, compared to the classical full-domain reference simulation. The overhead of 70-100 percent for the dynamic adapting block creation is significantly lower than the gain. The approach is not restricted to phase-field simulations, and can be employed in other domains of computational science to exploit sparsity of computing regions."",""1558-2183"","""",""10.1109/TPDS.2018.2819672"",""Ministry of Science, Research and the Arts Baden-Württemberg"; DFG (“Deutsche Forschungsgemeinschaft”);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8325488"",""Stencil code";distributed memory;scalable parallel algorithms;massively parallel performance;multi-hop network;load balancing;partial differential equation;"phase-field method"",""Computational modeling";Adaptation models;Mathematical model;Heuristic algorithms;Load modeling;Solid modeling;"Biological system modeling"","""",""8"","""",""56"",""IEEE"",""26 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"MCL: A Cost-Efficient Nonblocking Multicast Interconnection Network,""J. Duan";" Y. Yang"",""Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY";" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2046"",""2058"",""Interconnection networks lie in the heart of all types of parallel architectures, because how processors or memory modules are connected to each other has a significant impact on the scalability, reliability, cost and performance. For example, a nonblocking interconnection network delivers guaranteed path availability to any connection requests, without interference to existing connections. Also, an interconnection network with multicast capability can distribute data from a single source to all the destinations in a one-shot manner, eliminating unnecessary duplications and minimizing communication delays. However, implementing nonblocking multicast networks imposes great challenges to designers because both of them demand high hardware cost. To deal with this problem, in this paper we propose a novel interconnection network, named Multicast Capable Low-cost Network (MCL), which is both nonblocking for multicast traffic and cost-efficient. We first design the topologies and routing algorithms for MCL, and then prove its nonblocking multicast properties. Most importantly, we show that MCL achieves the lowest hardware cost in terms of asymptotic number of crosspoints. Specifically, the theoretical upper bound on the crosspoints of an N × N MCL is O(N log3.39N). The explicitly constructed instance of an N × N MCL has a cost of O(N5/4) and constant delay from the input ports to the output ports. These theoretical and practical costs are both the lowest compared to previous designs which deliver the same performance."",""1558-2183"","""",""10.1109/TPDS.2018.2817623"",""National Science Foundation(grant numbers:CCF-1320044,CCF-1717731)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320837"",""Parallel computing";interconnection networks;multicast;nonblocking;"cost-efficient networks"",""Multiprocessor interconnection";Routing;Wireless sensor networks;Hardware;Parallel processing;Unicast;"Data centers"","""",""5"","""",""57"",""IEEE"",""21 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"MeLoDy: A Long-Term Dynamic Quality-Aware Incentive Mechanism for Crowdsourcing,""H. Wang"; S. Guo; J. Cao;" M. Guo"",""Department of Computer Science and Engineering, Shang Jiao Tong University, Shanghai, China"; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Hong Kong;" Department of Computer Science and Engineering, Shang Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""901"",""914"",""Crowdsourcing allows requesters to allocate tasks to a group of workers on the Internet to make use of their collective intelligence. Quality control is a key design objective in incentive mechanisms for crowdsourcing as requesters aim at obtaining high-quality answers under a limited budget. However, when measuring workers' long-term quality, existing mechanisms either fail to utilize workers' historical information, or treat workers' quality as stable and ignore its temporal characteristics, hence performing poorly in a long run. In this paper we propose MELODY, a long-term dynamic quality-aware incentive mechanism for crowdsourcing. MELODY models interaction between requesters and workers as reverse auctions that run continuously. In each run of MELODY, we design a truthful, individual rational, budget feasible and quality-aware algorithm for task allocation with polynomial-time computation complexity and O(1) performance ratio. Moreover, taking into consideration the long-term characteristics of workers' quality, we propose a novel framework in MELODY for quality inference and parameters learning based on Linear Dynamical Systems at the end of each run, which takes full advantage of workers' historical information and predicts their quality accurately. Through extensive simulations, we demonstrate that MELODYoutperforms existing work in terms of both quality estimation (reducing estimation error by 17.6% ~ 24.2%) and social performance (increasing requester's utility by 18.2% ~ 46.6%) in long-term scenarios."",""1558-2183"","""",""10.1109/TPDS.2017.2775232"",""National Basic Research 973 Program of China(grant numbers:2015CB352403)"; National Natural Science Foundation of China (NSFC)(grant numbers:61602301); NSFC/RGC Joint Research Scheme(grant numbers:N_PolyU519/12); NSFC Key(grant numbers:61332004); The Hong Kong Polytechnic University(grant numbers:1-ZE26);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8115213"",""Crowdsourcing";incentive mechanism;quality control;approximation algorithm;"inference and learning"",""Crowdsourcing";Resource management;Quality control;Atmospheric measurements;Particle measurements;"Inference algorithms"","""",""50"","""",""51"",""IEEE"",""20 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Memory Hierarchy Characterization of NoSQL Applications through Full-System Simulation,""A. Colaso"; P. Prieto; J. A. Herrero; P. Abad; L. G. Menezo; V. Puente;" J. A. Gregorio"",""Computer Engineering Group, University of Cantabria, Santander, Cantabria, Spain"; Computer Engineering Group, University of Cantabria, Santander, Cantabria, Spain; Computer Engineering Group, University of Cantabria, Santander, Cantabria, Spain; Computer Engineering Group, University of Cantabria, Santander, Cantabria, Spain; Computer Engineering Group, University of Cantabria, Santander, Cantabria, Spain; Computer Engineering Group, University of Cantabria, Santander, Cantabria, Spain;" Computer Engineering Group, University of Cantabria, Santander, Cantabria, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1161"",""1173"",""In this work, we conduct a detailed memory characterization of a representative set of modern data-management software (Cassandra, MongoDB, OrientDB and Redis) running an illustrative NoSQL benchmark suite (YCSB). These applications are widely popular NoSQL databases with different data models and features such as in-memory storage. We compare how these data-serving applications behave with respect to other well-known benchmarks, such as SPEC CPU2006, PARSEC and NAS Parallel Benchmark. The methodology employed for evaluation relies on state-of-the-art full-system simulation tools, such as gem5. This allows us to explore configurations unattainable using performance monitoring units in actual hardware, being able to characterize memory properties. The results obtained suggest that NoSQL application behavior is not dissimilar to conventional workloads. Therefore, some of the optimizations present in state-of-the-art hardware might have a direct benefit. Nevertheless, there are some common aspects that are distinctive of conventional benchmarks that might be sufficiently relevant to be considered in architectural design. Strikingly, we also found that most database engines, independently of aspects such as workload or database size, exhibit highly uniform behavior. Finally, we show that different data-base engines make highly distinctive demands on the memory hierarchy, some being more stringent than others."",""1558-2183"","""",""10.1109/TPDS.2017.2787150"",""Spanish Government (Secretaría de Estado de Investigación, Desarrollo e Innovación)(grant numbers:TIN2015-66979-R,TIN2016-80512-R)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8239962"",""Memory hierarchy";big-data;NoSQL;cache hierarchy;"benchmark characterization"",""Benchmark testing";Software;Hardware;Data models;Distributed databases;"NoSQL databases"","""",""1"","""",""43"",""IEEE"",""25 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Metascheduling of HPC Jobs in Day-Ahead Electricity Markets,""P. Murali";" S. Vadhiyar"",""Computer Science Department, Princeton University, Princeton, NJ";" Department of Computational and Data Sciences, Indian Institute of Science, Bengaluru, Karnataka, India"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""614"",""627"",""High performance grid computing is a key enabler of large scale collaborative computational science. With the promise of exascale computing, high performance grid systems are expected to incur electricity bills that grow super-linearly overtime. In order to achieve cost effectiveness in these systems, it is essential for the scheduling algorithms to exploit electricity price variations, both in space and time, that are prevalent in the dynamic electricity price markets. In this paper, we present a metascheduling algorithm to optimize the placement of jobs in a compute grid which consumes electricity from the day-ahead wholesale market. We formulate the scheduling problem as a Minimum Cost Maximum Flow problem and leverage queue waiting time and electricity price predictions to accurately estimate the cost of job execution at a system. Using trace based simulation with real and synthetic workload traces, and real electricity price data sets, we demonstrate our approach on two currently operational grids, XSEDE and NorduGrid. Our experimental setup collectively constitute more than 433K processors spread across 58 compute systems in 17 geographically distributed locations. Experiments show that our approach simultaneously optimizes the total electricity cost and the average response time of the grid, without being unfair to users of the local batch systems."",""1558-2183"","""",""10.1109/TPDS.2017.2769082"",""Department of Science and Technology (DST), India(grant numbers:SR/S3/EECE/0095/2012)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8094010"",""Grids";supercomputers;batch queue systems;queue waiting times;response times;electricity prices;metascheduling;"network flow"",""Time factors";Processor scheduling;Prediction algorithms;Runtime;History;Data models;"Program processors"","""",""2"","""",""43"",""IEEE"",""2 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"MIA: Metric Importance Analysis for Big Data Workload Characterization,""Z. Yu"; W. Xiong; L. Eeckhout; Z. Bei; A. Mendelson;" C. Xu"",""Cloud Computing Center, Chinese Academy of Science, Shenzhen, China"; Cloud Computing Center, Chinese Academy of Science, Shenzhen, China; Ghent University, Ghent, 9000; Cloud Computing Center, Chinese Academy of Science, Shenzhen, China; Technion-Israel Institute of Technology, Haifa, Israel;" Cloud Computing Center, Chinese Academy of Science, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1371"",""1384"",""Data analytics is at the foundation of both high-quality products and services in modern economies and societies. Big data workloads run on complex large-scale computing clusters, which implies significant challenges for deeply understanding and characterizing overall system performance. In general, performance is affected by many factors at multiple layers in the system stack, hence it is challenging to identify the key metrics when understanding big data workload performance. In this paper, we propose a novel workload characterization methodology using ensemble learning, called Metric Importance Analysis (MIA), to quantify the respective importance of workload metrics. By focusing on the most important metrics, MIA reduces the complexity of the analysis without losing information. Moreover, we develop the MIA-based Kiviat Plot (MKP) and Benchmark Similarity Matrix (BSM) which provide more insightful information than the traditional linkage clustering based dendrogram to visualize program behavior (dis)similarity. To demonstrate the applicability of MIA, we use it to characterize three big data benchmark suites: HiBench, CloudRank-D and SZTS. The results show that MIA is able to characterize complex big data workloads in a simple, intuitive manner, and reveal interesting insights. Moreover, through a case study, we demonstrate that tuning the configuration parameters related to the important metrics found by MIA results in higher performance improvements than through tuning the parameters related to the less important ones."",""1558-2183"","""",""10.1109/TPDS.2017.2758781"",""National Key R&D Program of China(grant numbers:2016YFB1000204)"; NSFC(grant numbers:61672511,61702495); outstanding technical talent program of CAS; major scientific and technological project of Guangdong province(grant numbers:2014B010115003); Shenzhen Technology Research Project(grant numbers:JSGG20160510154-636747); Key technique research on Haiyun Data System of NICT(grant numbers:XDA06010500);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8057859"",""Big data";benchmarking;workload characterization;performance measurement;"MapReduce/hadoop"",""Measurement";Big Data;Benchmark testing;Hardware;Software;"Support vector machines"","""",""10"","""",""53"",""IEEE"",""4 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Minimal Cost Server Configuration for Meeting Time-Varying Resource Demands in Cloud Centers,""C. Liu"; K. Li;" K. Li"",""National Supercomputing Center in Changsha, Hunan, China"; National Supercomputing Center in Changsha, Hunan, China;" Department of Computer Science, State University of New York, New Paltz, New York"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2503"",""2513"",""We consider the minimal cost server configuration for meeting resource demands over multiple time slots. Specifically, there are some heterogeneous servers. Each server is specified by a cost, certain amounts of several resources, and an active interval, i.e., the time interval that the server is planed to work. There are different overall demands for each type of resource over different time slots. A feasible solution is a set of servers such that at any time slot, the resources provided by the selected servers are at least their corresponding demands. Notice that, a selected server can not provide resources for the time slots out of its active interval. The total cost of the solution is the summation of the costs of all selected servers. The goal is to find a feasible solution with minimal total cost. This problem is proved to be NP-hard due to a reduction from the multidimensional knapsack problem (MKP), which is a well-known NP-hard combinational optimization problem. To solve our problem, we present a randomized approximation algorithm called partial rounding algorithm ($\mathcal {PRA}$ ), which guarantees $O\left(\log \left(KT \right) \right)$ -approximation, i.e., $\eta \";"\log \left(KT \right)$ -approximation, where $K$  is the number of kinds of resources, $T$  is the number of time slots, and $\eta$  is a positive constant. Furthermore, to minimize $\eta$  as much as possible, we propose a varied Chernoff bound and apply it in $\mathcal {PRA}$ . We perform extensive experiments with random inputs and a specific application input. The results show that  $\mathcal {PRA}$  with our varied Chernoff conclusion can find solutions closing to the optimal one."",""1558-2183"","""",""10.1109/TPDS.2018.2836452"",""National Key R&D Program of China(grant numbers:SQ2018YFB020061,2015AA015305)"; National Natural Science Foundation of China(grant numbers:61702170,61602350,61602170,61370098,61672219); Key Program of National Natural Science Foundation of China(grant numbers:61432005); National Outstanding Youth Science Program; National Natural Science Foundation of China(grant numbers:61625202); Chinese Postdoctoral Science Foundation(grant numbers:2016M602409);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359091"",""Cost minimization";meeting resource demands;randomized approximation algorithm;server configuration;"varied Chernoff bound"",""Servers";Approximation algorithms;Resource management;Minimization;Cloud computing;Bandwidth;"Optimization"","""",""28"","""",""31"",""IEEE"",""15 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Minimize the Make-span of Batched Requests for FPGA Pooling in Cloud Computing,""Y. Zhao"; C. Tian; Z. Zhu; J. Cheng; C. Qiao;" A. X. Liu"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Engineering, Michigan State University, East Lansing, MI; Huawei Technologies Co., Ltd., Shenzhen, China; Department of Computer Science and Engineering, the State University of New York at Buffalo, Buffalo, NY;" Department of Computer Science and Engineering, Michigan State University, East Lansing, MI"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2514"",""2527"",""Using FPGA as accelerators is gaining popularity in Cloud computing. Usually, FPGA accelerators in a datacenter are managed as a single resource pool. By issuing a request to this pool, a tenant can transparently access FPGA resources. FPGA requests usually arrive in batches. The objective of scheduling is to minimize the make-span of a given batch of requests, which is the completion time of the entire batch of jobs. As a result, either the responsiveness is improved, or the system throughput is maximized. The key technical challenge is the existence of multiple resource bottlenecks. An FPGA job can be bottlenecked by either computation (i.e., computation-intensive) or network (i.e., network-intensive), and sometimes by both. To the best of our knowledge, this is the first work that minimizes the make-span of batched requests for an FPGA accelerator pool in Cloud computing that considers multiple resource bottlenecks. In this paper, we design several scheduling algorithms to address the challenge. We implement our scheduling algorithms in an IBM Cloud system. We conduct extensive evaluations on both a small scale testbed and a large-scale simulator. Compared with the Shortest-Job-First scheduling, our algorithms can reduce the make-span by 36.25 percent, and improve the system throughput by 36.05 percent."",""1558-2183"","""",""10.1109/TPDS.2018.2829860"",""National Science and Technology Major Project of China(grant numbers:2017ZX03001013-003)"; Fundamental Research Funds; Central Universities(grant numbers:0202-14380037); National Natural Science Foundation of China(grant numbers:61772265,61602194,61671130,61671124,61502229,61672276,61321491); Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship (Shuangchuang) Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8345755"",""Cloud computing";FPGA accelerators;"job scheduling"",""Field programmable gate arrays";Cloud computing;Scheduling algorithms;Task analysis;Throughput;"Approximation algorithms"","""",""6"","""",""33"",""IEEE"",""24 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"MPCA SGD—A Method for Distributed Training of Deep Learning Models on Spark,""M. Langer"; A. Hall; Z. He;" W. Rahayu"",""La Trobe University, VIC, Australia"; La Trobe University, VIC, Australia; La Trobe University, VIC, Australia;" La Trobe University, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2540"",""2556"",""Many distributed deep learning systems have been published over the past few years, often accompanied by impressive performance claims. In practice these figures are often achieved in high performance computing (HPC) environments with fast InfiniBand network connections. For average deep learning practitioners this is usually an unrealistic scenario, since they cannot afford access to these facilities. Simple re-implementations of algorithms such as EASGD [1] for standard Ethernet environments often fail to replicate the scalability and performance of the original works [2] . In this paper, we explore this particular problem domain and present MPCA SGD, a method for distributed training of deep neural networks that is specifically designed to run in low-budget environments. MPCA SGD tries to make the best possible use of available resources, and can operate well if network bandwidth is constrained. Furthermore, MPCA SGD runs on top of the popular Apache Spark [3] framework. Thus, it can easily be deployed in existing data centers and office environments where Spark is already used. When training large deep learning models in a gigabit Ethernet cluster, MPCA SGD achieves significantly faster convergence rates than many popular alternatives. For example, MPCA SGD can train ResNet-152 [4] up to 5.3x faster than state-of-the-art systems like MXNet [5] , up to 5.3x faster than bulk-synchronous systems like SparkNet [6] and up to 5.3x faster than decentral asynchronous systems like EASGD [1] ."",""1558-2183"","""",""10.1109/TPDS.2018.2833074"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8354695"",""Deep learning";distributed computing;machine learning;neural networks;spark;"stochastic gradient descent"",""Machine learning";Computational modeling;Sparks;Training;Optimization;Servers;"Bandwidth"","""",""21"","""",""39"",""IEEE"",""4 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"MSGD: A Novel Matrix Factorization Approach for Large-Scale Collaborative Filtering Recommender Systems on GPUs,""H. Li"; K. Li; J. An;" K. Li"",""College of Computer Science and Electronic Engineering and National Supercomputing Center in Changsha, Hunan University, Hunan, China"; College of Computer Science and Electronic Engineering and National Supercomputing Center in Changsha, Hunan University, Hunan, China; College of Computer Science and Electronic Engineering and National Supercomputing Center in Changsha, Hunan University, Hunan, China;" Department of Computer Science, State University of New York, New Paltz, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1530"",""1544"",""Real-time accurate recommendation of large-scale recommender systems is a challenging task. Matrix factorization (MF), as one of the most accurate and scalable techniques to predict missing ratings, has become popular in the collaborative filtering (CF) community. Currently, stochastic gradient descent (SGD) is one of the most famous approaches for MF. However, it is non-trivial to parallelize SGD for large-scale CF MF problems due to the dependence on the user and item pair, which can cause parallelization over-writing. To remove the dependence on the user and item pair, we propose a multi-stream SGD (MSGD) approach, for which the update process is theoretically convergent. On that basis, we propose a Compute Unified Device Architecture (CUDA) parallelization MSGD (CUMSGD) approach. CUMSGD can obtain high parallelism and scalability on Graphic Processing Units (GPUs). On Tesla K20m and K40c GPUs, the experimental results show that CUMSGD outperforms prior works that accelerated MF on shared memory systems, e.g., DSGD, FPSGD, Hogwild!, and CCD++. For large-scale CF problems, we propose multiple GPUs (multi-GPU) CUMSGD (MCUMSGD). The experimental results show that MCUMSGD can improve MSGD performance further. With a K20m GPU card, CUMSGD can be 5-10 times as fast compared with the state-of-the-art approaches on shared memory platform."",""1558-2183"","""",""10.1109/TPDS.2017.2718515"",""National Natural Science Foundation of China(grant numbers:61432005)"; National Outstanding Youth Science Program of National Natural Science Foundation of China(grant numbers:61625202); National Natural Science Foundation of China(grant numbers:61370095,61370097,61472124,61572175,61672224); The International Science & Technology Cooperation Program of China(grant numbers:2015DFA11240,2014DFB30010); National High-tech R&D Program of China(grant numbers:2015AA015305);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7955082"",""Collaborative filtering (CF)";CUDA parallelization algorithm;matrix factorization (MF);multi-GPU implementation;"stochastic gradient descent (SGD)"",""Graphics processing units";Recommender systems;Sparse matrices;Optimization;Scalability;Acceleration;"Collaboration"","""",""76"","""",""49"",""IEEE"",""22 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"mSNP: A Massively Parallel Algorithm for Large-Scale SNP Detection,""Y. Cui"; S. Peng; Y. Lu; X. Zhu; B. Wang; C. Wu;" X. Liao"",""College of Computer, National University of Defense Technology, Changsha, China"; College of Computer, National University of Defense Technology, Changsha, China; National Supercomputer Center in Guangzhou, Guangzhou, Guangdong, China; College of Computer, National University of Defense Technology, Changsha, China; National Supercomputer Center in Guangzhou, Guangzhou, Guangdong, China; College of Computer, National University of Defense Technology, Changsha, China;" College of Computer, National University of Defense Technology, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2557"",""2567"",""Single Nucleotide Polymorphism (SNP) detection is a fundamental procedure of whole genome analysis. SOAPsnp, a classic tool for detection, would take more than one week to analyze one typical human genome, which limits the efficiency of downstream analyses. In this paper, we present mSNP, an optimized version of SOAPsnp, which leverages Intel Xeon Phi coprocessors for large-scale SNP detection. Firstly, we redesigned the essential data structures of SOAPsnp, which significantly reduces memory footprint and improves computing efficiency. Then we developed a coordinated parallel framework for a higher hardware utilization of both CPU and Xeon Phi. Also, we tailored the data structures and operations to utilize the wide VPU of Xeon Phi to improve data throughput. Last but not the least, we proposed a read-based window division strategy to improve throughput and obtain better load balance. mSNP is the first SNP detection tool empowered by Xeon Phi. We achieved a 38x single thread speedup on CPU, without any loss in precision. Moreover, mSNP successfully scaled to 4,096 nodes on Tianhe-2. Our experiments demonstrate that mSNP is efficient and scalable for large-scale human genome SNP detection."",""1558-2183"","""",""10.1109/TPDS.2018.2839578"",""National Key R&D Program of China(grant numbers:2017YFB0202602,2017YFC1311003,2016YFC1302500,2016YFB0200400,2017YFB0202104)"; NSFC(grant numbers:61772543,U1435222,61625202,61272056); State Key Laboratory of Chemo/Biosensing and Chemometrics; Fundamental Research Funds for the Central Universities; and Guangdong Provincial Department of Science and Technology(grant numbers:2016B090918122);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8362678"",""SNP detection";SOAPsnp;Xeon Phi;MIC;sparse matrix;"vectorization"",""Microsoft Windows";Tools;Coprocessors;Genomics;Bioinformatics;Graphics processing units;"Data structures"","""",""4"","""",""33"",""IEEE"",""22 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Multi-Level Domain-Decomposition Strategy for Solving the Eikonal Equation with the Fast-Sweeping Method,""A. Shrestha";" I. Senocak"",""Department of Computer Science, Boise State University, Boise, ID";" Mechanical Engineering and Materials Science Department, University of Pittsburgh, PA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2297"",""2303"",""Distance field from a surface geometry is used in several scientific algorithms and applications from computer graphics, visualization, computational fluid dynamics and more. Distance field can be calculated efficiently by solving the eikonal equation at each grid point using the fast-sweeping method. There has been an increased interest to develop parallel algorithms for the fast-sweeping method to accelerate its computational turnaround time. Most parallel strategies have focused on shared-memory parallelism and do not readily extend to distributed-memory parallelism to handle large-scale problems. To address this issue, we propose a domain-decomposition strategy to enable distributed-memory parallelism for the fast-sweeping method on heterogeneous computing clusters with accelerators. In our strategy, we make use of the Cuthill-McKee ordering for both fine and coarse-grain parallelism such that parallel computations proceed in direction of solution characteristics. We consider both CUDA and OpenACC implementations to compute the distance field from arbitrarily complex geometries and demonstrate parallel computations of large-scale problems that necessitate distributed-memory parallelism. We also discuss the implications of the proposed strategy for scalability of the fastsweeping method."",""1558-2183"","""",""10.1109/TPDS.2018.2829869"",""US National Science Foundation(grant numbers:1229709,1440638)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8345737"",""CUDA";eikonal equation;fast-sweeping method;graphics processing units;MPI;OpenACC;"signed distance field"",""Parallel processing";Graphics processing units;Convergence;Mathematical model;Two dimensional displays;Geometry;"Heuristic algorithms"","""",""3"","""",""18"",""IEEE"",""24 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Multi-Objective Optimization for Virtual Machine Allocation and Replica Placement in Virtualized Hadoop,""C. Guerrero"; I. Lera; B. Bermejo;" C. Juiz"",""Computer Science Department, Balearic Islands University, Palma, Spain"; Computer Science Department, Balearic Islands University, Palma, Spain; Computer Science Department, Balearic Islands University, Palma, Spain;" Computer Science Department, Balearic Islands University, Palma, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2568"",""2581"",""Resource management is a key factor in the performance and efficient utilization of cloud systems, and many research works have proposed efficient policies to optimize such systems. However, these policies have traditionally managed the resources individually, neglecting the complexity of cloud systems and the interrelation between their elements. To illustrate this situation, we present an approach focused on virtualized Hadoop for a simultaneous and coordinated management of virtual machines and file replicas. Specifically, we propose determining the virtual machine allocation, virtual machine template selection, and file replica placement with the objective of minimizing the power consumption, physical resource waste, and file unavailability. We implemented our solution using the non-dominated sorting genetic algorithm-II, which is a multi-objective optimization algorithm. Our approach obtained important benefits in terms of file unavailability and resource waste, with overall improvements of approximately 400 and 170 percent compared to three other optimization strategies. The benefits for the power consumption were smaller, with an improvement of approximately 1.9 percent."",""1558-2183"","""",""10.1109/TPDS.2018.2837743"",""Spanish Government"; Agencia Estatal de Investigación; European Commission; Fondo Europeo de Desarrollo Regional(grant numbers:TIN2017-88547-P); MINECO/AEI/FEDER;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360490"",""Virtual machine allocation";file replica placement;hadoop;"evolutionary computing and genetic algorithms"",""Resource management";Power demand;Genetic algorithms;Optimization;Virtual machining;Cloud computing;"Sorting"","""",""35"","""",""59"",""IEEE"",""17 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Near-Memory Acceleration for Radio Astronomy,""L. Fiorin"; R. Jongerius; E. Vermij; J. van Lunteren;" C. Hagleitner"",""IBM Research, Johan Huizingalaan 765, 1066 VH Amsterdam, the Netherlands"; IBM Research, Johan Huizingalaan 765, 1066 VH Amsterdam, the Netherlands; IBM Research, Johan Huizingalaan 765, 1066 VH Amsterdam, the Netherlands; IBM Research—Zürich, Saeumerstrasse 4, Rueschlikon 8803, Switzerland;" IBM Research—Zürich, Saeumerstrasse 4, Rueschlikon 8803, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""115"",""128"",""Processing-in-memory and near-memory computing have recently been rediscovered as a way to alleviate the “memory wall problem” of traditional computing architectures. In this paper, we discuss the implementation of a 3D-stacked near-memory accelerator, targeting radio astronomy and scientific applications. After exploring the design space of the architecture by focusing on minimizing the execution power of the processing pipeline of the SKA1-Low central signal processor, we show that our accelerator can achieve an energy efficiency of up to 390 GFLOPS/W, corresponding to an energy consumption one order of magnitude lower than alternative state-of-the-art implementations. When running additional mathematical and streaming-oriented kernels, our accelerator achieves from 6.4x to 20x energy efficiency improvement compared to alternative solutions."",""1558-2183"","""",""10.1109/TPDS.2017.2748580"",""Netherlands Organisation for Scientific Research NWO"; Dutch Ministry of EL and I Province of Drenthe(grant numbers:DOME project);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8027117"",""Custom architecture";accelerators;GPU;Xeon Phi;FPGA;HPC;processing-in-memory;near-memory;"SKA"",""Radio astronomy";Computer architecture;Kernel;Pipelines;Signal processing algorithms;Acceleration;"Antennas"","""",""1"","""",""41"",""IEEE"",""7 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Neurostream: Scalable and Energy Efficient Deep Learning with Smart Memory Cubes,""E. Azarkhish"; D. Rossi; I. Loi;" L. Benini"",""Department of Electrical, Electronic and Information Engineering, University of Bologna, Bologna, Italy"; Department of Electrical, Electronic and Information Engineering, University of Bologna, Bologna, Italy; Department of Electrical, Electronic and Information Engineering, University of Bologna, Bologna, Italy;" Department of Electrical, Electronic and Information Engineering, University of Bologna, Bologna, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""420"",""434"",""High-performance computing systems are moving towards 2.5D and 3D memory hierarchies, based on High Bandwidth Memory (HBM) and Hybrid Memory Cube (HMC) to mitigate the main memory bottlenecks. This trend is also creating new opportunities to revisit near-memory computation. In this paper, we propose a flexible processor-in-memory (PIM) solution for scalable and energy-efficient execution of deep convolutional networks (ConvNets), one of the fastest-growing workloads for servers and high-end embedded systems. Our co-design approach consists of a network of Smart Memory Cubes (modular extensions to the standard HMC) each augmented with a many-core PIM platform called NeuroCluster. NeuroClusters have a modular design based on NeuroStream coprocessors (for Convolution-intensive computations) and general-purpose RISC-V cores. In addition, a DRAM-friendly tiling mechanism and a scalable computation paradigm are presented to efficiently harness this computational capability with a very low programming effort. NeuroCluster occupies only 8 percent of the total logic-base (LoB) die area in a standard HMC and achieves an average performance of 240 GFLOPS for complete execution of full-featured state-of-the-art (SoA) ConvNets within a power budget of 2.5 W. Overall 11 W is consumed in a single SMC device, with 22.5 GFLOPS/W energy-efficiency which is 3.5X better than the best GPU implementations in similar technologies. The minor increase in system-level power and the negligible area increase make our PIM system a cost-effective and energy efficient solution, easily scalable to 955 GFLOPS with a small network of just four SMCs."",""1558-2183"","""",""10.1109/TPDS.2017.2752706"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038819"",""Hybrid memory cube";convolutional neural networks;large-scale deep learning;"streaming floating-point"",""Random access memory";Standards;Memory management;Machine learning;Three-dimensional displays;Bandwidth;"Indexes"","""",""59"","""",""61"",""IEEE"",""15 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Non-Asymptotic Delay Bounds for Multi-Server Systems with Synchronization Constraints,""M. Fidler"; B. Walker;" Y. Jiang"",""Institute of Communications Technology, Leibniz Universität Hannover, Hannover, Germany"; Institute of Communications Technology, Leibniz Universität Hannover, Hannover, Germany;" Department of Information Security and Communication Technology, NTNU Trondheim, Trondheim, Norway"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1545"",""1559"",""Parallel computing has become a standard tool with architectures such as Google MapReduce, Hadoop, and Spark being broadly used in applications such as data processing and machine learning. Common to these systems are a fork operation, where jobs are first divided into tasks that are processed in parallel, and a join operation where completed tasks wait for the other tasks of the job before leaving the system. The synchronization constraint of the join operation makes the analysis of fork-join systems challenging, and few explicit results are known. In this work, we formulate a max-plus server model for parallel systems which allows us to derive performance bounds for a variety of systems in the GII GI and G I G cases. We contribute end-to-end delay bounds for multi-stage fork-join networks. We perform a detailed comparison of different multi-server configurations, including an analysis of single-queue fork-join systems that achieve a fundamental performance gain. We compare these results to both simulation and a live Spark system."",""1558-2183"","""",""10.1109/TPDS.2017.2779872"",""European Research Council(grant numbers:StG 306644)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8242677"",""Parallel computing";MapReduce;Hadoop;spark;performance analysis;"stochastic network calculus"",""Servers";Sparks;Delays;Stochastic processes;Load management;"Synchronization"","""",""13"","""",""43"",""OAPA"",""1 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Non-Preemptive Scheduling for Mixed-Criticality Real-Time Multiprocessor Systems,""H. Baek"; N. Jung; H. S. Chwa; I. Shin;" J. Lee"",""Department of Computer Science and Engineering, Sungkyunkwan University (SKKU), Suwon-si, Gyeonggi-do, Republic of Korea"; Department of Computer Science and Engineering, Sungkyunkwan University (SKKU), Suwon-si, Gyeonggi-do, Republic of Korea; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI; School of Computing, KAIST, Daejeon, Republic of Korea;" Department of Computer Science and Engineering, Sungkyunkwan University (SKKU), Suwon-si, Gyeonggi-do, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1766"",""1779"",""Real-time scheduling for Mixed-Criticality (MC) systems has received a growing attention as real-time embedded systems accommodate various tasks with different levels of criticality. While many studies have addressed how to guarantee timing requirements for MC systems with uniprocessor and multiprocessors, most of them have focused on supporting preemptive tasks. On the other hand, there have been few studies to address non-preemptive scheduling especially for MC multiprocessor platforms, in which the jobs under execution cannot be preempted by other jobs. In this paper, we develop schedulability tests for non-preemptive scheduling, which is the first attempt for MC multiprocessor systems. To this end, we first generalize an existing NP-EDF (Non-Preemptive Earliest Deadline First) schedulability test developed for single-criticality multiprocessor systems, towards for MC multiprocessor systems. For the generalization, we introduce new timing guarantee techniques for the system transition between two different criticalities, which is one of the key features in MC systems. We next extend the proposed NP-EDF schedulability test towards NP-EDFVD (NP-EDF with Virtual Deadlines) that is specialized for MC systems, and pose a virtual deadline assignment problem. We develop an optimal virtual deadline assignment policy using a control knob of the system-level deadline-reduction parameter and then a suboptimal one for the task-level parameter. Our simulation results demonstrate that the NP-EDFVD schedulability test with the proposed virtual deadline assignment policies finds a number of additional schedulable task sets, which are not schedulable by the NP-EDF schedulability test."",""1558-2183"","""",""10.1109/TPDS.2018.2806443"",""National Research Foundation of Korea"; Ministry of Science and ICT(grant numbers:2017R1A2B2002458,2017H1D8A2031628,2017K2A9A1A01092689); Ministry of Education(grant numbers:2016R1A6A3A11930688); IITP (Institute for Information & communications Technology Promotion); MSIT (Ministry of Science and ICT)(grant numbers:2015-0-00914,IITP-2017-2015-0-00742,2014-0-00065); Resilient Cyber-Physical Systems Research(grant numbers:2017M3C4A7065927);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8292931"",""Real-time scheduling";schedulability analysis;mixed-criticality;non-preemptive tasks;"real-time multiprocessor systems"",""Task analysis";Bars;Timing;Scheduling algorithms;"Real-time systems"","""",""12"","""",""27"",""IEEE"",""15 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"On Random Wiring in Practicable Folded Clos Networks for Modern Datacenters,""C. Camarero"; C. Martınez;" R. Beivide"",""Department of Computer Science and Electronics, Universidad de Cantabria, Santander, Spain"; Department of Computer Science and Electronics, Universidad de Cantabria, Santander, Spain;" Department of Computer Science and Electronics, Universidad de Cantabria, Santander, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1780"",""1793"",""Big scale, high performance and fault-tolerance, low-cost and graceful expandability are pursued features in current datacenter networks (DCN). Although there have been many proposals for DCNs, most modern installations are equipped with classical folded Clos networks. Recently, regular random topologies, as the Jellyfish, have been proposed for DCNs. However, their completely unstructured nature entails serious design problems. In this paper we propose Random Folded Clos (RFC) and Hydra networks in which the interconnection between certain switches levels is made randomly. Both RFCs and Hydras preserve important properties of Clos networks that provide a straightforward deadlock-free multi-path routing. The proposed networks leverage randomness to be gracefully expandable, thereby allowing for fine grain upgrading. RFCs and Hydras are compared in the paper, in topological and cost terms, against fat-trees, orthogonal fat-trees and random regular networks. Also, experiments are carried out to simulate their performance under synthetic traffic patterns emulating common loads present in warehouse scale computers. These theoretical and empirical studies reveal the interest of these topologies, concluding that Hydra constitutes a practicable alternative to current datacenter networks since it appropriately balance all the main design requirements. Moreover, Hydras perform better than the fat-trees, their natural competitor, being able to connect the same or more computing nodes with significant lower cost and latency while exhibiting comparable throughput."",""1558-2183"","""",""10.1109/TPDS.2018.2805344"",""Spanish Science and Technology Commission (CICYT)(grant numbers:TIN2016-76635-C2-2-R)"; European Union’s Horizon 2020 research and innovation program(grant numbers:671697,671610);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8290692"",""Datacenter network";random topologies;"folded Clos networks"",""Topology";Network topology;Routing;System recovery;Servers;Bandwidth;"Graph theory"","""",""3"","""",""40"",""IEEE"",""12 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"On the Synchronization Bottleneck of OpenStack Swift-Like Cloud Storage Systems,""M. Ruan"; T. Titcheu; E. Zhai; Z. Li; Y. Liu; J. E; Y. Cui;" H. Xu"",""School of Software, Tsinghua University, Beijing, China"; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Department of Computer Science, Yale University, New Haven, CT; School of Software, Tsinghua University, Beijing, China; Department of Computer Science, Binghamton University, NY; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science, City University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2059"",""2074"",""As one type of the most popular cloud storage services, OpenStack Swift and its follow-up systems replicate each object across multiple storage nodes and leverage object sync protocols to achieve high reliability and eventual consistency. The performance of object sync protocols heavily relies on two key parameters: $r$  (number of replicas for each object) and $n$  (number of objects hosted by each storage node). In existing tutorials and demos, the configurations are usually  $r=3$  and $n<1,000$  by default, and the sync process seems to perform well. However, we discover in data-intensive scenarios, e.g., when  $r>3$  and $n\gg 1,000$ , the sync process is significantly delayed and produces massive network overhead, referred to as the sync bottleneck problem. By reviewing the source code of OpenStack Swift, we find that its object sync protocol utilizes a fairly simple and network-intensive approach to check the consistency among replicas of objects. Hence in a sync round, the number of exchanged hash values per node is $\Theta (n\times r)$ . To tackle the problem, we propose a lightweight and practical object sync protocol, LightSync, which not only remarkably reduces the sync overhead, but also preserves high reliability and eventual consistency. LightSync derives this capability from three novel building blocks: 1) Hashing of Hashes, which aggregates all the $h$  hash values of each data partition into a single but representative hash value with the Merkle tree"; 2) Circular Hash Checking, which checks the consistency of different partition replicas by only sending the aggregated hash value to the clockwise neighbor;" and 3) Failed Neighbor Handling, which properly detects and handles node failures with moderate overhead to effectively strengthen the robustness of LightSync. The design of LightSync offers provable guarantee on reducing the per-node network overhead from $\Theta (n\times r)$ to $\Theta (\frac{n}{h})$ . Furthermore, we have implemented LightSync as an open-source patch and adopted it to OpenStack Swift, thus reducing the sync delay by up to 879 $\times$  and the network overhead by up to 47.5$\times$ ."",""1558-2183"","""",""10.1109/TPDS.2018.2810179"",""High-Tech R&D Program of China (“863–China Cloud” Major Program)(grant numbers:2015AA01A201)"; NSFC(grant numbers:61471217,61432002,61632020,61472337); National Research Fund, Luxembourg; NSF(grant numbers:CCF-1302327,CCF-1715387);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8303732"",""Cloud storage";OpenStack Swift;object synchronization;"performance bottleneck"",""Synchronization";Protocols;Cloud computing;Delays;Electronic mail;Reliability;"Open source software"","""",""6"","""",""44"",""IEEE"",""27 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Online Auction for IaaS Clouds: Towards Elastic User Demands and Weighted Heterogeneous VMs,""J. Li"; Y. Zhu; J. Yu; C. Long; G. Xue;" S. Qian"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Automation, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China;" Shanghai Institute for Advanced Communication and Data Science, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2075"",""2089"",""Auctions have been adopted by many major cloud providers, such as Amazon EC2. Unfortunately, only simple auctions have been implemented. Such simple auction has serious limitations, such as being unable to accept elastic user demands and having to allocate different types of VMs independently. These limitations create a big gap between the real needs of cloud users and the available services of cloud providers. In response to the limitations of the existing auction mechanisms, this paper proposes a novel online auction mechanism for IaaS clouds, with the unique features of an elastic model for inputting time-varying user demands and a unified model for requesting heterogeneous VMs together. However, several major challenges should be addressed, such as NP hardness of optimal VM allocation, time-varying user demands and potential misreports of private information of cloud users. We propose a truthful online auction mechanism for maximizing the profit of the cloud provider in IaaS clouds, which is composed of a price-based allocation rule and a payment rule. In the allocation rule, the online auction mechanism determines the number of VMs of each type to each user. In the payment rule, by introducing a marginal price function for each type of VMs, the mechanism determines how much the cloud provider should charge each cloud user. With solid theoretical analysis and trace-driven simulations, we demonstrate that our mechanism is truthful, fair and individually rational, and has a polynomial-time complexity. In addition, our auction achieves a competitive ratio for the profit of the cloud provider, compared against the offline optimal one."",""1558-2183"","""",""10.1109/TPDS.2018.2814566"",""National Key R&D Program of China(grant numbers:2017YFC0803700,2017YFC0803704)"; NSFC(grant numbers:61772341,61472254,61572324,61170238,61772334,61702151,61673275,61473184); STCSM(grant numbers:15DZ1100305); Singapore NRF(grant numbers:CREATE E2S2); National Natural Science Foundation of China(grant numbers:U1736207); Program for New Century Excellent Talents in University of China; Program for Changjiang Young Scholars in University of China; Program for China Top Young Talents; Program for Shanghai Top Young Talents; Shanghai Talent Development Fund;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8314090"",""IaaS clouds";online auctions;time-varying;elastic user demands;weighted heterogeneous VMs;"profit maximization"",""Cloud computing";Resource management;Pricing;Upper bound;Cost accounting;Solids;"Solid modeling"","""",""15"","""",""32"",""IEEE"",""12 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Online Tuning of EASY-Backfilling using Queue Reordering Policies,""E. Gaussier"; J. Lelong; V. Reis;" D. Trystram"",""University Grenoble Alpes, CNRS, Grenoble INP, LIG, Saint-Martin-d'Hères, France"; University Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, Saint-Martin-d’Hères, France; University Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, Saint-Martin-d’Hères, France;" University Grenoble Alpes, Inria, CNRS, Grenoble INP, LIG, Saint-Martin-d’Hères, France"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2304"",""2316"",""The EASY-FCFS heuristic is the basic building block of job scheduling policies in most parallel High Performance Computing platforms. Despite its simplicity, and the guarantee of no job starvation, it could still be improved on a per-system basis. Such tuning is difficult because of non-linearities in the scheduling process. The study conducted in this paper considers an online approach to the automatic tuning of the EASY heuristic for HPC platforms. More precisely, we consider the problem of selecting a reordering policy for the job queue under several feedback modes. We show via a comprehensive experimental validation on actual logs that periodic simulation of historical data can be used to recover existing in-hindsight results that allow to divide the average waiting time by almost 2. This results holds even when the simulator results are noisy. Moreover, we show that good performances can still be obtained without a simulator, under what is called bandit feedback - when we can only observe the performance of the algorithm that was picked on the live system. Indeed, a simple multi-armed bandit algorithm can reduce the average waiting time by 40 percent."",""1558-2183"","""",""10.1109/TPDS.2018.2820699"",""LabEx PERSYVAL-Lab(grant numbers:ANR-11-LABX-0025-01)"; French program Investissement d'avenir; Inria; CNRS; RENATER;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327923"",""parallel systems";scheduling;"learning"",""Tuning";Software;Scheduling;Uncertainty;Processor scheduling;Noise measurement;"Robustness"","""",""13"","""",""43"",""IEEE"",""29 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimization of Error-Bounded Lossy Compression for Hard-to-Compress HPC Data,""S. Di";" F. Cappello"",""Mathematics and Computer Science (MCS) division at Argonne National Laboratory, Lemont, IL";" Mathematics and Computer Science (MCS) division at Argonne National Laboratory, Lemont, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""129"",""143"",""Since today's scientific applications are producing vast amounts of data, compressing them before storage/transmission is critical. Results of existing compressors show two types of HPC data sets: highly compressible and hard to compress. In this work, we carefully design and optimize the error-bounded lossy compression for hard-to-compress scientific data. We propose an optimized algorithm that can adaptively partition the HPC data into best-fit consecutive segments each having mutually close data values, such that the compression condition can be optimized. Another significant contribution is the optimization of shifting offset such that the XOR-leading-zero length between two consecutive unpredictable data points can be maximized. We finally devise an adaptive method to select the best-fit compressor at runtime for maximizing the compression factor. We evaluate our solution using 13 benchmarks based on real-world scientific problems, and we compare it with 9 other state-of-the-art compressors. Experiments show that our compressor can always guarantee the compression errors within the user-specified error bounds. Most importantly, our optimization can improve the compression factor effectively, by up to 49 percent for hard-to-compress data sets with similar compression/ decompression time cost."",""1558-2183"","""",""10.1109/TPDS.2017.2749300"",""Exascale Computing Project (ECP)(grant numbers:17-SC-20-SC)"; DOE organizations-the Office of Science; National Nuclear Security Administration; UChicago Argonne; LLC; Operator of Argonne National Laboratory (Argonne); U.S. Department of Energy Office of Science laboratory(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8031063"",""Error-bounded lossy compression";floating-point data compression;high performance computing;"scientific simulation"",""Data models";Compressors;Benchmark testing;Solid modeling;Analytical models;Optimization;"Distributed databases"","""",""13"","""",""34"",""IEEE"",""11 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizations of Unstructured Aerodynamics Computations for Many-core Architectures,""M. A. Al Farhan";" D. E. Keyes"",""Extreme Computing Research Center, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia";" Extreme Computing Research Center, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2317"",""2332"",""We investigate several state-of-the-practice shared-memory optimization techniques applied to key routines of an unstructured computational aerodynamics application with irregular memory accesses. We illustrate for the Intel Knights Landing processor, as a representative of the processors in contemporary leading supercomputers, identifying and addressing performance challenges without compromising the floating point numerics of the original code. We employ low and high-level architecture-specific code optimizations involving thread and data-level parallelism. Our approach is based upon a multi-level hierarchical distribution of work and data across both the threads and the SIMD units within every hardware core. On a 64-core Knights Landing chip, we achieve nearly 2.9× speedup of the dominant routines relative to the baseline. These exhibit almost linear strong scalability up to 64 threads, and thereafter some improvement with hyperthreading. At substantially fewer Watts, we achieve up to 1.7× speedup relative to the performance of 72 threads of a 36-core Haswell CPU and roughly equivalent performance to 112 threads of a 56-core Skylake scalable processor. These optimizations are expected to be of value for many other unstructured mesh PDE-based scientific applications as multi and many-core architecture evolves."",""1558-2183"","""",""10.1109/TPDS.2018.2826533"",""KAUST Extreme Computing Research Center"; KAUST Supercomputing Laboratory; KAUST Information Technology Research Division; Intel Parallel Computing Centers;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8337750"",""Performance optimizations";thread-level parallelism;data-level parallelism;AVX-512;Knights Landing;SIMD;computational aerodynamics;unstructured meshes;"Intel Xeon Phi"",""Kernel";Optimization;Computer architecture;Computational modeling;Parallel processing;Aerodynamics;"Hardware"","""",""10"","""",""84"",""IEEE"",""13 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"OrthoNoC: A Broadcast-Oriented Dual-Plane Wireless Network-on-Chip Architecture,""S. Abadal"; J. Torrellas; E. Alarcón;" A. Cabellos-Aparicio"",""NaNoNetworking Center in Catalonia (N3Cat), Universitat Politècnica de Catalunya, Barcelona, Spain"; Department of Computer Science, University of Illinois at Urbana-Champaign, Champaign, IL; NaNoNetworking Center in Catalonia (N3Cat), Universitat Politècnica de Catalunya, Barcelona, Spain;" NaNoNetworking Center in Catalonia (N3Cat), Universitat Politècnica de Catalunya, Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""628"",""641"",""On-chip communication remains as a key research issue at the gates of the manycore era. In response to this, novel interconnect technologies have opened the door to new Network-on-Chip (NoC) solutions towards greater scalability and architectural flexibility. Particularly, wireless on-chip communication has garnered considerable attention due to its inherent broadcast capabilities, low latency, and system-level simplicity. This work presents ORTHONOC, a wired-wireless architecture that differs from existing proposals in that both network planes are decoupled and driven by traffic steering policies enforced at the network interfaces. With these and other design decisions, ORTHONOC seeks to emphasize the ordered broadcast advantage offered by the wireless technology. The performance and cost of ORTHONOC are first explored using synthetic traffic, showing substantial improvements with respect to other wired-wireless designs with a similar number of antennas. Then, the applicability of ORTHONOC in the multiprocessor scenario is demonstrated through the evaluation of a simple architecture that implements fast synchronization via ordered broadcast transmissions. Simulations reveal significant execution time speedups and communication energy savings for 64-threaded benchmarks, proving that the value of ORTHONOC goes beyond simply improving the performance of the on-chip interconnect."",""1558-2183"","""",""10.1109/TPDS.2017.2764901"",""Catalan Government(grant numbers:2014SGR-1427)"; Spanish State Ministry of Economy and Competitiveness(grant numbers:PCIN-2015-012); NSF(grant numbers:CCF 16-29431);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8078211"",""Network-on-Chip";wireless on-chip communication;broadcast;hybrid NoC;"manycore processors"",""Wireless communication";Throughput;System-on-chip;Bandwidth;Scalability;Program processors;"Coherence"","""",""38"","""",""51"",""IEEE"",""23 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"P3S: A Methodology to Analyze and Predict Application Scalability,""J. Panadero"; A. Wong; D. Rexachs;" E. Luque"",""Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain"; Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain; Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain;" Computer Architecture and Operating System Department, Universitat Autonoma de Barcelona, Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""642"",""658"",""Executing message-passing parallel applications on a large number of resources in an efficient way is not a trivial task. Due to the complex interaction between the parallel applications and the HPC system, many applications may suffer performance inefficiencies when they scale. To achieve an efficient use of these large-scale systems using thousands of cores, a point to consider before executing an application is to know its behavior in the system. In this work, we propose a novel methodology called P3S (Prediction of Parallel Program Scalability), which allows us to analyze and predict the scalability of message-passing applications on a given system. The methodology strives to use a bounded analysis time, and a reduced set of resources to predict the application behavior for large-scale. The experimental validation proves that the P3S is able to predict the application scalability with an average accuracy greater than 95 percent using a reduced set of resources."",""1558-2183"","""",""10.1109/TPDS.2017.2763148"",""MINECO Spain(grant numbers:TIN2014-53172)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8068239"",""HPC systems";scalability prediction;MPI application;"prediction of an application scalability"",""Scalability";Tools;Predictive models;Computational modeling;Mathematical model;Multicore processing;"Hardware"","""",""6"","""",""24"",""IEEE"",""16 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallel Algorithm for Incremental Betweenness Centrality on Large Graphs,""F. Jamour"; S. Skiadopoulos;" P. Kalnis"",""King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"; University of the Peloponnese, Tripoli, Greece;" King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""659"",""672"",""Betweenness centrality quantifies the importance of nodes in a graph in many applications, including network analysis, community detection and identification of influential users. Typically, graphs in such applications evolve over time. Thus, the computation of betweenness centrality should be performed incrementally. This is challenging because updating even a single edge may trigger the computation of all-pairs shortest paths in the entire graph. Existing approaches cannot scale to large graphs: they either require excessive memory (i.e., quadratic to the size of the input graph) or perform unnecessary computations rendering them prohibitively slow. We propose iCENTRAL"; a novel incremental algorithm for computing betweenness centrality in evolving graphs. We decompose the graph into biconnected components and prove that processing can be localized within the affected components. iCENTRAL is the first algorithm to support incremental betweeness centrality computation within a graph component. This is done efficiently, in linear space;" consequently, iCENTRAL scales to large graphs. We demonstrate with real datasets that the serial implementation of iCENTRAL is up to 3.7 times faster than existing serial methods. Our parallel implementation that scales to large graphs, is an order of magnitude faster than the state-of-the-art parallel algorithm, while using an order of magnitude less computational resources."",""1558-2183"","""",""10.1109/TPDS.2017.2763951"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8070346"",""Betweenness centrality";dynamic graph algorithms;"parallel graph algorithms"",""Measurement";Random access memory;Parallel algorithms;Heuristic algorithms;Algorithm design and analysis;Social network services;"Memory management"","""",""33"","""",""39"",""IEEE"",""17 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallel Computation of Component Trees on Distributed Memory Machines,""M. Götz"; G. Cavallaro; T. Géraud; M. Book;" M. Riedel"",""Jülich Supercomputing Center, Wilhelm-Johnen-Straße, Jülich, Germany"; Jülich Supercomputing Center, Wilhelm-Johnen-Straße, Jülich, Germany; EPITA Research and Development Laboratory (LRDE), Le Kremlin-Bicêtre, France; University of Iceland, Reykjavik, Iceland;" Jülich Supercomputing Center, Wilhelm-Johnen-Straße, Jülich, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2582"",""2598"",""Component trees are region-based representations that encode the inclusion relationship of the threshold sets of an image. These representations are one of the most promising strategies for the analysis and the interpretation of spatial information of complex scenes as they allow the simple and efficient implementation of connected filters. This work proposes a new efficient hybrid algorithm for the parallel computation of two particular component trees-the max- and min-tree-in shared and distributed memory environments. For the node-local computation a modified version of the flooding-based algorithm of Salembier is employed. A novel tuple-based merging scheme allows to merge the acquired partial images into a globally correct view. Using the proposed approach a speed-up of up to 44.88 using 128 processing cores on eight-bit gray-scale images could be achieved. This is more than a five-fold increase over the state-of-the-art shared-memory algorithm, while also requiring only one-thirty-second of the memory."",""1558-2183"","""",""10.1109/TPDS.2018.2829724"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360392"",""Component-trees";threshold decomposition;max-tree;connected component labeling;high-performance computing;hybrid application;MPI;"multithreading"",""Electronic mail";Image resolution;Gray-scale;Remote sensing;Morphology;Shape;"Merging"","""",""12"","""",""57"",""CCBY"",""17 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Penguin: Efficient Query-Based Framework for Replaying Large Scale Historical Data,""R. Gu"; Y. Zhou; Z. Wang; C. Yuan;" Y. Huang"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2333"",""2345"",""In the big data era, there are many demands for efficient and easy to use data replay services over large scale historical data. For example, stock security trading and on-line e-business services need historical data replay services to conduct system testing or ex-post review and analysis, just like replaying videos for security monitoring. Stream processing systems are designed for processing stream data, thus can not perform complex replay jobs over the static historical data. Database management systems support easy to use complex queries, but lack stream processing abilities. In this paper, we present a data replay model that combines the stream replay and complex query ability together, to allow the applications to replay large scale historical data from various data sources. First, to meet the demands of flexible replay semantics, we designed a set of easy to use replay operators to describe various replay behaviors and semantics. Users can use these operators to build up their complex replay jobs with diversified requirements. Then, we proposed a query mechanism to provide a flexible data loading service. Next, we presented the Penguin framework to support the proposed query-based replay model along with the replay operators. Penguin enables users to develop high-throughput and easy to use replay services over various large scale data sources with tunable replay speeds. To further improve the data replay performance, we proposed four system-level optimizations, including caching loading task results, cascading merging intermediate record queues, producing the replay queue in parallel, and caching remote file streams. Experimental results over replaying millions of records demonstrate that Penguin can achieve up to 4x and 144x speedup in data preparation and up to 16x and 9x speedup in replay speed compared to Apache Phoenix and Apache Hive respectively. As a case study, Penguin has been deployed in production environments of some Securities companies to provide online historical stock data replay services to large number of stock market users."",""1558-2183"","""",""10.1109/TPDS.2018.2829759"",""National Natural Science Foundation of China(grant numbers:61702254,61572250)"; Jiangsu Province Industry Support Program(grant numbers:BE2014131); Jiangsu Province National Natural Science Foundation(grant numbers:BK20170651); China Postdoctoral Science Foundation; Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8345689"",""Data replay framework";query-based stream replay;"distributed processing"",""Distributed databases";Structured Query Language;Sorting;Sparks;Data models;"Semantics"","""",""5"","""",""30"",""IEEE"",""24 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PEPS++: Towards Extreme-Scale Simulations of Strongly Correlated Quantum Many-Particle Models on Sunway TaihuLight,""L. He"; H. An; C. Yang; F. Wang; J. Chen; C. Wang; W. Liang; S. Dong; Q. Sun; W. Han; W. Liu; Y. Han;" W. Yao"",""CAS Key Laboratory of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; CCSE & CAPT, Peking University, Beijing, China; National Research Center of Parallel Computer Engineering and Technology, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; CAS Key Laboratory of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; CAS Key Laboratory of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China; Institute of Software, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; CAS Key Laboratory of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China; CAS Key Laboratory of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2838"",""2848"",""The study of strongly frustrated magnetic systems has drawn great attentions from both theoretical and experimental physics. Efficient simulations of these models are essential for understanding their exotic properties. Here we present PEPS++, a novel computational paradigm for simulating frustrated magnetic systems and other strongly correlated quantum many-body systems. PEPS++ can accurately solve these models at the extreme scale with low cost and high scalability on modern heterogeneous supercomputers. We implement PEPS++ on Sunway TaihuLight based on a carefully designed tensor computation library for manipulating high-rank tensors and optimize it by invoking various high-performance matrix and tensor operations. By solving a 2D strongly frustrated  $J_1$ -$J_2$  model with over ten million cores, PEPS++ demonstrates the capability of simulating strongly correlated quantum many-body problems at unprecedented scales with accuracy and time-to-solution far beyond the previous state of the art."",""1558-2183"","""",""10.1109/TPDS.2018.2848618"",""National Key Research and Development Program of China(grant numbers:2016YFB0201202,2016YFB0201902)"; National Natural Science Foundation of China(grant numbers:91530323);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8388281"",""Quantum spin liquid";PEPS++;"Sunway Taihulight"",""Tensile stress";Computational modeling;Quantum entanglement;Two dimensional displays;"Wave functions"","""",""13"","""",""40"",""IEEE"",""19 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Performance Model of MapReduce Iterative Applications for Hybrid Cloud Bursting,""F. J. Clemente-Castelló"; B. Nicolae; R. Mayo;" J. C. Fernández"",""Universitat Jaume I, Castellón, Spain"; Argonne National Laboratory, Lemont, IL; Universitat Jaume I, Castellón, Spain;" Universitat Jaume I, Castellón, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1794"",""1807"",""Hybrid cloud bursting (i.e., leasing temporary off-premise cloud resources to boost the overall capacity during peak utilization) can be a cost-effective way to deal with the increasing complexity of big data analytics, especially for iterative applications. However, the low throughput, high latency network link between the on-premise and off-premise resources (“weak link”) makes maintaining scalability difficult. While several data locality techniques have been designed for big data bursting on hybrid clouds, their effectiveness is difficult to estimate in advance. Yet such estimations are critical, because they help users decide whether the extra pay-as-you-go cost incurred by using the off-premise resources justifies the runtime speed-up. To this end, the current paper presents a performance model and methodology to estimate the runtime of iterative MapReduce applications in a hybrid cloud-bursting scenario. The paper focuses on the overhead incurred by the weak link at fine granularity, for both the map and the reduce phases. This approach enables high estimation accuracy, as demonstrated by extensive experiments at scale using a mix of real-world iterative MapReduce applications from standard big data benchmarking suites that cover a broad spectrum of data patterns. Not only are the produced estimations accurate in absolute terms compared with experimental results, but they are also up to an order of magnitude more accurate than applying state-of-art estimation approaches originally designed for single-site MapReduce deployments."",""1558-2183"","""",""10.1109/TPDS.2018.2802932"",""U.S. Department of Energy"; Office of Science(grant numbers:DE-AC02-06CH11357); Spanish CICYT projects(grant numbers:TIN2014-53495-R,TIN2017-82972-R);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8283575"",""Hybrid cloud";big data analytics;iterative applications;MapReduce;performance prediction;"runtime estimation"",""Cloud computing";Big Data;Estimation;Runtime;Task analysis;Data transfer;"Adaptation models"","""",""14"","""",""40"",""IEEE"",""6 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Power-Aware and Performance-Guaranteed Virtual Machine Placement in the Cloud,""H. Zhao"; J. Wang; F. Liu; Q. Wang; W. Zhang;" Q. Zheng"",""School of Computer Science and Technology, Xidian University, Xi'an, China"; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; Xi'an Jiaotong University, Xi'an, China;" Xi'an Jiaotong University, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 May 2018"",""2018"",""29"",""6"",""1385"",""1400"",""Cloud service providers offer virtual machines (VMs) as services to users over Internet. As VMs are running on physical machines (PMs), PM power consumption needs to be considered. Meanwhile, VMs running on the same PM share physical resources, and there exists great resource contention, which results in VM performance degradation. Therefore, how to place VMs to reduce PM power consumption and guarantee VM performance is still one major challenge. However, existing VMPs did not study VM performance degradation, so they could not guarantee VM performance. To solve the high power consumption and VMs performance degradation problems, this paper explores the balance between saving PM power and guaranteeing VM performance, and proposes a power-aware and performance-guaranteed VMP (PPVMP). First, we investigate the relationship between power consumption and CPU utilization to build a non-linear power model, which is helpful for the following VMP. Second, we construct VM performance models to present the VM performance degradation trend. Third, based on these models, we formulate VMP as a bi-objective optimization problem, which tries to minimize PM power consumption and guarantee VM performance. We then propose an algorithm based on ant colony optimization to solve it. Finally, the results show the efficiency of our algorithm."",""1558-2183"","""",""10.1109/TPDS.2018.2794369"",""National Science Foundation of China (NSFC)"; Natural Science Foundation of Shaanxi Province of China(grant numbers:2018JQ3526); Fundamental Research Funds for the Central Universities(grant numbers:JB170307,JB180306); NSFC(grant numbers:61702394,61702395,61472317,61772414); MOE Innovation Research Team(grant numbers:IRT13035); Project of China Knowledge Centre for Engineering Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8259446"",""Power consumption";performance-guaranteed;virtual machine placement;"ant colony optimization"",""Cloud computing";Power demand;Optimization;Degradation;Virtual machining;Resource management;"Approximation algorithms"","""",""97"","""",""64"",""IEEE"",""16 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Preserving Model Privacy for Machine Learning in Distributed Systems,""Q. Jia"; L. Guo; Z. Jin;" Y. Fang"",""Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY"; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY; Department of Electrical and Computer Engineering, Binghamton University, Binghamton, NY;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1808"",""1822"",""Machine Learning based data classification is a widely used data mining technique. By learning massive data collected from the real world, data classification helps learners discover hidden data patterns. These hidden data patterns are represented by the learned model in different machine learning schemes. Based on such models, a user can classify whether the new incoming data belongs to an existing class";" or, multiple entities may test the similarity of their datasets. However, due to data locality and privacy concerns, it is infeasible for large-scale distributed systems to share each individual's datasets for classifying or testing. On the one hand, the learned model is an entity's private asset and may leak private information, which should be well protected from all other non-collaborative entities. On the other hand, the new incoming data may contain sensitive information which cannot be disclosed directly for classification. To address the above privacy issues, we propose an approach to preserve the model privacy of the data classification and similarity evaluation for distributed systems. With our scheme, neither new data nor learned models are directly revealed during the classification and similarity evaluation procedures. Based on extensive real-world experiments, we have evaluated the privacy preservation, feasibility, and efficiency of the proposed scheme."",""1558-2183"","""",""10.1109/TPDS.2018.2809624"",""National Science Foundation(grant numbers:IIS-1722731,ECCS-1710996)"; National Science Foundation (NSF)(grant numbers:CNS-1422417,ECCS-1462473); National Science Foundation (NSF)(grant numbers:IIS-1722791);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8302601"",""Machine learning";privacy preservation;data classification;"model evaluation"",""Data models";Data privacy;Predictive models;Distributed databases;Support vector machines;Cryptography;"Privacy"","""",""29"","""",""44"",""IEEE"",""26 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PROSA: Protocol-Driven Network on Chip Architecture,""M. G. Alonso";" J. Flich"",""Department of Computing Engineering, Universitat Politecnica de Valencia, València, Spain";" Department of Computing Engineering, Universitat Politecnica de Valencia, València, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1560"",""1574"",""Nowadays chip multiprocessors (CMPs) tend to increase the number of cores, usually implementing a distributed shared last level cache (LLC). The network on chip (NoC) is in charge of interconnecting the cores, memory controller(s) and cache banks, largely impacting memory access latency. Packet switching (PS) is typically used in NoCs but circuit switching (CS) may complement PS achieving higher performance if the circuit is established before its need. In this paper we propose PROSA, an architecture to improve memory access latency by using CS. In PROSA, the coherence protocol steers the circuit setup logic in order to configure circuits before they are needed and only for the time they are required. PROSA uses a clustered router approach where groups of four routers are clustered and their circuit control logic is combined. Based on key design decisions, we present different PROSA versions, analyzing their impact on applications and NoC performance. PROSA is able to reduce applications' execution time by 35 percent while it significantly reduces average network fllt latency by 54 percent, leading to a reduction of miss load (and store) latency of 21 percent (in CMP systems with 64 processors). PROSA needs 8.4 percent more area, but reduces power consumption by 7 percent."",""1558-2183"","""",""10.1109/TPDS.2017.2784422"",""Spanish Ministerio de Economía y Competitividad (MINECO)"; FEDER funds(grant numbers:TIN2015-66972-C5-1-R);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8219740"",""Coherence protocol";circuit-packet switching;"network on chip"",""Coherence";Distributed databases;Routing protocols;Switching circuits;Program processors;"Delays"","""",""7"","""",""27"",""IEEE"",""18 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Quadboost: A Scalable Concurrent Quadtree,""K. Zhou"; G. Tan;" W. Zhou"",""State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Beijing, China"; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Beijing, China;" School of Software, Yunnan University, Kunming, Yunnan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""673"",""686"",""Building concurrent spatial trees is more complicated than binary search trees since a space hierarchy should be preserved during modifications. We present a non-blocking quadtree (quadboost) that supports concurrent insert, remove, move, and contain operations, in which the move operation combines the searches for different keys together and modifies different positions atomically. To increase its concurrency, a decoupling approach is proposed to separate physical adjustment from logical removal within the remove operation. In addition, we design a continuous find mechanism to reduce the search cost. Experimental results show that quadboost scales well on a multi-core system with 32 hardware threads. It outperforms existing concurrent trees in retrieving two-dimensional keys with up to 109 percent improvement when the number of threads is large. Furthermore, the move operation achieves better performance than the best-known algorithm with up to 47 percent."",""1558-2183"","""",""10.1109/TPDS.2017.2762298"",""National Natural Science Foundation of China(grant numbers:61762089,61363021,61521092,91430218,31327901,61472395,61272134,61432018)"; The National Key Research and Development Program of China(grant numbers:2016YFB0201305,2016YFB0200300,2016YFB0200504,2016YFB0200803);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8066340"",""Concurrent data structures";quadtree;continuous find;decoupling;"LCA"",""Concurrent computing";Algorithm design and analysis;Binary search trees;Instruction sets;Spatial databases;"Routing"","""",""3"","""",""22"",""IEEE"",""12 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Quantifying the Impact of Variability and Heterogeneity on the Energy Efficiency for a Next-Generation Ultra-Green Supercomputer,""F. Fraternali"; A. Bartolini; C. Cavazzoni;" L. Benini"",""Department of Electrical, Electronic and Information Engineering (DEI), University of Bologna, Bologna, Italy"; Department of Electrical, Electronic and Information Engineering (DEI), University of Bologna, Bologna, Italy; SCAI, Casalecchio di Reno, Bologna, Italy;" Integrated Systems Laboratory, ETH Zurich, Zürich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1575"",""1588"",""Supercomputers, nowadays, aggregate a large number of nodes featuring the same nominal HW components (e.g., processors and GPGPUS). In real-life machines, the chips populating each node are subject to a wide range of variability sources, related to performance and temperature operating points (i.e., ACPI p-states) as well as process variations and die binning. Eurora is a fully operational supercomputer prototype that topped July 2013 Green500 and it represents a unique 'living lab' for next-generation ultra-green supercomputers. In this paper we evaluate and quantify the impact of variability on Eurora's energy-performance tradeoffs under a wide range of workloads intensity. Our experiments demonstrate that variability comes from hardware component mismatches as well as from the interplay between run-time energy management and workload variations. Thus, variability has a significant impact on energy efficiency even at the moderate scale of the Eurora machine, thereby substantiating the critical importance of variability management in future green supercomputers."",""1558-2183"","""",""10.1109/TPDS.2017.2766151"",""FP7 ERC Advance project MULTITHERMAN(grant numbers:291125)"; H2020 FETHPC ANTAREX(grant numbers:671623);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8081827"",""Green500";high-performance computing;hardware variability;energy-efficient software design;energy-aware computing;green supercomputer;heterogeneous supercomputer;dynamic resource management;hardware accelerator;"DVFS"",""Supercomputers";Frequency measurement;Computer architecture;Power measurement;Hardware;Energy measurement;"Program processors"","""",""12"","""",""52"",""IEEE"",""24 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Queue Delegation Locking,""D. Klaftenegger"; K. Sagonas;" K. Winblad"",""Department of Information Technology, Uppsala University, Uppsala, Sweden"; Department of Information Technology, Uppsala University, Uppsala, Sweden;" Department of Information Technology, Uppsala University, Uppsala, Sweden"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""687"",""704"",""The scalability of parallel programs is often bounded by the performance of synchronization mechanisms used to protect critical sections. The performance of these mechanisms is in turn determined by their sequential execution time, efficient use of hardware, and ability to avoid waiting. In this article, we describe queue delegation (QD) locking, a family of locks that both delegate critical sections and enable detaching execution. Threads delegate work to the thread currently holding the lock and are able to detach, i.e., immediately continue their execution until they need a result from a previously delegated critical section. We show how to use queue delegation to build synchronization algorithms with lower overhead and higher throughput than existing algorithms, even when critical sections need to communicate results back immediately. Experiments when using up to 64 threads to access a shared priority queue show that QD locking provides 10 times higher throughput than Pthreads mutex locks and outperforms leading delegation algorithms. Also, when mixing parallel reads with delegated write operations, QD locking outperforms competing algorithms with an advantage ranging from 9.5 up to 207 percent increased throughput. Last but not least, continuing execution instead of waiting for the execution of critical sections leads to increased parallelism and better scalability. As we will see, queue delegation locking uses simple building blocks whose overhead is low even in uncontended use. All these make the technique useful in a wide variety of applications."",""1558-2183"","""",""10.1109/TPDS.2017.2767046"",""Vetenskapsrdet(grant numbers:UPMARC)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8093701"",""Locking";synchronization;delegation;detached execution;multi-core;"NUMA"",""Instruction sets";Message systems;Synchronization;Coherence;Scalability;Throughput;"Hardware"","""",""9"",""2"",""35"",""IEEE"",""1 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Race-Condition-Aware and Hardware-Oriented Task Partitioning and Scheduling Using Entropy Maximization,""S. Li"; Y. Zhang; H. Luo; Y. Chen; C. Lu;" D. Guo"",""Department of Electronic Engineering, Xiamen University, Fujian, China"; Department of Electrical and Computer Engineering, Southern Illinois University, Carbondale, IL; Department of Electronic Engineering, Xiamen University, Fujian, China; Department of Electronic Engineering, Xiamen University, Fujian, China; Department of Electrical and Computer Engineering, Southern Illinois University, Carbondale, IL;" Department of Electronic Engineering, Xiamen University, Fujian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1589"",""1604"",""In a multithreaded execution environment, race condition leads to computational errors and system hazards. Up to date, a series of task scheduling strategies have been presented in the literature to reduce the risk of race condition. Because of the increasing complexity of this problem in multi-core systems, existing task scheduling approaches are not very efficient. To deal with this challenge, in this work, we develop a race-condition-aware and hardware-oriented task partitioning and scheduling algorithm using entropy maximization model. We model uncertainty as a probabilistic occurrence within a time interval, and hence the characteristics of event ordering are analyzed through an uncertainty matrix. Next, a metric is developed to measure the uncertainty of task execution in various execution environments. Finally, a maximum entropy model is generated to ensure the lowest probability of race condition during task execution. The smallest one among maximum entropy values is chosen and used in our proposed task scheduling algorithm. Experimental results show that the proposed task scheduling strategy based on our maximum entropy model outperforms existing state-of-the-art approaches. For example, in a 128-core computing system, the task execution time, CPU utilization ratio, and throughput of our proposed task scheduling is improved by  $15.3\sim 36.4$  percent, $8.2\sim 17.6$  percent, and $20.7\sim 41.4$  percent, respectively. Moreover, our proposed scheduling algorithm exhibits low computational complexity and good adaptivity to diverse execution environments."",""1558-2183"","""",""10.1109/TPDS.2017.2784829"",""National Natural Science Foundation of China(grant numbers:61274133)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8226841"",""Race condition";uncertainty analysis;task partitioning;maximum entropy model;"execution environment"",""Entropy";Scheduling algorithms;Uncertainty;Analytical models;"Probabilistic logic"","""",""7"","""",""39"",""IEEE"",""19 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Random Regular Graph and Generalized De Bruijn Graph with $k$ -Shortest Path Routing,""P. Faizian"; M. A. Mollah; X. Yuan; Z. Alzaid; S. Pakin;" M. Lang"",""Department of Computer Science, Florida State University, Tallahassee, FL"; Department of Computer Science, Florida State University, Tallahassee, FL; Department of Computer Science, Florida State University, Tallahassee, FL; Department of Computer Science, Florida State University, Tallahassee, FL; Computer, Computational, and Statistical Sciences, Los Alamos National Laboratory, Los Alamos, NM;" Computer, Computational, and Statistical Sciences, Los Alamos National Laboratory, Los Alamos, NM"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""144"",""155"",""The Random regular graph (RRG) has recently been proposed as an interconnect topology for future large scale data centers and HPC clusters. An RRG is a special case of directed regular graph (DRG) where each link is unidirectional and all nodes have the same number of incoming and outgoing links. In this work, we establish bounds for DRGs on diameter, average k-shortest path length, and a load balancing property with k-shortest path routing, and use these bounds to evaluate RRGs. The results indicate that an RRG with k-shortest path routing is not ideal in terms of diameter and load balancing. We further consider the Generalized De Bruijn Graph (GDBG), a deterministic DRG, and prove that for most network configurations, a GDBG is near optimal in terms of diameter, average k-shortest path length, and load balancing with a k-shortest path routing scheme. Finally, we use modeling and simulation to exploit the strengths and weaknesses of RRGs for different traffic conditions by comparing RRGs with GDBGs."",""1558-2183"","""",""10.1109/TPDS.2017.2741492"",""U.S. Department of Energy"; Office of Science; Office of Advanced Scientific Computing Research(grant numbers:0000219853,DE-SC0016039);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8013054"",""Network";topology;random regular graph;generalized De Bruijn graph;"  $k$      -shortest path routing"",""Routing";Topology;Network topology;Switches;Load management;Bandwidth;"Load modeling"","""",""13"","""",""18"",""IEEE"",""18 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Rapid Calculation of Max-Min Fair Rates for Multi-Commodity Flows in Fat-Tree Networks,""M. A. Mollah"; X. Yuan; S. Pakin;" M. Lang"",""Department of Computer Science, Florida State University, Tallahassee, FL"; Department of Computer Science, Florida State University, Tallahassee, FL; Computer, Computational, and Statistical Sciences Division, Los Alamos National Laboratory, Los Alamos, NM;" Computer, Computational, and Statistical Sciences Division, Los Alamos National Laboratory, Los Alamos, NM"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""156"",""168"",""Max-min fairness is often used in the performance modeling of interconnection networks. Existing methods to compute max-min fair rates for multi-commodity flows have high complexity and are computationally infeasible for large networks. In this work, we show that by considering topological features, this problem can be solved efficiently for the fat-tree topology that is widely used in data centers and high performance compute clusters. Several efficient new algorithms are developed for this problem, including a parallel algorithm that can take advantage of multi-core and shared-memory architectures. Using these algorithms, we demonstrate that it is possible to find the max-min fair rate allocation for multi-commodity flows in fat-tree networks that support tens of thousands of nodes. We evaluate the run-time performance of the proposed algorithms and show improvement in orders of magnitude over the previously best known method. We further demonstrate a new application of max-min fair rate allocation that is only computationally feasible using our new algorithms."",""1558-2183"","""",""10.1109/TPDS.2017.2746078"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8017578"",""Max-min fairness";fat-tree topology;high-performance computing;"data center networks"",""Resource management";Topology;Fats;Network topology;Routing;Vegetation;"Bandwidth"","""",""8"","""",""19"",""IEEE"",""29 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"ReCA: An Efficient Reconfigurable Cache Architecture for Storage Systems with Online Workload Characterization,""R. Salkhordeh"; S. Ebrahimi;" H. Asadi"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1605"",""1620"",""In recent years, Solid-State Drives (SSDs) have gained tremendous attention in computing and storage systems due to significant performance improvement over Hard DiskDrives (HDDs). The cost per capacity of SSDs, however, prevents them from entirely replacing HDDs in such systems. One approach to effectively take advantage of SSDs is to use them as a caching layer to store performance critical data blocks in order to reduce the number of accesses to HDD-based disk subsystem. Due to characteristics of Flash-based SSDs such as limited write endurance and long latency on write operations, employing caching algorithms at the Operating System (OS) level necessitates to take such characteristics into consideration. Previous OS-level caching techniques are optimized towards only one type of application, which affects both generality and applicability. In addition, they are not adaptive when the workload pattern changes over time. This paper presents an efficient Reconfigurable Cache Architecture (ReCA) for storage systems using a comprehensive workload characterization to find an optimal cache configuration for I/O intensive applications. For this purpose, we first investigate various types of I/O workloads and classify them into five major classes. Based on this characterization, an optimal cache configuration is presented for each class of workloads. Then, using the main features of each class, we continuously monitor the characteristics of an application during system runtime and the cache organization is reconfigured if the application changes from one class to another class of workloads. The cache reconfiguration is done online and workload classes can be extended to emerging I/O workloads in order to maintain its efficiency with the characteristics of I/O requests. Experimental results obtained by implementing ReCA in a 4U rackmount server with SATA 6Gb/s disk interfaces running Linux 3.17.0 show that the proposed architecture improves performance and lifetime up to 24 and 33 percent, respectively."",""1558-2183"","""",""10.1109/TPDS.2018.2796100"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8265001"",""Solid-state drives";data storage systems;performance;endurance;"I/O caching"",""Performance evaluation";Computer architecture;Cache storage;Runtime;Metadata;Linux;"Hard disks"","""",""21"","""",""60"",""IEEE"",""22 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Reducing Cache Coherence Traffic with a NUMA-Aware Runtime Approach,""P. Caheny"; L. Alvarez; S. Derradji; M. Valero; M. Moretó;" M. Casas"",""Departament d’Arquitectura de Computadors at the Universitat Politècnica de Catalunya (UPC), Barcelona Supercomputing Centre (BSC), Barcelona, Spain"; Departament d’Arquitectura de Computadors at the Universitat Politècnica de Catalunya (UPC), Barcelona Supercomputing Centre (BSC), Barcelona, Spain; Bull/Atos Group, Les Clayes-sous-Bois, France; Departament d’Arquitectura de Computadors at the Universitat Politècnica de Catalunya (UPC), Barcelona Supercomputing Centre (BSC), Barcelona, Spain; Departament d’Arquitectura de Computadors at the Universitat Politècnica de Catalunya (UPC), Barcelona Supercomputing Centre (BSC), Barcelona, Spain;" Departament d’Arquitectura de Computadors at the Universitat Politècnica de Catalunya (UPC), Barcelona Supercomputing Centre (BSC), Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1174"",""1187"",""Cache Coherent NUMA (ccNUMA) architectures are a widespread paradigm due to the benefits they provide for scaling core count and memory capacity. Also, the flat memory address space they offer considerably improves programmability. However, ccNUMA architectures require sophisticated and expensive cache coherence protocols to enforce correctness during parallel executions, which trigger a significant amount of on- and off-chip traffic in the system. This paper analyses how coherence traffic may be best constrained in a large, real ccNUMA platform comprising 288 cores through the use of a joint hardware/software approach. For several benchmarks, we study coherence traffic in detail under the influence of an added hierarchical cache layer in the directory protocol combined with runtime managed NUMA-aware scheduling and data allocation techniques to make most efficient use of the added hardware. The effectiveness of this joint approach is demonstrated by speedups of 3.14× to 9.97× and coherence traffic reductions of up to 99 percent in comparison to NUMA-oblivious scheduling and data allocation."",""1558-2183"","""",""10.1109/TPDS.2017.2787123"",""Spanish Government(grant numbers:SEV2015-0493)"; Spanish Ministry of Science and Innovation(grant numbers:TIN2015-65316-P); Generalitat de Catalunya(grant numbers:2014-SGR-1051,2014-SGR-1272); RoMoL ERC Advanced(grant numbers:GA 321253); European HiPEAC Network of Excellence; EU's H2020 Framework Programme (H2020/2014-2020)(grant numbers:n° 671697); Ministry of Economy and Competitiveness(grant numbers:JCI-2012-15047); Secretary for Universities and Research of the Ministry of Economy and Knowledge of the Government of Catalonia and the Cofund programme of the Marie Curie Actions of the 7th R&D Framework Programme of the European Union(grant numbers:2013 BP_B 00243);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8239832"",""Cache coherence";NUMA;"task-based programming models"",""Coherence";Switched mode power supplies;Resource management;Runtime;Sockets;Protocols;"Computer architecture"","""",""7"","""",""29"",""IEEE"",""25 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Replication-Based Fault-Tolerance for Large-Scale Graph Processing,""R. Chen"; Y. Yao; P. Wang; K. Zhang; Z. Wang; H. Guan; B. Zang;" H. Chen"",""Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China"; Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; New York University, New York, NY; Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China;" Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1621"",""1635"",""The increasing algorithmic complexity and dataset sizes necessitate the use of networked machines for many graph-parallel algorithms, which also makes fault tolerance a must due to the increasing scale of machines. Unfortunately, existing large-scale graph-parallel systems usually adopt a distributed checkpoint mechanism for fault tolerance, which incurs not only notable performance overhead but also lengthy recovery time. This paper observes that the vertex replicas created for distributed graph computation can be naturally extended for fast in-memory recovery of graph states. This paper describes Imitator, a new fault tolerance mechanism, which supports cheap maintenance of vertex states by replicating them to their replicas during normal message exchanges, and provides fast in-memory reconstruction of failed vertices from replicas in other machines. Imitator has been implemented on Cyclops with edge-cut and PowerLyra with vertex-cut. Evaluation on a 50-node EC-2 like cluster shows that Imitator incurs an average of 1.37 and 2.32 percent performance overhead (ranging from -0.6 to 3.7 percent) for Cyclops and PowerLyra respectively, and can recover from failures of more than one million of vertices with less than 3.4 seconds."",""1558-2183"","""",""10.1109/TPDS.2017.2703904"",""National Key Research & Development Program(grant numbers:2016YFB1000500)"; National Natural Science Foundation of China(grant numbers:61402284,61572314,61525204); National Youth Top-notch Talent Support Program of China; Zhangjiang Hi-Tech program(grant numbers:201501-YP-B108-012); Singapore NRF(grant numbers:CREATE E2S2);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927721"",""Graph-parallel system";fault-tolerance;"replication"",""Fault tolerance";Fault tolerant systems;Clustering algorithms;Checkpointing;Runtime;Loading;"Synchronization"","""",""9"","""",""57"",""IEEE"",""15 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Resource Optimization Across the Cloud Stack,""Z. Á. Mann"",""University of Duisburg-Essen, Essen, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""169"",""182"",""Previous work on optimizing resource provisioning in virtualized environments focused either on mapping virtual machines (VMs) to physical machines (PMs) or mapping application components to VMs. In this paper, we argue that these two optimization problems influence each other significantly and in a highly non-trivial way. We define a sophisticated problem formulation for the joint optimization of the two mappings, taking into account sizing aspects, colocation constraints, license costs, and hardware affinity relations. As demonstrated by the empirical evaluation on a real-world workload trace, the combined optimization leads to significantly better overall results than considering the two problems in isolation."",""1558-2183"","""",""10.1109/TPDS.2017.2744627"",""Hungarian Scientific Research Fund(grant numbers:OTKA 108947)"; European Union's Horizon 2020 research and innovation programme(grant numbers:731678);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8016586"",""Virtual machines";VM placement;VM consolidation;VM selection;VM sizing;cloud computing;"data center"",""Optimization";Cloud computing;Licenses;Hardware;Energy consumption;Unified modeling language;"Heuristic algorithms"","""",""34"","""",""50"",""IEEE"",""25 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"RoB-Router : A Reorder Buffer Enabled Low Latency Network-on-Chip Router,""C. Li"; D. Dong; Z. Lu;" X. Liao"",""National Laboratory for Parallel and Distributed Processing, and Collaborative Innovation Center of High Performance Computing, National University of Defense Technology, Changsha, China"; National Laboratory for Parallel and Distributed Processing, and Collaborative Innovation Center of High Performance Computing, National University of Defense Technology, Changsha, China; School of Electrical Engineering and Computer Science, KTH Royal Institute of Technology, Stockholm, Sweden;" National Laboratory for Parallel and Distributed Processing, and Collaborative Innovation Center of High Performance Computing, National University of Defense Technology, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2090"",""2104"",""Traditional input-queued routers in network-on-chips (NoCs) only have a small number of virtual channels (VCs) and packets in a VC are organized in a fixed order. Such design is susceptible to head-of-line (HoL) blocking as only the packet at the head of a VC can be allocated by the switch allocator. Since switch allocation is the critical pipeline stage in on-chip routers, HoL blocking significantly degrades the performance of NoCs. In this paper, we propose to schedule packets in input buffers utilizing reorder buffer (RoB) techniques. We design VCs as RoBs to allow packets located not at the head of a VC to be allocated before the head packets. RoBs reduce the conflicts in switch allocation and mitigate the HoL blocking and thus improve the NoC performance. However, it is hard to reorder all the units in a VC due to circuit complexity and power overhead. We propose RoB-Router, which leverages elastic RoBs in VCs to only allow a part of a VC to act as RoB. RoB-Router automatically determines the length of RoB in a VC based on the number of buffered flits. This design minimizes the resource while achieving excellent efficiency. Furthermore, we propose two independent methods to improve the performance of RoB-Router. One is to optimize the packet order in input buffers by redesigning VC allocation strategy. The other combines RoB-Router with current most efficient switch allocator TS-Router. We perform evaluations and the results show that our design can achieve 46 and 15.7 percent performance improvement in packet latency under synthetic traffic and traces from PARSEC than TS-Router, and the cost of energy and area is moderate. Additionally, average packet latency reduction by our two improving methods under uniform traffic is 13 and 17 percent respectively."",""1558-2183"","""",""10.1109/TPDS.2018.2817552"",""National Key Research and Development Program of China(grant numbers:2016YFB0200400)"; NSFC(grant numbers:61672526); FANEDD(grant numbers:201450);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8320326"",""Network on chip";switch allocation;packets scheduling;"reorder buffer"",""Resource management";Switches;Pipelines;Scheduling;Network-on-chip;"Complexity theory"","""",""16"","""",""24"",""IEEE"",""20 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scalable Data Race Detection for Lock-Intensive Programs with Pending Period Representation,""X. Liao"; M. Lin; L. Zheng; H. Jin;" Z. Shao"",""Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China;" Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2599"",""2612"",""Most of dynamic data race detection essentially relies on the underlying happens-before orders to yield the precise reports. They are notoriously prone to a prohibitively basic overhead. Although there exist a wealth of research advances that succeed in significantly reducing the analysis overhead on memory accesses, there remains an open problem in handling a great deal of fundamentally unscalable synchronization overhead, which can be particularly serious for the large, lock-intensive programs with a long running time and a large number of threads. In this paper, we revisit the synchronization problem of off-the-shelf race detection with a comprehensive study. The key insight of this work is that a full collection of partial orders for synchronization operations in prior work is not necessarily tracked and analyzed from a new perspective of “global clock” representation. We therefore develop this insight into a novel pending-period based approach, aiming at reducing the overhead of monitoring and analysis on unnecessary synchronization operations. Further, we also enable a significant improvement for enhancing the efficiency of existing sampling techniques, in which synchronization operations are often conservatively identified. Our experimental results on a wide variety of programs show that our approach outperforms state-of-the-art by 5.85x (versus FastTrack), 3.51x (versus ThreadSanitizer) and 1.34x (versus IFRit) program execution slowdown improvement on average, which can be more significant as the number of threads is increasing. Particularly for the lock-intensive programs (e.g., barnes), our approach can be 26.04x faster than FastTrack. Further, our pending period extended sampling is more efficient than Pacer (with up to 31.28 percent improvement in the case of 10 percent sampling rate)."",""1558-2183"","""",""10.1109/TPDS.2018.2836899"",""National Natural Science Foundation of China(grant numbers:61732010,61702201,61628204)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359198"",""Data race";scalability;efficiency;happens-before relation;global clock;"sampling"",""Synchronization";Clocks;Instruction sets;Runtime;Monitoring;Scalability;"Message systems"","""",""2"","""",""44"",""IEEE"",""15 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scalable Deadlock-Free Deterministic Minimal-Path Routing Engine for InfiniBand-Based Dragonfly Networks,""G. Maglione-Mathey"; P. Yebenes; J. Escudero-Sahuquillo; P. J. Garcia; F. J. Quiles;" E. Zahavi"",""Computer Systems Department, Campus Universitario, Albacete s/n, Spain"; Computer Systems Department, Campus Universitario, Albacete s/n, Spain; Computer Systems Department, Campus Universitario, Albacete s/n, Spain; Computer Systems Department, Campus Universitario, Albacete s/n, Spain; Computer Systems Department, Campus Universitario, Albacete s/n, Spain;" Mellanox Technologies, Yokneam, Israel"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""183"",""197"",""Dragonfly topologies are gathering great interest nowadays as one of the most promising interconnect options for High-Performance Computing (HPC) systems. However, Dragonflies contain physical cycles that may lead to traffic deadlocks unless the routing algorithm prevents them properly. In general, existing deadlock-free routing algorithms, either deterministic or adaptive, proposed for Dragonflies, use Virtual Channels (VCs) to prevent cyclic dependencies. However, these topology-aware algorithms are difficult to implement, or even unfeasible, in systems based on the InfiniBand (IB) architecture, which is nowadays the most widely used network technology in HPC systems. This is due to some limitations in the IB specification, specifically regarding the way Virtual Lanes (VLs), which are considered as similar to VCs, can be assigned to traffic flows. Indeed, none of the routing engines currently available in the official releases of the IB control software has been specifically proposed for Dragonflies. In this paper, we present a new deterministic, minimal-path routing for Dragonfly that prevents deadlocks using VLs according to the IB specification, so that it can be straightforwardly implemented in IB-based networks. We have called this proposal D3R (Deterministic Deadlock-free Dragonfly Routing). Specifically, D3R maps each route to a single, specific VL depending on the destination group, and according to a specific order, so that cyclic dependencies (so deadlocks) are prevented. D3R is scalable as it requires only 2 VLs to prevent deadlocks regardless of network size, i.e., fewer VLs than the required by the deadlock-free routing engines available in IB that are suitable for Dragonflies. Alternatively, D3R achieves higher throughput if an additional VL is used to reduce internal contention in the Dragonfly groups. We have implemented D3R as a new routing engine in OpenSM, the control software including the subnet manager in IB. We have evaluated D3R by means of simulation and by experiments performed in a real IB-based cluster, the results showing that, in general, D3R outperforms other routing engines."",""1558-2183"","""",""10.1109/TPDS.2017.2742503"",""Spanish MINECO"; European Commission(grant numbers:TIN2012-38341-C04,UNCM13-1E-2456,TIN2015-66972-C5-2-R); FPI(grant numbers:BES-2013-063681); Junta de Comunidades de Castilla-La Mancha(grant numbers:POII10-0289-3724,PEII-2014-028-P); University of Castilla-La Mancha (UCLM); European Commission; Spanish System of Science, Technology and Innovation; UCLM research program(grant numbers:31/07/2014);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8013852"",""High-performance interconnection networks";InfiniBand;Dragonfly topology;routing algorithms;"deadlock-freedom"",""System recovery";Routing;Switches;Ports (Computers);Engines;Network topology;"Topology"","""",""17"","""",""52"",""IEEE"",""21 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scalable GPU Virtualization with Dynamic Sharing of Graphics Memory Space,""M. Xue"; J. Ma; W. Li; K. Tian; Y. Dong; J. Wu; Z. Qi; B. He;" H. Guan"",""Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China"; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Intel Asia-Pacific R&D Ltd, Shanghai, China; Intel Asia-Pacific R&D Ltd, Shanghai, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science, National University of Singapore, Singapore;" Department of Computer Science, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1823"",""1836"",""With increasing GPU-intensive workloads deployed on cloud, cloud service providers are seeking for practical and efficient GPU virtualization solutions. However, the cutting-edge GPU virtualization techniques such as gVirt still suffer from the restriction of scalability, which constrains the number of guest virtual GPU instances. This paper presents gScale, a scalable and practical open source GPU virtualization solution based on gVirt. gScale presents a sharing mechanism which combines partition and sharing together to break the hardware limitation of global graphics memory space. Particularly, we propose two approaches for gScale: (1) the private shadow graphics translation table (GTT) , which enables global graphics memory space sharing among virtual GPUs, (2) ladder mapping and fence memory space pool, which allows CPU access host physical memory space (serving the graphics memory) to bypass global graphics memory space. Furthermore, to mitigate the performance degradation caused by switching private shadow GTT when the number of vGPUs scales up, four other mechanisms are proposed: (1) slot sharing, which improves the performance of vGPU by dividing the high global graphics memory into multiple slots, (2) fine-grained slotting, which provides a flexible virtual graphics memory configuration, (3) predictive GTT copy mechanism, which reduces the performance loss by switching private shadow GTT before context switch, (4) predictive-copy aware scheduling, which maximizes the improvement of predictive GTT copy mechanism in cloud environment. Evaluation shows that gScale scales up to 15 guest virtual GPU instances in Linux or 12 guest virtual GPU instances in Windows, which is 5x and 4x, respectively, that of gVirt. At the same time, gScale incurs a slight but acceptable runtime overhead when hosting multiple virtual GPU instances."",""1558-2183"","""",""10.1109/TPDS.2018.2789883"",""National Key Research & Development Program of China(grant numbers:2016YFB1000502)"; National NSF of China(grant numbers:61672344,61525204,61732010); Shanghai Key Laboratory of Scalable Computing and Systems;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8247267"",""GPU";virtualization;scalability;"scheduling"",""Graphics processing units";Virtualization;Switches;Scalability;Registers;"Cloud computing"","""",""13"","""",""38"",""IEEE"",""5 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scalable Minimum-Cost Balanced Partitioning of Large-Scale Social Networks: Online and Offline Solutions,""R. J. Hada"; H. Wu;" M. Jin"",""Center for Advanced Computer Studies, University of Louisiana at Lafayette, Lafayette, LA"; Old Dominion University, Norfolk, VA;" Center for Advanced Computer Studies, University of Louisiana at Lafayette, Lafayette, LA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1636"",""1649"",""With the remarkable proliferation of intelligent mobile devices and fast growing broadband wireless technology, social networking is undergoing explosive growth in recent years as more and more users access social networks via mobile platforms. It is often expensive or even impossible to deploy a large online social network (OSN) on a single server. A cost-effective approach is horizontal scaling, where the OSN is partitioned and deployed on a set of low-cost servers. In this research, we study the problem of minimum-cost balanced partitioning of OSNs. Our goal is to achieve the best partitioning by minimizing the total inter-server traffic cost and at the same time balancing the load among servers. Given its NP-hardness, we propose new techniques and explore efficient heuristics to address the problem, especially for extremely large OSNs with an enormous volume of social nodes, social connections, and social data. Our key contributions include a localized approach with O(δ2) complexity to explicitly calculate the projected gain in inter-server traffic cost (named Server Change Benefit (SCB)). Built upon this technique, we devise two algorithms that offer online and offline solutions to achieving minimum-cost balanced partitioning of OSNs. The online algorithm is fast and highly efficient to process newly arrival individual nodes. The offline algorithm uses the current online result as a starting point. It further reduces inter-server traffic cost by applying relocation and swapping. It employs a merging process to group the nodes according to the social structure and swap the groups with similar size to further reduce the total inter-server traffic cost. We implement both algorithms and evaluate them based on a variety of real-world OSN datasets from Facebook, Arxiv, Gnutella, Amazon, and Twitter. The simulations demonstrate that the proposed algorithms can significantly reduce the execution time by an average of three folds and at the same time yield supreme performance (i.e., inter-server traffic cost) in comparison with existing solutions."",""1558-2183"","""",""10.1109/TPDS.2017.2694835"",""NSF(grant numbers:CCF-1054996,CNS-1320931)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7902221"",""Social networks";load balancing;inter-server traffic cost;"scalability"",""Servers";Facebook;Partitioning algorithms;Distributed databases;Wireless communication;"Explosives"","""",""5"","""",""28"",""IEEE"",""17 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scheduling Parallel Real-Time Recurrent Tasks on Multicore Platforms,""R. Pathan"; P. Voudouris;" P. Stenström"",""Department of Computer Science and Engineering, Chalmers University of Technology, Göteborg, Sweden"; Department of Computer Science and Engineering, Chalmers University of Technology, Göteborg, Sweden;" Department of Computer Science and Engineering, Chalmers University of Technology, Göteborg, Sweden"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""915"",""928"",""We consider the scheduling of a real-time application that is modeled as a collection of parallel and recurrent tasks on a multicore platform. Each task is a directed-acyclic graph (DAG) having a set of subtasks (i.e., nodes) with precedence constraints (i.e., directed edges) and must complete the execution of all its subtasks by some specified deadline. Each task generates potentially infinite number of instances where the releases of consecutive instances are separated by some minimum inter-arrival time. Each DAG task and each subtask of that DAG task is assigned a fixed priority. A two-level preemptive global fixed-priority scheduling (GFP) policy is proposed: a task-level scheduler first determines the highest-priority ready task and a subtask-level schedulerthen selects its highest-priority subtask for execution. To our knowledge, no earlier work considers a two-level GFP scheduler to schedule recurrent DAG tasks on a multicore platform. We derive a schedulability test for our proposed two-level GFP scheduler. If this test is satisfied, then it is guaranteed that all the tasks will meet their deadlines under GFP. We show that our proposed test is not only theoretically better but also empirically performs much better than the state-of-the-art test in scheduling randomly generated parallel DAG task sets."",""1558-2183"","""",""10.1109/TPDS.2017.2777449"",""MECCA(grant numbers:ERC-2013-AdG 340328-MECCA)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119822"",""Real-time systems";parallel DAG tasks;global fixed-priority scheduling;schedulability analysis;"multicore processors"",""Multicore processing";Real-time systems;Scheduling algorithms;Dynamic scheduling;"Program processors"","""",""34"","""",""25"",""IEEE"",""24 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scheduling Stochastic Multi-Stage Jobs to Elastic Hybrid Cloud Resources,""J. Zhu"; X. Li; R. Ruiz;" X. Xu"",""School of Computer Science, Jiangsu Key Laboratory of Big Data Security & Intelligent Processing"; Ministry of Education, Southeast University, Nanjing, China; Grupo de Sistemas de Optimización Aplicada, Camino de Vera s/n, València, Spain;" School of Computer Science, Nanjing University of Posts & Telecommunications, Yancheng, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1401"",""1415"",""We consider a special workflow scheduling problem in a hybrid-cloud-based workflow management system in which tasks are linearly dependent, compute-intensive, stochastic, deadline-constrained and executed on elastic and distributed cloud resources. This kind of problems closely resemble many real-time and workflow-based applications. Three optimization objectives are explored: number, usage time and utilization of rented VMs. An iterated heuristic framework is presented to schedule jobs event by event which mainly consists of job collecting and event scheduling. Two job collecting strategies are proposed and two timetabling methods are developed. The proposed methods are calibrated through detailed designs of experiments and sound statistical techniques. With the calibrated components and parameters, the proposed algorithm is compared to existing methods for related problems. Experimental results show that the proposal is robust and effective for the problems under study."",""1558-2183"","""",""10.1109/TPDS.2018.2793254"",""National Natural Science Foundation of China(grant numbers:71401079,61572127,61472192)"; National Basic Research Program of China (973 Program)(grant numbers:2017YFB1400801); Collaborative Innovation Center of Wireless Communications Technology; Spanish Ministry of Economy and Competitiveness(grant numbers:DPI2015-65895-R); FEDER funds;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8259004"",""Multi-stage job scheduling";linearly dependent tasks;stochastic;deadline-constraint;elastic;"cloud computing"",""Cloud computing";Dynamic scheduling;Processor scheduling;Heuristic algorithms;"Genetic algorithms"","""",""46"","""",""65"",""IEEE"",""15 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Secure Integrated Circuit Design via Hybrid Cloud,""X. Yuan"; J. Weng; C. Wang;" K. Ren"",""Faculty of Information Technology, Monash University, Clayton, VIC, Australia"; School of Information Technology, Jinan University, Guangzhou, Guangdong, China; City University of Hong Kong Shenzhen Research Institute, Shenzhen, China;" Institute of Cyber Security Research, Zhejiang University, Hangzhou, Zhejiang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1851"",""1864"",""In order to ease the burden of the in-house integrated circuit (IC) design, cloud-based IC design platforms advance rapidly, bringing benefits such as reduced capital costs and convenient design collaboration. However, such migration raises security challenges on IC Intellectual Property (IP) protection. Sensitive design data is unwillingly exposed to the cloud. In this paper, we initiate the first study for secure cloud-based IC design, and propose a hybrid cloud framework for privacy-assured IC timing analysis, i.e., an expensive procedure in the IC design flow for circuit delay evaluation. Our key observation is that more and more IP blocks are universally reused. After carefully extracting a small portion of sensitive blocks from the circuit, our framework only outsources non-sensitive design data to the public cloud. However, that “data splitting” hinders sequential delay evaluation. We then develop algorithms to enable the public cloud to derive intermediate results from non-sensitive data, which can be integrated with sensitive data at the private cloud. Additionally, we devise a practical verification protocol to assure the integrity of outsourced computation. Security analysis shows that our design is resilient to IC reverse engineering. Evaluations over large IC benchmarks demonstrate its efficiency and effectiveness."",""1558-2183"","""",""10.1109/TPDS.2018.2807844"",""Research Grants Council of Hong Kong(grant numbers:CityU 11276816,CityU 11212717,CityU C1008-16G)"; Innovation and Technology Commission of Hong Kong; ITF Project(grant numbers:ITS/168/17); National Natural Science Foundation of China(grant numbers:61572412,61772236,U1736203,61732021); National Key R&D Plan of China(grant numbers:2017YFB0802203);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8295269"",""Hybrid cloud";IP protection;"computation outsourcing"",""Cloud computing";Integrated circuits;Security;IP networks;Logic gates;"Delays"","""",""2"","""",""47"",""IEEE"",""20 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Sketch Acceleration on FPGA and its Applications in Network Anomaly Detection,""D. Tong";" V. K. Prasanna"",""VMware Inc., Palo Alto, CA";" Department of Electrical Engineering, University of Southern California, Los Angeles, CA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""929"",""942"",""Sketch, a highly accurate data stream summarization technique, has gained much interest in the research community in recent years. Because of its sub-linear memory complexity, Sketch-based techniques consume significantly less memory than the traditional per-item-state techniques for processing high throughput data streams. One of the major applications of Sketch is in network anomaly detection, which is critical for network management and security in both Internet and data centers. In these applications, throughput is a key performance metric. Due to the low memory complexity, Sketch-based techniques can be supported by the fast onchip storage of the state-of-the-art computing platforms to achieve high throughput. In this work, we first propose a generic architecture on FPGA to accelerate Sketch and adopt it to 2 widely used Sketches: Count-min Sketch and K-ary Sketch. We propose online Sketchbased algorithms for 2 key network anomaly detection tasks: heavy hitter detection and heavy change detection. We adopt the proposed generic architecture for Sketch to accelerate these online algorithms. The post place-and-route results on a state-of-the-art FPGA show that our generic architecture can accelerate both Count-min Sketch and K-ary Sketch to over 150Gbps, demonstrating significant throughput performance improvements compared with other Sketch acceleration techniques. Our architectures for online anomaly detection tasks sustain 100-150 Gbps throughput for various system configurations."",""1558-2183"","""",""10.1109/TPDS.2017.2766633"",""U.S. National Science Foundation (NSF)(grant numbers:ACI-1339756,CNS-1643351,CCF-1320211)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8085171"",""Network anomaly detection";sketch acceleration;FPGA;heavy hitter detection;"heavy change detection"",""Acceleration";Field programmable gate arrays;Throughput;Anomaly detection;Random access memory;"Memory management"","""",""34"","""",""29"",""IEEE"",""26 Oct 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"SMGuard: A Flexible and Fine-Grained Resource Management Framework for GPUs,""C. Yu"; Y. Bai; H. Yang; K. Cheng; Y. Gu; Z. Luan;" D. Qian"",""Sino-German Joint Software Institute, Beihang University, Beijing, China"; Sino-German Joint Software Institute, Beihang University, Beijing, China; Sino-German Joint Software Institute, Beihang University, Beijing, China; Sino-German Joint Software Institute, Beihang University, Beijing, China; Sino-German Joint Software Institute, Beihang University, Beijing, China; Sino-German Joint Software Institute, Beihang University, Beijing, China;" Sino-German Joint Software Institute, Beihang University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2849"",""2862"",""GPUs have been becoming an indispensable computing platform in data centers, and co-locating multiple applications on the same GPU is widely used to improve resource utilization. However, performance interference due to uncontrolled resource contention severely degrades the performance of co-locating applications and fails to deliver satisfactory user experience. In this paper, we present SMGuard, a software approach to flexibly manage the GPU resource usage of multiple applications under co-location. We also propose a capacity based GPU resource model CapSM, which provisions the GPU resources in a fine-grained granularity among co-locating applications. When co-locating latency-sensitive applications with batch applications, SMGuard can prevent batch applications from occupying resources without constraint using quota based mechanism, and guarantee the resource usage of latency-sensitive applications with reservation based mechanism. In addition, SMGuard supports dynamic resource adjustment through evicting the running thread blocks of batch applications to release the occupied resources and remapping the uncompleted thread blocks to the remaining resources, which avoids the relaunch of the preempted kernel. The SMGuard is a pure software solution that does not rely on special GPU hardware or programming model, which is easy to adopt on commodity GPUs in data centers. Our evaluation shows that SMGuard improves the average performance of latency-sensitive applications by 9.8× when co-located with batch applications. In the meanwhile, the GPU utilization can be improved by 35 percent on average."",""1558-2183"","""",""10.1109/TPDS.2018.2848621"",""National Key Research and Development Program of China(grant numbers:2016YFB1000503)"; National Science Foundation of China(grant numbers:61572062,61502019);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8388218"",""GPU";parallel computing;resource management;"application co-location"",""Graphics processing units";Task analysis;Instruction sets;Resource management;Quality of service;"Data centers"","""",""13"","""",""42"",""IEEE"",""19 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"SnapFiner: A Page-Aware Snapshot System for Virtual Machines,""L. Cui"; Z. Hao; L. Li;" X. Yun"",""Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China;" Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2613"",""2626"",""Virtual machine (VM) snapshot, enabling a VM to be resumed from a previously recorded state, is an essential part of cloud infrastructures. Unfortunately, the snapshot data are likely to be lost due to the high rate of disk failures, so that the associated VM fails to recover properly. To enhance data availability without compromising application performance upon rollback recovery, it is desired to place multiple replicas of snapshot across disperse disks. However, due to the large size of replica, it induces non-trivial storage cost when managing massive snapshots in clouds. In this paper, we investigate this problem and find out that the semantic gap existed between snapshot creation and snapshot storing is one key factor inducing high storage cost. To this end, we propose SnapFiner, a page-aware snapshot system for creating and storing massive snapshot files efficiently. First, SnapFiner acquires a fine-grained page categorization with an in-depth page exploration from three orthogonal views, thereby discovering more pages that can be excluded from the snapshot. Second, SnapFiner varies the number of replicas for different page categories based on a page-aware replication policy, achieving low storage cost without compromising availability and performance. Third, SnapFiner handles the loss of pages either intentionally dropped upon snapshot creation or unexpectedly damaged due to disk failures, enabling proper system execution after rollback recovery. We have implemented SnapFiner on QEMU/KVM to justify its practicality for Linux guests. The experimental results demonstrate that SnapFiner reduces the storage cost by 33 and 69.5 percent respectively compared to our previous work PARS and the naive approach on QEMU/KVM and HDFS."",""1558-2183"","""",""10.1109/TPDS.2018.2831202"",""National Natural Science Foundation of China(grant numbers:61602465,61601458)"; National Key Research and Development Program of China(grant numbers:2016QY04W0804); Beijing Natural Science Foundation(grant numbers:4172069); Research on Core Technologies of National Key Infrastructure Security Supervision Platform(grant numbers:Z161100002616032);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8352722"",""Replication";virtual machine snapshot;availability;storage cost;"semantic gap"",""Semantics";Kernel;Linux;Cloud computing;Virtual machining;Runtime;"Bridges"","""",""6"","""",""50"",""IEEE"",""30 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"SnapMig: Accelerating VM Live Storage Migration by Leveraging the Existing VM Snapshots in the Cloud,""Y. Yang"; B. Mao; H. Jiang; Y. Yang; H. Luo;" S. Wu"",""Microsoft, Redmond, WA"; Software School of Xiamen University, Xiamen, Fujian, China; Software School of Xiamen University, Xiamen, Fujian, China; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE;" Computer Science Department, Xiamen University, Xiamen, Fujian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1416"",""1427"",""Virtual Machine (VM) live storage migration is becoming increasingly important and indispensable in the current cloud data centers, for the purposes of load balance, hardware maintenance and system upgrade. Nevertheless, conventional VM migration approaches induce significant extra storage and network traffic to the source server that is already heavily loaded or scheduled for upgrade or repair. As a result, both the VM performance perceived by the application/user and the migration performance are degraded significantly. In this paper, we aim to address this problem by proposing a novel scheme, called SnapMig, to improve the VM live storage migration efficiency and eliminate its performance impact on user applications at the source server by effectively leveraging the existing VM snapshots in backup servers. By outsourcing the task of transferring VM base image and snapshots to the destination server to backup servers, the source server only needs to migrate the latest state changes to the destination server, leading to simultaneous improvement on VM performance, migration time and multiple-VM migration efficiency. Our lightweight prototype implementation of the SnapMig scheme demonstrates that, compared with the state-of-the-art approaches, SnagMig can significantly reduce the migration time and improve the source-server VM performance at the same time. Moreover, the performance improvement provided by SnapMig becomes much more pronounced with multiple concurrent VM migrations."",""1558-2183"","""",""10.1109/TPDS.2018.2790389"",""National Natural Science Foundation of China(grant numbers:61772439,U1705261,61472336,61402385)"; US National Science Foundation(grant numbers:CCF-1704504,CCF-1629625);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8248805"",""Virtual machine";live storage migration;cloud computing;VM snapshot;"performance evaluation"",""Servers";Cloud computing;Performance evaluation;Virtual machine monitors;Maintenance engineering;"Electronic mail"","""",""17"","""",""46"",""IEEE"",""8 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Sparse Geometries Handling in Lattice Boltzmann Method Implementation for Graphic Processors,""T. Tomczak";" R. G. Szafran"",""Faculty of Electronics, Computer Engineering at Wroclaw University of Science and Technology, Wroclaw, Poland";" Faculty of Chemistry, Department of Chemical Engineering, Wroclaw University of Science and Technology, Wroclaw, Poland"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1865"",""1878"",""We describe a high-performance implementation of the lattice Boltzmann method (LBM) for sparse geometries on graphic processors. In our implementation we cover the whole geometry with a uniform mesh of small tiles and carry out calculations for each tile independently with proper data synchronization at the tile edges. For this method, we provide both a theoretical analysis of complexity and the results for real implementations involving two-dimensional (2D) and three-dimensional (3D) geometries. Based on the theoretical model, we show that tiles offer significantly smaller bandwidth overheads than solutions based on indirect addressing. For2D lattice arrangements, a reduction in memory usage is also possible, although at the cost of diminished performance. We achieved a performance of 682 MLUPS on GTX Titan (72 percent of peak theoretical memory bandwidth) for the D3Q19 lattice arrangement and double-precision data."",""1558-2183"","""",""10.1109/TPDS.2018.2810237"",""National Science Centre’s(grant numbers:N N501 042140)"; Chair of Computer Engineering, Faculty of Electronics, Wroclaw University of Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8303717"",""GPU";CUDA;LBM;CFD;"parallel computing"",""Geometry";Computational modeling;Graphics processing units;Bandwidth;Memory management;"Lattice Boltzmann methods"","""",""9"","""",""30"",""IEEE"",""27 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"State of the Journal,""M. Parashar"",NA,""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""1"",""1"",""Welcome to 2018's first issue of the IEEE Transactions on Parallel and Distributed Systems (TPDS). I'm excited with my new role as incoming Editor-in-Chief (EIC) of TPDS and look forward to serving the community over the next few years. My goal as EIC is to continue to work on increasing the visibility and relevance and impact of TPDS, as well as the quality and timeliness of the review process, to ensure that TPDS is the premier Transactions in the field. The IEEE is a hallmark of quality for technical publication. The value TPDS brings to the international community is in its collection of the highest quality research that is relevant to academia, industry, and laboratories. I will investigate new opportunities for TPDS to capture the best research while maintaining its emphasis on highest quality papers. TPDS also needs to respond to a dynamic and rapidly evolving research and publication landscape. As EIC, I will work with the IEEE community to ensure that TPDS does respond appropriately, and will carefully work with the editorial board to revisit the scope and recruit new editorial board members as needed. An important and rapid growing conversation is related to the repeatability of published research and the submission of supplementary material such as code and data. I am part of this conversation, and will work with the EB and the community to explore how to bring these practices in meaningful and measured ways to TPDS."",""1558-2183"","""",""10.1109/TPDS.2017.2768538"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8173510"","""","""","""","""","""",""0"",""IEEE"",""11 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Storage, Communication, and Load Balancing Trade-off in Distributed Cache Networks,""M. J. Siavoshani"; A. Pourmiri;" S. P. Shariatpanahi"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, University of Isfahan, Isfahan, Iran;" School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2018"",""2018"",""29"",""4"",""943"",""957"",""We consider load balancing in a network of caching servers delivering contents to end users. Randomized load balancing via the so-called power of two choices is a well-known approach in parallel and distributed systems. In this framework, we investigate the tension between storage resources, communication cost, and load balancing performance. To this end, we propose a randomized load balancing scheme which simultaneously considers cache size limitation and proximity in the server redirection process. In contrast to the classical power of two choices setup, since the memory limitation and the proximity constraint cause correlation in the server selection process, we may not benefit from the power of two choices. However, we prove that in certain regimes of problem parameters, our scheme results in the maximum load of order Q(loglogn) (here n is the network size). This is an exponential improvement compared to the scheme which assigns each request to the nearest available replica. Interestingly, the extra communication cost incurred by our proposed scheme, compared to the nearest replica strategy, is small. Furthermore, our extensive simulations show that the trade-off trend does not depend on the network topology and library popularity profile details."",""1558-2183"","""",""10.1109/TPDS.2017.2781242"",""IPM";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8170231"",""Randomized algorithms";distributed caching servers;request routing;load balancing;communication cost;balls-into-bins;"content delivery networks"",""Servers";Load management;Load modeling;Correlation;Libraries;Topology;"Network topology"","""",""9"","""",""41"",""IEEE"",""8 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Strategy-Proof Mechanism for Provisioning and Allocation Virtual Machines in Heterogeneous Clouds,""X. Liu"; W. Li;" X. Zhang"",""School of Information Science and Engineering, Yunnan University, Kunming, Yunnan, PR China"; Department of Mathematics, Yunnan University, Kunming, Yunnan, PR China;" School of Information Science and Engineering, Yunnan University, Kunming, Yunnan, PR China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1650"",""1663"",""In this paper, we address the problem of heterogeneous physical machines resource management (HPMRM)"; that is, providing and allocating multiple virtual machine (VM) instances from heterogeneous physical machines to maximize social welfare. Although existing allocation mechanisms allocate VMs to users through the single-mapping mechanism, such allocations cannot guarantee maximum social welfare or efficient utilization of multiple types of resources for cloud providers. Thus, we consider the multi-mapping mechanism, which permits mapping VMs allocated to one user to physical machines for VM provisioning and allocation. This can result in improved social welfare and lead to less resource fragmentation. We formulate the HPMRM problem in an auction-based setting, and design optimal and approximate mechanisms to solve it. In addition, we show that our proposed mechanism is strategy-proof;" that is, our proposed mechanism drives the system into an equilibrium where no users have incentives to maximize their own profit by untruthfully reporting their requests. Furthermore, we analyze the approximation ratio of our proposed approximation algorithm. We also perform experiments to investigate the performance of our proposed approximation mechanism compared to the optimal mechanism. Experimental results demonstrate that our proposed approximation mechanism can obtain near optimal solutions and significantly improve allocation efficiency, while generating greater social welfare."",""1558-2183"","""",""10.1109/TPDS.2017.2785815"",""National Natural Science Foundation of China(grant numbers:61662088,61762091,11301466,11361048)"; Natural Science Foundation of Yunnan Province of China(grant numbers:2014FB114); Program for Excellent Young Talents, Yunnan University and IRTSTYN;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8241849"",""Resource allocation";strategy-proof;virtual machine provisioning;"cloud computing"",""Resource management";Cloud computing;Pricing;Approximation algorithms;Algorithm design and analysis;Virtual machining;"Dynamic scheduling"","""",""19"","""",""31"",""IEEE"",""29 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Stress-Aware Loops Mapping on CGRAs with Dynamic Multi-Map Reconfiguration,""J. Gu"; S. Yin; L. liu;" S. Wei"",""Institute of Microelectronics, Tsinghua University, Beijing, China"; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China;" Institute of Microelectronics, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2105"",""2120"",""With VLSI process technology scaling into nano-scale, the increasingly serious aging issues (e.g., NBTI and HCI aging effects) have brought a significant threat to system reliability. Coarse-grained reconfigurable architectures (CGRAs) exhibit the feature to reconfigure and execute different mapping schemes (Maps) dynamically, compensating for each other to mitigate aging issues effectively. In this paper, a two-stage stress-aware loops mapping algorithm is first proposed for the CGRA-mapped designs by jointing the intra-kernel and inter-kernel stress optimizations. With pipelining techniques, the intra-kernel stress optimization employs the stress-aware force-directed and effective MCC (Maximal Compatibility Class) methods to optimize operations' placement and mapping distribution on processing elements (PEs), which helps to avoid overmany operations to be mapped on the same PEs and reduce the accumulated stresses. By leveraging the dynamic reconfiguration feature, the inter-kernel stress optimization develops a multi-map scheduling method to reconfigure a set of ordered maps on CGRA dynamically, which diversifies the PEs' usage and compensates for the stresses on different PEs among them. Experimental results show that our approach can reduce the maximum stress by 82.0% for NBTI and 70.4% for HCI, and improve the aging efficiency by 6.01X and MTTF by 3.16X averagely, while keeping the optimized performance."",""1558-2183"","""",""10.1109/TPDS.2018.2816955"",""China National High Technologies Research Program(grant numbers:2015AA016601)"; China Major S&T Project(grant numbers:2013ZX01033001-001-003); NSFC(grant numbers:61774094);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8319523"",""CGRAs";aging mitigation;loop mapping;NBTI and HCI;stress optimization;intra-kernel;"inter-kernel"",""Stress";Aging;Human computer interaction;Negative bias temperature instability;Thermal variables control;Kernel;"Optimization"","""",""5"","""",""36"",""IEEE"",""19 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Symmetric Indefinite Linear Solver Using OpenMP Task on Multicore Architectures,""I. Yamazaki"; J. Kurzak; P. Wu; M. Zounon;" J. Dongarra"",""Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN"; Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN; Electrical Engineering and Computer Science, University of Tennessee, Knoxville, TN; School of Mathematics, The University of Manchester, Manchester, United Kingdom;" School of Mathematics, The University of Manchester, Manchester, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1879"",""1892"",""Recently, the Open Multi-Processing (OpenMP) standard has incorporated task-based programming, where a function call with input and output data is treated as a task. At run time, OpenMP's superscalar scheduler tracks the data dependencies among the tasks and executes the tasks as their dependencies are resolved. On a shared-memory architecture with multiple cores, the independent tasks are executed on different cores in parallel, thereby enabling parallel execution of a seemingly sequential code. With the emergence of many-core architectures, this type of programming paradigm is gaining attention-not only because of its simplicity, but also because it breaks the artificial synchronization points of the program and improves its thread-level parallelization. In this paper, we use these new OpenMP features to develop a portable high-performance implementation of a dense symmetric indefinite linear solver. Obtaining high performance from this kind of solver is a challenge because the symmetric pivoting, which is required to maintain numerical stability, leads to data dependencies that prevent us from using some common performance-improving techniques. To fully utilize a large number of cores through tasking, while conforming to the OpenMP standard, we describe several techniques. Our performance results on current many-core architectures-including Intel's Broadwell, Intel's Knights Landing, IBM's Power8, and Arm's ARMv8-demonstrate the portable and superior performance of our implementation compared with the Linear Algebra PACKage (LAPACK). The resulting solver is now available as a part of the PLASMA software package."",""1558-2183"","""",""10.1109/TPDS.2018.2808964"",""Exascale Computing Project(grant numbers:17-SC-20-SC)"; U.S. Department of Energy Office of Science; National Nuclear Security Administration(grant numbers:#DE-AC05-00OR22725); UT Battelle subaward(grant numbers:#4000152412);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8301559"",""Linear algebra";symmetric indefinite matrices;multithreading;"Runtime"",""Layout";Plasmas;Symmetric matrices;Task analysis;Computer architecture;Runtime;"Standards"","""",""5"","""",""30"",""IEEE"",""23 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TA-Update: An Adaptive Update Scheme with Tree-Structured Transmission in Erasure-Coded Storage Systems,""Y. Wang"; X. Pei; X. Ma;" F. Xu"",""National Key Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, P.R. China"; National Key Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, P.R. China; National Key Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, P.R. China;" National Key Laboratory for Parallel and Distributed Processing, College of Computer, National University of Defense Technology, Changsha, Hunan, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1893"",""1906"",""Erasure coding has received considerable attentions due to the better tradeoff between the space efficiency and reliability. The frequent update of the stored data in the distributed storage systems has posed a new challenge for erasure codes: how to update the erasure-coded data in a general, efficient and adaptive way. However, existing update schemes of erasure codes are inadequate to meet these requirements, since their code-related update manners lead to a low generality, their star-structured data transmission manners lead to a low update efficiency, and their redo manners when encountering the node failure lead to a low adaptivity. In this paper, we propose an adaptive update scheme with the tree-structured transmission, called TA-Update, which consists of a code-independent update framework and three algorithms: the rack-aware tree construction algorithm, the top-down data processing algorithm and the rollback-based failure processing algorithm. For generality, we propose a code-independent update framework with the tree structure to support the MDS code with any coding parameter. For efficiency, a rack-aware tree construction algorithm is proposed to achieve the high available bandwidth, which organizes the data node and parity nodes as an update tree. Moreover, a top-down data processing algorithm is proposed to achieve the high transmission and computation efficiency, which pipelines the data transmission along the update tree and distributes the encoding computations among all the participating nodes. For adaptivity, we propose a rollback-based failure processing algorithm to achieve high adaptivity, which handles the node failure during update with the existing update tree in a rollback manner. To evaluate the performance of TA-Update, we conduct experiments on HDFS-RAID under various parameter settings on both 30 physical and 200 virtual machines. Extensive experiments confirm that TA-Update could support the various erasure codes with any parameter, improve the update efficiency by 30 percent and the adaptivity by 47 percent on average compared with the state-of-the-art approaches under various parameter settings."",""1558-2183"","""",""10.1109/TPDS.2017.2717981"",""National Natural Science Foundation of China(grant numbers:61379052)"; National Basic Research Program of China (973 Program)(grant numbers:2016YFB1000101); Natural Science Foundation for Distinguished Young Scholars of Hunan Province(grant numbers:14JJ1026); Specialized Research Fund for the Doctoral Program of Higher Education(grant numbers:20124307110015); National Natural Science Foundation of China(grant numbers:61502511); National Natural Science Foundation of China(grant numbers:61502513);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954661"",""Distributed storage system";data update;update tree;"erasure codes"",""Distributed databases";Encoding;Data communication;Reliability;Data processing;Algorithm design and analysis;"Adaptive systems"","""",""29"","""",""28"",""IEEE"",""21 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TerrierTail: Mitigating Tail Latency of Cloud Virtual Machines,""E. Asyabi"; S. SanaeeKohroudi; M. Sharifi;" A. Bestavros"",""Computer Science Department, Boston University, Boston, MA"; School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran; School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran;" Computer Science Department, Boston University, Boston, MA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2346"",""2359"",""Large-scale online services parallelize sub-operations of a user's request across a large number of physical machines (service components) so as to enhance the responsiveness. Even a temporary spike in latency of any service component can notably inflate the end-to-end delay";" therefore, the tail of the latency distribution of service components has become a subject of intensive research. The key characteristics of clouds such as elasticity and on-demand resource provisioning have made clouds attractive for hosting large-scale online services wherein VMs are the building blocks of services. However, adherence to traditional hypervisor scheduling policies has led to unpredictable CPU access latencies for virtual CPUs (vCPUs) that are responsible for performing network IO processes. This has resulted in poor and unpredictable performance for network IO, exacerbating VMs' long tail latencies and discouraging the hosting of large-scale parallel web services on virtualized clouds. This paper presents TerrierTail, a hypervisor CPU scheduler whose primary goal is to trim the tail of the latency distribution of individual VMs in virtualized clouds. In TerrierTail, we have modified the network driver to identify vCPUs that are responsible for performing network IO processes. Leveraging this information, the TerrierTail scheduler mitigates the CPU access latencies of such vCPUs using novel scheduling policies, resulting in a higher and more predictable network IO performance and therefore lower tail latency. TerrierTail's gains come at no measurable negative impacts on other performance attributes (e.g., fairness) or on the performance of VMs running other types of workloads (e.g., CPU-intensive VMs). A prototype implementation of TerrierTail in the Xen hypervisor substantially outperforms the default Credit scheduler of Xen. For example, TerrierTail mitigates the tail latency of a Memcached server by up to 53 percent and an RPC server by up to 50 percent at 99.9th percentile."",""1558-2183"","""",""10.1109/TPDS.2018.2827075"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8338088"",""Cloud computing";tail latency;virtualization;CPU scheduler;"Xen"",""Cloud computing";Boosting;Virtual machine monitors;Servers;Task analysis;"Delays"","""",""10"","""",""36"",""IEEE"",""16 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Time- and Cost- Efficient Task Scheduling across Geo-Distributed Data Centers,""Z. Hu"; B. Li;" J. Luo"",""Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada"; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada;" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2018"",""2018"",""29"",""3"",""705"",""718"",""Typically called big data processing, analyzing large volumes of data from geographically distributed regions with machine learning algorithms has emerged as an important analytical tool for governments and multinational corporations. The traditional wisdom calls for the collection of all the data across the world to a central data center location, to be processed using data-parallel applications. This is neither efficient nor practical as the volume of data grows exponentially. Rather than transferring data, we believe that computation tasks should be scheduled near the data, while data should be processed with a minimum amount of transfers across data centers. In this paper, we design and implement Flutter, a new task scheduling algorithm that reduces both the completion times and the network costs of big data processing jobs across geographically distributed data centers. To cater to the specific characteristics of data-parallel applications, in the case of optimizing the job completion times only, we first formulate our problem as a lexicographical min-max integer linear programming (ILP) problem, and then transform the ILP problem into a nonlinear program problem with a separable convex objective function and a totally unimodular constraint matrix, which can be further solved using a standard linear programming solver efficiently in an online fashion. In the case of improving both time-and costefficiency, we formulate the general problem as an ILP problem and we find out that solving an LP problem can achieve the same goal in the real practice. Our implementation of Flutter is based on Apache Spark, a modern framework popular for big data processing. Our experimental results have shown convincing evidence that Flutter can shorten both job completion times and network costs by a substantial margin."",""1558-2183"","""",""10.1109/TPDS.2017.2773504"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8107572"",""Big data processing";task scheduling;"cloud computing"",""Bandwidth";Data transfer;Pricing;Big Data;"Distributed databases"","""",""53"","""",""43"",""IEEE"",""14 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TokenTLB+CUP: A Token-Based Page Classification with Cooperative Usage Prediction,""A. Esteve"; A. Ros; A. Robles;" M. E. Gómez"",""Department of Computer Engineering, Universitat Politècnica de València, València, Spain"; Departamento de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Department of Computer Engineering, Universitat Politècnica de València, València, Spain;" Department of Computer Engineering, Universitat Politècnica de València, València, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Apr 2018"",""2018"",""29"",""5"",""1188"",""1201"",""Discerning the private or shared condition of the data accessed by the applications is an increasingly decisive approach to achieving efficiency and scalability in multiand many-core systems. Since most memory accesses in both sequential and parallel applications are either private (accessed only by one core) or read-only (not written) data, devoting the full cost of coherence to every memory access results in sub-optimal performance and limits the scalability and efficiency of the multiprocessor. This paper introduces TokenTLB, a TLB-based page classification approach based on exchange and count of tokens. Token counting on TLBs is a natural and efficient way for classifying memory pages, and it does not require the use of complex and undesirable persistent requests or arbitration. In addition, classification is extended with Cooperative Usage Predictor (CUP), a token-based system-wide page usage predictor retrieved through TLB cooperation, in order to perform a classification unaffected by TLB size. Through cycle-accurate simulation we observed that TokenTLB spends 43.6 percent of cycles as private per page on average, and CUP further increases the time spent as private by 22.0 percent. CUP avoids 4 out of 5 TLB invalidations when compared to state-of-the-art predictors, thus proving far better prediction accuracy and making usage prediction an attractive mechanism for the first time."",""1558-2183"","""",""10.1109/TPDS.2017.2782808"",""MINECO and European Commission (FEDER funds)(grant numbers:TIN2015-66972-C5-1-R,TIN2015-66972-C5-3-R)"; Fundación Seneca-Agencia de Ciencia y Tecnología de la Región de Murcia(grant numbers:18956/JLI/13);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8194915"",""Data classification";token counting;TLB;private-shared;read-only data;"TLB usage predictor"",""Coherence";Protocols;Optimization;Proposals;Scalability;Memory management;"Data models"","""",""2"","""",""41"",""IEEE"",""13 Dec 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Toward High Mobile GPU Performance Through Collaborative Workload Offloading,""C. Wu"; B. Yang; W. Zhu;" Y. Zhang"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""435"",""449"",""The ever increasing of display resolution on mobile devices raises high demand for GPU rendering details. However, the challenge of poor hardware support but fine-grained rendering details often makes user unsatisfied especially in calling for high frame rate scenarios, e.g., game. To resolve such issue, we propose ButterFly, a novel system which collaboratively utilizes mobile GPUs to process high-quality rendering details for on-the-go mobile users. In particular, ButterFly achieves three technical contributions for the collaborative design: (1) a mobile device can migrate GPU workloads in buffer queue to peers, (2) the collaborative rendering mechanism benefits user high quality details while significant power saving performance, and (3) unnecessary 3D texture rendering can be clipped for further optimization. All the techniques are compatible with the OpenGL ES standards. Furthermore, a 40-person survey perceives that ButterFly can provide excellent user experience of both rendering details and frame rate over Wi-Fi network. In addition, our comprehensive trace-driven experiments on Android prototype reveal the benefits of Butterfly have more superior performance over state-of-the-art systems, which achieves more than 28.3 percent power saving."",""1558-2183"","""",""10.1109/TPDS.2017.2754482"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8047282"",""Mobile applications";distributed system;code offload;"performance optimization"",""Rendering (computer graphics)";Graphics processing units;Mobile communication;Mobile handsets;Collaboration;Hardware;"Androids"","""",""13"","""",""39"",""IEEE"",""20 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Towards Bandwidth Guarantee for Virtual Clusters Under Demand Uncertainty in Multi-Tenant Clouds,""L. Yu"; H. Shen; Z. Cai; L. Liu;" C. Pu"",""School of Computer Science, Georgia Institute of Technology, Atlanta, GA"; Department of Computer Science, University of Virginia, Charlottesville, VA; Department of Computer Science, Georgia State University, Atlanta, GA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA;" School of Computer Science, Georgia Institute of Technology, Atlanta, GA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2018"",""2018"",""29"",""2"",""450"",""465"",""In the cloud, multiple tenants share the resource of datacenters and their applications compete with each other for scarce network bandwidth. Current studies have shown that the lack of bandwidth guarantee causes unpredictable network performance, leading to poor application performance. To address this issue, several virtual network abstractions have been proposed which allow the tenants to reserve virtual clusters with specified bandwidth between the Virtual Machines (VMs) in the datacenters. However, all these existing proposals require the tenants to deterministically characterize the bandwidth demands in the abstractions, which can be difficult and result in inefficient bandwidth reservation due to the demand uncertainty. In this paper, we explore a virtual cluster abstraction with stochastic bandwidth characterization to address the bandwidth demand uncertainty. We propose Stochastic Virtual Cluster (SVC), which models the bandwidth demand between VMs in a probabilistic way. Based on SVC, we develop a stochastic framework for virtual cluster allocation, in which the admitted virtual cluster's bandwidth demands are satisfied with a high probability. Efficient VM allocation algorithms are proposed to implement the framework while reducing the possibility of link congestion through minimizing the maximum bandwidth occupancy of a virtual cluster on physical links. Using simulations, we show that SVC achieves the trade-off between the job concurrency and the average job running time, and demonstrate its effectiveness for accommodating cloud application workloads with highly volatile bandwidth demands and its improvement to work-conserving bandwidth enforcement."",""1558-2183"","""",""10.1109/TPDS.2017.2754366"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8046038"",""Cloud computing";bandwidth guarantee;network abstraction;virtual network;"virtual machine placement"",""Bandwidth";Cloud computing;Resource management;Static VAr compensators;Stochastic processes;Uncertainty;"Clustering algorithms"","""",""36"","""",""25"",""IEEE"",""19 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Towards Memory-Efficient Allocation of CNNs on Processing-in-Memory Architecture,""Y. Wang"; W. Chen; J. Yang;" T. Li"",""College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China"; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Experimental and Innovation Practice Center, Harbin Institute of Technology, Shenzhen, China;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2018"",""2018"",""29"",""6"",""1428"",""1441"",""Convolutional neural networks (CNNs) have been successfully applied in artificial intelligent systems to perform sensory processing, sequence learning, and image processing. In contrast to conventional computing-centric applications, CNNs are known to be both computationally and memory intensive. The computational and memory resources of CNN applications are mixed together in the network weights. This incurs a significant amount of data movement, especially for high-dimensional convolutions. The emerging Processing-in-Memory (PIM) alleviates this memory bottleneck by integrating both processing elements and memory into a 3D-stacked architecture. Although this architecture can offer fast near-data processing to reduce data movement, memory is still a limiting factor of the entire system. We observe that an unsolved key challenge is how to efficiently allocate convolutions to 3D-stacked PIM to combine the advantages of both neural and computational processing. This paper presents MemoNet, a memory-efficient data allocation strategy for convolutional neural networks on 3D PIM architecture. MemoNet offers fine-grained parallelism that can fully exploit the computational power of PIM architecture. The objective is to capture the characteristics of neural network applications and perfectly match the underlining hardware resources provided by PIM, resulting in a hardware-independent design to transparently allocate data. We formulate the target problem as a dynamic programming model and present an optimal solution. To demonstrate the viability of the proposed MemoNet, we conduct a set of experiments using a variety of realistic convolutional neural network applications. The extensive evaluations show that, MemoNet can significantly improve the performance and the cache utilization compared to representative schemes."",""1558-2183"","""",""10.1109/TPDS.2018.2791440"",""National Natural Science Foundation of China(grant numbers:61502309)"; Natural Science Foundation of Guangdong Province(grant numbers:2016A030313045,2017B030314073); Shenzhen Science and Technology Foundation(grant numbers:JCYJ20150529164656096,JCYJ20170302153955969); Natural Science Foundation of SZU(grant numbers:803/000026080154,827-000073); State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences(grant numbers:CARCH201608);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8252752"",""Processing-in-memory";neuromorphic computing;non-volatile memory;scheduling;data allocation;"parallel computing"",""Computer architecture";Resource management;Hardware;Engines;Standards;"Buffer storage"","""",""18"","""",""43"",""IEEE"",""9 Jan 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Towards Quality Aware Information Integration in Distributed Sensing Systems,""W. Jiang"; C. Miao; L. Su; Q. Li; S. Hu; S. Wang; J. Gao; H. Liu; T. F. Abdelzaher; J. Han; X. Liu; Y. Gao;" L. Kaplan"",""State University of New York at Buffalo, Buffalo, NY"; State University of New York at Buffalo, Buffalo, NY; State University of New York at Buffalo, Buffalo, NY; State University of New York at Buffalo, Buffalo, NY; University of Illinois at Urbana-Champaign, Urbana, IL; University of Illinois at Urbana-Champaign, Urbana, IL; State University of New York at Buffalo, Buffalo, NY; University of Science and Technology of China, Hefei, China; University of Illinois at Urbana-Champaign, Urbana, IL; University of Illinois at Urbana-Champaign, Urbana, IL; McGill University, Montreal, Quebec, QC, Canada; LinkedIn, 2029 Stierlin Ct, Mountain View, CA;" Army Research Laboratory, Adelphi, MD"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""198"",""211"",""In this paper, we present GDA, a generalized decision aggregation framework that integrates information from distributed sensor nodes for decision making in a resource efficient manner. Different from traditional approaches, our proposed GDA framework is able to not only estimate the reliability of each sensor, but also take advantage of its confidence information, and thus achieves higher decision accuracy. Targeting generalized problem domains, our framework can naturally handle the scenarios where different sensor nodes observe different sets of events whose numbers of possible classes may also be different. GDA also makes no assumption about the availability level of ground truth label information, while being able to take advantage of any if present. For these reasons, our approach can be applied to a much broader spectrum of sensing scenarios. In this paper, we also propose two extensions of the GDA framework, i.e., incremental GDA (I-GDA) and parallel GDA (P-GDA) to deal with streaming and large-scale data. The advantages of our proposed methods are demonstrated through both theoretic analysis and extensive experiments."",""1558-2183"","""",""10.1109/TPDS.2017.2712630"",""US National Science Foundation(grant numbers:CNS-1566374,CNS-1652503,IIS-1319973,IIS-1553411)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7940106"",""Information integration";distributed sensing system;participatory sensing;crowd sensing;social sensing;"quality"",""Sensors";Reliability;Electronic mail;Decision making;Data integration;Roads;"Monitoring"","""",""11"","""",""41"",""IEEE"",""6 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Towards Stable Flow Scheduling in Data Centers,""T. Zhang"; F. Ren;" R. Shu"",""Department of Computer Science and Techonolgy, Beijing National Research Center for Information Science and Technology, Beijing, 100084, China"; Department of Computer Science and Techonolgy, Beijing National Research Center for Information Science and Technology, Beijing, 100084, China;" Department of Computer Science and Techonolgy, Beijing National Research Center for Information Science and Technology, Beijing, 100084, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2018"",""2018"",""29"",""11"",""2627"",""2640"",""At present, soft real-time data center applications are in a booming development and impose stringent delay requirements on internal data transfers. In this context, many recently proposed data center transport protocols share a common goal of minimizing Flow Completion Time (FCT), and the Shortest Remaining Processing Time (SRPT) scheduling algorithm has attracted widespread attentions for its superior performance in average FCT. However, SRPT suffers from the instability problem, incurring more and more flows left uncompleted even if the traffic load is within the fabric capacity, which implies unnecessary bandwidth waste. To solve the problem, this paper proposes a backlog-aware flow scheduling algorithm (BASRPT) for both giant switch and general topologies. Because of taking into account queue backlogs other than flow sizes at scheduling, we prove that BASRPT is stable and still maintains good FCT performance. To overcome the huge computation overhead and enable distributed implementation, a fast and practical approximation algorithm called fast BASRPT is also developed. Extensive flow-level simulations show that fast BASRPT indeed stabilizes the queue length and obtains a higher throughput while being able to push the FCT arbitrarily close to the optimal value in the condition of feasible traffic loads."",""1558-2183"","""",""10.1109/TPDS.2018.2833458"",""Suzhou-Tsinghua Special Project for Leading Innovation"; IEEE ICDCS; Nara;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8355701"",""Stable";flow scheduling;backlog-aware;"data center"",""Data centers";Scheduling algorithms;Fabrics;Topology;Delays;Stability analysis;"Switches"","""",""4"","""",""28"",""IEEE"",""7 May 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Triggered-Issuance and Triggered-Execution: A Control Paradigm to Minimize Pipeline Stalls in Distributed Controlled Coarse-Grained Reconfigurable Arrays,""Y. Lu"; L. Liu; Y. Deng; J. Weng; S. Yin; Y. Shi;" S. Wei"",""Institute of Microelectronics, Tsinghua University, Beijing, China"; Tsinghua University, Beijing, Beijing, CN; School of Software, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Departments of Computer Science and Engineering and Electrical Engineering, University of Notre Dame, Notre Dame, IN;" Institute of Microelectronics, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2360"",""2372"",""Distributed controlled coarse-grained reconfigurable arrays (CGRAs) enable efficient execution of irregular control flows by reconciling divergence in the processing elements (PEs). To further improve performance by better exploiting spatial parallelism, the triggered instruction architecture (TIA) eliminates the program counter and branch instructions by converting control flows into predicate dependencies as triggers. However, pipeline stalls, which occur in pipelines composed of both intra and inter-PEs, remain a major obstacle to the overall performance. In fact, the stalls in distributed controlled CGRAs pose a unique problem that is difficult to resolve by previous techniques. This work presents a triggered-issuance and triggered-execution (TITE) paradigm in which the issuance and execution of instructions are separately triggered to further relax the predicate dependencies in TIA. In this paradigm, instructions are paired as dual instructions to eliminate stalls caused by control divergence. Tags that identify the data transmitted between PEs are forwarded for acceleration. As a result, pipeline stalls of both intra- and inter-PEs can be significantly minimized. Experiments show that TITE improves performance by 21 percent, energy efficiency by 17 percent, and area efficiency by 12 percent compared with a baseline TIA."",""1558-2183"","""",""10.1109/TPDS.2018.2822708"",""National High Technology Research and Development Program of China(grant numbers:2012AA012701)"; National Natural Science Foundation of China(grant numbers:61672317);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8330047"",""Coarse-grained reconfigurable array";pipeline stall;"triggered-issuance and triggered-execution"",""Pipelines";Pipeline processing;Arrays;Hazards;Switches;"Distributed databases"","""",""1"","""",""31"",""IEEE"",""3 Apr 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TripleID-Q: RDF Query Processing Framework Using GPU,""C. Chantrapornchai";" C. Choksuchat"",""Department of Computer Engineering, Faculty of Engineering, Kasetsart University, Bangkok, Thailand";" Silpakorn University, Nakhon Pathom, Thailand"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2121"",""2135"",""Resource Description Framework (RDF) data represents information linkage around the Internet. It uses Internationalized Resources Identifier (IRI) which can be referred to external information. Typically, an RDF data is serialized as a large text file which contains millions of relationships. In this work, we propose a framework based on TripleID-Q, for query processing of large RDF data in a GPU. The key elements of the framework are 1) a compact representation suitable for a Graphics Processing Unit (GPU) and 2) its simple representation conversion method which optimizes the preprocessing overhead. Together with the framework, we propose parallel algorithms which utilize thousands of GPU threads to look for specific data for a given query as well as to perform basic query operations such as union, join, and filter. The TripleID representation is smaller than the original representation 3-4 times. Querying from TripleID using a GPU is up to 108 times faster than using the traditional RDF tool. The speedup can be more than 1,000 times over the traditional RDF store when processing a complex query with union and join of many subqueries."",""1558-2183"","""",""10.1109/TPDS.2018.2814567"",""The Thailand Research Fund (TRF) through the Royal Golden Jubilee Ph.D. Program(grant numbers:PHD/0005/2554)"; DAAD (German Academic Exchange Service) Scholarship(grant numbers:57084841); NVIDIA Hardware; Faculty of Engineering at Kasetsart University Research(grant numbers:57/12/MATE);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8314130"",""Query processing";parallel processing;entailment,TripleID;GPU;"RDF"",""Graphics processing units";Resource description framework;Instruction sets;Query processing;Data structures;"Tools"","""",""6"","""",""50"",""OAPA"",""12 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Tsumiki: A Meta-Platform for Building Your Own Testbed,""J. Cappos"; Y. Zhuang; A. Rafetseder;" I. Beschastnikh"",""New York University, New York, NY, USA"; University of Colorado, Colorado Springs, CO, USA; New York University, New York, NY, USA;" University of British Columbia, Vancouver, BC, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Nov 2018"",""2018"",""29"",""12"",""2863"",""2881"",""Network testbeds are essential research tools that have been responsible for valuable network measurements and major advances in distributed systems research. However, no single testbed can satisfy the requirements of every research project, prompting continual efforts to develop new testbeds. The common practice is to re-implement functionality anew for each testbed. This work introduces a set of ready-to-use software components and interfaces called Tsumiki to help researchers to rapidly prototype custom networked testbeds without substantial effort. We derive Tsumiki's design using a set of component and interface design principles, and demonstrate that Tsumiki can be used to implement new, diverse, and useful testbeds. We detail a few such testbeds: a testbed composed of Android devices, a testbed that uses Docker for sandboxing, and a testbed that shares computation and storage resources among Facebook friends. A user study demonstrated that students with no prior experience with networked testbeds were able to use Tsumiki to create a testbed with new functionality and run an experiment on this testbed in under an hour. Furthermore, Tsumiki has been used in production in multiple testbeds, resulting in installations on tens of thousands of devices and use by thousands of researchers."",""1558-2183"","""",""10.1109/TPDS.2018.2846242"",""NSF(grant numbers:1547290,1405904,1405907,1205415,0834243,1223588)"; NSERC Postdoctoral Fellowship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8382260"",""Networked testbeds";"distributed systems"",""Monitoring";Resource management;Computer architecture;Distributed databases;"Prototypes"","""",""2"","""",""77"",""IEEE"",""12 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Unifying Fixed Code Mapping, Communication, Synchronization and Scheduling Algorithms for Efficient and Scalable Loop Pipelining,""A. Mastoras";" T. R. Gross"",""Department of Computer Science, ETH Zurich, Zürich, Switzerland";" Department of Computer Science, ETH Zurich, Zürich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2136"",""2149"",""Pipelining allows the execution of loop iterations with cross-iteration dependences to overlap in time, provided that the loop body is partitioned into stages such that the data dependences are not violated. Then, the stages are mapped onto threads and communication and synchronization between stages is typically achieved using queues. Pipelining techniques that rely on static scheduling perform poorly for load-imbalanced loops. Moreover, previous research efforts that achieve load-balancing are restricted to work-stealing and imply high overhead for fine-grained loops. In this article, we present URTS, a unified runtime system with compiler support that provides a lightweight dynamic scheduler by combining mapping, communication and synchronization algorithms with a suitable data structure and an efficient ticket mechanism. Particularly, URTS shows that it is possible to combine the efficiency of static scheduling with the load-imbalance tolerance of work-stealing by using a unified design that exploits the properties of a novel data structure. The evaluation on 8- and 32-core machines shows that URTS implies low overhead, of the same order as a static scheduler, for a set of benchmarks chosen from widely-used collections. URTS is a scalable solution that performs efficient dynamic scheduling for fine-grained loops, i.e., a class of interesting loops that is poorly handled by the state-of-the-art due to high overhead."",""1558-2183"","""",""10.1109/TPDS.2018.2817207"",""SNF(grant numbers:206021_133835)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8319946"",""Linear pipeline";partitioning;fixed code mapping;communication;synchronization;multi-threading: dynamic scheduling;parallelization directives;"runtime system"",""Pipeline processing";Synchronization;Dynamic scheduling;Pipelines;Message systems;Data structures;"Runtime"","""",""3"","""",""46"",""IEEE"",""19 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Unleashing Fine-Grained Parallelism on Embedded Many-Core Accelerators with Lightweight OpenMP Tasking,""G. Tagliavini"; D. Cesarini;" A. Marongiu"",""Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, BO, Italy"; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, BO, Italy;" Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, BO, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2018"",""2018"",""29"",""9"",""2150"",""2163"",""In recent years, programmable many-core accelerators (PMCAs) have been introduced in embedded systems to satisfy stringent performance/Watt requirements. This has increased the urge for programming models capable of effectively leveraging hundreds to thousands of processors. Task-based parallelism has the potential to provide such capabilities, offering high-level abstractions to outline abundant and irregular parallelism in embedded applications. However, efficiently supporting this programming paradigm on embedded PMCAs is challenging, due to the large time and space overheads it introduces. In this paper we describe a lightweight OpenMP tasking runtime environment (RTE) design for a state-of-the-art embedded PMCA, the Kalray MPPA 256. We provide an exhaustive characterization of the costs of our RTE, considering both synthetic workload and real programs, and we compare to several other tasking RTEs. Experimental results confirm that our solution achieves near-ideal parallelization speedups for tasks as small as 5K cycles, and an average speedup of 12x for real benchmarks, which is 60% higher than what we observe with the original Kalray OpenMP implementation."",""1558-2183"","""",""10.1109/TPDS.2018.2814602"",""EU FP7 project P-SOCRATES(grant numbers:611016)"; EU Horizon 2020 RIA project HERCULES(grant numbers:688860);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8314096"",""Heterogeneous embedded systems on chip";programmable many-core accelerators;tasking;"OpenMp"",""Task analysis";Parallel processing;Wool;Embedded systems;Real-time systems;"Parallel programming"","""",""19"","""",""46"",""IEEE"",""12 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Unraveling Network-Induced Memory Contention: Deeper Insights with Machine Learning,""T. L. Groves"; R. E. Grant; A. Gonzales;" D. Arnold"",""Sandia National Laboratories, Albuquerque, NM"; Sandia National Laboratories, Albuquerque, NM; Sandia National Laboratories, Albuquerque, NM;" Sandia National Laboratories, Albuquerque, NM"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2018"",""2018"",""29"",""8"",""1907"",""1922"",""Remote Direct Memory Access (RDMA) is expected to be an integral communication mechanism for future exascale systems-enabling asynchronous data transfers, so that applications may fully utilize CPU resources while simultaneously sharing data amongst remote nodes. In this work we examine Network-induced Memory Contention (NiMC) on Infiniband networks. We expose the interactions between RDMA, main-memory and cache, when applications and out-of-band services compete for memory resources. We then explore NiMC's resulting impact on application-level performance. For a range of hardware technologies and HPC workloads, we quantify NiMC and show that NiMC's impact grows with scale resulting in up to 3X performance degradation at scales as small as 8K processes even in applications that previously have been shown to be performance resilient in the presence of noise. Additionally, this work examines the problem of predicting NiMC's impact on applications by leveraging machine learning and easily accessible performance counters. This approach provides additional insights about the root cause of NiMC and facilitates dynamic selection of potential solutions. Lastly, we evaluated three potential techniques to reduce NiMC's impact, namely hardware offloading, core reservation and software-based network throttling."",""1558-2183"","""",""10.1109/TPDS.2017.2773483"",""United States Department of Energy’s"; National Nuclear Security Administration(grant numbers:DE-AC04-94AL85000);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8116691"",""Measurement";performance;memory contention;networks;asynchronous communication;"machine learning"",""Bandwidth";Hardware;Asynchronous communication;Distributed databases;Data transfer;Market research;"Multicore processing"","""",""3"","""",""38"",""IEEE"",""21 Nov 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Using Hardware-Transactional-Memory Support to Implement Thread-Level Speculation,""J. Salamanca"; J. N. Amaral;" G. Araujo"",""Institute of Computing, UNICAMP, São Paulo, Brazil"; Department of Computing Science, University of Alberta, Edmonton, AB, Canada;" Institute of Computing, UNICAMP, São Paulo, Brazil"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2018"",""2018"",""29"",""2"",""466"",""480"",""This paper presents a detailed analysis of the application of Hardware Transactional Memory (HTM) support for loop parallelization with Thread-Level Speculation (TLS) and describes a careful evaluation of the implementation of TLS on the HTM extensions available in such machines. The sample implementation of TLS over HTM described in this paper also provides evidence that the programming effort to implement TLS over HTM support is non-trivial. Thus the paper also describes an extension to OpenMP that both makes TLS more accessible to OpenMP programmers and allows for the easytuning of TLS parameters. As a result, it provides evidence to support several important claims about the performance of TLS over HTM in the Intel Core and the IBM POWER8 architectures. Experimental results reveal that by implementing TLS on top of HTM, speed-ups of up to 3.8x can be obtained for some loops."",""1558-2183"","""",""10.1109/TPDS.2017.2752169"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8038067"",""Thread-level speculation";hardware transactional memory;"transactional memory"",""Hardware";Instruction sets;Strips;Benchmark testing;Runtime;Feature extraction;"Programming"","""",""16"","""",""20"",""IEEE"",""14 Sep 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Value the Recent Past: Approximate Causal Consistency for Partially Replicated Systems,""T. -Y. Hsu";" A. D. Kshemkalyani"",""Department of Electrical and Computer Engineering, University of Illinois at Chicago, Chicago, IL";" Department of Computer Science, University of Illinois at Chicago, Chicago, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""212"",""225"",""In wide-area distributed systems, data replication provides fault tolerance and low latency. Causal consistency in such systems is an interesting consistency model. Most existing works assume the data is fully replicated because this greatly simplifies the design of the algorithms to implement causal consistency. Recently, we proposed causal consistency under partial replication because it reduces the number of messages used under a wide range of workloads. One drawback of partial replication is that its meta-data tends to be relatively large when the message size is small. In this paper, we propose an algorithm Approx-Opt-Track which provides approximate causal consistency whereby we can reduce the meta-data at the cost of some violations of causal consistency. The amount of violations can be made arbitrarily small by controlling a tunable parameter, that we call credits. We present the analytic data to show the performance of Approx-Opt-Track. We then give simulation results to show the potential benefit of Approx-Opt-Track, viz., its ability to provide almost the same guarantees as causal consistency, at a smaller cost."",""1558-2183"","""",""10.1109/TPDS.2017.2740174"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010836"",""Causal consistency";causality;distributed shared memory;"partial replication"",""Protocols";Approximation algorithms;Social network services;Real-time systems;Message passing;History;"Distributed databases"","""",""5"","""",""30"",""IEEE"",""15 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Virtual Network Function Placement Considering Resource Optimization and SFC Requests in Cloud Datacenter,""D. Li"; P. Hong; K. Xue;" j. Pei"",""Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China. No. 96 Jinzhai Road, Hefei, Anhui Province, P. R. China"; Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China. No. 96 Jinzhai Road, Hefei, Anhui Province, P. R. China; Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China. No. 96 Jinzhai Road, Hefei, Anhui Province, P. R. China;" Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China. No. 96 Jinzhai Road, Hefei, Anhui Province, P. R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2018"",""2018"",""29"",""7"",""1664"",""1677"",""Network function virtualization (NFV) brings great conveniences and benefits for the enterprises to outsource their network functions to the cloud datacenter. In this paper, we address the virtual network function (VNF) placement problem in cloud datacenter considering users' service function chain requests (SFCRs). To optimize the resource utilization, we take two less-considered factors into consideration, which are the time-varying workloads, and the basic resource consumptions (BRCs) when instantiating VNFs in physical machines (PMs). Then the VNF placement problem is formulated as an integer linear programming (ILP) model with the aim of minimizing the number of used PMs. Afterwards, a Two-StAge heurisTic solution (T-SAT) is designed to solve the ILP. T-SAT consists of a correlation-based greedy algorithm for SFCR mapping (first stage) and a further adjustment algorithm for virtual network function requests (VNFRs) in each SFCR (second stage). Finally, we evaluate T-SAT with the artificial data we compose with Gaussian function and trace data derived from Google's datacenters. The simulation results demonstrate that the number of used PMs derived by T-SAT is near to the optimal results and much smaller than the benchmarks. Besides, it improves the network resource utilization significantly."",""1558-2183"","""",""10.1109/TPDS.2018.2802518"",""National Natural Science Foundation of China(grant numbers:61671420,61672106)"; Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2016394); Fundamental Research Funds for the Central Universities;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8281644"",""Virtual network function placement";resource optimization;service function chain;time-varying workloads;multi-tenancy;basic resource consumptions;"correlation-based algorithm"",""Cloud computing";Bandwidth;Optimization;Resource management;Servers;Virtualization;"Noise measurement"","""",""117"","""",""32"",""IEEE"",""5 Feb 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"VOLAP: A Scalable Distributed Real-Time OLAP System for High-Velocity Data,""F. Dehne"; D. E. Robillard; A. Rau-Chaplin;" N. Burke"",""School of Computer Science, Carleton University, Ottawa, ON, Canada"; School of Computer Science, Carleton University, Ottawa, ON, Canada; Faculty of Computer Science, Dalhousie University, Halifax, Canada;" Faculty of Computer Science, Dalhousie University, Halifax, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2017"",""2018"",""29"",""1"",""226"",""239"",""This paper presents VelocityOLAP (VOLAP), a distributed real-time OLAP system for high-velocity data. VOLAP makes use of dimension hierarchies, is highly scalable, exploits both multi-core and multi-processor parallelism, and can guarantee serializable execution of insert and query operations. In contrast to other high performance OLAP systems such as SAP HANA or IBM Netezza that rely on vertical scaling or special purpose hardware, VOLAP supports cost-efficient horizontal scaling on commodity hardware or modest cloud instances. Experiments on 20 Amazon EC2 nodes with TPC-DS data show that VOLAP is capable of bulk ingesting data at over 600 thousand items per second, and processing streams of interspersed insertions and aggregate queries at a rate of approximately 50 thousand insertions and 20 thousand aggregate queries per second with a database of 1 billion items. VOLAP is designed to support applications that perform large aggregate queries, and provides similar high performance for aggregations ranging from a few items to nearly the entire database."",""1558-2183"","""",""10.1109/TPDS.2017.2743072"",""IBM Center for Advanced Studies Canada"; Natural Sciences and Engineering Research Council of Canada;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8014458"","""",""Servers";Aggregates;Real-time systems;Distributed databases;Hardware;"Indexes"","""",""3"","""",""45"",""IEEE"",""22 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Workload Scheduling for Massive Storage Systems with Arbitrary Renewable Supply,""D. Li"; X. Qu; J. Wan; J. Wang; Y. Xia; X. Zhuang;" C. Xie"",""National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, P.R. China"; National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, P.R. China; National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, P.R. China; School of Electrical Engineering and Computer Science, University of Central Florida, Orlando, FL; School of Computer Science and Engineering, Ohio State University, Columbus, OH; National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, P.R. China;" National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2018"",""2018"",""29"",""10"",""2373"",""2387"",""As datacenters grow in scale, increasing energy costs and carbon emissions have led data centers to seek renewable energy, such as wind and solar energy. However, tackling the challenges associated with the intermittency and variability of renewable energy is difficult. This paper proposes a scheme called GreenMatch, which deploys an SSD cache to match green energy supplies with a time-shifting workload schedule while maintaining low latency for online data-intensive services. With the SSD cache, the process for a latency-sensitive request to access a disk is divided into two stages: a low-energy/low-latency online stage and a high-energy/high-latency off-line stage. As the process in the latter stage is off-line, it offers opportunities for time-shifting workload scheduling in response to variations of green energy supplies. We also allocate an HDD cache to guarantee data availability when renewable energy is inadequate. Furthermore, we design a novel replacement policy called Inactive P-disk First for the HDD cache to avoid inactive disk accesses. The experimental results show that GreenMatch can make full use of renewable energy while minimizing the negative impacts of intermittency and variability on performance and availability."",""1558-2183"","""",""10.1109/TPDS.2018.2820070"",""National Natural Science Foundation of China(grant numbers:61472152,61300047,61432007,61572209)"; Higher Education Discipline Innovation Project(grant numbers:B07038); Director Fund of WNLO; Key Laboratory of Data Storage System, Ministry of Education;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8327505"",""Renewable energy";storage system;"SSD cache"",""Green products";Renewable energy sources;Power supplies;Energy consumption;Prefetching;Cache storage;"Power system management"","""",""3"","""",""42"",""IEEE"",""28 Mar 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;