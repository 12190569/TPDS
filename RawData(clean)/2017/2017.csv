"A Cloud Reservation System for Big Data Applications,""D. C. Marinescu"; A. Paya;" J. P. Morrison"",""Computer Science Department, University of Central Florida, Orlando, FL"; Computer Science Department, University of Central Florida, Orlando, FL;" Computer Science Department, University College Cork, Cork, Ireland"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""606"",""618"",""Emerging Big Data applications increasingly require resources beyond those available from a single server and may be expressed as a complex workflow of many components and dependency relationships-each component potentially requiring its own specific, and perhaps specialized, resources for its execution. Efficiently supporting this type of Big Data application is a challenging resource management problem for existing cloud environments. In response, we propose a two-stage protocol for solving this resource management problem. We exploit spatial locality in the first stage by dynamically forming rack-level coalitions of servers to execute a workflow component. These coalitions only exist for the duration of the execution of their assigned component and are subsequently disbanded, allowing their resources to take part in future coalitions. The second stage creates a package of these coalitions, designed to support all the components in the complete workflow. To minimize the communication and housekeeping overhead needed to form this package of coalitions, the technique of combinatorial auctions is adapted from market-based resource allocation. This technique has a considerably lower overhead for resource aggregation than the traditional hierarchically organized models. We analyze two strategies for coalition formation: the first, history-based uses information from past auctions to pre-form coalitions in anticipation of predicted demand";" the second one is a just-in-time-that builds coalitions only when support for specific workflow components is requested."",""1558-2183"","""",""10.1109/TPDS.2016.2594783"",""US National Science Foundation CCR(grant numbers:1525943)"; Simulation of Quantum Many-Body Systems Feasible on the Cloud(grant numbers:H2020 EU); Irish Centre for Cloud Computing and Commerce;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523396"",""Big data applications";cloud resource management;hierarchical organization;coalition formation;"combinatorial auctions"",""Cloud computing";Resource management;Servers;Big data;Organizations;Computers;"Google"","""",""15"","""",""39"",""IEEE"",""27 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Clustering Algorithm for Communication-Aware Scheduling of Task Graphs on Multi-Core Reconfigurable Systems,""A. Yoosefi";" H. R. Naji"",""Department of Electrical and Computer Engineering, Graduate University of Advanced Technology, Kerman, Iran";" Department of Electrical and Computer Engineering, Graduate University of Advanced Technology, Kerman, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2718"",""2732"",""In this paper, a clustering algorithm called the Reconfigurable Dominant Sequence Clustering (ReDSC), is proposed. The experiments show that ReDSC reduces the parallel run time efficiently by 30 percent. Moreover, an empirical comparison of ReDSC with its peer in the multi-processor model, i.e., DSC algorithm, is provided and the results demonstrate the 35 percent improvement of ReDSC over DSC. Then, a hardware implementation of a dynamic scheduler for run-time communication-aware scheduling of hardware tasks is proposed. The proposed scheduler makes use of the ReDSC algorithm for allocating tasks to the processing reconfigurable cores, dynamically considering the communication costs. The simulation results illustrate the benefits of the proposed scheduler as compared to that of other static and basic schedulers in the technical literature."",""1558-2183"","""",""10.1109/TPDS.2017.2703123"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924424"",""Clustering";field-programmable gate array (FPGA);multi-core;reconfigurable computing;"task scheduling"",""Hardware";Dynamic scheduling;Clustering algorithms;Processor scheduling;Heuristic algorithms;"Field programmable gate arrays"","""",""18"","""",""33"",""IEEE"",""10 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Collision-Mitigation Cuckoo Hashing Scheme for Large-Scale Storage Systems,""Y. Sun"; Y. Hua; D. Feng; L. Yang; P. Zuo; S. Cao;" Y. Guo"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China."; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China.; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China.; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China.; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China.; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China.;" Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China."",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""619"",""632"",""With the rapid growth of the amount of information, cloud computing servers need to process and analyze large amounts of high-dimensional and unstructured data timely and accurately. This usually requires many query operations. Due to simplicity and ease of use, cuckoo hashing schemes have been widely used in real-world cloud-related applications. However, due to the potential hash collisions, the cuckoo hashing suffers from endless loops and high insertion latency, even high risks of re-construction of entire hash table. In order to address these problems, we propose a cost-efficient cuckoo hashing scheme, called MinCounter. The idea behind MinCounter is to alleviate the occurrence of endless loops in the data insertion by selecting unbusy kicking-out routes. MinCounter selects the “cold” (infrequently accessed), rather than random, buckets to handle hash collisions. We further improve the concurrency of the MinCounter scheme to pursue higher performance and adapt to concurrent applications. MinCounter has the salient features of offering efficient insertion and query services and delivering high performance of cloud servers, as well as enhancing the experiences for cloud users. We have implemented MinCounter in a large-scale cloud testbed and examined the performance by using three realworld traces. Extensive experimental results demonstrate the efficacy and efficiency of MinCounter."",""1558-2183"","""",""10.1109/TPDS.2016.2594763"",""National Key Research and Development(grant numbers:2016YFB1000202)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523403"",""Cuckoo hashing";cloud storage;"data insertion and query"",""Cloud computing";Servers;Real-time systems;Radiation detectors;Indexes;Data structures;"Complexity theory"","""",""6"","""",""61"",""IEEE"",""27 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Comprehensive Study of MapReduce Over Lustre for Intermediate Data Placement and Shuffle Strategies on HPC Clusters,""M. Wasi-ur-Rahman"; N. S. Islam; X. Lu;" D. K. Panda"",""Department of Computer Science and Engineering, The Ohio State University, Columbus, OH"; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH; Department of Computer Science and Engineering, The Ohio State University, Columbus, OH;" Department of Computer Science and Engineering, The Ohio State University, Columbus, OH"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""633"",""646"",""With high performance interconnects and parallel file systems, running MapReduce over modern High Performance Computing (HPC) clusters has attracted much attention due to its uniqueness of solving data analytics problems with a combination of Big Data and HPC technologies. Since the MapReduce architecture relies heavily on the availability of local storage media, the Lustre-based global storage in HPC clusters poses many new opportunities and challenges. In this paper, we perform a comprehensive study on different MapReduce over Lustre deployments and propose a novel high-performance design of YARN MapReduce on HPC clusters by utilizing Lustre as the additional storage provider for intermediate data. With a deployment architecture where both local disks and Lustre are utilized for intermediate data storage, we propose a novel priority directory selection scheme through which RDMA-enhanced MapReduce can choose the best intermediate storage during runtime by on-line profiling. Our results indicate that, we can achieve 44 percent performance benefit for shuffle-intensive workloads in leadership-class HPC systems. Our priority directory selection scheme can improve the job execution time by 63 percent over default MapReduce while executing multiple concurrent jobs. To the best of our knowledge, this is the first such comprehensive study for YARN MapReduce with Lustre and RDMA."",""1558-2183"","""",""10.1109/TPDS.2016.2591947"",""National Science Foundation(grant numbers:#CNS-1419123,#IIS-1447804,#ACI-1450440)"; Extreme Science and Engineering Discovery Environment (XSEDE); National Science Foundation(grant numbers:#OCI-1053575);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514945"",""Big data";high performance computing;RDMA;MapReduce;"lustre"",""Computer architecture";Servers;High performance computing;Data analysis;Big data;"Memory"","""",""18"","""",""40"",""IEEE"",""18 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Conflict-Free Replicated JSON Datatype,""M. Kleppmann";" A. R. Beresford"",""Computer Laboratory, University of Cambridge, Cambridge, United Kingdom";" Computer Laboratory, University of Cambridge, Cambridge, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2733"",""2746"",""Many applications model their data in a general-purpose storage format such as JSON. This data structure is modified by the application as a result of user input. Such modifications are well understood if performed sequentially on a single copy of the data, but if the data is replicated and modified concurrently on multiple devices, it is unclear what the semantics should be. In this paper we present an algorithm and formal semantics for a JSON data structure that automatically resolves concurrent modifications such that no updates are lost, and such that all replicas converge towards the same state (a conflict-free replicated datatype or CRDT). It supports arbitrarily nested list and map types, which can be modified by insertion, deletion and assignment. The algorithm performs all merging client-side and does not depend on ordering guarantees from the network, making it suitable for deployment on mobile devices with poor network connectivity, in peer-to-peer networks, and in messaging systems with end-to-end encryption."",""1558-2183"","""",""10.1109/TPDS.2017.2697382"",""The Boeing Company";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7909007"",""CRDTs";collaborative editing;P2P;JSON;optimistic replication;operational semantics;"eventual consistency"",""Data structures";Collaboration;Semantics;Registers;Servers;Data models;"Mobile handsets"","""",""49"","""",""44"",""IEEE"",""24 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Declarative Optimization Engine for Resource Provisioning of Scientific Workflows in Geo-Distributed Clouds,""A. C. Zhou"; B. He; X. Cheng;" C. T. Lau"",""Inria Rennes Bretagne Atlantique, Rennes, France"; National University of Singapore, Singapore; Nanyang Technological University, Singapore;" Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""647"",""661"",""Geo-distributed clouds are becoming increasingly popular for cloud providers, and data centers with different regions often offer different prices, even for the same type of virtual machines. Resource provisioning in geo-distributed clouds is an important and complicated problem for budget and performance optimizations of scientific workflows. Scientists are facing the complexities resulted from various cloud offerings in the geo-distributed settings, severe cloud performance dynamics and evolving user requirements on performance and cost. To address those complexities, we propose a declarative optimization engine named Geco for resource provisioning of scientific workflows in geo-distributed clouds. Geco allows users to specify their workflow optimization goals and constraints of specific problems with an extended declarative language. We propose a novel probabilistic optimization approach for evaluating the declarative optimization goals and constraints to address the cloud dynamics. Additionally, we develop runtime optimizations to more effectively utilize the cloud resources at runtime. To accelerate the solution finding, Geco leverages the power of GPUs to find the solution in a fast and timely manner. Our evaluations with four common workflow provisioning problems demonstrate that, Geco is able to achieve more effective performance/cost optimizations in geo-distributed cloud environments than the state-of-the-art approaches."",""1558-2183"","""",""10.1109/TPDS.2016.2599529"",""MoE"; AcRF(grant numbers:T1 251RES1610); NUS;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542141"",""Resource provisioning";scientific workflows;declarative optimizations;"geo-distributed clouds"",""Cloud computing";Optimization;Engines;Asia;Complexity theory;Probabilistic logic;"Runtime"","""",""13"","""",""46"",""IEEE"",""11 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Domain Specific Approach to High Performance Heterogeneous Computing,""G. Inggs"; D. B. Thomas;" W. Luk"",""Circuits and Systems Group, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom"; Circuits and Systems Group, Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom;" Custom Computing Group in the Department of Computing, Imperial College London, London, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Dec 2016"",""2017"",""28"",""1"",""2"",""15"",""Users of heterogeneous computing systems face two problems: first, in understanding the trade-off relationships between the observable characteristics of their applications, such as latency and quality of the result, and second, how to exploit knowledge of these characteristics to allocate work to distributed computing platforms efficiently. A domain specific approach addresses both of these problems. By considering a subset of operations or functions, models of the observable characteristics or domain metrics may be formulated in advance, and populated at run-time for task instances. These metric models can then be used to express the allocation of work as a constrained integer program. These claims are illustrated using the domain of derivatives pricing in computational finance, with the domain metrics of workload latency and pricing accuracy. For a large, varied workload of 128 Black-Scholes and Heston model-based option pricing tasks, running upon a diverse array of 16 Multicore CPUs, GPUs and FPGAs platforms, predictions made by models of both the makespan and accuracy are generally within 10 percent of the run-time performance. When these models are used as inputs to machine learning and MILP-based workload allocation approaches, a latency improvement of up to 24 and 270 times over the heuristic approach is seen."",""1558-2183"","""",""10.1109/TPDS.2016.2563427"",""South African National Research Foundation"; Oppenheimer Memorial Trust; Nallatech, Altera, Xilinx, Intel, and Maxeler university programs;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7465804"",""Distributed computing, programming environments, accelerator architectures, high performance computing, application software"",""Resource management";Measurement;Computational modeling;Pricing;Biological system modeling;Context;"Benchmark testing"","""",""6"","""",""31"",""IEEE"",""5 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Dynamic Approach for Workload Partitioning on GPU Architectures,""F. Busato";" N. Bombieri"",""Department of Computer Science, University of Verona, VR, Italy";" Department of Computer Science, University of Verona, VR, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1535"",""1549"",""Workload partitioning and the subsequent work item-to-thread mapping are key aspects to face when implementing any efficient GPU application. Different techniques have been proposed to deal with such issues, ranging from the computationally simplest static to the most complex dynamic ones. Each of them finds the best use depending on the workload characteristics (static for more regular workloads, dynamic for irregular workloads). Nevertheless, no one of them provides a sound tradeoff when applied in both cases. Static approaches lead to load unbalancing with irregular problems, while the computational overhead introduced by the dynamic or semi-dynamic approaches often worsens the overall application performance when run on regular problems. This article presents an efficient dynamic technique for workload partitioning and work item-to-thread mapping whose complexity is significantly reduced with respect to the other dynamic approaches in literature. The article shows how the partitioning and mapping algorithm has been implemented by fully taking advantage of the GPU device characteristics with the aim of minimizing the involved computational overhead. The article shows, compares, and analyses the experimental results obtained by applying the proposed approach and several static, dynamic, and semi-dynamic techniques at the state of the art to different benchmarks and over different GPU technologies (i.e., NVIDIA Fermi, Kepler, and Maxwell) to understand when and how each technique best applies."",""1558-2183"","""",""10.1109/TPDS.2016.2631166"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7750565"",""Computer society";IEEE;IEEEtran;journal;prefix-scan;load balacing;"GPU"",""Graphics processing units";Instruction sets;Indexes;Heuristic algorithms;Message systems;Parallel processing;"Benchmark testing"","""",""5"","""",""29"",""IEEE"",""21 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Fast and Accurate Hardware String Matching Module with Bloom Filters,""S. Zengin";" E. G. Schmidt"",""Defense Industries Research and Development Institute, Scientific and Technological Research Council of Turkey, Ankara, Turkey";" Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""305"",""317"",""Many fields of computing such as Deep Packet Inspection (DPI) employ string matching modules (SMM) that search for a given set of positive strings in their input. An SMM is expected to produce correct outcomes while scanning the input data at high rates. Furthermore the string sets that are searched for are usually large and their sizes increase steadily. Bloom Filters (BFs) are hashing data structures which are fast but their false positive results require further processing. That is, their speed can be exploited for Standard Bloom Filter SMMs (SBFs) as long as the positive probability is low. Multiple BFs in parallel can further increase the throughput. In this paper, we propose the Double Bloom Filter SMM (DBF) which achieves a higher throughput than the SBF and maintains a high throughput even for large positive probabilities. The second Bloom Filter of DBF stores a small enough subset of the positive strings such that its false positive probability is approximately zero. We develop an analytical model of the DBF and show that the throughput advantage of DBF over SBF becomes more prominent if the positive probability and the fraction of matches in the second Bloom Filter increase. Accordingly, we propose a heuristic algorithm that stores the strings that are more frequently matched in the second Bloom Filter according to localities identified in the input. Our numerical results are obtained using realistic values from an FPGA implementation and are validated by SystemC simulations."",""1558-2183"","""",""10.1109/TPDS.2016.2577033"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7485864"",""Bloom filter";string matching;deep packet inspection;"field programmable gate array (FPGA)"",""Throughput";Matched filters;Analytical models;Hardware;Field programmable gate arrays;Time factors;"Metadata"","""",""6"","""",""39"",""IEEE"",""6 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Fuzzy Approach Based on Heterogeneous Metrics for Scaling Out Public Clouds,""V. Persico"; D. Grimaldi; A. Pescapè; A. Salvi;" S. Santini"",""University of Napoli “Federico II”, Napoli, Italy"; University of Napoli “Federico II”, Napoli, Italy; University of Napoli “Federico II”, Napoli, Italy; University of Napoli “Federico II”, Napoli, Italy;" University of Napoli “Federico II”, Napoli, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2117"",""2130"",""Thanks to resource elasticity, cloud systems allow to build high performance applications by dynamically adapting resources to workload dynamics. In this paper, we present a novel approach for horizontally scaling cloud resources. The approach is based on an optimized feedback control scheme that leverages fuzzy logic to self-adjust its parameters in order to cope with unpredictable and highly time-varying public-cloud operating conditions. The proposed approach takes as input heterogeneous monitoring metrics related to distinct aspects of interest (i.e., CPU and network load) merged through a fitness function. Therefore, it is able to accomplish the application needs from different viewpoints. The extensive experimental evaluation performed in the Amazon EC2 environment showed how the proposed approach is robust against a number of realistic workloads-also when VM failures happen- and that it is flexible, as being suitable for applications with different needs. Finally, it also achieves better performance when compared to previously proposed solutions."",""1558-2183"","""",""10.1109/TPDS.2017.2651810"",""MIUR(grant numbers:11 DM 593/2000)"; AWS;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7814313"",""Cloud computing";cloud control;cloud monitoring;autoscaling;PID;fuzzy logic;"QoS/QoE metrics"",""Cloud computing";Measurement;Quality of service;Elasticity;Fuzzy logic;Servers;"Adaptation models"","""",""20"","""",""69"",""IEEE"",""11 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A General-Purpose Architecture for Replicated Metadata Services in Distributed File Systems,""D. Stamatakis"; N. Tsikoudis; E. Micheli;" K. Magoutis"",""Department of Computer Science, Brandeis University, Waltham, MA"; Department of Computer Science, Brandeis University, Waltham, MA; Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece;" Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2747"",""2759"",""A large class of modern distributed file systems treat metadata services as an independent system component, separately from data servers. The availability of the metadata service is key to the availability of the overall system. Given the high rates of failures observed in large-scale data centers, distributed file systems usually incorporate high-availability (HA) features. A typical approach in the development of distributed file systems is to design and develop metadata services from the ground up, at significant cost in terms of complexity and time, often leading to functional shortcomings. Our motivation in this paper was to improve on this state of things by defining a general-purpose architecture for HA metadata services (which we call RMS) that can be easily incorporated and reused in new or existing file systems, reducing development time. Taking two prominent distributed file systems as case studies, PVFS and HDFS, we developed RMS variants that improve on functional shortcomings of the original HA solutions, while being easy to build and test. Our extensive evaluation of the RMS variant of HDFS shows that it does not incur an overall performance or availability penalty compared to the original implementation."",""1558-2183"","""",""10.1109/TPDS.2017.2700272"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917318"",""Distributed file systems";high availability;system recovery;"metadata services"",""Metadata";Servers;Distributed databases;Computer architecture;Peer-to-peer computing;"File systems"","""",""3"","""",""57"",""IEEE"",""2 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A GPU-Architecture Optimized Hierarchical Decomposition Algorithm for Support Vector Machine Training,""J. Vaněk"; J. Michálek;" J. Psutka"",""University of West Bohemia, New Technologies for the Information Society, Pilsen, Czech Republic"; University of West Bohemia, New Technologies for the Information Society, Pilsen, Czech Republic;" University of West Bohemia, New Technologies for the Information Society, Pilsen, Czech Republic"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3330"",""3343"",""In the last decade, several GPU implementations of Support Vector Machine (SVM) training with nonlinear kernels were published. Some of them even with source codes. The most effective ones are based on Sequential Minimal Optimization (SMO). They decompose the restricted quadratic problem into a series of smallest possible subproblems, which are then solved analytically. For large datasets, the majority of elapsed time is spent by a large amount of matrix-vector multiplications that cannot be computed efficiently on current GPUs because of limited memory bandwidth. In this paper, we introduce a novel GPU approach to the SVM training that we call Optimized Hierarchical Decomposition SVM (OHD-SVM). It uses a hierarchical decomposition iterative algorithm that fits better to actual GPU architecture. The low decomposition level uses a single GPU multiprocessor to efficiently solve a local subproblem. Nowadays a single GPU multiprocessor can run thousand or more threads that are able to synchronize quickly. It is an ideal platform for a single kernel SMO-based local solver with fast local iterations. The high decomposition level updates gradients of entire training set and selects a new local working set. The gradient update requires many kernel values that are costly to compute. However, solving a large local subproblem offers an efficient kernel values computation via a matrix-matrix multiplication that is much more efficient than the matrix-vector multiplication used in already published implementations. Along with a description of our implementation, the paper includes an exact comparison of five publicly available C++ SVM training GPU implementations. In this paper, the binary classification task and RBF kernel function are taken into account as it is usual in most of the recent papers. According to the measured results on a wide set of publicly available datasets, our proposed approach excelled significantly over the other methods in all datasets. The biggest difference was on the largest dataset where we achieved speed-up up to 12 times in comparison with the fastest already published GPU implementation. Moreover, our OHD-SVM is the only one that can handle dense as well as sparse datasets. Along with this paper, we published the source-codes at https://github.com/OrcusCZ/OHD-SVM."",""1558-2183"","""",""10.1109/TPDS.2017.2731764"",""Grant Agency of the Czech Republic(grant numbers:GBP103/12/G084)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990554"",""Support vector machines";SVM training;GPU;CUDA;"optimization"",""Graphics processing units";Support vector machines;Computer architecture;Open source software;"Optimization"","""",""13"","""",""43"",""IEEE"",""25 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Group-Ordered Fast Iterative Method for Eikonal Equations,""S. Hong";" W. -K. Jeong"",""School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea";" School of Electrical and Computer Engineering, Ulsan National Institute of Science and Technology, Ulsan, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""318"",""331"",""In the past decade, many numerical algorithms for the Eikonal equation have been proposed. Recently, the research of Eikonal equation solver has focused more on developing efficient parallel algorithms in order to leverage the computing power of parallel systems, such as multi-core CPUs and GPUs (Graphics Processing Units). In this paper, we introduce an efficient parallel algorithm that extends Jeong et al.'s FIM (Fast Iterative Method, [1]), originally developed for the GPU, for multi-core shared memory systems. First, we propose a parallel implementation of FIM using a lock-free local queue approach and provide an in-depth analysis of the parallel performance of the method. Second, we propose a new parallel algorithm, Group-Ordered Fast Iterative Method (GO-FIM), that exploits causality of grid blocks to reduce redundant computations, which was the main drawback of the original FIM. In addition, the proposed GO-FIM method employs clustering of blocks based on the updating order where each cluster can be updated in parallel using multi-core parallel architectures. We discuss the performance of GO-FIM and compare with the state-of-the-art parallel Eikonal equation solvers."",""1558-2183"","""",""10.1109/TPDS.2016.2567397"",""Institute for Information & communications Technology Promotion (IITP) Korea government (MSIP)(grant numbers:R0190-15-2012)"; High Performance Big Data Analytics Platform Performance Acceleration Technologies Development; Basic Science Research Program; National Research Foundation of Korea (NRF); Ministry of Education(grant numbers:NRF-2014R1A1A2058773);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469401"",""Eikonal equation";GPU;"parallel computing"",""Graphics processing units";Iterative methods;Parallel algorithms;Algorithm design and analysis;Data structures;"Parallel architectures"","""",""2"","""",""30"",""IEEE"",""12 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Hardware Approach to Fairly Balance the Inter-Thread Interference in Shared Caches,""V. Selfa"; J. Sahuquillo; S. Petit;" M. E. Gómez"",""Department of Computing Engineering, Universitat Politècnica de València, València, Spain"; Department of Computing Engineering, Universitat Politècnica de València, València, Spain; Department of Computing Engineering, Universitat Politècnica de València, València, Spain;" Department of Computing Engineering, Universitat Politècnica de València, València, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3021"",""3032"",""Shared caches have become the common design choice in the vast majority of modern multi-core and many-core processors, since cache sharing improves throughput for a given silicon area. Sharing the cache, however, has a downside: the requests from multiple applications compete among them for cache resources, so the execution time of each application increases over isolated execution. The degree in which the performance of each application is affected by the interference becomes unpredictable yielding the system to unfairness situations. This paper proposes Fair-Progress Cache Partitioning (FPCP), a low-overhead hardware-based cache partitioning approach that addresses system fairness. FPCP reduces the interference by allocating to each application a cache partition and adjusting the partition sizes at runtime. To adjust partitions, our approach estimates during multicore execution the time each application would have taken in isolation, which is challenging. The proposed approach has two main differences over existing approaches. First, FPCP distributes cache ways incrementally, which makes the proposal less prone to estimation errors. Second, the proposed algorithm is much less costly than the state-of-the-art ASM-Cache approach. Experimental results show that, compared to ASM-Cache, FPCP reduces unfairness by 48 percent in four-application workloads and by 28 percent in eight-application workloads, without harming the performance."",""1558-2183"","""",""10.1109/TPDS.2017.2713778"",""Spanish Ministerio de Economía y Competitividad (MINECO)(grant numbers:TIN2014-62246-EXP,TIN2015-66972-C5-1-R)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944620"",""Cache partitioning";multi-cores;fairness;progress;slowdown;"execution time in isolation"",""Interference";Proposals;Instruction sets;Hardware;Quality of service;"Measurement"","""",""6"","""",""56"",""IEEE"",""8 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A High Performance Block Eigensolver for Nuclear Configuration Interaction Calculations,""H. M. Aktulga"; M. Afibuzzaman; S. Williams; A. Buluç; M. Shao; C. Yang; E. G. Ng; P. Maris;" J. P. Vary"",""Michigan State University, 428 S. Shaw Lane, Room 3115, East Lansing, MI"; Michigan State University, 428 S. Shaw Lane, Room 3115, East Lansing, MI; Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Rd, MS 50F-1650, Berkeley, CA; Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Rd, MS 50F-1650, Berkeley, CA; Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Rd, MS 50F-1650, Berkeley, CA; Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Rd, MS 50F-1650, Berkeley, CA; Computational Research Division, Lawrence Berkeley National Laboratory, 1 Cyclotron Rd, MS 50F-1650, Berkeley, CA; Department of Physics and Astronomy, Iowa State University, Ames, IA;" Department of Physics and Astronomy, Iowa State University, Ames, IA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1550"",""1563"",""As on-node parallelism increases and the performance gap between the processor and the memory system widens, achieving high performance in large-scale scientific applications requires an architecture-aware design of algorithms and solvers. We focus on the eigenvalue problem arising in nuclear Configuration Interaction (CI) calculations, where a few extreme eigenpairs of a sparse symmetric matrix are needed. We consider a block iterative eigensolver whose main computational kernels are the multiplication of a sparse matrix with multiple vectors (SpMM), and tall-skinny matrix operations. We present techniques to significantly improve the SpMM and the transpose operation SpMMT by using the compressed sparse blocks (CSB) format. We achieve 3-4× speedup on the requisite operations over good implementations with the commonly used compressed sparse row (CSR) format. We develop a performance model that allows us to correctly estimate the performance of our SpMM kernel implementations, and we identify cache bandwidth as a potential performance bottleneck beyond DRAM. We also analyze and optimize the performance of LOBPCG kernels (inner product and linear combinations on multiple vectors) and show up to 15× speedup over using high performance BLAS libraries for these operations. The resulting high performance LOBPCG solver achieves 1.4× to 1.8× speedup over the existing Lanczos solver on a series of CI computations on high-end multicore architectures (Intel Xeons). We also analyze the performance of our techniques on an Intel Xeon Phi Knights Corner (KNC) processor."",""1558-2183"","""",""10.1109/TPDS.2016.2630699"",""U.S. DOE's ASCR"; NP Offices as part of the Applied Mathematics Program and Scientific Discovery through Advanced Computing (SciDAC) Program; NUCLEI Project(grant numbers:DESC0008485); FASTMath Institute; SUPER Institute; NERSC; Lawrence Berkeley National Laboratory(grant numbers:DE-AC02-05CH11231); Iowa State University(grant numbers:DE-FG02-87ER40371);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748453"",""Sparse matrix multiplication";block eigensolver;configuration interaction;extended roofline model;"tall-skinny matrices"",""Sparse matrices";Eigenvalues and eigenfunctions;Symmetric matrices;Kernel;Wave functions;Bandwidth;"Computer architecture"","""",""10"","""",""35"",""IEEE"",""18 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"A Hybrid MPI-OpenMP Strategy to Speedup the Compression of Big Next-Generation Sequencing Datasets,""S. Vargas-Pérez";" F. Saeed"",""Department of Computer Science, Western Michigan University, Kalamazoo, MI";" Department of Electrical and Computer Engineering and Department of Computer Science, Western Michigan University, Kalamazoo, MI"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2760"",""2769"",""DNA sequencing has moved into the realm of Big Data due to the rapid development of high-throughput, low cost Next-Generation Sequencing (NGS) technologies. Sequential data compression solutions that once were sufficient to efficiently store and distribute this information are now falling behind. In this paper we introduce phyNGSC, a hybrid MPI-OpenMP strategy to speedup the compression of big NGS data by combining the features of both distributed and shared memory architectures. Our algorithm balances work-load among processes and threads, alleviates memory latency by exploiting locality, and accelerates I/O by reducing excessive read/write operations and inter-node message exchange. To make the algorithm scalable, we introduce a novel timestamp-based file structure that allows us to write the compressed data in a distributed and non-deterministic fashion while retaining the capability of reconstructing the dataset with its original order. Our experimental results show that phyNGSC achieved compression times for big NGS datasets that were 45 to 98 percent faster than NGS-specific sequential compressors with throughputs of up to 3 GB/s. Our theoretical analysis and experimental results suggest strong scalability with some datasets yielding super-linear speedups and constant efficiency. We were able to compress 1 terabyte of data in under 8 minutes compared to more than 5 hours taken by NGS-specific compression algorithms running sequentially. Compared to other parallel solutions, phyNGSC achieved up to 6x speedups while maintaining a higher compression ratio. The code for this implementation is available at https://github.com/pcdslab/PHYNGSC."",""1558-2183"","""",""10.1109/TPDS.2017.2692782"",""US National Science Foundation(grant numbers:CRII CCF-1464268)"; US National Science Foundation(grant numbers:CAREER ACI-1651724); XSEDE(grant numbers:TG-CCR150017); SDSC;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7895161"",""NGS";parallel;hybrid;HPC;MPI;OpenMP;big data;FASTQ;"timestamps"",""Writing";DNA;Genomics;Bioinformatics;Parallel algorithms;Big Data;"Instruction sets"","""",""11"","""",""36"",""IEEE"",""12 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Load Balancing and Multi-Tenancy Oriented Data Center Virtualization Framework,""J. Duan";" Y. Yang"",""Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY";" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2131"",""2144"",""Virtualization is an essential step before a bare-metal data center being ready for commercial usage, because it bridges the foreground interface for cloud tenants and the background resource management on underlying infrastructures. A concept at the heart of the foreground is multi-tenancy, which deals with logical isolation of shared virtual computing, storage, and network resources and provides adaptive capability for heterogeneous demands from various tenants. A crucial problem in the background is load balancing, which affects multiple issues including cost, flexibility and availability. In this work, we propose a virtualization framework that consider these two problems simultaneously. Our framework takes advantage of the flourishing application of distributed virtual switch (DVS), and leverages the blooming adoption of OpenFlow protocols. First, the framework accommodates heterogeneous network communication patterns by supporting arbitrary traffic matrices among virtual machines (VMs) in virtual private clouds (VPCs). The only constraint on the network flows is that the bandwidth of a server's network interface. Second, our framework achieves load balancing using an elaborately designed link establishment algorithm. The algorithm takes the configurations of the bare-metal data center and the dynamic network environment as inputs, and adaptively applies a globally bounded oversubscription on every link. Our framework concentrates on the fat-tree architecture, which is widely used in today's data centers."",""1558-2183"","""",""10.1109/TPDS.2017.2657633"",""U.S. National Science Foundation(grant numbers:CCF-1320044)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831441"",""Virtualization";load balancing;multi-tenancy;data center;data center networks;OpenFlow;"distributed virtual switch"",""Cloud computing";Switches;Virtualization;Servers;Load management;Voltage control;"Hardware"","""",""31"","""",""45"",""IEEE"",""24 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A multi-core CPU and many-core GPU based fast parallel shuffled complex evolution global optimization approach,""G. Kan"; T. Lei; K. Liang; J. Li; L. Ding; X. He; H. Yu; D. Zhang; D. Zuo; Z. Bao; M. Amo-Boateng; Y. Hu;" M. Zhang"",""State Key Laboratory of Simulation and Regulation of Water Cycle in River Basin, China Institute of Water Resources and Hydropower Research, Beijing, P. R. China"; State Key Laboratory of Simulation and Regulation of Water Cycle in River Basin, China Institute of Water Resources and Hydropower Research, Beijing, P. R. China; College of Hydrology and Water Resources, Hohai University, Nanjing, P. R. China; State Key Laboratory of Simulation and Regulation of Water Cycle in River Basin, China Institute of Water Resources and Hydropower Research, Beijing, P. R. China; State Key Laboratory of Simulation and Regulation of Water Cycle in River Basin, China Institute of Water Resources and Hydropower Research, Beijing, P. R. China; State Key Laboratory of Simulation and Regulation of Water Cycle in River Basin, China Institute of Water Resources and Hydropower Research, Beijing, P. R. China; State Key Laboratory of Simulation and Regulation of Water Cycle in River Basin, China Institute of Water Resources and Hydropower Research, Beijing, P. R. China; State Key Laboratory of Simulation and Regulation of Water Cycle in River Basin, China Institute of Water Resources and Hydropower Research, Beijing, P. R. China; College of Water Sciences, Beijing Normal University, Beijing, P. R. China; Nanjing Hydraulic Research Institute, Nanjing, P. R. China; University of Energy and Natural Resources, Sunyani, Ghana; Hydrologic Bureau (Information Center) of the Huaihe River Commission, Bengbu, P. R. China;" Department of Water Resources (DWR), China Institute of Water Resources and Hydropower Research, Beijing, P. R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""332"",""344"",""In the field of hydrological modelling, the global and automatic parameter calibration has been a hot issue for many years. Among automatic parameter optimization algorithms, the shuffled complex evolution developed at the University of Arizona (SCE-UA) is the most successful method for stably and robustly locating the global “best” parameter values. Ever since the invention of the SCE-UA, the profession suddenly has a consistent way to calibrate watershed models. However, the computational efficiency of the SCE-UA significantly deteriorates when coping with big data and complex models. For the purpose of solving the efficiency problem, the recently emerging heterogeneous parallel computing (parallel computing by using the multi-core CPU and many-core GPU) was applied in the parallelization and acceleration of the SCE-UA. The original serial and proposed parallel SCE-UA were compared to test the performance based on the Griewank benchmark function. The comparison results indicated that the parallel SCE-UA converged much faster than the serial version and its optimization accuracy was the same as the serial version. It has a promising application prospect in the field of fast hydrological model parameter optimization."",""1558-2183"","""",""10.1109/TPDS.2016.2575822"",""IWHR Research"; Development Support Program(grant numbers:JZ0145B052016); IWHR Scientific Research Projects of Outstanding Young Scientists; Research and application on the fast global optimization method for the Xinanjiang model parameters based on the high performance heterogeneous computing(grant numbers:KY1605); China Postdoctoral Science Foundation on Grant(grant numbers:2016M591214); Specific Research of China Institute of Water Resources and Hydropower Research(grant numbers:Fangji 1240); Third Sub-Project: Flood Forecasting, Controlling and Flood Prevention Aided Software Development; Flood Control Early Warning Communication System and Flood Forecasting, Controlling and Flood Prevention Aided Software Development for Poyang Lake Area of Jiangxi Province(grant numbers:0628-136006104242,JZ0205A432013,SLXMB200902); NNSF of China; Study on the integrated assessment model for risk and benefit of dynamic control of reservoir waterlevel in flood season(grant numbers:51509268); NNSF of China; Numerical Simulation Technology of Flash Flood based on Godunov Scheme and Its Mechanism Study by Experiment(grant numbers:51509263); NVIDIA Corporation with the donation of the Tesla K40 GPU;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7491261"",""parameter optimization";SCE-UA;multi-core CPU;many-core GPU;"parallel computing"",""Optimization";Water resources;Algorithm design and analysis;Parallel processing;Computational modeling;Calibration;"Genetic algorithms"","""",""21"","""",""29"",""IEEE"",""14 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;
"A Multi-Objective Model Oriented Mapping Approach for NoC-based Computing Systems,""C. Wu"; C. Deng; L. Liu; J. Han; J. Chen; S. Yin;" S. Wei"",""Institute of Microelectronics and the National Lab for Information Science and Technology, Tsinghua University, Beijing, China"; Institute of Microelectronics and the National Lab for Information Science and Technology, Tsinghua University, Beijing, China; Institute of Microelectronics and the National Lab for Information Science and Technology, Tsinghua University, Beijing, China; ECE Department, University of Alberta, Edmonton, AB, Canada; Institute of Microelectronics and the National Lab for Information Science and Technology, Tsinghua University, Beijing, China; Institute of Microelectronics and the National Lab for Information Science and Technology, Tsinghua University, Beijing, China;" Institute of Microelectronics and the National Lab for Information Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""662"",""676"",""In this paper, a multi-objective, i.e., reliability, communication energy, performance, co-optimization model oriented mapping approach is proposed to find optimal mappings when applications are mapped onto network-on-chip (NoC) based reconfigurable architectures. A co-optimization model, defined as reliability efficiency model (REM), is developed to evaluate the overall reliability efficiency of a mapping. In REM, reliability efficiency is defined as the reliability profit at the same energy latency product. Based on REM, a mapping approach, referred to as priority and compensation factor oriented branch and bound (PCBB), is introduced to figure out the best mapping pattern. Two techniques, priority allocation and compensation factor utilization, are adopted to make a tradeoff between search efficiency and accuracy. Experimental results show that the proposed approach has three major contributions compared to state-of-the-art approaches. (1) PCBB is highly efficient in finding best mappings, with a 3x and 720x speedup compared to branch and bound (BB) and simulated annealing (SA). (2) PCBB is able to dynamically remap after the reconfiguration of the architecture. (3) General quantitative evaluation for reliability, communication energy and performance are made respectively before integrated into the unified model REM, whereas other similar models only touch upon two of them quantitatively."",""1558-2183"","""",""10.1109/TPDS.2016.2589934"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508918"",""Multi-objective";Network-on-Chip (NoC);reconfigurable architecture;"mapping approach"",""Reliability engineering";Parallel processing;Routing;Reconfigurable architectures;Topology;"Computational modeling"","""",""22"","""",""47"",""IEEE"",""11 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Parallel Random Forest Algorithm for Big Data in a Spark Cloud Computing Environment,""J. Chen"; K. Li; Z. Tang; K. Bilal; S. Yu; C. Weng;" K. Li"",""College of Computer Science and Electronic Engineering, Hunan University, National Supercomputing Center in Changsha, Hunan, Changsha, China"; College of Computer Science and Electronic Engineering, Hunan University, National Supercomputing Center in Changsha, Hunan, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, National Supercomputing Center in Changsha, Hunan, Changsha, China; Qatar University, Doha, Qatar; School of Information Technology, Deakin University, Melbourne, Vic., Australia; School of Computer Science and Software Engineering, East China Normal University, Shanghai, China;" College of Computer Science and Electronic Engineering, Hunan University, National Supercomputing Center in Changsha, Hunan, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""919"",""933"",""With the emergence of the big data age, the issue of how to obtain valuable knowledge from a dataset efficiently and accurately has attracted increasingly attention from both academia and industry. This paper presents a Parallel Random Forest (PRF) algorithm for big data on the Apache Spark platform. The PRF algorithm is optimized based on a hybrid approach combining dataparallel and task-parallel optimization. From the perspective of data-parallel optimization, a vertical data-partitioning method is performed to reduce the data communication cost effectively, and a data-multiplexing method is performed is performed to allow the training dataset to be reused and diminish the volume of data. From the perspective of task-parallel optimization, a dual parallel approach is carried out in the training process of RF, and a task Directed Acyclic Graph (DAG) is created according to the parallel training process of PRF and the dependence of the Resilient Distributed Datasets (RDD) objects. Then, different task schedulers are invoked for the tasks in the DAG. Moreover, to improve the algorithm's accuracy for large, high-dimensional, and noisy data, we perform a dimension-reduction approach in the training process and a weighted voting approach in the prediction process prior to parallelization. Extensive experimental results indicate the superiority and notable advantages of the PRF algorithm over the relevant algorithms implemented by Spark MLlib and other studies in terms of the classification accuracy, performance, and scalability. With the expansion of the scale of the random forest model and the Spark cluster, the advantage of the PRF algorithm is more obvious."",""1558-2183"","""",""10.1109/TPDS.2016.2603511"",""National Natural Science Foundation of China(grant numbers:61133005,61432005)"; National Natural Science Foundation of China(grant numbers:61370095,61472124,61202109,61472126,61672221); National Research Foundation of Qatar NPRP(grant numbers:8-519-1-108); Natural Science Foundation of Hunan Province of China(grant numbers:2015JJ4100,2016JJ4002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7557062"",""Apache spark";big data;cloud computing;data parallel;random forest;"task parallel"",""Training";Radio frequency;Optimization;Sparks;Big data;Decision trees;"Distributed databases"","""",""310"","""",""33"",""IEEE"",""31 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"A Recursive Hypergraph Bipartitioning Framework for Reducing Bandwidth and Latency Costs Simultaneously,""O. Selvitopi"; S. Acer;" C. Aykanat"",""Department of Computer Engineering, Bilkent University, Ankara, Turkey"; Department of Computer Engineering, Bilkent University, Ankara, Turkey;" Department of Computer Engineering, Bilkent University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""345"",""358"",""Intelligent partitioning models are commonly used for efficient parallelization of irregular applications on distributed systems. These models usually aim to minimize a single communication cost metric, which is either related to communication volume or message count. However, both volume- and message-related metrics should be taken into account during partitioning for a more efficient parallelization. There are only a few works that consider both of them and they usually address each in separate phases of a two-phase approach. In this work, we propose a recursive hypergraph bipartitioning framework that reduces the total volume and total message count in a single phase. In this framework, the standard hypergraph models, nets of which already capture the bandwidth cost, are augmented with message nets. The message nets encode the message count so that minimizing conventional cutsize captures the minimization of bandwidth and latency costs together. Our model provides a more accurate representation of the overall communication cost by incorporating both the bandwidth and the latency components into the partitioning objective. The use of the widely-adopted successful recursive bipartitioning framework provides the flexibility of using any existing hypergraph partitioner. The experiments on instances from different domains show that our model on the average achieves up to 52 percent reduction in total message count and hence results in 29 percent reduction in parallel running time compared to the model that considers only the total volume."",""1558-2183"","""",""10.1109/TPDS.2016.2577024"",""Scientific and Technological Research Council of Turkey (TUBITAK)(grant numbers:EEEAG-114E545)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7485866"",""Communication cost";bandwidth;latency;partitioning;hypergraph;recursive bipartitioning;load balancing;sparse matrix vector multiplication;"combinatorial scientific computing"",""Bandwidth";Measurement;Program processors;Solid modeling;Standards;Computational modeling;"Sparse matrices"","""",""4"","""",""32"",""IEEE"",""6 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Resilient 2-D Waveguide Communication Fabric for Hybrid Wired-Wireless NoC Design,""M. O. Agyeman"; Q. -T. Vien; A. Ahmadinia; A. Yakovlev; K. -F. Tong;" T. Mak"",""Computing and Immersive Technologies, University of Northampton, Northampton, United Kingdom"; School of Science and Technology, Middlesex University, London, United Kingdom; Department of Computer Science, California State University San Marcos, San Marcos, CA; School of EECE, University of Newcastle upon Tyne, Newcastle upon Tyne, Tyne and Wear, United Kingdom; Department of Electrical and Electronic Engineering, UCL, London, United Kingdom;" School of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""359"",""373"",""Hybrid wired-wireless Network-on-Chip (WiNoC) has emerged as an alternative solution to the poor scalability and performance issues of conventional wireline NoC design for future System-on-Chip (SoC). Existing feasible wireless solution for WiNoCs in the form of millimeter wave (mm-Wave) relies on free space signal radiation which has high power dissipation with high degradation rate in the signal strength per transmission distance. Moreover, over the lossy wireless medium, combining wireless and wireline channels drastically reduces the total reliability of the communication fabric. Surface wave has been proposed as an alternative wireless technology for low power on-chip communication. With the right design considerations, the reliability and performance benefits of the surface wave channel could be extended. In this paper, we propose a surface wave communication fabric for emerging WiNoCs that is able to match the reliability of traditional wireline NoCs. First, we propose a realistic channel model which demonstrates that existing mm-Wave WiNoCs suffers from not only free-space spreading loss (FSSL) but also molecular absorption attenuation (MAA), especially at high frequency band, which reduces the reliability of the system. Consequently, we employ a carefully designed transducer and commercially available thin metal conductor coated with a low cost dielectric material to generate surface wave signals with improved transmission gain. Our experimental results demonstrate that the proposed communication fabric can achieve a 5 dB operational bandwidth of about 60 GHz around the center frequency (60 GHz). By improving the transmission reliability of wireless layer, the proposed communication fabric can improve maximum sustainable load of NoCs by an average of 20:9 and 133:3 percent compared to existing WiNoCs and wireline NoCs, respectively."",""1558-2183"","""",""10.1109/TPDS.2016.2575836"",""University of Northampton";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484291"",""Hybrid wired-wireless Network-on-Chip";reliability;surface wave;mm-Wave;WiNoC;waveguide;"wireless channel"",""Wireless communication";Fabrics;Reliability;Surface waves;System-on-chip;Channel models;"Optical surface waves"","""",""31"","""",""31"",""OAPA"",""2 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Self-Routing on-Chip Network,""A. Y. Oruç"",""Department of Electrical and Computer Engineering, University of Maryland, College Park, MD"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1229"",""1239"",""This paper introduces a new nonblocking self-routing network, called a multi-root binary tree, which may be used to interconnect the cores in multicore chips. The multi-root binary tree network differs from other binary tree-based networks in that the cores are placed at the roots rather than the leaves or the interior nodes of the trees. The self-routing property of a multi-root binary tree is built on the concept of replication and clustering. A new replication and clustering method, called the triangular shift pairing is given to connect the cores together over dedicated paths. It is shown that connecting cores in clusters requires at least n=2 columns of pairings for an n-core network in a grid layout model and triangular shift wiring method matches this lower bound. The replication and clustering concept leads to a simple self-routing scheme that pairs cores by decoding both unicast and multicast connection requests using lg n-bit cluster address bits. In particular, it is established that cores can identify the cluster with which they pair with other cores using simple modulo addition of their own ids and addresses of the targeted cores in 2lg n single bit-decoding steps. It is also shown that, if connection requests are uniformly distributed among the cores then blocking due to target conflicts can be avoided if cores serve connection requests at a small multiple of the frequencies with which they receive them."",""1558-2183"","""",""10.1109/TPDS.2016.2622266"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723924"",""Bit decoding";interconnection network;multi-root binary tree;on-chip network;"self-routing network"",""System-on-chip";Switches;Measurement;Routing;Network topology;Integrated circuit interconnections;"Unicast"","""",""1"","""",""30"",""IEEE"",""27 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Survey of Techniques for Architecting and Managing GPU Register File,""S. Mittal"",""Future Technologies Group, Oak Ridge National Laboratory, Oak Ridge, TN"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""16"",""28"",""To support their massively-multithreaded architecture, GPUs use very large register file (RF) which has a capacity higher than even L1 and L2 caches. In total contrast, traditional CPUs use tiny RF and much larger caches to optimize latency. Due to these differences, along with the crucial impact of RF in determining GPU performance, novel and intelligent techniques are required for managing GPU RF. In this paper, we survey the techniques for designing and managing GPU RF. We discuss techniques related to performance, energy and reliability aspects of RF. To emphasize the similarities and differences between the techniques, we classify them along several parameters. The aim of this paper is to synthesize the state-of-art developments in RF management and also stimulate further research in this area."",""1558-2183"","""",""10.1109/TPDS.2016.2546249"",""U.S. Department of Energy"; Office of Science; Advanced Scientific Computing Research;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448930"",""Review";classification;GPGPU;GPU;register file;reliability;performance;power management;non-volatile memory;"embedded DRAM (eDRAM)"",""Radio frequency";Graphics processing units;Registers;Random access memory;Reliability;Message systems;"Power demand"","""",""21"","""",""55"",""IEEE"",""7 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Technique for Efficient Query Estimation over Distributed Data Streams,""Z. Shah"; A. N. Mahmood; Z. Tari;" A. Y. Zomaya"",""University of New South Wales, Canberra, Australia"; Department of Computer Science, School of Engineering and Mathematical Sciences, La Trobe University, Melbourne, Vic, Australia; School of Science, RMIT, Melbourne, Vic, Australia;" Center for Distributed and High Performance Computing, University of Sydney, Camperdown, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2770"",""2783"",""Distributed data stream mining in a sliding window has emerged recently, due to its applications in many domains including large Telecoms and Internet Service Providers, financial tickers, ATM and credit card operations in banks and transactions in retail chains. Many of these large-scale applications prohibit monitoring data centrally at a single location due to their massive volume of the data"; therefore, data acquisition, processing, and mining tasks are often distributed to a number of processing nodes, which monitor their local streams and exchange only the summary of data either periodically or on demand. While this offer many advantages, distributed stream applications possess significant challenges including problems related to an online analysis of the recent data, communication efficiency and various estimation of various complex queries. There are few existing techniques which solve problems related to distributed sliding window data stream; however, those techniques are focused on solving only simple problems and require high space, query, and communication cost, which can be a bottleneck for many of these large scale applications. In this paper, we propose an efficient query estimation technique by constructing a small sketch of the data stream. The constructed sketch uses a deterministic sliding window model and can estimate various complex queries, for both centralized and distributed applications; including point queries (i.e., range queries and heavy hitter queries), quantiles, inner product, and self-join size queries, with deterministic guarantees on the precision. The proposed approach improves upon recent existing work for these problems, in terms of the memory and query cost in a centralized setting and in terms of communication cost and merge complexity in a distributed setting. It requires O(1/ε2 1 log (εN)) memory (where 0 <; ε <;" 1 is a user defined parameter), can provide estimates in O(1) time, and processes each incoming record in O(1) amortized time. Detailed experimental analysis, both in centralized and distributed settings demonstrates that in practice the proposed approach uses about six times less memory, and has about eight times less query time when compared to ECM sketches. In a distributed application, the proposed technique also significantly improves (around seven times) on the communication cost between distributed sites."",""1558-2183"","""",""10.1109/TPDS.2017.2693983"",""Australian Research Council (ARC)(grant numbers:LP150101213)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898394"",""Distributed query estimation";distributed heavy hitters;"distributed data streams"",""Distributed databases";Memory management;Monitoring;Histograms;Estimation;"Radiation detectors"","""",""9"","""",""23"",""IEEE"",""12 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Very Fast Trace-Driven Simulation Platform for Chip-Multiprocessors Architectural Explorations,""M. E. S. Elrabaa"; A. Hroub; M. F. Mudawar; A. Al-Aghbari; M. Al-Asli;" A. Khayyat"",""Computer Engineering Department, King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia"; Computer Engineering Department, King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia; Computer Engineering Department, King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia; Computer Engineering Department, King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia; Computer Engineering Department, King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia;" Computer Engineering Department, King Fahd University of Petroleum & Minerals, Dhahran, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3033"",""3045"",""Simulation is the main tool for computer architects and parallel application developers for developing new architectures and parallel algorithms on many-core machines. Simulating a many-core architecture represent a challenge to software simulators even with parallelization of these SW on multi-cores. Field Programmable Gate Arrays offer an excellent implementation platform due to inherent parallelism. Existing FPGA-based simulators however, are mostly execution-driven which consumes too many FPGA resources. Hence, they still trade-off accuracy with simulation speed as SW simulators do. In this work, an application-level trace-driven FPGA-based many-core simulator is presented. A parameterized Verilog template was developed that can generate any number of simulator tiles. The input trace has an architecturally agnostic format that is directly interpreted by the FPGA-based timing model to re-construct the execution events of the original application with accurate timing. This allows fitting a large number of simulation tiles on a single FPGA without sacrificing simulation speed or accuracy. Experimental results show that the simulator's average accuracy is ~14 percent with simulation speeds ranging from 100's of MIPs to over 2,200 MIPS for a 16-core target architecture. Hence, with accuracy similar to SW simulators, its speed is higher than all other FPGA-based simulators."",""1558-2183"","""",""10.1109/TPDS.2017.2713782"",""King Fahd University of Petroleum and Minerals"; NSTIP(grant numbers:12-INF3015-04);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944586"",""Trace-driven simulations";hardware simulators;chip-multiprocessors;multithreading;"field-programmable gate arrays"",""Field programmable gate arrays";Timing;Software;Tools;Computational modeling;"Multicore processing"","""",""3"","""",""37"",""IEEE"",""8 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating Decision Tree Based Traffic Classification on FPGA and Multicore Platforms,""D. Tong"; Y. R. Qu;" V. K. Prasanna"",""VMware, Inc., 3401 Hillview Ave, Palo Alto, CA"; Xilinx, Inc., 2100 Logic Dr, San Jose, CA;" Department of Electrical Engineering, University of Southern California, Los Angeles, CA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Oct 2017"",""2017"",""28"",""11"",""3046"",""3059"",""Machine learning (ML) algorithms have been shown to be effective in classifying a broad range of applications in the Internet traffic. In this paper, we propose algorithms and architectures to realize online traffic classification using flow level features. First, we develop a traffic classifier based on C4.5 decision tree algorithm and Entropy-MDL (Minimum Description Length) discretization algorithm. It achieves an overall accuracy of 97.92 percent for classifying eight major applications. Next we propose approaches to accelerate the classifier on FPGA (Field Programmable Gate Array) and multicore platforms. We optimize the original classifier by merging it with discretization. Our implementation of this optimized decision tree achieves 7500+ Million Classifications Per Second (MCPS) on a state-of-the-art FPGA platform and 75-150 MCPS on two state-of-the-art multicore platforms. We also propose a divide and conquer approach to handle imbalanced decision trees. Our implementation of the divide-and-conquer approach achieves 10,000+ MCPS on a state-of-the-art FPGA platform and 130-340 MCPS on two state-of-the-art multicore platforms. We conduct extensive experiments on both platforms for various application scenarios to compare the two approaches."",""1558-2183"","""",""10.1109/TPDS.2017.2714661"",""U.S. National Science Foundation (NSF)(grant numbers:CCF-1320211,ACI-1339756)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7946151"",""Traffic classification";machine learning;decision tree;multicore;FPGA;"high throughput"",""Decision trees";Field programmable gate arrays;Classification algorithms;Multicore processing;Machine learning algorithms;Throughput;"Acceleration"","""",""53"","""",""40"",""IEEE"",""12 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"ACStor: Optimizing Access Performance of Virtual Disk Images in Clouds,""S. Wu"; Y. Wang; W. Luo; S. Di; H. Chen; X. Xu; R. Zheng;" H. Jin"",""Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China"; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; Argonne National Laboratory, Lemont, IL; Chuzhou University, Chuzhou, China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China;" Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2414"",""2427"",""In virtualized data centers, virtual disk images (VDIs) serve as the containers in virtual environment, so their access performance is critical for the overall system performance. Some distributed VDI chunk storage systems have been proposed in order to alleviate the I/O bottleneck for VM management. As the system scales up to a large number of running VMs, however, the overall network traffic would become unbalanced with hot spots on some VMs inevitably, leading to I/O performance degradation when accessing the VMs. In this paper, we propose an adaptive and collaborative VDI storage system (ACStor) to resolve the above performance issue. In comparison with the existing research, our solution is able to dynamically balance the traffic workloads in accessing VDI chunks, based on the run-time network state. Specifically, compute nodes with lightly loaded traffic will be adaptively assigned more chunk access requests from remote VMs and vice versa, which can effectively eliminate the above problem and thus improves the I/O performance of VMs. We implement a prototype based on our ACStor design, and evaluate it by various benchmarks on a real cluster with 32 nodes and a simulated platform with 256 nodes. Experiments show that under different network traffic patterns of data centers, our solution achieves up to 2-8× performance gain on VM booting time and VM's I/O throughput, in comparison with the other state-of-the-art approaches."",""1558-2183"","""",""10.1109/TPDS.2017.2675988"",""863 Hi-Tech Research and Development Program(grant numbers:2015AA01A203)"; National Key Research and Development Program(grant numbers:2016YFB1000501); National Science Foundation of China(grant numbers:61472151,61232008); National Science Foundation of Anhui Province(grant numbers:1608085QF147); Excellent Youth Scholars in Colleges and Universities of Anhui Province(grant numbers:gxyqZD2016332); U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research Program(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7866840"",""Virtual disk image (VDI)";collaborative;uneven traffic;"adaptive"",""Collaboration";Peer-to-peer computing;Cloud computing;Degradation;Booting;Computational modeling;"Containers"","""","""","""",""32"",""IEEE"",""2 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Adaptive Particle Swarm Optimization with Heterogeneous Multicore Parallelism and GPU Acceleration,""M. P. Wachowiak"; M. C. Timson;" D. J. DuVal"",""Department of Computer Science and Mathematics, Nipissing University, North Bay, ON, Canada"; Department of Computer Science and Mathematics, Nipissing University, North Bay, ON, Canada;" Department of Computer Science and Mathematics, Nipissing University, North Bay, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2784"",""2793"",""Much progress has recently been made in global optimization, with particular attention devoted to robust nature-inspired stochastic methods for difficult, high-dimensional problems. This paper presents a computational study of an adaptation of one such method, particle swarm optimization (PSO), which is analyzed for parallelization on readily-available heterogeneous parallel computational hardware: specifically, multicore technologies accelerated by graphics processing units (GPUs), as well as Intel Xeon Phi co-processors accelerated with vectorization. In this heterogeneous approach, computationally-intensive, task-parallel components are performed with multicore parallelism and data-parallel elements are executed via co-processing (GPUs or vectorization). A computationally intensive adaptive PSO technique is parallelized according to this schema. In experiments with two high-dimensional and complex functions, large speedups can be obtained. Thus, a heterogeneous approach mitigates the time complexity of PSO adaptations, suggesting that other time-intensive stochastic methods can also benefit from the techniques proposed here."",""1558-2183"","""",""10.1109/TPDS.2017.2687461"",""Natural Sciences and Engineering Research Council of Canada(grant numbers:386586-2011)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886331"",""Applications";parallel and vector implementations;optimization;stochastic programming;"unconstrained optimization"",""Multicore processing";Graphics processing units;Cost function;Parallel processing;Acceleration;"Hardware"","""",""28"","""",""56"",""IEEE"",""24 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Aggregating Uncertain Incast Transfers in BCube-Like Data Centers,""D. Guo"",""College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""934"",""946"",""Many data-intensive applications like MapReduce are network-bound in data centers, due to transfer massive amount of flows across successive processing stages. Data flows in such an incast or shuffle transfer are highly correlated and aggregated at the receiver side. Prior work aims to aggregate correlated flows of each transfer, during the transmission phase as early as possible, so as to directly lower down the network traffic. However, many applications do not constrain the flows' endpoints of each transfer as long as certain constraints are satisfied. Such uncertain transfers bring new opportunities and challenges to lower down the network traffic than prior deterministic transfers. In this paper, we focus on aggregating an uncertain incast transfer and minimizing the amount of caused network traffic. Prior approaches, relying on deterministic incast transfers, remain inapplicable. This paper makes the first step towards the study of aggregating uncertain incast transfer. We propose efficient approaches from two aspects, i.e., the initialization of uncertain senders and the incast tree building. We first design two initialization methods to pick the best deterministic senders for an uncertain incast transfer, so as to form the least number of disjoint sender groups. Thus, flows from each group would be aggregated as one flow on a common one-hop neighbor, irrespective of the location of a picked receiver. Accordingly, we propose the interstage-based and intrastage-based incast tree building methods to exploit the benefits of our initialization methods. We provide evidence to show that our approach can achieve the benefits of in-network aggregation for any uncertain incast transfer. Moreover, an uncertain incast transfer significantly outperforms any related deterministic one, in terms of the reduced network traffic and the saved network resources."",""1558-2183"","""",""10.1109/TPDS.2016.2612660"",""National Natural Science Foundation"; Outstanding Excellent young scholars of China(grant numbers:61422214); National Basic Research Program (973 program)(grant numbers:2014CB347800); Program for New Century Excellent Talents in University; Hunan Provincial Natural Science Fund for Distinguished Young Scholars(grant numbers:2016JJ1002); NUDT(grant numbers:JQ14-05-02,ZDYYJCYJ20140601);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574280"",""Data center";uncertain incast;"data aggregation"",""Receivers";Buildings;Servers;Data models;Computational modeling;Data transfer;"Distributed databases"","""",""10"","""",""28"",""IEEE"",""22 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"aHDFS: An Erasure-Coded Data Archival System for Hadoop Clusters,""Y. Chen"; Y. Zhou; S. Taneja; X. Qin;" J. Huang"",""Department of Computer Science and Software Engineering, Auburn University, AL"; Department of Computer Science and Software Engineering, Auburn University, AL; Department of Computer Science and Software Engineering, Auburn University, AL; Department of Computer Science and Software Engineering, Auburn University, AL;" Wuhan National Lab. for Optoelectronics, Huazhong University of Science and Technology (HUST), Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3060"",""3073"",""In this paper, we propose an erasure-coded data archival system called aHDFS for Hadoop clusters, where RS(k + r";" k) codes are employed to archive data replicas in the Hadoop distributed file system or HDFS. We develop two archival strategies (i.e., aHDFS-Grouping and aHDFS-Pipeline) in aHDFSto speed up the data archival process. aHDFS-Groupinga MapReduce-based data archiving scheme - keeps each mapper's intermediate output Key-Value pairs in a local key-value store. With the local store in place, aHDFS-Grouping merges all the intermediate key-value pairs with the same key into one single key-value pair, followed by shuffling the single Key-Value pair to reducers to generate final parity blocks. aHDFS-Pipeline forms a data archival pipeline using multiple data node in a Hadoop cluster. aHDFS-Pipeline delivers the merged single key-value pair to a subsequent node's local key-value store. Last node in the pipeline is responsible for outputting parity blocks. We implement aHDFS in a real-world Hadoop cluster. The experimental results show that aHDFS-Grouping and aHDFS-Pipeline speed up Baseline's shuffle and reduce phases by a factor of 10 and 5, respectively. When block size is larger than 32 MB, aHDFS improves the performance of HDFS-RAID and HDFS-EC by approximately 31.8 and 15.7 percent, respectively."",""1558-2183"","""",""10.1109/TPDS.2017.2706686"",""U.S. National Science Foundation(grant numbers:IIS-1618669,CNS-0917137,CCF-0845257)"; National Science Foundation of China(grant numbers:61572209); Fundamental Research Funds for the Central Universities(grant numbers:HUST: 2015MS006);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932189"",""Hadoop distributed file system";replica-based storage clusters;archival performance;erasure codes;"parallel encoding"",""Mathematical model";Distributed databases;Redundancy;Encoding;Programming;Pipelines;"Data models"","""",""14"","""",""32"",""IEEE"",""19 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Algorithms for Balanced Graph Colorings with Applications in Parallel Computing,""H. Lu"; M. Halappanavar; D. Chavarría-Miranda; A. H. Gebremedhin; A. Panyala;" A. Kalyanaraman"",""Washington State University, Pullman, WA"; Pacific Northwest National Laboratory, Richland, WA; Pacific Northwest National Laboratory, Richland, WA; Washington State University, Pullman, WA; Pacific Northwest National Laboratory, Richland, WA;" Washington State University, Pullman, WA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1240"",""1256"",""Graph coloring-in a generic sense-is used to identify subsets of independent tasks in parallel scientific computing applications. Traditional coloring heuristics aim to reduce the number of colors used as that number also corresponds to the number of parallel steps in the application. However, if the color classes produced have a skew in their sizes, utilization of hardware resources becomes inefficient, especially for the smaller color classes. Equitable coloring is a theoretical formulation of coloring that guarantees a perfect balance among color classes, and its practical relaxation is referred to here as balanced coloring. In this paper, we consider balanced coloring models in the context of parallel computing. The goal is to achieve a balanced coloring of an input graph without increasing the number of colors that an algorithm oblivious to balance would have used. We propose and study multiple heuristics that aim to achieve such a balanced coloring for two variants of coloring problem, distance-1 coloring (the standard coloring problem) and partial distance-2 coloring (defined on a bipartite graph). We present parallelization approaches for multi-core and manycore architectures and cross-evaluate their effectiveness with respect to the quality of balance achieved and performance. Furthermore, we study the impact of the proposed balanced coloring heuristics on a concrete application-viz. parallel community detection, which is an example of an irregular application. In addition, we propose several extensions to our basic balancing schemes and evaluate their balancing efficacy and performance characteristics. The thorough treatment of balanced coloring presented in this paper from algorithms to application is expected to serve as a valuable resource to parallel application developers who seek to improve parallel performance of their applications using coloring."",""1558-2183"","""",""10.1109/TPDS.2016.2620142"",""U.S. Department of Energy(grant numbers:DE-SC-0006516,DE-AC05-76RL01830,DE-SC-0010205)"; U.S. Department of Defense; Autotuning for Power, Energy & Resilience (ATPER); National Science Foundation(grant numbers:IIS-1553528);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7605439"",""Balanced coloring";parallel graph coloring;distance-1 coloring;partial distance-2 coloring;Tilera manycore architecture;community detection;"graph algorithms"",""Color";Image color analysis;Context;Standards;Bipartite graph;"Processor scheduling"","""",""15"","""",""35"",""IEEE"",""21 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Adaptive Parallel Algorithm for Computing Connected Components,""C. Jain"; P. Flick; T. Pan; O. Green;" S. Aluru"",""Georgia Institute of Technology, Atlanta, GA"; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA; Georgia Institute of Technology, Atlanta, GA;" Georgia Institute of Technology, Atlanta, GA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2428"",""2439"",""We present an efficient distributed memory parallel algorithm for computing connected components in undirected graphs based on Shiloach-Vishkin's PRAM approach. We discuss multiple optimization techniques that reduce communication volume as well as load-balance the algorithm. We also note that the efficiency of the parallel graph connectivity algorithm depends on the underlying graph topology. Particularly for short diameter graph components, we observe that parallel Breadth First Search (BFS) method offers better performance. However, running parallel BFS is not efficient for computing large diameter components or large number of small components. To address this challenge, we employ a heuristic that allows the algorithm to quickly predict the type of the network by computing the degree distribution and follow the optimal hybrid route. Using large graphs with diverse topologies from domains including metagenomics, web crawl, social graph and road networks, we show that our hybrid implementation is efficient and scalable for each of the graph types. Our approach achieves a runtime of 215 seconds using 32 K cores of Cray XC30 for a metagenomic graph with over 50 billion edges. When compared against the previous state-of-the-art method, we see performance improvements up to 24 ×."",""1558-2183"","""",""10.1109/TPDS.2017.2672739"",""National Science Foundation(grant numbers:IIS-1416259)"; National Science Foundation(grant numbers:CNS-1229081);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862286"",""Parallel algorithms";distributed memory;breadth first search;"undirected graphs"",""Partitioning algorithms";Algorithm design and analysis;Heuristic algorithms;Phase change random access memory;Topology;Parallel algorithms;"Runtime"","""",""16"","""",""42"",""IEEE"",""22 Feb 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Alternating Direction Method Approach to Cloud Traffic Management,""C. Feng"; H. Xu;" B. Li"",""School of Engineering, University of British Columbia, Kelowna, BC, Canada"; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong;" Edward S. Rogers Sr. Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2145"",""2158"",""In this paper, we introduce a unified framework for studying various cloud traffic management problems, ranging from geographical load balancing to backbone traffic engineering. We first abstract these real-world problems as a multi-facility resource allocation problem, and then present two distributed optimization algorithms by exploiting the special structure of the problem. Our algorithms are inspired by Alternating Direction Method of Multipliers (ADMM), enjoying a number of unique features. Compared to dual decomposition, they converge with non-strictly convex objective functions";" compared to other ADMM-type algorithms, they not only achieve faster convergence under weaker assumptions, but also have lower computational complexity and lower message-passing overhead. The simulation results not only confirm these desirable features of our algorithms, but also highlight several additional advantages, such as scalability and fault-tolerance."",""1558-2183"","""",""10.1109/TPDS.2017.2658620"",""NSERC(grant numbers:RGPIN-2016-05310)"; RGC(grant numbers:ECS-21201714,CRF-C7036-15G);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833029"",""Cloud traffic management";load balancing;traffic engineering;datacenters;ADMM;"distributed optimization"",""Load management";Convergence;Cloud computing;Servers;Optimization;Resource management;"Distributed databases"","""",""27"","""",""52"",""IEEE"",""25 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Economical and SLO-Guaranteed Cloud Storage Service Across Multiple Cloud Service Providers,""G. Liu"; H. Shen;" H. Wang"",""Department of ECE, Clemson University, Clemson, SC"; Department of Computer Science, University of Virginia, Charlottesville, VA;" Department of Computer Science, University of Virginia, Charlottesville, VA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2440"",""2453"",""It is important for cloud service brokers to provide a multi-cloud storage service to minimize their payment cost to cloud service providers (CSPs) while providing service level objective (SLO) guarantee to their customers. Many multi-cloud storage services have been proposed or payment cost minimization or SLO guarantee. However, no previous works fully leverage the current cloud pricing policies (such as resource reservation pricing) to reduce the payment cost. Also, few works achieve both cost minimization and SLO guarantee. In this paper, we propose a multi-cloud Economical and SLO-guaranteed Storage Service (ES3), which determines data allocation and resource reservation schedules with payment cost minimization and SLO guarantee. ES3 incorporates (1) a coordinated data allocation and resource reservation method, which allocates each data item to a datacenter and determines the resource reservation amount on datacenters by leveraging all the pricing policies";" (2) a genetic algorithm based data allocation adjustment method, which reduce data Get/Put rate variance in each datacenter to maximize the reservation benefit. We also propose several algorithms to enhance the cost efficient and SLO guarantee performance of ES3 including i) dynamic request redirection, ii) grouped Gets for cost reduction, iii) lazy update for cost-efficient Puts, and iv) concurrent requests for rigid Get SLO guarantee. Our trace-driven experiments on a supercomputing cluster and on real clouds (i.e., Amazon S3, Windows Azure Storage and Google Cloud Storage) show the superior performance of ES3 in payment cost minimization and SLO guarantee in comparison with previous methods."",""1558-2183"","""",""10.1109/TPDS.2017.2675422"",""U.S. NSF(grant numbers:IIS-1354123,CNS-1733596,CNS-1249603,CNS-1049947,CNS-0917056,CNS-1025652)"; Microsoft Research(grant numbers:8300751);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7865999"",""Cloud storage";SLO;data availability;"payment cost minimization"",""Cloud computing";Resource management;Pricing;Minimization;Schedules;Genetic algorithms;"Google"","""",""20"","""",""32"",""IEEE"",""1 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Efficient Protection Technique for Last Level STT-RAM Caches in Multi-Core Processors,""Z. Azad"; H. Farbeh; A. M. H. Monazzah;" S. G. Miremadi"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1564"",""1577"",""Due to serious problems of SRAM-based caches in nano-scale technologies, researchers seek for new alternatives. Among the existing options, STT-RAM seems to be the most promising alternative. With high density and negligible leakage power, STT-RAMs open a new doorto respond to future demands of multi-core systems, i.e., large on-chip caches. However, several problems in STT-RAMs should be overcome to make it applicable in on-chip caches. High probability of write error due to stochastic switching is a major problem in STT-RAMs. Conventional Error-Correcting Codes (ECCs) impose significant area and energy consumption overheads to protect STT-RAM caches. These overheads in multi-core processors with large last-level caches are not affordable. In this paper, we propose Asymmetry-Aware Protection Technique (A2PT) to efficiently protect the STT-RAM caches. A2PT benefits from error rate asymmetry of STT-RAM write operations to provide the required level of cache protection with significantly lower overheads. Compared with the conventional ECC configuration, the evaluation results show that A2PT reduces the area and energy consumption overheads by about 42 and 50 percent, respectively, while providing the same level of protection. Moreover, A2PT decreases the number of bit switching in write operations by 28 percent, which leads to about 25 percent saving in write energy consumption."",""1558-2183"","""",""10.1109/TPDS.2016.2628742"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744670"",""Asymmetric switching";error-correcting codes (ECCs);multi-core processors;non-uniform protection;"STT-RAM caches"",""Switches";Magnetic tunneling;Error analysis;Error correction codes;Random access memory;Energy consumption;"Magnetic fields"","""",""23"","""",""42"",""IEEE"",""15 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Energy-Efficient Directory Based Multicore Architecture with Wireless Routers to Minimize the Communication Latency,""A. Asaduzzaman"; K. K. Chidella;" D. Vardha"",""Wichita State University, Wichita, KS"; Wichita State University, Wichita, KS;" Wichita State University, Wichita, KS"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""374"",""385"",""Multicore architectures suffer from high core-to-core communication latency primarily due to the cache's dynamic behavior. Studies suggest that a directory-approach can be helpful to reduce communication latency by storing the cached block information. Recent studies also indicate that a wireless router has potential to help decrease communication latency in multicore architectures. In this work, we propose a directory based multicore architecture with wireless routers to minimize communication latency. We simulate systems with mesh (used in the Standford Directory Architecture for SHared memory (DASH) architecture), wireless network-on-chip (WNoC), and the proposed directory based architecture with wireless routers. According to the experimental results, our proposed architecture outperforms the WNoC and the mesh architectures. It is observed that the proposed architecture helps decrease the communication delay by up to 15.71 percent and the total power consumption by up to 67.58 percent when compared with the mesh architecture. Similarly, the proposed architecture helps decrease the communication delay by up to 10.00 percent and the total power consumption by upto 58.10 percent when compared with the WNoC architecture. This is due to the fact that the proposed directory based mechanism helps reduce the number of core-to-core communication and the wireless routers help reduce the total number of hops."",""1558-2183"","""",""10.1109/TPDS.2016.2571282"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475919"",""Communication latency";directory based multicore architecture;energy-efficient multicore architecture;mesh topology;wireless network-on-chip architecture;"wireless router"",""Multicore processing";Wireless communication;Network topology;Topology;Routing;"System-on-chip"","""",""6"","""",""41"",""IEEE"",""20 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Energy-Efficient Storage Strategy for Cloud Datacenters Based on Variable K-Coverage of a Hypergraph,""T. Yang"; H. Pen; W. Li; D. Yuan;" A. Y. Zomaya"",""Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianjin, China"; Key Laboratory of Smart Grid of Ministry of Education, Tianjin University, Tianjin, China; School of Information Technology, The University of Sydney, Camperdown, NSW, Australia; School of Information Technology, The University of Sydney, Camperdown, NSW, Australia;" School of Information Technology, The University of Sydney, Camperdown, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3344"",""3355"",""Distributed storage systems, e.g., Hadoop Distributed File System (HDFS), have been widely used in datacenters for handling large amounts of data due to their excellent performance in terms of fault tolerance, reliability and scalability. However, these storage systems usually adopt the same replication and storage strategy to guarantee data availability, i.e., creating the same number of replicas for all data sets and randomly storing them across data nodes. Such strategies do not fully consider the difference requirements of data availability on different data sets. More servers than necessary should thus be used to store replicas of rarely-used data, which will lead to increased energy consumption. To address this issue, we propose an energy-efficient storage strategy for cloud datacenters based on a novel hypergraph coverage model. According to users' requirements of data availability in different applications, our proposed algorithm can selectively determine the corresponding minimum hyperedge coverage, which represents the minimum set of data nodes required in the datacenter. Hence, some other data nodes can be turned off for the purpose of energy saving. We have also implemented our proposed algorithm as a dynamic runtime strategy in a HDFS based prototype datacenter for performance evaluation. Experimental results show that the variable hypergraph coverage based strategy can not only reduce energy consumption, but can also improve the network performance in the datacenter."",""1558-2183"","""",""10.1109/TPDS.2017.2723004"",""National Key Research and Development Program of China(grant numbers:2017YFB0903000)"; National Natural Science Foundation of China(grant numbers:61571324); Natural Science Foundation of Tianjin(grant numbers:16JCZDJC30900); National program of international S&T cooperation(grant numbers:2013DFA11040); Australian Research Council(grant numbers:LP150101213);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968334"",""Energy saving";cloud datacenters;data availability;hypergraph;"minimum K-coverage"",""Distributed databases";Energy efficiency;Cloud computing;Energy storage;Heuristic algorithms;Energy consumption;"Algorithm design and analysis"","""",""7"","""",""33"",""IEEE"",""4 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Exploration of Designing a Hybrid Scale-Up/Out Hadoop Architecture Based on Performance Measurements,""Z. Li"; H. Shen; W. Ligon;" J. Denton"",""Department of Electrical and Computer Engineering, Clemson University, Clemson, SC"; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC;" CITI group of CCIT, SC"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""386"",""400"",""Scale-up machines perform better for jobs with small and median (KB, MB) data sizes, while scale-out machines perform better for jobs with large (GB, TB) data size. Since a workload usually consists of jobs with different data size levels, we propose building a hybrid Hadoop architecture that includes both scale-up and scale-out machines, which however is not trivial. The first challenge is workload data storage. Thousands of small data size jobs in a workload may overload the limited local disks of scale-up machines. Jobs from scale-up and scale-out machines may both request the same set of data, which leads to data transmission between the machines. The second challenge is to automatically schedule jobs to either scale-up or scale-out cluster to achieve the best performance. We conduct a thorough performance measurement of different applications on scale-up and scale-out clusters, configured with Hadoop Distributed File System (HDFS) and a remote file system (i.e., OFS), respectively. We find that using OFS rather than HDFS can solve the data storage challenge. Also, we identify the factors that determine the performance differences on the scale-up and scale-out clusters and their cross points to make the choice. Accordingly, we design and implement the hybrid scale-up/out Hadoop architecture. Our trace-driven experimental results show that our hybrid architecture outperforms both the traditional Hadoop architecture with HDFS and with OFS in terms of job completion time, throughput and job failure rate."",""1558-2183"","""",""10.1109/TPDS.2016.2573820"",""U.S. NSF(grant numbers:NSF-1404981,NSF 1228312 MRI,IIS-1354123,CNS-1254006)"; Microsoft Research Faculty Fellowship(grant numbers:8300751);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7480403"",""Hadoop";scale-up;scale-out;remote file system;"hybrid architecture"",""Computer architecture";Facebook;Measurement;Distributed databases;Random access memory;"Data communication"","""",""22"","""",""34"",""IEEE"",""27 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Incrementally Scalable and Cost-Efficient Interconnection Structure for Data Centers,""J. Xie"; Y. Deng; G. Min;" Y. Zhou"",""Department of Computer Science, Jinan University, Guangzhou, China"; Department of Computer Science, Jinan University, Guangzhou, China; College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, United Kingdom;" Department of Computer Science, Jinan University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1578"",""1592"",""The explosive growth in the volume of data storing and complexity of data processing drive data center networks (DCNs) to become incrementally scalable and cost-efficient while to maintain high network capacity and fault tolerance. To address these challenges, this paper proposes a new structure, called Totoro, which is defined recursively and hierarchically: dual-port servers and commodity switches are used to make Totoro affordable"; a bunch of servers are connected to an intra-switch to form a basic partition;" to construct a high-level structure, a half of the backup ports of servers in the low-level structures are connected by inter-switches in order to incrementally build a larger partition. Totoro is incrementally scalable since expanding the structure does not require any rewiring or routing alteration. We further design a distributed and fault-tolerant routing protocol to handle multiple types of failures. Experimental results demonstrate that Totoro is able to satisfy the demands of fault tolerance and high throughput. Furthermore, architecture analysis indicates that Totoro balances between performance and costs in terms of robustness, structural properties, bandwidth, economic costs and power consumption."",""1558-2183"","""",""10.1109/TPDS.2016.2629508"",""NSF of China(grant numbers:61272073,61572232)"; NSF of Guangdong Province(grant numbers:S2013020012865);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745911"",""Data center network";scalability;network capacity;cost efficiency;"fault tolerance"",""Servers";Ports (Computers);Fault tolerance;Fault tolerant systems;Scalability;Routing;"Computer architecture"","""",""34"","""",""32"",""IEEE"",""16 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Online Auction Mechanism for Dynamic Virtual Cluster Provisioning in Geo-Distributed Clouds,""W. Shi"; C. Wu;" Z. Li"",""Department of Computer Science, University of Hong Kong, Hong Kong"; Department of Computer Science, University of Hong Kong, Hong Kong;" Department of Computer Science, University of Calgary, Calgary, AB, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""677"",""688"",""It is common for cloud users to require clusters of inter-connected virtual machines (VMs) in a geo-distributed IaaS cloud, to run their services. Compared to isolated VMs, key challenges on dynamic virtual cluster (VC) provisioning (computation + communication resources) lie in two folds: (1) optimal placement of VCs and inter-VM traffic routing involve NP-hard problems, which are non-trivial to solve offline, not to mention if an online efficient algorithm is sought";" (2) an efficient pricing mechanism is missing, which charges a market-driven price for each VC as a whole upon request, while maximizing system efficiency or provider revenue over the entire span. This paper proposes efficient online auction mechanisms to address the above challenges. We first design SWMOA, a novel online algorithm for dynamic VC provisioning and pricing, achieving truthfulness, individual rationality, computation efficiency, and (1 + 2 log μ)-competitiveness in social welfare, where m is related to the problem size. Next, applying a randomized reduction technique, we convert the social welfare maximizing auction into a revenue maximizing online auction, PRMOA, achieving O(log μ)-competitiveness in provider revenue, as well as truthfulness, individual rationality and computation efficiency. We investigate auction design in different cases of resource cost functions in the system. We validate the efficacy of the mechanisms through solid theoretical analysis and trace-driven simulations."",""1558-2183"","""",""10.1109/TPDS.2016.2601905"",""RGC"; HKU(grant numbers:718513,17204715,17225516,C7036-15G); CRF; NSERC; MITACS;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7548335"",""Cloud computing";auction mechanism design;online algorithm;"resource allocation"",""Cloud computing";Bandwidth;Resource management;Heuristic algorithms;Algorithm design and analysis;Pricing;"Routing"","""",""21"","""",""40"",""IEEE"",""24 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Optimal Single-Path Routing Algorithm in the Datacenter Network DPillar,""A. Erickson"; A. E. Kiasari; J. Navaridas;" I. A. Stewart"",""School of Engineering and Computing Sciences, Durham University, South Road, Durham, United Kingdom"; School of Computer Science, University of Manchester, Oxford Road, Manchester, United Kingdom; School of Computer Science, University of Manchester, Oxford Road, Manchester, United Kingdom;" School of Engineering and Computing Sciences, Durham University, South Road, Durham, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""689"",""703"",""DPillar has recently been proposed as a server-centric datacenter network and is combinatorially related to (but distinct from) the well-known wrapped butterfly network. We explain the relationship between DPillar and the wrapped butterfly network before proving that the underlying graph of DPillar is a Cayley graph";" hence, the datacenter network DPillar is node-symmetric. We use this symmetry property to establish a single-path routing algorithm for DPillar that computes a shortest path and has time complexity O(k), where k parameterizes the dimension of DPillar (we refer to the number of ports in its switches as n). Our analysis also enables us to calculate the diameter of DPillar exactly. Moreover, our algorithm is trivial to implement, being essentially a conditional clause of numeric tests, and improves significantly upon a routing algorithm earlier employed for DPillar. Furthermore, we provide empirical data in order to demonstrate this improvement. In particular, we empirically show that our routing algorithm improves the average length of paths found, the aggregate bottleneck throughput, and the communication latency. A secondary, yet important, effect of our work is that it emphasises that datacenter networks are amenable to a closer combinatorial scrutiny that can significantly improve their computational efficiency and performance."",""1558-2183"","""",""10.1109/TPDS.2016.2591011"",""EPSRC(grant numbers:EP/K015680/1,EP/K015699/1)"; European Union's Horizon 2020 programme(grant numbers:671553);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7511760"",""Datacenter networks";routing algorithms;shortest paths;"symmetry"",""Servers";Routing;Algorithm design and analysis;Ports (Computers);Throughput;Switches;"Time complexity"","""",""10"","""",""24"",""IEEE"",""13 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Analysis of Minimum Interaction Time for Continuous Distributed Interactive Computing,""L. Zhang"; X. Tang;" B. He"",""Computer Science and Computer Engineering Department, University of Arkansas, Fayetteville, AR"; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore;" School of Computing, National University of Singapore, Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""401"",""415"",""Distributed interactive computing allows participants at different locations to interact with each other in real time. In this paper, we study the interaction times of continuous Distributed Interactive Applications (DIAs) in which the application states change due to not only user-initiated operations but also time passing. Given the clients and servers of a continuous DIA, its interaction time is directly affected by how the clients are assigned to the servers as well as the simulation time settings of the servers. We formulate the Minimum Interaction Time (MIT) problem as a combinatorial problem of these two tuning knobs and prove that it is NP-hard. We then approximate the problem by fixing the client assignment or the simulation time offsets among the servers. When the client assignment is fixed, we show that finding the minimum achievable interaction time can be reduced to a weighted bipartite matching problem. We further show that this approach establishes a tight approximation factor of 3 to the MIT problem if each client is assigned to its nearest server. When the simulation time offsets among the servers are fixed, we show that finding the minimum achievable interaction time is still NP-hard. This approach can approximate the MIT problem by a factor within 2 if the simulation times of all servers are synchronized. A mix of the above two approaches better approximates the MIT problem within a factor of 5/3. We further conduct experimental evaluation of these approaches with three real Internet latency datasets."",""1558-2183"","""",""10.1109/TPDS.2016.2585140"",""Singapore Ministry of Education Academic Research Fund Tier 2(grant numbers:MOE2013-T2-2-067)"; Academic Research Fund Tier 1(grant numbers:2014-T1-001-145);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500040"",""Distributed interactive computing";interaction time;consistency;"approximation algorithm"",""Servers";Synchronization;Delays;Real-time systems;Computational modeling;Internet;"Games"","""",""7"","""",""33"",""IEEE"",""27 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Analysis, Classification and Comparison of Scheduling Techniques for Software Transactional Memories,""P. Di Sanzo"",""Department of Computer, Control, and Management Engineering, Sapienza University of Rome, Roma, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3356"",""3373"",""Transactional Memory (TM) is a practical programming paradigm for developing concurrent applications. Performance is a critical factor for TM implementations, and various studies demonstrated that specialised transaction/thread scheduling support is essential for implementing performance-effective TM systems. After one decade of research, this article reviews the wide variety of scheduling techniques proposed for Software Transactional Memories. Based on peculiarities and differences of the adopted scheduling strategies, we propose a classification of the existing techniques, and we discuss the specific characteristics of each technique. Also, we analyse the results of previous evaluation and comparison studies, and we present the results of a new experimental study encompassing techniques based on different scheduling strategies. Finally, we identify potential strengths and weaknesses of the different techniques, as well as the issues that require to be further investigated."",""1558-2183"","""",""10.1109/TPDS.2017.2740285"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010909"",""Transactional memory";transaction scheduling;concurrent applications;"performance optimization"",""Benchmark testing";Job shop scheduling;Message systems;Synchronization;Instruction sets;Programming;"Throughput"","""",""11"","""",""53"",""IEEE"",""15 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Analyzing and Enhancing Dynamic Threshold Policy of Data Center Switches,""D. Shan"; W. Jiang;" F. Ren"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; School of Information Science and Engineering, Central South University, Changsha, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2454"",""2470"",""Today's data center switches usually employ on-chip shared memory";" buffer management policy in them is essential to ensure fair sharing of memory among all ports. Among various polices, Dynamic Threshold (DT) policy is widely used by switch vendors. Meanwhile, in data centers, distributed applications such as MapReduce often introduce micro-burst traffic into network and the packet dropping caused by micro-burst usually leads to serious performance degradation. When micro-burst traffic arrives at switches, DT is unable to fully utilize the buffer to absorb it. Therefore, in this paper, we theoretically deduce the sufficient conditions for packet dropping caused by micro-burst traffic, and quantitatively estimate the free buffer size when packets are dropped. The results show that the free buffer size can be very large when the number of overloaded ports is small. What's worse, to ensure fair sharing of memory among output ports, packets from micro-burst traffic may be dropped even when the traffic size is much smaller than the buffer size. In light of these results, we propose the Enhanced Dynamic Threshold (EDT) policy, which can alleviate packet dropping caused by micro-burst traffic through fully utilizing the switch buffer and temporarily relaxing the fairness constraint. The simulation results show that EDT can absorb more micro-burst traffic than DT."",""1558-2183"","""",""10.1109/TPDS.2017.2671429"",""National High-Tech Research and Development Plan of China (863 Plan)(grant numbers:2015AA020101)"; National Natural Science Foundation of China (NSFC)(grant numbers:61225011); Suzhou-Tsinghua;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859368"",""Shared memory";dynamic threshold;buffer management;micro-burst;"data center network"",""Ports (Computers)";Memory management;Bandwidth;Distributed databases;System-on-chip;"Law enforcement"","""",""15"","""",""57"",""IEEE"",""20 Feb 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"AppBooster: Boosting the Performance of Interactive Mobile Applications with Computation Offloading and Parameter Tuning,""W. Liu"; J. Cao; L. Yang; L. Xu; X. Qiu;" J. Li"",""University of Science and Technology of China"; Hong Kong Polytechnic University, Kowloon, Hong Kong, China; Hong Kong Polytechnic University, Kowloon, Hong Kong, China; University of Science and Technology of China, Hefei, Anhui, China; Hong Kong Polytechnic University, Kowloon, Hong Kong, China;" University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1593"",""1606"",""Interactive mobile applications attract lots of attentions recently. They utilize complex algorithms (e.g., machine learning) to provide advanced functions (e.g., object recognition), thus lead to long response time while running on mobile devices. To reduce the response time, researchers propose offloading some compute-intensive parts of mobile applications onto cloud. Existing works aim to optimize general performance (e.g., response time), but ignore the enhancement of application quality (e.g., recognition accuracy), which is also critical to user experience. In this paper, we develop AppBooster, a mobile cloud platform which boosts both general performance and application quality for interactive mobile applications. AppBooster jointly leverages the quality adaptation, computation offloading and parallel speedup to boost the comprehensive performance, which is defined by developers based on the metrics of application quality and general performance. Through combining history-based platform-learned knowledge, developer-provided information and the platform-monitored environment conditions (e.g., workload, network), AppBooster manages applications with optimal computation partitioning scheme and tunable parameter setting thus obtain high comprehensive performance. We evaluate AppBooster with an object recognition application in various network conditions and show AppBooster can significantly boost application performance and obtain 1.3 to 3.5 times better performance than existing strategies."",""1558-2183"","""",""10.1109/TPDS.2016.2624733"",""The fundings for Project of Strategic Importance"; The Hong Kong Polytechnic University(grant numbers:1-ZE26); National High-Technology Research and Development Program (863 Program) of China(grant numbers:2014AA01A302); Hong Kong RGC; GRF(grant numbers:PolyU 5104/13E); National Natural Science Foundation of China(grant numbers:61502312);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7733131"",""Mobile cloud computing";computation offloading;"interactive mobile application"",""Mobile applications";Mobile communication;Time factors;Measurement;Optimization;Mobile handsets;"Cloud computing"","""",""26"","""",""27"",""IEEE"",""3 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"APPLES: Efficiently Handling Spin-lock Synchronization on Virtualized Platforms,""J. Shan"; X. Ding;" N. Gehani"",""Computer Science Department, New Jersey Institute of Technology, Newark, NJ"; Computer Science Department, New Jersey Institute of Technology, Newark, NJ;" Computer Science Department, New Jersey Institute of Technology, Newark, NJ"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1811"",""1824"",""Spin-locks are widely used in software for efficient synchronization. However, they cause serious performance degradation on virtualized platforms, such as the Lock Holder Preemption (LHP) problem and the Lock Waiter Preemption (LWP) problem, due to excessive spinning by virtual CPUs (VCPUs). The excessive spinning occurs when a VCPU waits to acquire a spin-lock. To address the performance degradation, hardware facilities, such as Intel PLE and AMD PF, are provided on processors to preempt VCPUs when they spin excessively. Although these facilities have been predominantly used on mainstream virtualization systems, using them in a manner that achieves the highest performance is still a challenging issue. There are two core problems in using these hardware facilities to reduce excessive spinning. One is to determine the best time to preempt a spinning VCPU (i.e., the selection of spinning thresholds). The other is which VCPU should be scheduled to run after the spinning VCPU is descheduled. Due to the semantic gap between different software layers, the virtual machine monitor (VMM) does not have information about the computation characteristics on VCPUs, which is needed to address the above problems. This makes the problems inherently challenging. We propose a framework named AdPtive Pause-Loop Exiting and Scheduling (APPLES) to address these problems. APPLES monitors the overhead caused by excessive spinning and preempting spinning VCPUs, and periodically adjusts spinning thresholds to reduce the overhead. APPLES also evaluates and schedules “ready” VCPUs in a VM by their potential to reduce the spinning incurred by the spin-lock synchronization. The evaluation is based on the causality and the time of VCPU preemptions. The implementation of APPLES incurs only minimal changes to existing systems (about 100 lines of code in KVM). Experiments show that APPLES can improve performance by 3  $\sim$ 49 percent (14 percent on average) for the workloads with frequent spin-lock operations."",""1558-2183"","""",""10.1109/TPDS.2016.2625249"",""National Science Foundation(grant numbers:CNS 1409523,CCF 1617749)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7736153"",""Virtualization";multi-core;cloud computing;spin-lock synchronization;lock holder preemption;"scheduling"",""Spinning";Program processors;Virtual machine monitors;Hardware;Synchronization;Schedules;"Virtualization"","""",""5"","""",""41"",""IEEE"",""4 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Application Execution Time Prediction for Effective CPU Provisioning in Virtualization Environment,""H. -W. Li"; Y. -S. Wu; Y. -Y. Chen; C. -M. Wang;" Y. -N. Huang"",""Department of Computer Science, National Chiao-Tung University, Hsinchu City, Taiwan"; Department of Computer Science, National Chiao-Tung University, Hsinchu City, Taiwan; Department of Computer Science, National Chiao-Tung University, Hsinchu City, Taiwan; Department of Computer Science, National Chiao-Tung University, Hsinchu City, Taiwan;" Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3074"",""3088"",""Provisioning of hardware resources through virtual machines (VMs) has been widely used for supporting server consolidation and infrastructure-as-a-cloud computing. We propose NICBLE to support accurate CPU resource provisioning for application workload running on VMs. While CPU is essential for any application workload, not every workload requires the same level of CPU resource. The VM tenants may also have different expectations of application performance and preferences. NICBLE models the execution of an application workload and employs a simulation-based algorithm to predict the impact on application execution time for a hypothetical VM configuration change on the number of CPUs. One may use NICBLE to reason about whether changing the number of CPUs will significantly affect the application performance. We built the NICBLE prototype on top of the Xen hypervisor [1]. NICBLE does not require modification to the guest systems. The performance overhead on the guest system is negligible. Our evaluation indicates that NICBLE is able to provide accurate prediction with an average error rate of less than 15 percent for non-adaptive application workload."",""1558-2183"","""",""10.1109/TPDS.2017.2707543"",""Ministry of Science and Technology of the Republic of China(grant numbers:104-2221-E-009-104-MY3,103-2221-E-001-028-MY3)"; Industrial Technology Research Institute, Taiwan;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7933268"",""Application execution time";virtualization;CPUs;resource provisioning;"auto-scaling"",""Schedules";Virtual machining;Predictive models;Resource management;Cloud computing;Virtualization;"Servers"","""",""11"","""",""40"",""IEEE"",""24 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Application Scheduling, Placement, and Routing for Power Efficiency in Cloud Data Centers,""A. Dalvandi"; M. Gurusamy;" K. C. Chua"",""Department of Electrical and Computer Engineering, National University of Singapore (NUS), Singapore"; Department of Electrical and Computer Engineering, National University of Singapore (NUS), Singapore;" Department of Electrical and Computer Engineering, National University of Singapore (NUS), Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""947"",""960"",""Increased power usage and network performance degradation due to best-effort bandwidth sharing significantly affect tenancy cost, cloud adoption, and data center efficiencies. In this article, we propose a novel Sliding-Scheduled Tenant request model which enables tenants to specify the required duration of their application within a certain window, in addition to its resource requirement graph. We investigate the sliding-scheduled application placement and routing problem, which selects the start-time of requests within their specified time-window to reserve both server and network resources for their required duration, and therefore provide resource guarantees with predictable performance. Using the multi-component utilization-based power model, we formulate the problem as an optimization problem that maximizes the acceptance rate while consuming as low power as possible. We develop fast online heuristics that adopt power, acceptance and adaptive spread-based scheduling policies while allocating the resources with the consideration of request duration and current shutdown-time of the devices. We demonstrate the effectiveness of the proposed algorithms in terms of power saving and acceptance rate, 1) for small data centers, by comparing their performance with the numerical results obtained from solving the optimization problem using CPLEX and 2) for large data centers using comprehensive simulation results."",""1558-2183"","""",""10.1109/TPDS.2016.2607743"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563845"",""VM-placement";routing;bandwidth guarantee;power efficiency;"sliding-scheduling tenant application requests"",""Servers";Routing;Bandwidth;Data models;Adaptation models;Optimization;"Cloud computing"","""",""12"","""",""29"",""IEEE"",""9 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Astro: Auto-Generation of Synthetic Traces Using Scaling Pattern Recognition for MPI Workloads,""J. Chen";" R. M. Clapp"",""Intel Corporation, Hillsboro, OR";" Intel Corporation, Hillsboro, OR"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2159"",""2171"",""Performance modeling of scale-out MPI workloads is critical in assessing trade-offs for high-performance system designs. Traces and workload skeletons are the two main vehicles used to date to accomplish this task. However, the ever-increasing scale and complexity of MPI workloads makes it difficult, sometimes infeasible, to collect the traces and/or skeletonize the workloads, due to the constraints of computing resources or the unavailability of workload source code. This paper presents Astro, a framework that leverages machine learning techniques to automatically recognize the scaling patterns from training traces, and generate high-quality synthetic traces which mimic original trace behavior and extrapolate it to arbitrary scale. Experimental results show that compared with original traces, the synthetic traces yield less than 15 percent error against a range of metrics for up to 8 K MPI ranks. This framework enables large-scale performance modeling with limited computing resources, and allows modeling proprietary workloads in a portable and secure way."",""1558-2183"","""",""10.1109/TPDS.2017.2649518"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7809142"",""Scaling pattern recognition";trace synthesis;"MPI performance modeling"",""Computational modeling";Skeleton;Load modeling;Pattern recognition;Training;Measurement;"Predictive models"","""",""5"","""",""31"",""IEEE"",""9 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Asynchronous Non-Generational Model to Parallelize Metaheuristics: A Bioinformatics Case Study,""S. Santander-Jiménez";" M. A. Vega-Rodríguez"",""Department of Computer and Communications Technologies, University of Extremadura, Caceres, Spain";" Department of Computer and Communications Technologies, University of Extremadura, Caceres, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1825"",""1838"",""The integration of parallel computing techniques into metaheuristics has traditionally represented a promising approach to tackle computationally demanding optimization problems. In the last years, metaheuristics have evolved by including more complex search mechanisms whose parallelization often leads to performance issues under classic parallel schemes. This work investigates the asynchronous non-generational parallelization model, which is aimed at dealing with performance pitfalls by allowing worker threads to behave as asynchronous independent agents. We incorporate asynchronous principles into a recently proposed metaheuristic for multiobjective optimization, the Indicator-Based Multiobjective Bat Algorithm, and apply the resulting approach to solve a real-world problem in the bioinformatics domain: the reconstruction of evolutionary histories. Experiments on multicore multiprocessor systems comprising up to 64 cores reveal the suitability of the model to address the main challenges of the metaheuristic design under study, outperforming other implementations and methods in terms of parallel performance while also achieving significant solution quality."",""1558-2183"","""",""10.1109/TPDS.2016.2645764"",""Spanish Ministry of Economy and Competitiveness"; European Regional Development Fund (ERDF)(grant numbers:TIN2016-76259-P); Junta de Extremadura(grant numbers:GR15011,TIC015); University of Extremadura(grant numbers:ACCION-III-04);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801038"",""Parallel algorithms";performance evaluation of algorithms and systems;soft computing;"biology and genetics"",""Phylogeny";Optimization;Algorithm design and analysis;Parallel processing;Linear programming;Bioinformatics;"Multicore processing"","""",""7"","""",""47"",""IEEE"",""28 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"ATOM: Efficient Tracking, Monitoring, and Orchestration of Cloud Resources,""M. Du";" F. Li"",""School of Computing, University of Utah, Salt Lake City, UT";" School of Computing, University of Utah, Salt Lake City, UT"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2172"",""2189"",""The emergence of Infrastructure as a Service framework brings new opportunities, which also accompanies with new challenges in auto scaling, resource allocation, and security. A fundamental challenge underpinning these problems is the continuous tracking and monitoring of resource usage in the system. In this paper, we present ATOM, an efficient and effective framework to automatically track, monitor, and orchestrate resource usage in an Infrastructure as a Service (IaaS) system that is widely used in cloud infrastructure. We use novel tracking method to continuously track important system usage metrics with low overhead, and develop a Principal Component Analysis (PCA) based approach to continuously monitor and automatically find anomalies based on the approximated tracking results. We show how to dynamically set the tracking threshold based on the detection results, and further, how to adjust tracking algorithm to ensure its optimality under dynamic workloads. Lastly, when potential anomalies are identified, we use introspection tools to perform memory forensics on VMs guided by analyzed results from tracking and monitoring to identify malicious behavior inside a VM. We demonstrate the extensibility of ATOM through virtual machine (VM) clustering. The performance of our framework is evaluated in an open source IaaS system."",""1558-2183"","""",""10.1109/TPDS.2017.2652467"",""US National Science Foundation(grant numbers:CNS-1314945,CNS-1514520)"; US National Science Foundation(grant numbers:IIS-1251019);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7815424"",""Infrastructure as a service";cloud;tracking;monitoring;anomaly detection;"virtual machine introspection"",""Monitoring";Cloud computing;Atomic measurements;Security;Principal component analysis;"Resource management"","""",""16"","""",""57"",""IEEE"",""16 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Automated Synthesis of Distributed Network Access Controls: A Formal Framework with Refinement,""M. A. Rahman";" E. Al-Shaer"",""Department of Computer Science, Tennessee Tech University, Cookeville, TN";" Department of Software and Information Systems, University of North Carolina at Charlotte, Charlotte, NC"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""416"",""430"",""Due to the extensive use of network services and emerging security threats, enterprise networks deploy varieties of security devices for controlling resource access based on organizational security requirements. These requirements need fine-grained access control rules based on heterogeneous isolation patterns like access denial, trusted communication, and payload inspection. Organizations are also seeking for usable and optimal security configurations that can harden the network security within enterprise budget constraints. In order to design a security architecture, i.e., the distribution of security devices along with their security policies, that satisfies the organizational security requirements as well as the business constraints, it is required to analyze various alternative security architectures considering placements of network security devices in the network and the corresponding access controls. In this paper, we present an automated formal framework for synthesizing network security configurations. The main design alternatives include different kinds of isolation patterns for network traffic flows. The framework takes security requirements and business constraints along with the network topology as inputs. Then, it synthesizes cost-effective security configurations satisfying the constraints and provides placements of different security devices, optimally distributed in the network, according to the given network topology. In addition, we provide a hypothesis testing-based security architecture refinement mechanism that explores various security design alternatives using ConfigSynth and improves the security architecture by systematically increasing the security requirements. We demonstrate the execution of ConfigSynth and the refinement mechanism using case studies. Finally, we evaluate their scalability using simulated experiments."",""1558-2183"","""",""10.1109/TPDS.2016.2585108"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7500078"",""Security configuration";automatic synthesis;formal modeling;security metrics;"isolation"",""Security";Usability;Network topology;Business;Payloads;Inspection;"Computer architecture"","""",""7"","""",""30"",""IEEE"",""27 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Beehive: Erasure Codes for Fixing Multiple Failures in Distributed Storage Systems,""J. Li";" B. Li"",""Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada";" Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1257"",""1270"",""In distributed storage systems, erasure codes have been increasingly deployed to tolerate server failures without loss of data. Traditional erasure codes, such as Reed-Solomon codes, suffer from a high volume of network transfer and disk I/O to recover unavailable data at failed storage servers. Typically, unavailable data at different failed storage servers in a distributed storage system are fixed separately. It has been shown that it is possible to reduce the volume of network transfer significantly by reconstructing data from multiple storage servers at the same time. However, there has been no construction of erasure codes to achieve it without imposing strict constraints on system parameters. In this paper, we propose Beehive codes, designed for optimizing the volume of network transfers to fix the data on multiple failed storage servers. Beehive codes can be constructed over a wide range of system parameters at code rate no more than 0.5, while incurring slightly more storage overhead than Reed-Solomon codes. To achieve the optimal storage overhead as Reed-Solomon codes, we further extend vanilla Beehive codes to MDS Beehive codes, which incurs near-optimal volumes of network transfers during reconstruction. We implement both Beehive and MDS Beehive Codes in C++ and evaluate their performance on Amazon EC2. Our evaluation results have clearly shown that the volume of both network transfers and disk I/O can be conserved by a substantial margin."",""1558-2183"","""",""10.1109/TPDS.2016.2623309"",""NSERC"; Discovery research program; SAVI; NSERC;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7726018"",""Distributed storage system";cooperative regenerating codes;MDS;"interference alignment"",""Servers";Distributed databases;Reed-Solomon codes;Systematics;C++ languages;Decoding;"Indexes"","""",""16"","""",""38"",""IEEE"",""31 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Binary-Tree Based Estimation of File Requests for Efficient Data Replication,""S. Souravlas";" A. Sifaleras"",""Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece";" Department of Applied Informatics, University of Macedonia, Thessaloniki, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1839"",""1852"",""Recently, data replication has received considerable attention in the field of grid computing. The main goal of data replication algorithms is to optimize data access performance by replicating the most popular files. When a file does not exist in the node where it was requested, it necessarily has to be transferred from another node, causing delays in the completion the file requests. The general idea behind data replication is to keep track of the most popular files requested in the grid and create copies of them in selected nodes. In this way, more file requests can be completed over a period of time and average job execution time is reduced. In this paper, we introduce an algorithm that estimates the potential of the files located in each node of the grid, using a binary tree structure. Also, the file scope and the file type are taken into account. By potential of a file, we mean its increasing or decreasing demand over a period of time. The file scope generally refers to the extent of the group of users which are interested or potentially interested in a file. The file types are divided into read and write intensive. Our scheme mainly promotes the high-potential files for replication, based on the temporal locality principle. The simulation results indicate that the proposed scheme can offer better data access performance in terms of the hit ratio and the average job execution time, compared to other state-of-the-art strategies."",""1558-2183"","""",""10.1109/TPDS.2017.2650228"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7811179"",""Data replication";binary trees;data grid;"file popularity"",""Distributed databases";Simulation;Heuristic algorithms;Servers;Estimation;"Delays"","""",""12"","""",""23"",""IEEE"",""9 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Bridging the Gap Between OpenMP and Task-Based Runtime Systems for the Fast Multipole Method,""E. Agullo"; O. Aumage; B. Bramas; O. Coulaud;" S. Pitoiset"",""Inria Centre de recherche Bordeaux Sud-Ouest, HiePACS, Talence, France"; Inria Centre de recherche Bordeaux Sud-Ouest, HiePACS, Talence, France; Inria Centre de recherche Bordeaux Sud-Ouest, HiePACS, Talence, France; Inria Centre de recherche Bordeaux Sud-Ouest, HiePACS, Talence, France;" Inria Centre de recherche Bordeaux Sud-Ouest, HiePACS, Talence, France"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2794"",""2807"",""With the advent of complex modern architectures, the low-level paradigms long considered sufficient to build High Performance Computing (HPC) numerical codes have met their limits. Achieving efficiency, ensuring portability, while preserving programming tractability on such hardware prompted the HPC community to design new, higher level paradigms while relying on runtime systems to maintain performance. However, the common weakness of these projects is to deeply tie applications to specific expert-only runtime system APIs. The OpenMP specification, which aims at providing common parallel programming means for shared-memory platforms, appears as a good candidate to address this issue thanks to the latest task-based constructs introduced in its revision 4.0. The goal of this paper is to assess the effectiveness and limits of this support for designing a high-performance numerical library, ScalFMM, implementing the fast multipole method (FMM) that we have deeply re-designed with respect to the most advanced features provided by OpenMP 4. We show that OpenMP 4 allows for significant performance improvements over previous OpenMP revisions on recent multicore processors and that extensions to the 4.0 standard allow for strongly improving the performance, bridging the gap with the very high performance that was so far reserved to expert-only runtime system APIs."",""1558-2183"","""",""10.1109/TPDS.2017.2697857"",""Inria PlaFRIM"; Bordeaux INP; LABRI; IMB; Conseil Régional d’Aquitaine; Université de Bordeaux; CNRS; ANR; d'investissements d’Avenir;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7912335"",""High performance computing";fast multipole method;runtime system;OpenMP;compiler;parallel programming model;priority;commutativity;"multicore architecture"",""Runtime";Programming;Libraries;Computational modeling;Multicore processing;Parallel processing;"Program processors"","""",""12"","""",""28"",""IEEE"",""26 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Building an Efficient Put-Intensive Key-Value Store with Skip-Tree,""Y. Yue"; B. He; Y. Li;" W. Wang"",""Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"; National University of Singapore, Singapore; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China;" Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Mar 2017"",""2017"",""28"",""4"",""961"",""973"",""Multi-component based Log-Structured Merge-tree (LSM-tree) has been becoming one of the mainstream indexes. LSM-tree adopts component-by-component KV item flowing down mechanism to push each KV item from one smaller component to the adjacent larger component during compaction procedures until the KV items reach the largest component. This process incurs significant write amplification and limits the write throughput. In this paper, we propose one multi-component Skip-tree to aggressively push the KV items to the non-adjacent larger components via skipping some components and then make the KV items' top-down move more efficient. We develop adaptive and reliable KV item movements among components. By reducing the number of steps during the flowing process from memory-resident component to the disk-resident largest component, Skip-tree can effectively reduce the write amplification and thus improve the system throughput. We design and implement one high performance key-value store, named SkipStore, based on Skip-tree. The experiments demonstrate that SkipStore outperforms the state-of-the-art open-sourced system RocksDB in Facebook by 66.5 percent under HDD and 61 percent under SSD."",""1558-2183"","""",""10.1109/TPDS.2016.2609912"",""National Science Foundation of China(grant numbers:61303056)"; Youth Innovation Promotion Association, CAS(grant numbers:2016146); MoE; AcRF(grant numbers:T1 251RES1610); NUS;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7569086"",""Storage system";KV store;LSM-tree;compaction;"performance optimization"",""Indexes";Compaction;Facebook;Internet;Throughput;Reliability;"Performance evaluation"","""",""25"","""",""39"",""IEEE"",""15 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Building NVRAM-Aware Swapping Through Code Migration in Mobile Devices,""K. Zhong"; D. Liu; L. Long; J. Ren; Y. Li;" E. H. -M. Sha"",""College of Computer Science, Chongqing University, No. 174, Shazhengjie, Shapingba, Chongqing, China"; College of Computer Science, Chongqing University, No. 174, Shazhengjie, Shapingba, Chongqing, China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; College of Computer Science, Chongqing University, No. 174, Shazhengjie, Shapingba, Chongqing, China; College of Computer Science, Chongqing University, No. 174, Shazhengjie, Shapingba, Chongqing, China;" College of Computer Science, Chongqing University, No. 174, Shazhengjie, Shapingba, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3089"",""3099"",""Mobile applications are becoming increasingly feature-rich and powerful, but also dependent on large main memories, which consume a large portion of system energy, especially for devices equipped with 4/6 GB DRAM. Swapping inactive DRAM pages to byte-addressable, non-volatile memory (NVRAM) is a promising solution to this problem. However, most NVRAMs have limited write endurance and the current victim pages selecting algorithm does not aware it. Therefore, to make it practical, the design of an NVRAM based swapping system must also consider endurance. In this paper, we target at prolonging the lifetime of NVRAM based swap area in mobile devices by reducing the write activities to NVRAM based swap area. Different from traditional wisdom, such as wear leveling and hot/cold data identification, we propose to build a system called nCode, which exploits the fact that code pages are easy to identify, read-only, and therefore a perfect candidate for swapping. Utilizing NVRAM’s byte-addressability, we support execute-in-place (XIP) of the code pages in the swap area, without copying them back to DRAM based main memory. Experimental results based on the Google Nexus 5 smartphone show that nCode can effectively prolong the lifetime of NVRAM under various workloads."",""1558-2183"","""",""10.1109/TPDS.2017.2713780"",""National Natural Science Foundation of China(grant numbers:61672116,61601067,61472052)"; National 863 Program(grant numbers:2015AA015304); Chongqing High-Tech Research Program(grant numbers:cstc2016jcyjA0332,cstc2014yykfB40007); Science and Technology Research Program; Chongqing Municipal Education Commission(grant numbers:KJ1704085); ACM; IEEE;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7944530"",""Smartphone";swapping;non-volatile memory;"application relaunching delay"",""Random access memory";Nonvolatile memory;Phase change materials;Mobile handsets;Google;Energy consumption;"Switches"","""",""7"","""",""43"",""IEEE"",""8 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"CIACP: A Correlation- and Iteration- Aware Cache Partitioning Mechanism to Improve Performance of Multiple Coarse-Grained Reconfigurable Arrays,""C. Yang"; L. Liu; K. Luo; S. Yin;" S. Wei"",""Institute of Microelectronics Department, Tsinghua University, HaiDian District, Beijing, China"; Institute of Microelectronics Department, Tsinghua University, HaiDian District, Beijing, China; Institute of Microelectronics Department, Tsinghua University, HaiDian District, Beijing, China; Institute of Microelectronics Department, Tsinghua University, HaiDian District, Beijing, China;" Institute of Microelectronics Department, Tsinghua University, HaiDian District, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""29"",""43"",""Multiple coarse-grained reconfigurable arrays (CGRA), which are organized in parallel or pipeline to complete applications, have become a productive solution to balance the performance with the flexibility. One of the keys to obtain high performance from multiple CGRAs is to manage the shared on-chip cache efficiently to reduce off-chip memory bandwidth requirements. Cache partitioning has been viewed as a promising technique to enhance the efficiency of a shared cache. However, the majority of prior partitioning techniques were developed for multi-core platform and aimed at multi-programmed workloads. They cannot directly address the adverse impacts of data correlation and computation imbalance among competing CGRAs in multi-CGRA platform. This paper proposes a correlation- and iteration- aware cache partitioning (CIACP) mechanism for shared cache partitioning in multiple CGRAs systems. This mechanism employs correlation monitors (CMONs) to trace the amount of overlapping data among parallel CGRAs, and iteration monitors (IMONs) to track the computation load of each CGRA. Using the information collected by CMONs and IMONs, the CIACP mechanism can eliminate redundant cache utilization of the overlapping data and can also shorten the total execution time of pipelined CGRAs. Experimental results showed that CIACP outperformed state-of-the-art utility-based cache partitioning techniques by up to 16 percent in performance."",""1558-2183"","""",""10.1109/TPDS.2016.2554278"",""National High Technology Research and Development Program of China(grant numbers:2012AA012701)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7452657"",""Coarse-grained reconfigurable array (CGRA)";cache partitioning;data correlation;"computation balance"",""Context";Monitoring;Arrays;Program processors;Bandwidth;"Correlation"","""",""13"","""",""50"",""IEEE"",""14 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CLAP: Component-Level Approximate Processing for Low Tail Latency and High Result Accuracy in Cloud Online Services,""R. Han"; S. Huang; Z. Wang;" J. Zhan"",""Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; School of Software, Tsinghua University, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China;" Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2190"",""2203"",""Modern latency-critical online services such as search engines often process requests by consulting large input data spanning massive parallel components. Hence the tail latency of these components determines the service latency. To trade off result accuracy for tail latency reduction, existing techniques use the components responding before a specified deadline to produce approximate results. However, they skip a large proportion of components when load gets heavier, thus incurring large accuracy losses. In this paper, we propose CLAP to enable component-level approximate processing of requests for low tail latency and small accuracy losses. CLAP aggregates information of input data to create small aggregated data points. Using these points, CLAP reduces latency variance of parallel components and allows them to produce initial results quickly"; CLAP also identifies the parts of input data most related to requests' result accuracies, thus first using these parts to improve the produced results to minimize accuracy losses. We evaluated CLAP using real services and datasets. The results show: (i) CLAP reduces tail latency by 6.46 times with accuracy losses of 2.2 percent compared to existing exact processing techniques;" (ii) when using the same latency, CLAP reduces accuracy losses by 31.58 times compared to existing approximate processing techniques."",""1558-2183"","""",""10.1109/TPDS.2017.2650988"",""National Natural Science Foundation of China(grant numbers:61502451)"; National Key Research and Development Plan of China(grant numbers:2016YFB1000601);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7812758"",""Cloud online services";tail latency;result accuracy;component-level approximate processing;"aggregated data points"",""Indexes";Web pages;Search engines;Aggregates;Recommender systems;Correlation;"Web search"","""",""4"","""",""62"",""IEEE"",""10 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cloudde: A Heterogeneous Differential Evolution Algorithm and Its Distributed Cloud Version,""Z. -H. Zhan"; X. -F. Liu; H. Zhang; Z. Yu; J. Weng; Y. Li; T. Gu;" J. Zhang"",""School of Computer Science and Engineering, South China University of Technology, Guangzhou, China"; School of Computer Science and Engineering, South China University of Technology, Guangzhou, China; School of Information Science and Engineering, Shandong Normal University, Jinan, China; School of Information Engineering and Automation, Kunming University of Science and Technology, Kunming, China; College of Information Science and Technology, Jinan University, Guangzhou, China; College of Computer Science and Technology, Dongguan University of Technology, Dongguan, China; School of Computer Science and Engineering, Guilin University of Electronic Technology, Guilin, China;" School of Computer Science and Engineering, South China University of Technology, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""704"",""716"",""Existing differential evolution (DE) algorithms often face two challenges. The first is that the optimization performance is significantly affected by the ad hoc configurations of operators and parameters for different problems. The second is the long runtime for real-world problems whose fitness evaluations are often expensive. Aiming at solving these two problems, this paper develops a novel double-layered heterogeneous DE algorithm and realizes it in cloud computing distributed environment. In the first layer, different populations with various parameters and/or operators run concurrently and adaptively migrate to deliver robust solutions by making the best use of performance differences among multiple populations. In the second layer, a set of cloud virtual machines run in parallel to evaluate fitness of corresponding populations, reducing computational costs as offered by cloud. Experimental results on a set of benchmark problems with different search requirements and a case study with expensive design evaluations have shown that the proposed algorithm offers generally improved performance and reduced computational time, compared with not only conventional and a number of state-of-the-art DE variants, but also a number of other distributed DE and high-performing evolutionary algorithms. The speedup is significant especially on expensive problems, offering high potential in a broad range of real-world applications."",""1558-2183"","""",""10.1109/TPDS.2016.2597826"",""National Natural Science Foundations of China (NSFC)(grant numbers:61402545)"; Natural Science Foundations of Guangdong Province(grant numbers:2014A030306038); Pearl River New Star in Science and Technology(grant numbers:201506010047); GDUPS; NSFC(grant numbers:61332002,U1201258);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530859"",""Differential evolution";evolutionary algorithm;adaptive migration strategy;cloud computing;"heterogeneous parallelism"",""Cloud computing";Sociology;Statistics;Algorithm design and analysis;Optimization;Robustness;"Computational efficiency"","""",""135"","""",""64"",""OAPA"",""3 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"CloudFog: Leveraging Fog to Extend Cloud Gaming for Thin-Client MMOG with High Quality of Service,""Y. Lin";" H. Shen"",""Department of Electrical and Computer Engineering, Clemson University, Clemson, SC";" Department of Electrical and Computer Engineering, Clemson University, Clemson, SC"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""431"",""445"",""With the increasing popularity of Massively Multiplayer Online Game (MMOG) and fast growth of mobile gaming, cloud gaming exhibits great promises over the conventional MMOG gaming model as it frees players from the requirement of hardware and game installation on their local computers. However, as the graphics rendering is offloaded to the cloud, the data transmission between the end-users and the cloud significantly increases the response latency and limits the user coverage, thus preventing cloud gaming to achieve high user Quality of Service (QoS). To solve this problem, previous research suggested deploying more datacenters, but it comes at a prohibitive cost. We propose a lightweight system called CloudFog, which incorporates “fog” consisting of supernodes that are responsible for rendering game videos and streaming them to their nearby players. Fog enables the cloud to be only responsible for the intensive game state computation and sending update information to supernodes, which significantly reduce the traffic hence the latency and bandwidth consumption. To further enhance QoS, we propose the reputation based supernode selection strategy to assign each player with a suitable supernode that can provide satisfactory game video streaming service, the receiver-driven encoding rate adaptation strategy to increase the playback continuity, the social network based server assignment strategy to avoid the communication interaction between servers in a datacenter to reduce latency, and the dynamic supernode provisioning strategy to deal with user churns. Experimental results from PeerSim and PlanetLab show the effectiveness and efficiency of CloudFog and our individual strategies in increasing user coverage, reducing response latency and bandwidth consumption."",""1558-2183"","""",""10.1109/TPDS.2016.2563428"",""U.S. NSF(grant numbers:NSF-1404981,IIS-1354123,CNS-1254006,CNS-1249603)"; Microsoft Research Faculty Fellowship(grant numbers:8300751);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7465785"",""Cloud gaming";P2P network;online gaming;"quality of service"",""Servers";Videos;Quality of service;"Bandwidth"","""",""63"","""",""60"",""IEEE"",""5 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CloudScout: A Non-Intrusive Approach to Service Dependency Discovery,""J. Yin"; X. Zhao; Y. Tang; C. Zhi; Z. Chen;" Z. Wu"",""College of Computer Science, Zhejiang University, Hangzhou, China"; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; College of Computer Science, Zhejiang University, Hangzhou, China; National Parallel Computing Engineering Research Center, Chinese Academy of Engineering, Beijing, China;" College of Computer Science, Zhejiang University, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1271"",""1284"",""Nowadays, numerous enterprises are migrating their applications into cloud computing environments. Typically, the applications are composed of several dependent service components that span many hosts and network devices. In light of this, exploring the dependency between service components can be beneficial for achieving fast network application response time. Moreover, it is significant to consolidate service components according to resource constraints, service dependency, and network structure. However, it is a tedious task to discover the dependency among service components without expert knowledge of the running application. In this paper, we propose CloudScout, a non-intrusive approach that is capable of automatically discovering dependent service components. CloudScout analyzes the correlation among service components based on the time-series information from system monitoring logs. We address two key challenges in CloudScout: service distance calculation and dependent service clustering. We conduct experiments on five applications with 290 service components that span 20 physical hosts across two data centers. The experimental results demonstrate that CloudScout can successfully discover the dependency among service components and facilitate reducing the network latency of network applications and distributed applications."",""1558-2183"","""",""10.1109/TPDS.2016.2619715"",""National Natural Science Foundation of China(grant numbers:61272129)"; National Science and Technology(grant numbers:2015BAH18F02); New-Century Excellent Talents; Ministry of Education(grant numbers:NCET-12-0491); Zhejiang Provincial Natural Science Foundation(grant numbers:LR13F020002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7604145"",""Monitoring data analysis";log mining;service dependency discovery;"VM consolidation"",""Switches";Cloud computing;Servers;Network topology;Distributed databases;Correlation;"Data mining"","""",""19"","""",""27"",""IEEE"",""20 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cluster-Aware Virtual Machine Collaborative Migration in Media Cloud,""W. Zhang"; Y. Chen; X. Gao; Z. Mo; Q. Zheng;" Z. Lu"",""Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, Shaanxi, China"; Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, Shaanxi, China; Department of Computer Science and Technology, Xi'an Jiaotong University, Xi'an, Shaanxi, China;" Department of Computer Science and Engineering, Pennsylvania State University, PA"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2808"",""2822"",""Media cloud has become a promising paradigm for deploying large-scale streaming media applications at a reduced cost. Due to dynamic and diverse demands of users, media cloud presents two crucial characteristics: high resource consumption and dynamic traffic among media servers. Consequently, Virtual Machine (VM) migration in media cloud is highly required to suit varying resource requirements and the dynamic traffic patterns. Moreover, migration of such bandwidth-intensive media applications in media cloud needs cautious handling, especially for the internal traffic of Data Center Networks (DCN). However, existing media cloud resource management schemes or traffic-aware VM deployment approaches are insufficient for media cloud, ignoring the characteristics of either cloud infrastructure or media streaming requirements. In this paper, we propose a cluster-aware VM collaborative migration scheme for media cloud, tightly integrating clustering, placement, and dynamic migration process. The scheme employs a clustering algorithm and a placement algorithm to obtain ideal migration strategies for newly perceived media server clusters, and a migration algorithm to effectively accomplish the migration process of media servers. Evaluation results demonstrate that our scheme can effectively migrate virtual media servers in media cloud, while reducing the total internal traffic in DCN under the resource consumption constraints of media streaming applications."",""1558-2183"","""",""10.1109/TPDS.2017.2697381"",""National Key Research and Development Program of China(grant numbers:2016YFB1000903)"; National Science Foundation of China(grant numbers:61472317,61502379,61532015,61532004); MOE(grant numbers:IRT13035); Key Lab of Shaanxi Province(grant numbers:2013SZS05-Z01); Centre for Engineering Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7909001"",""Cluster-aware";virtual machine collaborative migration;media cloud;"data center network"",""Media";Cloud computing;Servers;Clustering algorithms;Streaming media;Heuristic algorithms;"Dynamic scheduling"","""",""8"","""",""43"",""IEEE"",""24 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Combining Vertex-Centric Graph Processing with SPARQL for Large-Scale RDF Data Analytics,""I. Abdelaziz"; R. Harbi; S. Salihoglu;" P. Kalnis"",""King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"; Saudi Aramco, Thuwal, Saudi Arabia; University of Waterloo, Waterloo, ON, Canada;" King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3374"",""3388"",""Modern applications require sophisticated analytics on RDF graphs that combine structural queries with generic graph computations. Existing systems support either declarative SPARQL queries, or generic graph processing, but not both. We bridge the gap by introducing Spartex, a versatile framework for complex RDF analytics. Spartex extends SPARQL to combine seamlessly generic graph algorithms (e.g., PageRank, Shortest Paths, etc.) with SPARQL queries. Spartex builds on existing vertex-centric graph processing frameworks, such as Graphlab or Pregel. It implements a generic SPARQL operator as a vertex-centric program that interprets SPARQL queries and executes them efficiently using a built-in optimizer. In addition, any graph algorithm implemented in the underlying vertex-centric framework, can be executed in Spartex. We present various scenarios where our framework simplifies significantly the implementation of complex RDF data analytics programs. We demonstrate that Spartex scales to datasets with billions of edges, and show that our core SPARQL engine is at least as fast as the state-of-the-art specialized RDF engines. For complex analytical tasks that combine generic graph processing with SPARQL, Spartex is at least an order of magnitude faster than existing alternatives."",""1558-2183"","""",""10.1109/TPDS.2017.2720174"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959641"",""RDF data";graph analytics;SPARQL;"vertex-centric"",""Resource description framework";Graphical models;Matched filters;Pattern matching;Filtering algorithms;Databases;"Algorithm design and analysis"","""",""17"","""",""44"",""IEEE"",""27 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Computer Generation of High Throughput and Memory Efficient Sorting Designs on FPGA,""R. Chen";" V. K. Prasanna"",""Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, CA";" Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, CA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3100"",""3113"",""Accelerating sorting using dedicated hardware to fully utilize the memory bandwidth for Big Data applications has gained much interest in the research community. Recently, parallel sorting networks have been widely employed in hardware implementations due to their high data parallelism and low control overhead. In this paper, we propose a systematic methodology for mapping large-scale bitonic sorting networks onto FPGA. To realize data permutations in the sorting network, we develop a novel RAM-based design by vertically “folding” the classic Clos network. By utilizing the proposed design for data permutation, we develop a hardware generator to automatically build bitonic sorting architectures on FPGAs. For given input size, data width and data parallelism, the hardware generator specializes both the datapath and the control unit for sorting and generates a design in high level hardware description language. We demonstrate trade-offs among throughput, latency and area using two illustrative sorting designs including a high throughput design and a resource efficient design. With a data parallelism of p (2 ≤ p ≤ N/2), the high throughput design sorts an N-key sequence with latency 6N=p + o(N), throughputp results per cycle and uses 6N + o(N) memory. This achieves optimal memory efficiency (defined as the ratio of throughput to the amount of on-chip memory used by the design) and outperforms the state-of-the-art. Experimental results show that the designs obtained by our proposed hardware generator achieve 49 to 112 percent improvement in energy efficiency and 56 to 430 percent higher memory efficiency compared with the state-of-the-art."",""1558-2183"","""",""10.1109/TPDS.2017.2705128"",""U.S. National Science Foundation (NSF)(grant numbers:CCF-1320211,ACI-1339756)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930437"",""Parallel sorting";computer generation;large-scale sorting;bitonic sorting network;Clos network;FPGA;hardware acceleration;energy efficiency;memory efficiency;throughput;"database operations"",""Sorting";Hardware;Throughput;Field programmable gate arrays;Generators;Parallel processing;"Bandwidth"","""",""19"","""",""28"",""IEEE"",""17 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Computing Maximum Cardinality Matchings in Parallel on Bipartite Graphs via Tree-Grafting,""A. Azad"; A. Buluç;" A. Pothen"",""Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA"; Computational Research Division, Lawrence Berkeley National Laboratory, Berkeley, CA;" Department of Computer Science, Purdue University, West Lafayette, IN"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""44"",""59"",""It is difficult to obtain high performance when computing matchings on parallel processors because matching algorithms explicitly or implicitly search for paths in the graph, and when these paths become long, there is little concurrency. In spite of this limitation, we present a new algorithm and its shared-memory parallelization that achieves good performance and scalability in computing maximum cardinality matchings in bipartite graphs. Our algorithm searches for augmenting paths via specialized breadth-first searches (BFS) from multiple source vertices, hence creating more parallelism than single source algorithms. Algorithms that employ multiple-source searches cannot discard a search tree once no augmenting path is discovered from the tree, unlike algorithms that rely on single-source searches. We describe a novel tree-grafting method that eliminates most of the redundant edge traversals resulting from this property of multiple-source searches. We also employ the recent direction-optimizing BFS algorithm as a subroutine to discover augmenting paths faster. Our algorithm compares favorably with the current best algorithms in terms of the number of edges traversed, the average augmenting path length, and the number of iterations. We provide a proof of correctness for our algorithm. Our NUMA-aware implementation is scalable to 80 threads of an Intel multiprocessor and to 240 threads on an Intel Knights Corner coprocessor. On average, our parallel algorithm runs an order of magnitude faster than the fastest algorithms available. The performance improvement is more significant on graphs with small matching number."",""1558-2183"","""",""10.1109/TPDS.2016.2546258"",""US Department of Energy"; Office of Science; Office of Advanced Scientific Computing Research, Applied Mathematics(grant numbers:DE-AC02-05CH11231); National Science Foundation(grant numbers:1218196,1552323); DOE(grant numbers:DE-FG02-13ER26135);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440857"",""Cardinality matching";bipartite graph;tree grafting;"parallel algorithms"",""Bipartite graph";Vegetation;Parallel algorithms;Algorithm design and analysis;Instruction sets;"Impedance matching"","""",""12"","""",""33"",""IEEE"",""24 Mar 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Conflict-Free Loop Mapping for Coarse-Grained Reconfigurable Architecture with Multi-Bank Memory,""S. Yin"; X. Yao; T. Lu; D. Liu; J. Gu; L. Liu;" S. Wei"",""Institute of Microelectronics, Tsinghua University, Beijing, China"; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China;" Institute of Microelectronics, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2471"",""2485"",""Coarse-grained reconfigurable architecture (CGRA) is a promising architecture with high performance, high power-efficiency and attraction of flexibility. The computation-intensive parts of an application (e.g., loops) are often mapped on CGRA for acceleration. Due to the high parallel data access demands, the architecture with multi-bank memory is proposed to improve parallelism. For CGRA with multi-bank memory, a joint solution, which simultaneously considers the memory partitioning and modulo scheduling, is proposed to achieve a valid mapping with better performance. In this solution, the modulo scheduling and operator scheduling are used to achieve a valid loop mapping and a valid data placement without any memory access conflicts. By avoiding the pipelining stalls caused by conflicts, the performance of loop mapping is greatly improved. The experimental results on benchmarks of the Livermore, Polybench and Mediabench show that our approach can improve the performance of loops on CGRA to 1.89×, 1.49× and 1.37× compared with REGIMap, HTDM and REGIMap with memory partitioning, at cost of an acceptable increase in compilation time."",""1558-2183"","""",""10.1109/TPDS.2017.2682241"",""China National High Technologies Research Program(grant numbers:2015AA016601)"; China Major S&T Project(grant numbers:2013ZX01033001-001-003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7879331"",""Multi-bank";modulo scheduling;data placement;"access pattern"",""Arrays";Pipeline processing;Registers;Routing;"Memory management"","""",""17"","""",""22"",""IEEE"",""15 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cooper: Expedite Batch Data Dissemination in Computer Clusters with Coded Gossips,""Y. Liu"; D. Niu;" M. Khabbazian"",""EMC Corporation, Edmonton, AB, Canada"; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada;" Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2204"",""2217"",""Data transfers happen frequently in server clusters for software and application deployment, and in parallel computing clusters to transmit intermediate results in batches among servers between computation stages. This paper presents Cooper, an optimized prototype system to speedup multi-batch data transfers among a cluster of servers, leveraging a theoretically proven optimal algorithm called “coded permutation gossip,” which employs a simple random topology control scheme to best utilize bandwidth and decentralized random linear network coding to maximize the useful information transmitted. On a process-level coding-transfer pipeline, we investigate the best block division, batch division and inter-batch scheduling strategies to minimize the broadcast finish time in a realistic setting. For batch-based transfers, we propose a scheduling algorithm with low overhead that overlaps the transfers of consecutive batches and temporarily prioritizes later batches, to further reduce the broadcast finish time. We describe an asynchronous and distributed implementation of Cooper and have deployed it on Amazon EC2 for evaluation. Based on results from real experiments, we show that Cooper can almost double the speed of data transfers in computing clusters, as compared to state-of-the-art content distribution tools like BitTorrent, at a low CPU overhead."",""1558-2183"","""",""10.1109/TPDS.2017.2654242"",""NSERC"; NSERC; RGPIN(grant numbers:436170);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820201"",""Content distribution";computer cluster;gossips;random linear code;parallelism;pipelining;"scheduling"",""Encoding";Servers;Computers;Data transfer;Pipeline processing;Data dissemination;"Clustering algorithms"","""","""","""",""27"",""IEEE"",""17 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CoRE: Cooperative End-to-End Traffic Redundancy Elimination for Reducing Cloud Bandwidth Cost,""L. Yu"; H. Shen; K. Sapra; L. Ye;" Z. Cai"",""School of Computer Science, Georgia Institute of Technology, Atlanta, GA"; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC; Department of Electrical and Computer Engineering, Clemson University, Clemson, SC; Department of Computer Science and Technology, Harbin Institute of Technology, Harbin, China;" Department of Computer Science, Georgia State University, Atlanta, GA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""446"",""461"",""The pay-as-you-go service model impels cloud customers to reduce the usage cost of bandwidth. Traffic Redundancy Elimination (TRE) has been shown to be an effective solution for reducing bandwidth costs, and thus has recently captured significant attention in the cloud environment. By studying the TRE techniques in a trace driven approach, we found that both short-term (time span of seconds) and long-term (time span of hours or days) data redundancy can concurrently appear in the traffic, and solely using either sender-based TRE or receiver-based TRE cannot simultaneously capture both types of traffic redundancy. Also, the efficiency of existing receiver-based TRE solution is susceptible to the data changes compared to the historical data in the cache. In this paper, we propose a Cooperative end-to-end TRE solution (CoRE) that can detect and remove both short-term and long-term redundancy through a two-layer TRE design with cooperative operations between layers. An adaptive prediction algorithm is further proposed to improve TRE efficiency through dynamically adjusting the prediction window size based on the hit ratio of historical predictions. Besides, we enhance CoRE to adapt to different traffic redundancy characteristics of cloud applications to improve its operation cost. Extensive evaluation with several real traces show that CoRE is capable of effectively identifying both short-term and long-term redundancy with low additional cost while ensuring TRE efficiency from data changes."",""1558-2183"","""",""10.1109/TPDS.2016.2578928"",""U.S. NSF(grant numbers:CNS-1249603,OCI-1064230,CNS-1254006,CNS-1049947,CNS-1156875,CNS-0917056,CNS-1057530,CNS-1025652,CNS-0938189)"; Microsoft Research Faculty Fellowship(grant numbers:8300751); U.S. Department of Energy; Oak Ridge National Laboratory including the Extreme Scale Systems Center located; ORNL; DoD(grant numbers:4000111689);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488173"",""Traffic redundancy elimination";cloud computing;network bandwidth;bandwidth cost;"end-to-end"",""Redundancy";Cloud computing;Bandwidth;Servers;Receivers;Elasticity;"Electronic mail"","""",""18"","""",""28"",""IEEE"",""9 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cost Minimization Algorithms for Data Center Management,""L. Shi"; Y. Shi; X. Wei; X. Ding;" Z. Wei"",""Department of Computer and Information, Hefei University of Technology, Hefei, Anhui, China"; Intelligent Automation Inc., Rockville, MD; Department of Computer and Information, Hefei University of Technology, Hefei, Anhui, China; Institute of Industry and Equipment Technology, Hefei University of Technology, Hefei, Anhui, China;" Department of Computer and Information, Hefei University of Technology, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""60"",""71"",""Due to the increasing usage of cloud computing applications, it is important to minimize energy cost consumed by a data center, and simultaneously, to improve quality of service via data center management. One promising approach is to switch some servers in a data center to the idle mode for saving energy while to keep a suitable number of servers in the active mode for providing timely service. In this paper, we design both online and offline algorithms for this problem. For the offline algorithm, we formulate data center management as a cost minimization problem by considering energy cost, delay cost (to measure service quality), and switching cost (to change servers's active/idle mode). Then, we analyze certain properties of an optimal solution which lead to a dynamic programming based algorithm. Moreover, by revising the solution procedure, we successfully eliminate the recursive procedure and achieve an optimal offline algorithm with a polynomial complexity. For the online algorithm, We design it by considering the worst case scenario for future workload. In simulation, we show this online algorithm can always provide near-optimal solutions."",""1558-2183"","""",""10.1109/TPDS.2016.2549016"",""National Natural Science Foundation of China(grant numbers:61370088,61501161)"; International Science & Technology Cooperation Program of China(grant numbers:2014DFB10060);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7445250"",""Data center management";offline algorithm;dynamic programming;"online algorithm"",""Servers";Algorithm design and analysis;Switches;Heuristic algorithms;Optimization;Minimization;"Delays"","""",""13"","""",""20"",""IEEE"",""31 Mar 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cost-Aware Big Data Processing Across Geo-Distributed Datacenters,""W. Xiao"; W. Bao; X. Zhu;" L. Liu"",""Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, P.R. China"; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, P.R. China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, P.R. China;" College of Computing, Georgia Institute of Technology, 266 Ferst Drive, Atlanta, GA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3114"",""3127"",""With the globalization of service, organizations continuously produce large volumes of data that need to be analysed over geo-dispersed locations. Traditionally central approach that moving all data to a single cluster is inefficient or infeasible due to the limitations such as the scarcity of wide-area bandwidth and the low latency requirement of data processing. Processing big data across geo-distributed datacenters continues to gain popularity in recent years. However, managing distributed MapReduce computations across geo-distributed datacenters poses a number of technical challenges: how to allocate data among a selection of geo-distributed datacenters to reduce the communication cost, how to determine the Virtual Machine (VM) provisioning strategy that offers high performance and low cost, and what criteria should be used to select a datacenter as the final reducer for big data analytics jobs. In this paper, these challenges is addressed by balancing bandwidth cost, storage cost, computing cost, migration cost, and latency cost, between the two MapReduce phases across datacenters. We formulate this complex cost optimization problem for data movement, resource provisioning and reducer selection into a joint stochastic integer nonlinear optimization problem by minimizing the five cost factors simultaneously. The Lyapunov framework is integrated into our study and an efficient online algorithm that is able to minimize the long-term time-averaged operation cost is further designed. Theoretical analysis shows that our online algorithm can provide a near optimum solution with a provable gap and can guarantee that the data processing can be completed within pre-defined bounded delays. Experiments on WorldCup98 web site trace validate the theoretical analysis results and demonstrate that our approach is close to the offline-optimum performance and superior to some representative approaches."",""1558-2183"","""",""10.1109/TPDS.2017.2708120"",""National Natural Science Foundation of China(grant numbers:61572511)"; National University of Defense Technology(grant numbers:ZK16-03-57,ZK16-03-09); China Post-doctoral Science Foundation(grant numbers:2016M602960); Southwest Electron & Telecom Technology Institute(grant numbers:2015014); Georgia Tech; US National Science Foundation(grant numbers:NSF 1547102,SaTC 1564097); IBM;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7934132"",""Big data processing";cloud computing;data movement;virtual machine scheduling;"online algorithm"",""Distributed databases";Cloud computing;Bandwidth;Big Data;Algorithm design and analysis;"Optimization"","""",""39"","""",""38"",""IEEE"",""25 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cost-Aware Region-Level Data Placement in Multi-Tiered Parallel I/O Systems,""S. He"; Y. Wang; Z. Li; X. -H. Sun;" C. Xu"",""State Key Laboratory of Software Engineering, Computer School, Wuhan University, Luojiashan, Wuhan, Hubei, China"; Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Xueyuan Blvd. 1068, Shenzhen, China; School of Computer Sciences, Western Illinois University, Macomb, IL; Department of Computer Science, Illinois Institute of Technology, Chicago, IL;" Shenzhen Institute of Advanced Technology, Chinese Academy of Science, Xueyuan Blvd. 1068, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1853"",""1865"",""Multi-tiered Parallel I/O systems that combine traditional HDDs with emerging SSDs mitigate the cost burden of SSDs while benefiting from their superior I/O performance. While a multi-tiered parallel I/O system is promising for data-intensive applications in high-performance (HPC) domains, placing data on each tier of the system to achieve high I/O performance remains a challenge. In this paper, we propose a cost-aware region-level (CARL) data placement scheme in multi-tiered parallel I/O systems. CARL divides a large file into several small regions, and then places regions on different types of servers based on region access costs. CARL includes a static policy S-CARL and a dynamic policy D-CARL. For applications whose I/O access patterns are completely known, S-CARL calculates the region costs within the entire workload duration, and uses a static data placement scheme to selectively place regions on the proper servers. To adapt to applications whose access patterns are unknown in advance, D-CARL uses a dynamic data placement scheme which migrates data among different servers within each time window. We have implemented CARL under MPI-IO library and OrangeFS parallel file system environment. Our evaluation with representative benchmarks and an application shows that CARL is both feasible and able to improve I/O performance significantly."",""1558-2183"","""",""10.1109/TPDS.2016.2636837"",""National Science Foundation of China(grant numbers:61572377,61672513,U1401258,61550110250)"; China National Basic Research Program 973 Program(grant numbers:2015CB352400); Natural Science Foundation of Hubei Province(grant numbers:2014CFB239); Open Fund from HPCL(grant numbers:201512-02); Open Fund from SKLSE(grant numbers:2015-A-06); Science and Technology Planning Project of Guangdong Province(grant numbers:2015B010129011,2016A030313183);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7776880"",""Parallel I/O system";parallel file system;data placement;"solid state drive"",""Servers";Data models;System performance;Optimization;Distributed databases;Software;"Computers"","""",""10"","""",""50"",""IEEE"",""7 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CRED: Cloud Right-Sizing with Execution Deadlines and Data Locality,""M. Xu"; S. Alamro; T. Lan;" S. Subramaniam"",""Department of Electrical and Computer Engineering, George Washington University, Washington, DC"; Department of Electrical and Computer Engineering, George Washington University, Washington, DC; Department of Electrical and Computer Engineering, George Washington University, Washington, DC;" Department of Electrical and Computer Engineering, George Washington University, Washington, DC"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3389"",""3400"",""As demands for cloud-based data processing continue to grow, cloud providers seek effective techniques that deliver value to the businesses without violating Service Level Agreements (SLAs). Cloud right-sizing has emerged as a very promising technique for making cloud services more cost-effective. In this paper, we present CRED, a novel framework for cloud right-sizing with execution deadlines and data locality constraints. CRED jointly optimizes data placement and task scheduling in data centers with the aim of minimizing the number of nodes needed while meeting users' SLA requirements. We formulate CRED as an integer optimization problem and present a heuristic algorithm with provable performance guarantees to solve the problem. Competitive ratios of the proposed algorithm are quantified in closed form for arbitrary task parameters and cloud configurations. We also extend our work to obtain a resilient solution, which allows successful recovery at run time from any single node failure and is guaranteed to meet both deadline and locality constraints. Simulation results using Google trace show that our proposed algorithm significantly outperforms existing heuristics such as first-fit by reducing the number of required active servers by up to 47 percent, and achieves near-optimal performance. We also show that our algorithm can significantly improve utilization of both computational resources and storage space by up to 28 and 15 percent, respectively."",""1558-2183"","""",""10.1109/TPDS.2017.2726071"",""US National Science Foundation(grant numbers:CSR-1320226)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7976384"",""Cloud right-sizing";scheduling;data locality;"failure recovery"",""Cloud computing";Scheduling;Optimization;Heuristic algorithms;Failure analysis;Data models;"Distributed databases"","""",""6"","""",""27"",""IEEE"",""12 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cutting Latency Tail: Analyzing and Validating Replication without Canceling,""Z. Qiu"; J. F. Pérez; R. Birke; L. Chen;" P. G. Harrison"",""Department of Computing, Kensington, London, United Kingdom"; Department of Applied Mathematics and Computer Science, Universidad del Rosario, Bogotá, Colombia; IBM Research Zurich, Saumerstrasse, Ruschlikon, Switzerland; IBM Research Zurich, Saumerstrasse, Ruschlikon, Switzerland;" Department of Computing, Kensington, London, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3128"",""3141"",""Response time variability in software applications can severely degrade the quality of the user experience. To reduce this variability, request replication emerges as an effective solution by spawning multiple copies of each request and using the result of the first one to complete. Most previous studies have mainly focused on the mean latency for systems implementing replica cancellation, i.e., all replicas of a request are canceled once the first one finishes. Instead, we develop models to obtain the response-time distribution for systems where replica cancellation may be too expensive or infeasible to implement, as in “fast” systems, such as web services, or in legacy systems. Furthermore, we introduce a novel service model to explicitly consider correlation in the processing times of the request replicas, and design an efficient algorithm to parameterize the model from real data. Extensive evaluations on a MATLAB benchmark and a three-tier web application (MediaWiki) show remarkable accuracy, e.g., 7 (4 percent) average error on the 99th percentile response time for the benchmark (respectively, MediaWiki), the requests of which execute in the order of seconds (respectively, milliseconds). Insights into optimal replication levels are thereby gained from this precise quantitative analysis, under a wide variety of system scenarios."",""1558-2183"","""",""10.1109/TPDS.2017.2706268"",""Australian Research Council(grant numbers:Centre of Excellence for Mathematical and Statisti)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932099"",""Software quality engineering";speculative computing;matrix analytic methods;"correlated service times"",""Time factors";Load modeling;Servers;Cloud computing;Computational modeling;"Analytical models"","""",""7"","""",""38"",""IEEE"",""19 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Deadline-Constrained Cost Optimization Approaches for Workflow Scheduling in Clouds,""Q. Wu"; F. Ishikawa; Q. Zhu; Y. Xia;" J. Wen"",""College of Computer Science, Chongqing University, Chongqing, China"; National Institute of Informatics, Tokyo, Japan; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China;" College of Software Engineering, Chongqing University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3401"",""3412"",""Nowadays it is becoming more and more attractive to execute workflow applications in the cloud because it enables workflow applications to use computing resources on demand. Meanwhile, it also challenges traditional workflow scheduling algorithms that only concentrate on optimizing the execution time. This paper investigates how to minimize execution cost of a workflow in clouds under a deadline constraint and proposes a metaheuristic algorithm L-ACO as well as a simple heuristic ProLiS. ProLiS distributes the deadline to each task, proportionally to a novel definition of probabilistic upward rank, and follows a two-step list scheduling methodology: rank tasks and sequentially allocates each task a service which meets the sub-deadline and minimizes the cost. L-ACO employs ant colony optimization to carry out deadline-constrained cost optimization: the ant constructs an ordered task list according to the pheromone trail and probabilistic upward rank, and uses the same deadline distribution and service selection methods as ProLiS to build solutions. Moreover, the deadline is relaxed to guide the search of L-ACO towards constrained optimization. Experimental results show that compared with traditional algorithms, the performance of ProLiS is very competitive and L-ACO performs the best in terms of execution costs and success ratios of meeting deadlines."",""1558-2183"","""",""10.1109/TPDS.2017.2735400"",""National Natural Science Foundation of China(grant numbers:61472051)"; Fundamental Research Funds for the Central Universities(grant numbers:0903005203386); CDJXY(grant numbers:0216005202069);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000634"",""Ant colony optimization";cloud computing;constrained optimization;deadline;"workflow scheduling"",""Scheduling";Cloud computing;Optimization;Processor scheduling;Computational modeling;"Probabilistic logic"","""",""135"","""",""41"",""IEEE"",""3 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Delegation of Computation with Verification Outsourcing: Curious Verifiers,""G. Xu"; G. T. Amariucai;" Y. Guan"",""Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, 50011"; Iowa State University, Ames, IA;" Iowa State University, Ames, IA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""717"",""730"",""In the Cloud Computing paradigm, a user often reduces financial, personnel, and computational burdens by outsourcing computation and other IT services to a professional service provider. However, to be able to assure the correctness of the result, the user still needs to perform the verification himself. Such verification may be tedious and expensive. Consequently, users are likely to outsource (again) the verification workload to a third party. Other scenarios such as auditing and arbitrating may also require the use of third-party verification. Outsourcing verification will introduce new security challenges. One such challenge is to protect the computational task and the results from the untrusted third party verifier. In this work, we address this problem by proposing an efficient verification outsourcing scheme. To our knowledge, this is the first solution to the verification outsourcing problem. We show that, without using expensive fully-homomorphic encryption, an honest-but-curious third party can help to verify the result of an outsourced computational task without having to learn either the computational task or the result thereof. We have implemented our design by combining a novel commitment protocol and an additive-homomorphic encryption in the argument system model. The total cost of the verification in our design is less than the verifier's cost in the state-of-the-art argument systems that rely only on standard cryptographic assumptions."",""1558-2183"","""",""10.1109/TPDS.2016.2598342"",""US National Science Foundation(grant numbers:CNS-0644238,CNS-0831470)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7533495"",""Delegation of verification";argument systems;PCPs;cloud computing;verifiable computation;"fully confidentiality-preserving verifiable computation"",""Outsourcing";Cloud computing;Computational modeling;Servers;Encryption;Protocols;"Logic gates"","""",""6"","""",""41"",""IEEE"",""4 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Design and Implementation of a Communication-Optimal Classifier for Distributed Kernel Support Vector Machines,""Y. You"; J. Demmel; K. Czechowski; L. Song;" R. Vuduc"",""Computer Science Division, UC Berkeley, CA"; Computer Science Division, UC Berkeley, CA; College of Computing, Georgia Tech, GA; College of Computing, Georgia Tech, GA;" College of Computing, Georgia Tech, GA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""974"",""988"",""We consider the problem of how to design and implement communication-efficient versions of parallel kernel support vector machines, a widely used classifier in statistical machine learning, for distributed memory clusters and supercomputers. The main computational bottleneck is the training phase, in which a statistical model is built from an input data set. Prior to our study, the parallel isoefficiency of a state-of-the-art implementation scaled as W = Ω(P3), where W is the problem size and P the number of processors";" this scaling is worse than even a one-dimensional block row dense matrix vector multiplication, which has W = Ω(P2). This study considers a series of algorithmic refinements, leading ultimately to a Communication-Avoiding SVM method that improves the isoefficiency to nearly W = Ω(P). We evaluate these methods on 96 to 1,536 processors, and show average speedups of 3 - 16x (7× on average) over Dis-SMO, and a 95 percent weak-scaling efficiency on six real-world datasets, with only modest losses in overall classification accuracy. The source code can be downloaded at [1]."",""1558-2183"","""",""10.1109/TPDS.2016.2608823"",""U.S. Department of Energy"; Office of Science; Office of Advanced Scientific Computing Research; Applied Mathematics program(grant numbers:DE-SC0010200); U.S. Department of Energy; Office of Science; Office of Advanced Scientific Computing Research(grant numbers:DE-SC0008700,AC02-05CH11231); DARPA(grant numbers:HR0011-12-2-0016); Intel; Google; HP; Huawei; LGE; Nokia; NVIDIA; Oracle; Samsung; NSF; NIH; BIGDATA(grant numbers:1R01GM108341,ONR N00014-15-1- 2340,NSF IIS-1218749); NSF; CAREER(grant numbers:IIS-1350983); Intel; NVIDIA;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7565530"",""Distributed memory algorithms";communication-avoidance;"statistical machine learning"",""Support vector machines";Training;Kernel;Data models;Program processors;Partitioning algorithms;"Optimization"","""",""9"","""",""37"",""IEEE"",""13 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Design Space Exploration of 2-D Processor Array Architectures for Similarity Distance Computation,""A. Kanan"; F. Gebali;" A. Ibrahim"",""Department of Electrical and Computer Engineering, University of Victoria, Victoria, BC, Canada"; Department of Electrical and Computer Engineering, University of Victoria, Victoria, BC, Canada;" Department of Microelectronics, Electronics Research Institute, Cairo, Egypt"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2218"",""2228"",""We present a systematic methodology for exploring the design space of similarity distance computation in machine learning algorithms. Previous architectures proposed in the literature have been obtained using ad hoc techniques that do not allow for design space exploration. The size and dimensionality of the input datasets have not been taken into consideration in previous works. This may result in impractical designs that are not amenable for hardware implementation. The methodology presented in this work is used to obtain the 3-D computation domain of the similarity distance computation algorithm. A scheduling function determines whether an algorithm variable is pipelined or broadcast. Four linear scheduling functions are presented, and six possible 2-D processor array architectures are obtained and classified based on the size and dimensionality of the input datasets. The obtained designs are analyzed in terms of speed and area, and compared with previously obtained designs. The proposed designs achieve better time and area complexities."",""1558-2183"","""",""10.1109/TPDS.2017.2656900"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829398"",""Processor arrays";similarity distance;machine learning;data mining;big data analytics;"parallel processing"",""Parallel processing";Algorithm design and analysis;Space exploration;Machine learning algorithms;Scheduling;"Arrays"","""",""3"","""",""33"",""IEEE"",""23 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"DGLB: Distributed Stochastic Geographical Load Balancing over Cloud Networks,""T. Chen"; A. G. Marques;" G. B. Giannakis"",""Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN"; Department of Signal Theory and Communications, Rey Juan Carlos University, Madrid, Spain;" Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1866"",""1880"",""Contemporary cloud networks are being challenged by the rapid increase of user demands and growing concerns about global warming, due to their substantial energy consumption. This requires future data centers to be both energy efficient and sustainable, which calls for leveraging cutting-edge features and the flexibility provided by the modern smart grids. To fulfill those goals, this paper puts forward a systematic approach to designing energy-aware traffic-efficient geographical load balancing schemes for data-center networks that are not only optimal, but also computationally efficient and amenable to distributed implementation. Under this comprehensive approach, workload and power balancing schemes are designed jointly across the network, both delay-tolerant and interactive workloads are accommodated, novel smart-grid features such as energy storage units are incorporated to cope with renewables, and incentive pricing mechanisms are adopted in the design. To further account for the spatio-temporal variation of demands, energy prices and renewables, the task is formulated as a two-timescale stochastic optimization. Leveraging dual stochastic approximation and the fast iterative shrinkage-thresholding algorithm (FISTA), the proposed optimization is decomposed across time slots (first-stage) and data centers (second-stage). While the resultant online algorithm is strictly feasible and provably optimal under a Markovian assumption for the underlying random processes, extensive numerical tests further demonstrate that it also works well in real-data scenarios, where the underlying randomness is highly correlated across time."",""1558-2183"","""",""10.1109/TPDS.2016.2636210"",""National Science Foundation(grant numbers:1509040,1508993,1423316,1442686)"; MINECO(grant numbers:TEC2013-41604-R); CAM(grant numbers:S2013/ICE-2933);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7775032"",""Data center";renewables;energy storages;incentive payment;network resource allocation;"stochastic programming"",""Manganese";Optimization;Resource management;Load management;Servers;Cooling;Routing;"Global warming"","""",""46"","""",""43"",""IEEE"",""6 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Diagnosing Performance Variations by Comparing Multi-Level Execution Traces,""F. Doray";" M. Dagenais"",""Computer and Software Engineering Department, Ecole Polytechnique de Montreal, Montreal, QC, Canada";" Computer and Software Engineering Department, Ecole Polytechnique de Montreal, Montreal, QC, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""462"",""474"",""Tracing allows the analysis of task interactions with each other and with the operating system. Locating performance problems in a trace is not trivial because of their large size. Furthermore, deep knowledge of all components of the observed system is required to decide whether observed behavior is normal. We introduce TraceCompare, a framework that automatically identifies differences between groups of executions of the same task at the user space and kernel levels. Many performance problems manifest themselves as variations that are easily identified by our framework. Our comparison algorithm takes into account all threads that affect the completion time of analyzed executions. Differences are correlated with application code to facilitate the correction of identified problems. Performance characteristics of task executions are represented by a new data structure called enhanced calling context tree (ECCT). We demonstrate the efficiency of our approach by presenting four case studies in which TraceCompare was used to uncover serious performance problems in enterprise and open source applications, without any prior knowledge of their codebase. We also show that the overhead of our tracing solution is between 0.2 and 9 percent depending on the type of application."",""1558-2183"","""",""10.1109/TPDS.2016.2567390"",""Natural Sciences and Engineering Council of Canada";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7469402"",""Performance analysis";tracing;software visualization;concurrency;"operating systems"",""Instruction sets";Libraries;Kernel;Google;Aerospace electronics;"History"","""",""12"","""",""34"",""IEEE"",""12 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Diamond Tiling: Tiling Techniques to Maximize Parallelism for Stencil Computations,""U. Bondhugula"; V. Bandishti;" I. Pananilath"",""Department of Computer Science and Automation, Indian Institute of Science, Bangalore, Karnataka, India"; Department of Computer Science and Automation, Indian Institute of Science, Bangalore, Karnataka, India;" Department of Computer Science and Automation, Indian Institute of Science, Bangalore, Karnataka, India"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1285"",""1298"",""Most stencil computations allow tile-wise concurrent start, i.e., there always exists a face of the iteration space and a set of tiling directions such that all tiles along that face can be started concurrently. This provides load balance and maximizes parallelism. However, existing automatic tiling frameworks often choose hyperplanes that lead to pipelined start-up and load imbalance. We address this issue with a new tiling technique, called diamond tiling, that ensures concurrent start-up as well as perfect load-balance whenever possible. We first provide necessary and sufficient conditions for a set of tiling hyperplanes to allow concurrent start for programs with affine data accesses. We then provide an approach to automatically find such hyperplanes. Experimental evaluation on a 12-core Intel Westmere shows that diamond tiled code is able to outperform a tuned domain-specific stencil code generator by 10 to 40 percent, and previous compiler techniques by a factor of 1.3x to 10.1x."",""1558-2183"","""",""10.1109/TPDS.2016.2615094"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582549"",""Compilers";program transformation;loop tiling;parallelism;locality;"stencils"",""Face";Diamond;Parallel processing;Shape;Indexes;Silicon;"Optimization"","""",""40"","""",""54"",""IEEE"",""4 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Dissecting GPU Memory Hierarchy Through Microbenchmarking,""X. Mei";" X. Chu"",""Department of Computer Science, Hong Kong Baptist University, Kowloon, Hong Kong";" HKBU Institute of Research and Continuing Education"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""72"",""86"",""Memory access efficiency is a key factor in fully utilizing the computational power of graphics processing units (GPUs). However, many details of the GPU memory hierarchy are not released by GPU vendors. In this paper, we propose a novel fine-grained microbenchmarking approach and apply it to three generations of NVIDIA GPUs, namely Fermi, Kepler, and Maxwell, to expose the previously unknown characteristics of their memory hierarchies. Specifically, we investigate the structures of different GPU cache systems, such as the data cache, the texture cache and the translation look-aside buffer (TLB). We also investigate the throughput and access latency of GPU global memory and shared memory. Our microbenchmark results offer a better understanding of the mysterious GPU memory hierarchy, which will facilitate the software optimization and modelling of GPU architectures. To the best of our knowledge, this is the first study to reveal the cache properties of Kepler and Maxwell GPUs, and the superiority of Maxwell in shared memory performance under bank conflict."",""1558-2183"","""",""10.1109/TPDS.2016.2549523"",""GRF"; HKBU(grant numbers:210412); HKBU(grant numbers:FRG2/14-15/059); Shenzhen Basic Research(grant numbers:SCI-2015-SZTIC-002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7445236"",""GPU";CUDA;memory hierarchy;cache structure;"throughput"",""Graphics processing units";Throughput;Computer architecture;Instruction sets;Benchmark testing;Hardware;"Kernel"","""",""104"","""",""37"",""IEEE"",""31 Mar 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Mining of Contrast Patterns,""D. Savage"; X. Zhang; P. Chou; X. Yu;" Q. Wang"",""School of Computer Science and Information Technology, RMIT University, Melbourne, VIC, Australia"; School of Computer Science and Information Technology, RMIT University, Melbourne, VIC, Australia; Australian Transaction Reports and Analysis Centre, Melbourne, VIC, Australia; Platform Technologies Research Institute, RMIT University, Melbourne, VIC, Australia;" Platform Technologies Research Institute, RMIT University, Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1881"",""1890"",""In this paper we propose a novel algorithm for mining contrast patterns using a distributed, map-reduce like framework. Contrast patterns describe differences between contrasted data sets and have previously been used for building highly accurate classifiers. However, mining for contrast patterns is a computationally expensive task and existing algorithms are designed to run in a sequential manner on a single machine. Consequently, existing approaches are unable to handle dense, high volume and high dimensional databases. Our algorithm addresses this problem by partitioning the search-space for contrast patterns into small, independent units. These units can be mined in parallel, providing a scalable solution for mining large data sets. Using three different real-world data sets we test an implementation of our algorithm on a Spark cluster. Results of these tests indicate that our algorithm achieves a high-degree of parallelism and scalability."",""1558-2183"","""",""10.1109/TPDS.2016.2637914"",""Australian Research Council"; Australian Transaction Reports and Analysis Centre(grant numbers:LP120200128);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782416"",""Contrast patterns";emerging patterns;distributed;parallel;"spark"",""Data mining";Algorithm design and analysis;Sparks;Distributed databases;Clustering algorithms;"Distributed algorithms"","""",""5"","""",""24"",""IEEE"",""13 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Shortcut Networks: Low-Latency Low-Degree Non-Random Topologies Targeting the Diameter and Cable Length Trade-Off,""N. T. Truong"; I. Fujiwara; M. Koibuchi;" K. -V. Nguyen"",""Graduate University for Advanced Studies (SOKENDAI), Hayama, Kanagawa Prefecture, Japan"; National Institute of Informatics, 2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo, Japan; Graduate University for Advanced Studies (SOKENDAI), Hayama, Kanagawa Prefecture, Japan;" Ha Noi University of Science and Technology, 1 Dai Co Viet Road, Ha Noi, Viet Nam"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""989"",""1001"",""Low communication latency becomes a main concern in highly parallel computers and supercomputers that reach millions of processing cores. Random network topologies are better suited to achieve low average shortest path length and low diameter in terms of the hop counts between nodes. However, random topologies lead to two problems: (1) increased aggregate cable length on a machine room floor that would become dominant for communication latency in next-generation custom supercomputers, and (2) high routing complexity that typically requires a routing table at each node (e.g., topology-agnostic deadlock-free routing). In this context, we first propose low-degree non-random topologies that exploit the small-world effect, which has been well modeled by some random network models. Our main idea is to carefully design a set of various-length shortcuts that keep the diameter small while maintaining a short cable length for economical passive electric cables. We also propose custom routing that uses the regularity of the various-length shortcuts. Our experimental graph analyses show that our proposed topology has low diameter and low average shortest path length, which are considerably better than those of the counterpart 3-D torus and are near to those of a random topology with the same average degree. The proposed topology has average cable length drastically shorter than that of the counterpart random topology, which leads to low cost of interconnection networks. Our custom routing takes non-minimal paths to provide lower zero-load latency than the minimal custom routings on different counterpart topologies. Our discrete-event simulation results using SimGrid show that our proposed topology is suitable for applications that have irregular communication patterns or non-nearest neighbor collective communication patterns."",""1558-2183"","""",""10.1109/TPDS.2016.2613043"",""JST"; CREST; MIC; SCOPE(grant numbers:152103004,KAKENHI #16H02816); Vietnamese Ministry of Sci. and Tech;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576694"",""Network topologies";small-world networks;interconnection networks;"high-performance computing"",""Topology";Network topology;Switches;Delays;Routing;Communication cables;"Supercomputers"","""",""7"","""",""28"",""OAPA"",""26 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Durable Address Translation in PCM-Based Flash Storage Systems,""D. Liu"; K. Zhong; T. Wang; Y. Wang; Z. Shao; E. H. -M. Sha;" J. Xue"",""College of Computer Science, Chongqing University, Chongqing, China"; College of Computer Science, Chongqing University, Chongqing, China; Department of Computer Science, University of Toronto, Toronto, ON, Canada; College of Computer Science & Software Engineering, Shenzhen University, Shenzhen, China; Department of Computing, the Hong Kong Polytechnic University, Hung Hom, Hong Kong; College of Computer Science, Chongqing University, Chongqing, China;" School of Computer Science and Engineering, University of New South Wales, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""475"",""490"",""Phase change memory (PCM) is a promising DRAM alternative because of its non-volatility, high density, low standby power and close-to-DRAM performance. These features make PCM an attractive solution to optimize the management of NAND flash memory in embedded systems. However, PCM's limited write endurance hinders its application in embedded systems. Therefore, how to manage flash memory with PCM-particularly guarantee PCM a reasonable lifetime-becomes a challenging issue. In this paper, we propose to partially replace DRAM using PCM to optimize the management of flash memory metadata for better system reliability in the presence of power failure and system crash. To prolong PCM's lifetime, we present a write-activity-aware PCM-assisted flash memory management scheme, called PCM-FTL. By differentiating sequential and random I/O behaviors, a novel two-level mapping mechanism and a customized wear-leveling scheme are developed to reduce writes to PCM and extend its lifetime. We evaluate PCM-FTL with a variety of general-purpose and mobile I/O workloads. Experimental results show that PCM-FTL can significantly reduce write activities and achieve an even distribution of writes in PCM with very low overhead."",""1558-2183"","""",""10.1109/TPDS.2016.2586059"",""National Natural Science Foundation of China(grant numbers:61309004,61272103,61373049,61472052,61402061)"; Research Fund for the Doctoral Program of Higher Education of China(grant numbers:20130191120030); Chongqing High-Tech Research Program(grant numbers:cstc2016jcyjA0332,cstc2016jcyjA0274); Fundamental Research Funds for the Central Universities(grant numbers:CDJZR14185501); Chongqing University(grant numbers:2012T0006); Research Grants Council of the Hong Kong Special Administrative Region, China(grant numbers:GRF 152138/14E,GRF 15222315/15E); Hong Kong Polytechnic University(grant numbers:4-ZZD7,G-YK24,G-YM10,G-YN36,ARC DP150102109);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501824"",""Phase change memory";NAND flash memory;flash translation layer;"write activity"",""Phase change materials";Random access memory;Microprocessors;Embedded systems;Metadata;"Memory management"","""",""34"","""",""53"",""IEEE"",""29 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Dynamic Associativity Management in Tiled CMPs by Runtime Adaptation of Fellow Sets,""S. Das";" H. K. Kapoor"",""Department of CSE, Indian Institute of Information Technology Guwahati, Assam, India";" Department of CSE, Indian Institute of Technology Guwahati, Assam, India"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2229"",""2243"",""The non-uniform distribution of memory accesses among the cache sets results in some sets being used heavily while certain others remaining underutilized. Dynamic associativity management (DAM) is a technique to allow the heavily used sets to distribute their load among the lightly used sets thus improving the overall utilization of the cache. CMP-SVR is a previously proposed DAM based technique, where each set is divided into two sections: normal storage (NT) and reserve storage (RT). Some number of ways (25 to 50 percent) from each set are reserved for RT and the remaining ways belong to NT. The sets are divided into groups called fellow-groups and a set can use the reserve-ways of its fellow sets to increase its associativity during execution. Though CMP-SVR improves performance the formation of its fellow-groups is static: once created it never changes. It has been observed that some fellow-groups have more number of heavily used sets than the other fellow-groups. As a result the cache loads are not uniformly distributed among the fellow-groups. Also the behavior of sets changes dynamically: a lightly used set may become heavily used after a number of execution cycles. This paper studies the behavior of each set in detail and proposes a DAM based technique which improves the performance compared to other DAM based techniques. The proposed technique called FS-DAM dynamically creates fellow-groups based on the current set loads ensuring that the heavily used sets are evenly distributed among all the fellow-groups. Such distribution increases the utilization of the cache and hence improves performance. Full system simulation shows an average of 6.62 and 16.74 percent improvements, in FS-DAM as compared to CMP-SVR, in terms of CPI (Cycles Per Instruction) and MPKI (Miss Per Thousand Instructions) respectively. Comparing with Z-Cache the improvements are 6.21 percent (CPI) and 14.65 percent (MPKI). The proposed policy also shows better performance over V-Way and SBC."",""1558-2183"","""",""10.1109/TPDS.2017.2657512"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7831453"",""DAM";CMP-SVR;victim retention;NUCA;"V-way"",""Indexes";Decision support systems;Benchmark testing;Mathematical model;Electronic mail;Arrays;"Energy storage"","""",""6"","""",""42"",""IEEE"",""24 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Dynamic Configuration of Partitioning in Spark Applications,""A. Gounaris"; G. Kougka; R. Tous; C. T. Montes;" J. Torres"",""Aristotle University of Thessaloniki, Thessaloniki, Greece"; Aristotle University of Thessaloniki, Thessaloniki, Greece; Universitat Politècnica de Catalunya, Barcelona, Spain; Barcelona Supercomputing Centre, Barcelona, Spain;" Barcelona Supercomputing Centre, Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1891"",""1904"",""Spark has become one of the main options for large-scale analytics running on top of shared-nothing clusters. This work aims to make a deep dive into the parallelism configuration and shed light on the behavior of parallel spark jobs. It is motivated by the fact that running a Spark application on all the available processors does not necessarily imply lower running time, while may entail waste of resources. We first propose analytical models for expressing the running time as a function of the number of machines employed. We then take another step, namely to present novel algorithms for configuring dynamic partitioning with a view to minimizing resource consumption without sacrificing running time beyond a user-defined limit. The problem we target is NP-hard. To tackle it, we propose a greedy approach after introducing the notions of dependency graphs and of the benefit from modifying the degree of partitioning at a stage";" complementarily, we investigate a randomized approach. Our polynomial solutions are capable of judiciously use the resources that are potentially at user's disposal and strike interesting trade-offs between running time and resource consumption. Their efficiency is thoroughly investigated through experiments based on real execution data."",""1558-2183"","""",""10.1109/TPDS.2017.2647939"",""Spanish Ministry of Economy and Competitivity(grant numbers:TIN2015-65316-P)"; SGR programme(grant numbers:2014-SGR-1051); Catalan Government;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7807262"",""Data repartitioning";data flow optimization;data flow profiling;"spark"",""Sparks";Benchmark testing;Parallel processing;Aggregates;Analytical models;Heuristic algorithms;"Electronic mail"","""",""45"","""",""31"",""IEEE"",""5 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Dynamic Resizing on Active Warps Scheduler to Hide Operation Stalls on GPUs,""M. K. Yoon"; Y. Oh; S. H. Kim; S. Lee; D. Kim;" W. W. Ro"",""School of Electrical and Electronic Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea"; School of Electrical and Electronic Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea; School of Electrical and Electronic Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea;" School of Electrical and Electronic Engineering, Yonsei University, Seodaemun-gu, Seoul, Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3142"",""3156"",""This paper conducts a detailed study of the factors affecting the operation stalls in terms of the fetch group size on the warp scheduler of GPUs. Throughout this paper, we reveal that the size of a fetch group is highly involved for hiding various types of operation stalls: short latency stalls, long latency stalls, and Load/Store Unit (LSU) stalls. The scheduler with a small fetch group cannot hide short latency stalls due to the limited number of warps in a fetch group. In contrast, the scheduler with a large fetch group cannot hide long latency and LSU stalls due to the limited number of fetch groups and the lack of memory subsystems, respectively. To hide various types of stalls, this paper proposes a Dynamic Resizing on Active Warps (DRAW) scheduler which adjusts the size of a fetch group dynamically based on the execution phases of applications. For the applications that have the best performance at LRR (one fetch group), the DRAW scheduler matches the performance of LRR and outperforms TL (multiple fetch groups) by 22.7 percent. In addition, for the applications that have the best performance at TL, our scheduler achieves 11.0 and 5.5 percent better performance compared to LRR and TL, respectively."",""1558-2183"","""",""10.1109/TPDS.2017.2704080"",""National Research Foundation of Korea (NRF)"; MSIP(grant numbers:NRF-2015R1A2A2A01008281); GPU; IEEE International Symposium on Performance Analysis of Systems and Software(grant numbers:ISPASS 2015);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7927466"",""Graphics Processing Units (GPUs)";General Purpose on GPUs (GPGPUs);"warp scheduler"",""Graphics processing units";Message systems;Instruction sets;Registers;Scheduling;"System-on-chip"","""",""3"","""",""45"",""IEEE"",""12 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Dynamic Service Placement for Mobile Micro-Clouds with Predicted Future Costs,""S. Wang"; R. Urgaonkar; T. He; K. Chan; M. Zafer;" K. K. Leung"",""IBM T. J. Watson Research Center, NY, United States"; Amazon Inc., Seattle, WA; School of Electrical Engineering and Computer Science, Pennsylvania State University, PA, United States; Army Research Laboratory, Adelphi, MD; Nyansa Inc., Palo Alto, CA;" Department of Electrical and Electronic Engineering, Imperial College London, London, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1002"",""1016"",""Mobile micro-clouds are promising for enabling performance-critical cloud applications. However, one challenge therein is the dynamics at the network edge. In this paper, we study how to place service instances to cope with these dynamics, where multiple users and service instances coexist in the system. Our goal is to find the optimal placement (configuration) of instances to minimize the average cost overtime, leveraging the ability of predicting future cost parameters with known accuracy. We first propose an offline algorithm that solves for the optimal configuration in a specific look-ahead time-window. Then, we propose an online approximation algorithm with polynomial time-complexity to find the placement in real-time whenever an instance arrives. We analytically show that the online algorithm is 0(1)-competitive for a broad family of cost functions. Afterwards, the impact of prediction errors is considered and a method for finding the optimal look-ahead window size is proposed, which minimizes an upper bound of the average actual cost. The effectiveness of the proposed approach is evaluated by simulations with both synthetic and real-world (San Francisco taxi) usermobility traces. The theoretical methodology used in this paper can potentially be applied to a larger class of dynamic resource allocation problems."",""1558-2183"","""",""10.1109/TPDS.2016.2604814"",""Army Research Laboratory"; U.K. Ministry of Defence(grant numbers:W911NF-06-3-0001); U.S. Army Research Laboratory; U.K. Ministry of Defence;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7557016"",""Cloud computing";fog/edge computing;online approximation algorithm;optimization;resource allocation;"wireless networks"",""Approximation algorithms";Heuristic algorithms;Prediction algorithms;Cloud computing;Electronic mail;Algorithm design and analysis;"Cost function"","""",""184"","""",""32"",""IEEE"",""31 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"EAFR: An Energy-Efficient Adaptive File Replication System in Data-Intensive Clusters,""Y. Lin";" H. Shen"",""Department of Electrical and Computer Engineering, Clemson University, Clemson, SC";" Department of Computer Science, University of Virginia, Charlottesville, VA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1017"",""1030"",""In data intensive clusters, a large amount of files are stored, processed and transferred simultaneously. To increase the data availability, some file systems create and store three replicas for each file in randomly selected servers across different racks. However, they neglect the file heterogeneity and server heterogeneity, which can be leveraged to further enhance data availability and file system efficiency. As files have heterogeneous popularities, a rigid number of three replicas may not provide immediate response to an excessive number of read requests to hot files, and waste resources (including energy) for replicas of cold files that have few read requests. Also, servers are heterogeneous in network bandwidth, hardware configuration and capacity (i.e., the maximal number of service requests that can be supported simultaneously), it is crucial to select replica servers to ensure low replication delay and request response delay. In this paper, we propose an Energy-Efficient Adaptive File Replication System (EAFR), which incorporates three components. It is adaptive to time-varying file popularities to achieve a good tradeoff between data availability and efficiency. Higher popularity of a file leads to more replicas and vice versa. Also, to achieve energy efficiency, servers are classified into hot servers and cold servers with different energy consumption, and cold files are stored in cold servers. EAFR then selects a server with sufficient capacity (including network bandwidth and capacity) to hold a replica. To further improve the performance of EAFR, we propose a dynamic transmission rate adjustment strategy to prevent potential incast congestion when replicating a file to a server, a networkaware data node selection strategy to reduce file read latency, and a load-aware replica maintenance strategy to quickly create file replicas under replica node failures. Experimental results on a real-world cluster show the effectiveness of EAFR and proposed strategies in reducing file read latency, replication time, and power consumption in large clusters."",""1558-2183"","""",""10.1109/TPDS.2016.2613989"",""US National Science Foundation(grant numbers:NSF-1404981,IIS-1354123,CNS-1254006,CNS-1249603)"; Microsoft Research; Faculty Fellowship(grant numbers:8300751);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7577866"",""Data-intensive clusters";file replication;replica placement;"energy-efficient"",""Servers";Energy consumption;Power demand;Adaptive systems;Bandwidth;Hardware;"Reliability"","""",""15"","""",""47"",""IEEE"",""27 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Edge Provisioning with Flexible Server Placement,""H. Yin"; X. Zhang; H. H. Liu; Y. Luo; C. Tian; S. Zhao;" F. Li"",""Research Institute of Information Technology, Tsinghua University, Beijing, China"; Research Institute of Information Technology, Tsinghua University, Beijing, China; Mobility and Networking Research Group, Microsoft Research, Redmond, WA; Department of Electrical and Computer Engineering, University of Massachusetts Lowell, Lowell, MA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Research Institute of Information Technology, Tsinghua University, Beijing, China;" Research Institute of Information Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1031"",""1045"",""We present $\sf {Tentacle}$ , a decision support framework to provision edge servers for online services providers (OSPs). $\sf {Tentacle}$  takes advantage of the increasingly flexible edge server placement, which is enabled by new technologies such as edge computing platforms, cloudlets and network function virtualization, to optimize the overall performance and cost of edge infrastructures. The key difference between  $\sf {Tentacle}$  and traditional server placement approaches lies on that  $\sf {Tentacle}$  can discover proper unforeseen edge locations which significantly improve the efficiency and reduce the cost of edge provisioning. We show how $\sf {Tentacle}$  effectively identifies promising edge locations which are close to a collection of users merely with inaccurate network distance estimation methods, e.g., geographic coordinate (GC) and network coordinate systems (NC). We also show how $\sf {Tentacle}$  comprehensively considers various pragmatic concerns in edge provisioning, such as traffic limits by law or ISP policy, edge site deployment and resource usage cost, over-provisioning for fault tolerance, etc., with a simple optimization model. We simulate $\sf {Tentacle}$  using real network data at global and county-wide scales. Measurement-driven simulations show that with a given cost budget  $\sf {Tentacle}$  can improve user performance by around 10-45 percent at global scale networks and 15-35 percent at a country-wide scale network."",""1558-2183"","""",""10.1109/TPDS.2016.2604803"",""National Basic Research Program of China (973 Program)(grant numbers:2012CB315801)"; National Key Research and Development Program(grant numbers:2016YFB1000102); National Natural Science Foundation of China(grant numbers:61672318,61602194); Tsinghua University(grant numbers:20131089304); Tsinghua National Laboratory for Information Science and Technology (TNList); European Seventh Framework Programme (FP7)(grant numbers:PIRSES-GA-2012-318939); National Science Foundation of United States(grant numbers:ACI-1547428,ACI-1541434,ACI-1440737,ACI-1450996);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7556983"",""Edge provisioning";optimization;decoupling;"unforeseen locations"",""Servers";Delays;Complexity theory;Cloud computing;"Electronic mail"","""",""100"","""",""47"",""IEEE"",""31 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Approximation Algorithms for the Bounded Flexible Scheduling Problem in Clouds,""L. Guo";" H. Shen"",""College of Mathematics and Computer Science, Fuzhou University, Fuzhou Shi, China";" School of Information Science and Technology, Sun Yat-Sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3511"",""3520"",""Clouds, such as Amazon Infrastructure-as-a-Service (IaaS) clouds and EMC Hybrid Cloud, impose growing requirements of resource-efficiency scheduling. The bounded flexible scheduling (BFS) problem is one of the problems proposed to meet such requirements. In BFS, we are given a set of identical machines and a set of jobs, each of which is with a value, a workload, a deadline and a parallelism degree, i.e., the maximum number of machines on which the job can execute concurrently. The problem is to compute an assignment of the given jobs to the machines, such that the total value of the jobs successfully completed by their deadlines is maximized. This paper presents a factor C/C-k approximation algorithm for BFS, where k is the maximum parallelism degree and C is the capacity of the system (i.e., the number of machines). Since C ≫ k in BFS, our result significantly improves the known best approximation ratio of (2C-k/C-k)(1-ϵ) for tight deadlines [17], and C/C-k · s/s-1/s for loose deadlines [18] on a slackness ratios > 1 that is the maximum ratio between a job's earliest actual finish time and its deadline. We first propose feasibility condition to determine whether an instance of BFS is feasible, i.e., whether there exists a scheduling according to which all jobs can finish before their deadlines, which is the key to achieve the ratio improvement of our algorithm. To prove the correctness of the feasibility condition, we give a simple linear program (LP) for a weaker version of BFS, and show that it is with an integral polyhedron and hence the version of BFS is polynomial-time solvable. Then we present a greedy algorithm and its equivalent primal-dual algorithm for the complementary problem of BFS. Both algorithms have an approximation ratio of C/C-k, and time complexity O(n2 + nT), where n is the number of jobs and T is the number of time slots. As a by-product, we show that the BFS admits a polynomial-time approximation scheme (PTAS) when T is fixed."",""1558-2183"","""",""10.1109/TPDS.2017.2731843"",""University of Adelaide"; Australian Research Council(grant numbers:DP150104871); Natural Science Foundation of China(grant numbers:61300025); Sun Yat-Sen University(grant numbers:985);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990567"",""Approximation algorithm";primal dual method;bounded flexible scheduling;"resource allocation"",""Approximation algorithms";Processor scheduling;Scheduling;Time complexity;Cloud computing;"Algorithm design and analysis"","""",""27"","""",""27"",""IEEE"",""25 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Data Center Flow Scheduling Without Starvation Using Expansion Ratio,""S. Zhang"; Z. Qian; H. Wu;" S. Lu"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3157"",""3170"",""Existing data center transport protocols are usually based on the Processor Sharing (PS) policy and/or the Shortest Remaining Processing Time (SRPT) policy. PS divides link bandwidth equally between competing flows, thus it fails to achieve optimal average flow completion time (FCT). SRPT prioritizes flows that have the shortest remaining processing time and provides near-optimal average FCT, but it may cause long flows to suffer unfair delays, or even starve them. In fact, these two types of policies represent two directions in the design space: PS prefers fairness (in terms of starvation freedom) while SRPT favors efficiency (in terms of average FCT). In this paper, we propose a novel metric, expansion ratio, which enables us to strike a balance between SRPT and PS. We design MERP that achieves efficient flow scheduling without starvation. MERP takes care of both average and tail FCTs by minimizing the expansion ratio of competing flows in a lexicographically manner. MERP controls the sending rate of competing flows via synchronized virtual deadlines and routes flows in a downstream-aware manner that reacts quickly to link failures. We evaluate MERP using extensive NS2-based simulations. Results show that, under various traffic loads, MERP reduces the tail FCT significantly with a negligible increase of average FCT compared with pFabric, and MERP reduces the average FCT notably compared with ECMP and CONGA when link failures occur."",""1558-2183"","""",""10.1109/TPDS.2017.2706290"",""NSFC(grant numbers:61502224,61472181,61472185,61321491)"; China Postdoctoral Science Foundation(grant numbers:2015M570434,2016T90444); CCF-Tencent Open Research Fund(grant numbers:AGR20160104); Jiangsu NSF(grant numbers:BK20151392,BK20151390); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932173"",""Data center";flow completion time;efficiency;starvation freedom;"expansion ratio"",""Switches";Bandwidth;Measurement;Schedules;Distributed databases;"Transport protocols"","""",""5"","""",""40"",""IEEE"",""19 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Resource Constrained Scheduling Using Parallel Two-Phase Branch-and-Bound Heuristics,""M. Chen"; Y. Bao; X. Fu; G. Pu;" T. Wei"",""Shanghai Key Lab of Trustworthy Computing, East China Normal University, Shanghai, China"; Shanghai Key Lab of Trustworthy Computing, East China Normal University, Shanghai, China; Department of Electrical and Computer Engineering, University of Houston, Houston, TX; Shanghai Key Lab of Trustworthy Computing, East China Normal University, Shanghai, China;" Shanghai Key Lab of Trustworthy Computing, East China Normal University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1299"",""1314"",""Branch-and-bound (B&B) approaches are widely investigated in resource constrained scheduling (RCS). However, due to the lack of approaches that can generate a tight schedule at the beginning of the search, B&B approaches usually start with a large initial search space, which makes the following search of an optimal schedule time-consuming. To address this problem, this paper proposes a parallel two-phase B&B approach that can drastically reduce the overall RCS time. This paper makes three major contributions: i) it proposes three partial-search heuristics that can quickly find a tight schedule to compact the initial search space"; ii) it presents a two-phase search framework that supports the efficient parallel search of an optimal schedule;" iii) it investigates various bound sharing and speculation techniques among collaborative tasks to further improve the parallel search performance at different search phases. The experimental results based on well-established benchmarks demonstrate the efficacy of our proposed approach."",""1558-2183"","""",""10.1109/TPDS.2016.2621768"",""Natural Science Foundation of China(grant numbers:91418203,61672230,61572197)"; US National Science Foundation(grant numbers:CCF-1351054); CAREER; Shanghai Municipal Education Commission(grant numbers:14ZZ047); NSF(grant numbers:16ZR1409000);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723936"",""Resource constrained scheduling";branch-and-bound;parallel two-phase pruning;"high-level synthesis"",""Optimal scheduling";Schedules;Estimation;Collaboration;Job shop scheduling;"Dispatching"","""",""4"","""",""23"",""IEEE"",""27 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Self-Invalidation/Self-Downgrade for Critical Sections with Relaxed Semantics,""A. Ros"; C. Leonardsson; C. Sakalis;" S. Kaxiras"",""Department of Electrical and Computer Engineering, Universidad de Murcia, Murcia, Spain"; Department of Information Technology, Uppsala University, Uppsala, Sweden; Department of Information Technology, Uppsala University, Uppsala, Sweden;" Department of Information Technology, Uppsala University, Uppsala, Sweden"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3413"",""3425"",""Cache coherence protocols based on self-invalidation allow simpler hardware implementation compared to traditional write-invalidation protocols, by relying on data-race-free semantics and applying self-invalidation on synchronization points. Their simplicity lies in the absence of invalidation traffic. This eliminates the need to track readers in a directory, and reduces the number of transient protocol states. Similarly, the use of self-downgrade on synchronization eliminates directory indirection, and hence the need to track writers in a directory. These protocols, effectively without a directory, have the potential to reduce area, energy consumption, and complexity, without sacrificing performance-provided, that self-invalidation and self-downgrade are performed prudently. In this work we examine how self-invalidation and self-downgrade are performed in relation to atomicity and ordering. We show that self-invalidation and self-downgrade do not need to be applied conservatively, as so far implemented. Our key observation is that, often, critical sections which are not ordered in time, are intended to provide only atomicity and not thread synchronization. We thus propose a new type of self-invalidation, forward-self-invalidation (FSI), which invalidates solely data that are going to be accessed inside a critical section. Based on the same reasoning, we propose a new type of self-downgrade, forward self-downgrade (FSD), also restricted to writes in critical sections. Finally, we define the semantics of locks using FSI and FSD, which resemble the semantics of relaxed atomic operations in C++. Our evaluation for 64-core multiprocessors shows significant improvements using the proposed FSI and FSD-where applicable-in Splash-3 and PARSEC benchmarks, over a directory-based protocol (17.1 percent in execution time and 33.9 percent in energy consumption) and also over a state-of-the-art self-invalidation/self-downgrade protocol (7.6 percent in execution time and 9.1 percent in energy consumption), while still retaining the design simplicity of the protocol."",""1558-2183"","""",""10.1109/TPDS.2017.2720744"",""Fundación Séneca-Agencia de Ciencia y Tecnología de la Región de Murcia"; Jóvenes Líderes en Investigación(grant numbers:18956/JLI/13); MINECO; FEDER(grant numbers:TIN2015-66972-C5-3-R); Uppsala Programming for Multicore Architectures Research Center; Swedish VR(grant numbers:621-2012-5332);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961235"",""Cache coherence";memory consistency;self-invalidation;self-downgrade;critical section;"atomicity"",""Synchronization";Semantics;Instruction sets;Energy consumption;"Cache memory"","""",""1"","""",""45"",""IEEE"",""28 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Elastic Reliability Optimization Through Peer-to-Peer Checkpointing in Cloud Computing,""J. Zhao"; Y. Xiang; T. Lan; H. H. Huang;" S. Subramaniam"",""Electrical and Computer Engineering, University of Massachusetts-Lowell, Lowell, Massachusetts"; Department of Cloud Platform Software Research, AT&T Labs, Bedminster, NJ; Department of Electrical and Computer Engineering, The George Washington University, Washington, DC; Department of Electrical and Computer Engineering, The George Washington University, Washington, DC;" Department of Electrical and Computer Engineering, The George Washington University, Washington, DC"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""491"",""502"",""Modern day data centers coordinate hundreds of thousands of heterogeneous tasks and aim at delivering highly reliable cloud computing services. Although offering equal reliability to all users benefits everyone at the same time, users may find such an approach either inadequate or too expensive to fit their individual requirements, which may vary dramatically. In this paper, we propose a novel method for providing elastic reliability optimization in cloud computing. Our scheme makes use of peer-to-peer checkpointing and allows user reliability levels to be jointly optimized based on an assessment of their individual requirements and total available resources in the data center. We show that the joint optimization can be efficiently solved by a distributed algorithm using dual decomposition. The solution improves resource utilization and presents an additional source of revenue to data center operators. Our validation results suggest a significant improvement of reliability over existing schemes."",""1558-2183"","""",""10.1109/TPDS.2016.2571281"",""US National Science Foundation(grant numbers:1320226)"; CAREER Award(grant numbers:CNS-1350766);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7475901"",""Cloud computing";data center;reliability;checkpoint;"optimization"",""Reliability";Checkpointing;Optimization;Bandwidth;Interference;Peer-to-peer computing;"Cloud computing"","""",""17"","""",""44"",""IEEE"",""20 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Elastic State Machine Replication,""A. Nogueira"; A. Casimiro;" A. Bessani"",""LaSIGE, Universidade de Lisboa, Lisboa, Portugal"; LaSIGE, Universidade de Lisboa, Lisboa, Portugal;" LaSIGE, Universidade de Lisboa, Lisboa, Portugal"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2486"",""2499"",""State machine replication (SMR) is a fundamental technique for implementing stateful dependable systems. A key limitation of this technique is that the performance of a service does not scale with the number of replicas hosting it. Some works have shown that such scalability can be achieved by partitioning the state of the service into shards. The few SMR-based systems that support dynamic partitioning implement ad-hoc state transfer protocols and perform scaling operations as background tasks to minimize the performance degradation during reconfigurations. In this work we go one step further and propose a modular partition transfer protocol for creating and destroying such partitions at runtime, thus providing fast elasticity for crash and Byzantine fault tolerant replicated state machines and making them more suitable for cloud systems."",""1558-2183"","""",""10.1109/TPDS.2017.2686383"",""FCT(grant numbers:UID/CEC/00408/2013)"; IRCoC(grant numbers:PTDC/EEI-SCR/6970/2014); European Commission(grant numbers:643964);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885120"",""State machine replication";replication;elasticity;fault tolerance;Byzantine fault tolerance;partitioning;"scaling"",""Protocols";Elasticity;Throughput;Servers;Libraries;Scalability;"Fault tolerance"","""",""10"","""",""61"",""IEEE"",""22 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Enabling Efficient and Reliable Transition from Replication to Erasure Coding for Clustered File Systems,""R. Li"; Y. Hu;" P. P. C. Lee"",""Chinese University of Hong Kong, Shatin, N.T., Hong Kong"; Huazhong University of Science and Technology, Wuhan, P. R. China;" Chinese University of Hong Kong, Shatin, N.T., Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2500"",""2513"",""To balance performance and storage efficiency, modern clustered file systems often first store data with replication, followed by encoding the replicated data with erasure coding. We argue that the commonly used random replication does not take into account erasure coding in its design, thereby raising both performance and availability issues in the subsequent encoding operation. We propose encoding-aware replication, which carefully places the replicas so as to (i) eliminate cross-rack downloads of data blocks during the encoding operation, (ii) preserve availability without data relocation after the encoding operation, and (iii) maintain load balancing across replicas as in random replication before the encoding operation. We conduct extensive HDFS-based testbed experiments and discrete-event simulations, and demonstrate the performance gains of encoding-aware replication over random replication."",""1558-2183"","""",""10.1109/TPDS.2017.2678505"",""University Grants Committee"; Research Grants Council(grant numbers:AoE/E-02/08,CRF-C7036-15); National Natural Science Foundation of China(grant numbers:61502191,61502190); ZTE Corporation; Hubei Provincial Natural Science Foundation(grant numbers:2016CFB226);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872497"",""Replication";erasure codes;distributed storage systems;"experiments and implementation"",""Encoding";Ear;Fault tolerant systems;Redundancy;Bandwidth;"Load management"","""",""39"","""",""37"",""IEEE"",""6 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Enabling Parallel Simulation of Large-Scale HPC Network Systems,""M. Mubarak"; C. D. Carothers; R. B. Ross;" P. Carns"",""Mathematics and Computer Science (MCS) Division at Argonne National Laboratory, 9700 South Cass Ave., Lemont, IL"; Computer Science Department, Rensselaer Polytechnic Institute, 110 8th Street, Troy, NY; Mathematics and Computer Science (MCS) Division at Argonne National Laboratory, 9700 South Cass Ave., Lemont, IL;" Mathematics and Computer Science (MCS) Division at Argonne National Laboratory, 9700 South Cass Ave., Lemont, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""87"",""100"",""With the increasing complexity of today's high-performance computing (HPC) architectures, simulation has become an indispensable tool for exploring the design space of HPC systems-in particular, networks. In order to make effective design decisions, simulations of these systems must possess the following properties: (1) have high accuracy and fidelity, (2) produce results in a timely manner, and (3) be able to analyze a broad range of network workloads. Most state-of-the-art HPC network simulation frameworks, however, are constrained in one or more of these areas. In this work, we present a simulation framework for modeling two important classes of networks used in today's IBM and Cray supercomputers: torus and dragonfly networks. We use the Co-Design of Multi-layer Exascale Storage Architecture (CODES) simulation framework to simulate these network topologies at a flit-level detail using the Rensselaer Optimistic Simulation System (ROSS) for parallel discrete-event simulation. Our simulation framework meets all the requirements of a practical network simulation and can assist network designers in design space exploration. First, it uses validated and detailed flit-level network models to provide an accurate and high-fidelity network simulation. Second, instead of relying on serial time-stepped or traditional conservative discrete-event simulations that limit simulation scalability and efficiency, we use the optimistic event-scheduling capability of ROSS to achieve efficient and scalable HPC network simulations on today's high-performance cluster systems. Third, our models give network designers a choice in simulating a broad range of network workloads, including HPC application workloads using detailed network traces, an ability that is rarely offered in parallel with high-fidelity network simulations."",""1558-2183"","""",""10.1109/TPDS.2016.2543725"",""US Department of Energy"; Office of Science; Office of Advanced Scientific Computer Research(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7448965"",""Massively parallel discrete-event simulation";interconnect networks;"trace-based simulation"",""Computational modeling";Routing;Space exploration;Analytical models;Network topology;Processor scheduling;"Bandwidth"","""",""53"","""",""48"",""IEEE"",""7 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Energy Efficiency for Clustered Heterogeneous Multicores,""S. Pagani"; A. Pathania; M. Shafique; J. -J. Chen;" J. Henkel"",""Chair for Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany"; Chair for Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany; Embedded Computing Systems Group, Vienna University of Technology (TU Wien), Wien, Austria; Department of Informatics, TU Dortmund, Dortmund, Germany;" Chair for Embedded Systems (CES), Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1315"",""1330"",""Heterogeneous multicore systems clustered in multiple Voltage Frequency Islands (VFIs) are the next-generation solution for power and energy efficient computing systems. Due to the heterogeneity, the power consumption and execution time of a task changes not only with Dynamic Voltage and Frequency Scaling (DVFS), but also according to the task-to-island assignment, presenting major challenges for power management and energy minimization techniques. This paper focuses on energy minimization of periodic real-time tasks (or performance-constrained tasks) on such systems, in which the cores in an island are homogeneous and share the same voltage and frequency, but different islands have different types and numbers of cores and can be executed at other voltages and frequencies. We present an efficient algorithm to minimize the total energy consumption while satisfying the timing constraints of all tasks. Our technique consists of the coordinated selection of the voltage and frequency levels for each island, together with a task partitioning strategy that considers the energy consumption of the task executing on different islands and at different frequencies, as well as the impact of the frequency and the underlying core architecture to the resulting execution time. Every task is then mapped to the most energy efficient island for the selected voltage and frequency levels, and to a core inside the island such that the workloads of the cores in a VFI are balanced. We experimentally evaluate our technique and compare it to state-of-the-art solutions, resulting in average in 25 percent less energy consumption (and up to 87 percent for some cases), while guaranteeing that all tasks meet their deadlines."",""1558-2183"","""",""10.1109/TPDS.2016.2623616"",""German Research Foundation (DFG)"; Transregional Collaborative Research Centre(grant numbers:SFB/TR 89);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728098"",""Energy efficiency";energy minimization;task partitioning;heterogeneous multicores;voltage frequency islands (VFI);single frequency approximation (SFA);"power management"",""Multicore processing";Minimization;Energy consumption;Partitioning algorithms;Timing;Time-frequency analysis;"Real-time systems"","""",""32"","""",""34"",""IEEE"",""1 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Energy-Aware Scheduling of Embarrassingly Parallel Jobs and Resource Allocation in Cloud,""L. Shi"; Z. Zhang;" T. Robertazzi"",""Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY"; Department of Computer Science, Xiaomen University, Xiamen, China;" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1607"",""1620"",""In cloud computing, with full control of the underlying infrastructures, cloud providers can flexibly place user jobs on suitable physical servers and dynamically allocate computing resources to user jobs in the form of virtual machines. As a cloud provider, scheduling user jobs in a way that minimizes their completion time is important, as this can increase the utilization, productivity, or profit of a cloud. In this paper, we focus on the problem of scheduling embarrassingly parallel jobs composed of a set of independent tasks and consider energy consumption during scheduling. Our goal is to determine task placement plan and resource allocation plan for such jobs in a way that minimizes the Job Completion Time (JCT). We begin with proposing an analytical solution to the problem of optimal resource allocation with pre-determined task placement. In the following, we formulate the problem of scheduling a single job as a Non-linear Mixed Integer Programming problem and present a relaxation with an equivalent Linear Programming problem. We further propose an algorithm named TaPRA and its simplified version TaPRA-fast that solve the single job scheduling problem. Lastly, to address multiple jobs in online scheduling, we propose an online scheduler named OnTaPRA. By comparing with the start-of-the-art algorithms and schedulers via simulations, we demonstrate that TaPRA and TaPRA-fast reduce the JCT by 40-430 percent and the OnTaPRA scheduler reduces the average JCT by 60-280 percent. In addition, TaPRA-fast can be 10 times faster than TaPRA with around 5 percent performance degradation compared to TaPRA, which makes the use of TaPRA-fast very appropriate in practice."",""1558-2183"","""",""10.1109/TPDS.2016.2625254"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7736137"",""Cloud computing";task scheduling;resource management;convex optimization;"non-linear programming"",""Cloud computing";Energy consumption;Resource management;Schedules;Processor scheduling;Servers;"Scheduling"","""",""54"","""",""34"",""IEEE"",""4 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Energy-Efficient Scheduling Algorithms for Real-Time Parallel Applications on Heterogeneous Distributed Embedded Systems,""G. Xie"; G. Zeng; X. Xiao; R. Li;" K. Li"",""College of Computer Science and Electronic Engineering, Key Laboratory for Embedded and Network Computing of Hunan Province, Changsha, Hunan, China"; Graduate School of Engineering, Nagoya University, Aichi, Japan; College of Computer Science and Electronic Engineering, Key Laboratory for Embedded and Network Computing of Hunan Province, Changsha, Hunan, China; College of Computer Science and Electronic Engineering, Key Laboratory for Embedded and Network Computing of Hunan Province, Changsha, Hunan, China;" College of Computer Science and Electronic Engineering, Key Laboratory for Embedded and Network Computing of Hunan Province, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3426"",""3442"",""Energy consumption minimization is one of the primary design requirements for heterogeneous distributed systems. State-of-the-art algorithms are used to study the problem of minimizing the energy consumption of a real-time parallel application with precedence constrained tasks on a heterogeneous distributed system by introducing the concept of latest finish time (LFT) to reclaim the slack time based on the dynamic voltage and frequency scaling (DVFS) energy-efficient design optimization technique. However, the use of DVFS technique alone is insufficient, and the energy consumption reduction is limited because scaling down the frequency is restricted in practice. Furthermore, these studies merely minimize energy consumption through a local energy-efficient scheduling algorithm, such as reducing the energy consumption for each task on the fixed processor, rather than a global energy-efficient scheduling algorithm, such as reducing the energy consumption for each task on different processors. This study solves the problem of minimizing the energy consumption of a real-time parallel application on heterogeneous distributed systems by using the combined non-DVFS and global DVFS-enabled energy-efficient scheduling algorithms. The non-DVFS energy-efficient scheduling (NDES) algorithm is solved by introducing the concept of deadline slacks to reduce the energy consumption while satisfying the deadline constraint. The global DVFS-enabled energy-efficient scheduling (GDES) algorithm is presented by moving the tasks to the processor slacks that generate minimum dynamic energy consumptions. Results of the experiments show that the combined NDES&GDES algorithm can save up to 36.25-55.65 percent of energy compared with state-of-the-art counterparts under different scales, parallelism, and heterogeneity degrees of parallel applications."",""1558-2183"","""",""10.1109/TPDS.2017.2730876"",""National Key Research and Development Plan of China(grant numbers:2016YFB0200405)"; National Natural Science Foundation of China(grant numbers:61672217,61432005,61379115,61402170,61370097,61502162,61502405); CERNET(grant numbers:NGII20161003); China Postdoctoral Science Foundation(grant numbers:2016M592422);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7990045"",""Directed acyclic graph (DAG)";dynamic voltage and frequency scaling (DVFS);energy-efficient scheduling;heterogeneous distributed embedded systems;"real-time"",""Energy consumption";Heuristic algorithms;Dynamic scheduling;Scheduling algorithms;"Real-time systems"","""",""59"","""",""23"",""IEEE"",""24 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Enhancing the Malloc System with Pollution Awareness for Better Cache Performance,""X. Liao"; R. Guo; H. Jin; J. Yue;" G. Tan"",""Service Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Service Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; Service Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; Auburn University, Auburn, AL;" SIAT, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""731"",""745"",""Cache pollution, by which weak-locality data unduly replaces strong-locality data, may notably degrade application performance in a shared-cache multicore machine. This paper presents NightWatch, a cache management subsystem that provides general, transparent and low-overhead pollution control to applications. NightWatch is based on the observation that data within the same memory chunk or chunks within the same allocation context often share similar locality property. NightWatch embodies this observation by online monitoring current cache locality to predict future behavior and restricting potential cache polluters proactively. We have integrated NightWatch into two popular allocators, tcmalloc and ptmalloc2. Experiments with SPEC CPU2006 show that NightWatch improves application performance by up to 45 percent (18 percent on average), with an average monitoring overhead of 0.57 percent (up to 3.02 percent)."",""1558-2183"","""",""10.1109/TPDS.2016.2587644"",""National High-tech Research and Development Program of China (863 Program)(grant numbers:2015AA015303)"; National Natural Science Foundation of China(grant numbers:61322210,61272408,61433019,61379135); Ministry of Education of China(grant numbers:20130142110048); Shenzhen Oversea High-Level Talents Innovation and Entrepreneurship Funds(grant numbers:KQCX20140520154115026);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506061"",""Cache pollution";multicore;"malloc"",""Resource management";Pollution;Monitoring;Image color analysis;Context;Memory management;"Multicore processing"","""",""4"","""",""44"",""IEEE"",""7 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Ephemeral Content Popularity at the Edge and Implications for On-Demand Caching,""N. Carlsson";" D. Eager"",""Department of Computer and Information Science, Linköping University, Linköping, Sweden";" Department of Computer Sciene, University of Saskatchewan, Saskatoon, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1621"",""1634"",""The ephemeral content popularity seen with many content delivery applications can make indiscriminate on-demand caching in edge networks highly inefficient, since many of the content items that are added to the cache will not be requested again from that network. In this paper, we address the problem of designing and evaluating more selective edge-network caching policies. The need for such policies is demonstrated through an analysis of a dataset recording YouTube video requests from users on an edge network over a 20-month period. We then develop a novel workload modelling approach for such applications and apply it to study the performance of alternative edge caching policies, including indiscriminate caching and cache on kth request for different k. The latter policies are found able to greatly reduce the fraction of the requested items that are inserted into the cache, at the cost of only modest increases in cache miss rate. Finally, we quantify and explore the potential room for improvement from use of other possible predictors of further requests. We find that although room for substantial improvement exists when comparing performance to that of a perfect “oracle” policy, such improvements are unlikely to be achievable in practice."",""1558-2183"","""",""10.1109/TPDS.2016.2614805"",""Industrial Information Technology (CENIIT)"; Natural Sciences and Engineering Research Council (NSERC) of Canada;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7581058"",""Ephemeral content popularity";one-timers;one-hit-wonders;edge network;measurements;"caching"",""YouTube";Data collection;Aggregates;Internet;Computers;Metadata;"Privacy"","""",""34"","""",""37"",""IEEE"",""3 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Evaluating Scalable Distributed Erlang for Scalability and Reliability,""N. Chechina"; K. MacKenzie; S. Thompson; P. Trinder; O. Boudeville; V. Fördős; C. Hoch; A. Ghaffari;" M. M. Hernandez"",""University of Glasgow, Glasgow, United Kingdom"; University of Glasgow, Glasgow, United Kingdom; University of Kent, Canterbury, United Kingdom; University of Glasgow, Glasgow, United Kingdom; EDF R&D, Clamart, France; Erlang Solutions AB, Budapest, Hungary; Erlang Solutions AB, Budapest, Hungary; University of Glasgow, Glasgow, United Kingdom;" University of Glasgow, Glasgow, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2244"",""2257"",""Large scale servers with hundreds of hosts and tens of thousands of cores are becoming common. To exploit these platforms software must be both scalable and reliable, and distributed actor languages like Erlang are a proven technology in this area. While distributed Erlang conceptually supports the engineering of large scale reliable systems, in practice it has some scalability limits that force developers to depart from the standard language mechanisms at scale. In earlier work we have explored these scalability limitations, and addressed them by providing a Scalable Distributed (SD) Erlang library that partitions the network of Erlang Virtual Machines (VMs) into scalable groups (s_groups). This paper presents the first systematic evaluation of SD Erlang s_groups and associated tools, and how they can be used. We present a comprehensive evaluation of the scalability and reliability of SD Erlang using three typical benchmarks and a case study. We demonstrate that s_groups improve the scalability of reliable and unreliable Erlang applications on up to 256 hosts (6,144 cores). We show that SD Erlang preserves the class-leading distributed Erlang reliability model, but scales far better than the standard model. We present a novel, systematic, and tool-supported approach for refactoring distributed Erlang applications into SD Erlang. We outline the new and improved monitoring, debugging and deployment tools for large scale SD Erlang applications. We demonstrate the scaling characteristics of key tools on systems comprising up to 10 K Erlang VMs."",""1558-2183"","""",""10.1109/TPDS.2017.2654246"",""EU(grant numbers:287510)"; EPSRC(grant numbers:EP/L000687/1);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7820204"",""Scalability";reliability;actors;"Erlang"",""Scalability";Servers;Software reliability;Benchmark testing;Monitoring;"Reliability engineering"","""",""9"","""",""47"",""CCBY"",""17 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Evaluation of a Heterogeneous Multicore Architecture by Design and Test of an OFDM Receiver,""S. Nouri"; W. Hussain;" J. Nurmi"",""Laboratory of Electronics and Communications Engineering, Tampere University of Technology, Tampere, Finland"; Norwegian University of Science and Technology, Hogskoleringen, Trondheim, Norway;" Laboratory of Electronics and Communications Engineering, Tampere University of Technology, Tampere, Finland"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3171"",""3187"",""This paper presents an evaluation of a Heterogeneous Multicore Architecture (HMA) by implementing Orthogonal Frequency-Division Multiplexing (OFDM) receiver blocks as designs for the test of functionality. OFDM receiver consists of computationally intensive and general-purpose processing tasks that can provide maximum coverage to test and evaluate a massively-parallel as well as a general-purpose platform like the HMA. The blocks of the receiver are primarily designed by crafting template-based Coarse-Grained Reconfigurable Array (CGRA) devices and then arranging them in a sequence over a Network-on-Chip (NoC) structure along with a few RISC cores for complete OFDM processing. The OFDM blocks such as Fast Fourier Transform (FFT) and Time Synchronization are computationally intensive and require parallel processing. The OFDM receiver also contains tasks such as frequency offset estimation which require the processing of Taylor series and CORDIC algorithms that are serial in nature. Such a combination of serial and parallel algorithms can perform a thorough exploration and evaluation of almost all the design features of an HMA. The OFDM implementation has led to scale CGRAs to different dimensions, instantiate Processing Elements (PEs) as multiple arithmetic resources and to establish almost all possible ways of PE interconnections. It further explores time-multiplexed patterns for data placement in the CGRA memories. Nevertheless, the data can also be exchanged among different nodes over NoC structure simultaneously and independently by using direct memory access devices. In this experimental work, the performance of each CGRA, the collective performance of the whole platform and the NoC traffic are recorded in terms of the number of clock cycles and several high-level performance metrics. Today's HMAs are generally over or under resourced for the applications that they are designed for and thus not an optimal choice for the end user. Apart from the interesting comparisons to the other state-of-the-art, our experimental setup has provided important insight and guidelines that the designers can use to implement near-optimal solutions for their target applications."",""1558-2183"","""",""10.1109/TPDS.2017.2706691"",""Laboratory of Electronics and Communications Engineering"; Tampere University of Technology; Academy of Finland(grant numbers:# 258,506); (DEFT: Design of a Highly-parallel Heterogeneous MP-SoC Architecture for Future Wireless Technologis); Tuula and Yrjö Neuvo fund; HPY Research Foundation; Tekniikan Edistämissäätiö (TES); HiPEAC Collaboration Grant;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932115"",""Reconfigurable";CGRA;network-on-chip;heterogeneous;accelerator;multicore;FFT;time synchronization;channel estimation;frequency offset estimation;receiver;"OFDM"",""OFDM";Computer architecture;Receivers;Reduced instruction set computing;Performance evaluation;"Field programmable gate arrays"","""",""15"","""",""33"",""IEEE"",""19 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"ExCCC-DCN: A Highly Scalable, Cost-Effective and Energy-Efficient Data Center Structure,""Z. Zhang"; Y. Deng; G. Min; J. Xie;" S. Huang"",""Department of Computer Science, Jinan University, Guangzhou, Guangdong, China"; Department of Computer Science, Jinan University, Guangzhou, Guangdong, China; College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, United Kingdom; Department of Computer Science, Jinan University, Guangzhou, Guangdong, China;" Network and Educational Technology Center, Jinan University, Guangzhou, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1046"",""1060"",""Over the past decade, many data centers have been constructed around the world due to the explosive growth of data volume and type. The cost and energy consumption have become the most important challenges of building those data centers. Data centers today use commodity computers and switches instead of high-end servers and interconnections for cost-effectiveness. In this paper, we propose a new type of interconnection networks called Exchanged Cube-Connected Cycles (ExCCC). The ExCCC network is an extension of Exchanged Hypercube (EH) network by replacing each node with a cycle. The EHnetwork is based on link removal from a Hypercube network, which makes the EHnetwork more cost-effective as it scales up. After analyzing the topological properties of ExCCC, we employ commodity switches to construct a new class of data center network models, namely ExCCC-DCN, by leveraging the advantages of the ExCCC architecture. The analysis and experimental results demonstrate that the proposed ExCCC-DCN models significantly outperform four state-of-the-art data center network models in terms of the total cost, power consumption, scalability, and other static characteristics. It achieves the goals of low cost, low energy consumption, high network throughput, and high scalability simultaneously."",""1558-2183"","""",""10.1109/TPDS.2016.2609428"",""National Natural Science Foundation (NSF) of China(grant numbers:61572232,61272073)"; Natural Science Foundation of Guangdong Province(grant numbers:S2013020012865); Science and Technology Planning Project of Guangdong Province(grant numbers:2016B010124008); Fundamental Research Funds for the Central Universities(grant numbers:11615443);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7567529"",""Data center";interconnection;topology;cost-effectiveness;"scalability"",""Servers";Ports (Computers);Hypercubes;Data models;Network topology;"Analytical models"","""",""25"","""",""36"",""IEEE"",""14 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Exploiting Locality in Sparse Matrix-Matrix Multiplication on Many-Core Architectures,""K. Akbudak";" C. Aykanat"",""Computer Engineering Department, Bilkent University, Ankara, Turkey";" Computer Engineering Department, Bilkent University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2258"",""2271"",""Exploiting spatial and temporal localities is investigated for efficient row-by-row parallelization of general sparse matrix-matrix multiplication (SpGEMM) operation of the form C=AB on many-core architectures. Hypergraph and bipartite graph models are proposed for 1D rowwise partitioning of matrix A to evenly partition the work across threads with the objective of reducing the number of B-matrix words to be transferred from the memory and between different caches. A hypergraph model is proposed for B-matrix column reordering to exploit spatial locality in accessing entries of thread-private temporary arrays, which are used to accumulate results for C-matrix rows. A similarity graph model is proposed for B-matrix row reordering to increase temporal reuse of these accumulation array entries. The proposed models and methods are tested on a wide range of sparse matrices from real applications and the experiments were carried on a 60-core Intel Xeon Phi processor, as well as a two-socket Xeon processor. Results show the validity of the models and methods proposed for enhancing the locality in parallel SpGEMM operations."",""1558-2183"","""",""10.1109/TPDS.2017.2656893"",""Scientific and Technological Research Council of Turkey (TUBITAK)(grant numbers:EEEAG-115E212)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7829317"",""Data locality";sparse matrix;sparse matrix-matrix multiplication;SpGEMM;computational hypergraph model;hypergraph partitioning;hypergraph clustering;graph model;bipartite graph model;graph partitioning;graph clustering;many-core architecture;"Intel Xeon Phi"",""Sparse matrices";Computer architecture;Instruction sets;Computational modeling;Bipartite graph;Parallel processing;"Data models"","""",""25"","""",""58"",""IEEE"",""23 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Exploiting the Parallelism Between Conflicting Critical Sections with Partial Reversion,""L. Zheng"; X. Liao; H. Jin;" H. Liu"",""Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei Sheng, China"; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei Sheng, China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei Sheng, China;" Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei Sheng, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3443"",""3457"",""The critical sections with the lock protection greatly limit the concurrency of multi-threaded applications. The prior lock elision based technique is presented to exploit the parallelism between critical sections accessing the disjoint shared data, but still fails to notice and expose a high degree of concurrency between critical sections that contend for the same shared data, i.e., conflicting critical sections (CCS). This paper focuses on exploiting the CCS parallelism. The key insight of this work is that, for each running CCS, a large proportion (>73.4% ) of parallelism between CCSs can be exploited as fully as possible by simply allowing the parallel execution of their first conflict-free code fragment at runtime. We therefore present BSOptimizer, a new microarchitecture, to perform the partial reversion integrated with a series of sophisticated hardware and software strategies for the CCS parallelization. We complement the off-the-shelf cache coherency protocol to perceive the conflict location of CCS, present a predictive checkpoint mechanism to register and predict the concerned conflict point in a lightweight and accurate fashion, and redefine the traditional mutual exclusive semantics with a binary relationship. With these collaborative techniques, each CCS can be scheduled in parallel. Our experimental results on a wide variety of real programs and PARSEC benchmarks show that, compared to the native execution and two state-of-the-art lock elision techniques (including SLE and SLR), BSOptmizer can dramatically improves the performance of programs with a slight (<";0.8% ) energy consumption and (<;"3.9% ) extra runtime overhead. Our evaluation on a micro-benchmark with software based optimization also verifies that BSOptimizer can accurately exploit the CCS parallelism as promised."",""1558-2183"","""",""10.1109/TPDS.2017.2727485"",""NSFC(grant numbers:61272408,61322210)"; CCCPC;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7983450"",""Lock elision";parallelism;critical section;multi-threaded programs;"lock synchronization"",""Parallel processing";Runtime;Concurrent computing;Synchronization;Energy consumption;"Distance measurement"","""",""1"","""",""41"",""IEEE"",""18 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Exploring Efficient Hardware Support for Applications with Irregular Memory Patterns on Multinode Manycore Architectures,""M. Ceriani"; S. Secchi; O. Villa; A. Tumeo;" G. Palermo"",""Dipartimento di Elettronica e Informazione, Politecnico di Milano, Milano, Italy"; ARM Ltd, Cambridge, U.K; NVIDIA Research, Santa Clara, CA; High Performance Computing Group, Pacific Northwest National Laboratory, 902 Battelle Blvd, MSIN, Richland, WA;" Dipartimento di Elettronica e Informazione, Politecnico di Milano, Milano, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1635"",""1648"",""With computing systems becoming ubiquitous, numerous data sets of extremely large size are becoming available for analysis. Often the data collected have complex, graph based structures, which makes them difficult to process with traditional tools. Moreover, the irregularities in the data sets, and in the analysis algorithms, hamper the scaling of performance in large distributed high-performance systems, optimized for locality exploitation and regular data structures. In this paper we present an approach to system design that enable efficient execution of applications with irregular memory patterns on a distributed, many-core architecture, based on off-the-shelf cores. We introduce a set of hardware and software components, which provide a distributed global address space, fine-grained synchronization and latency hiding of remote accesses with multithreading. An FPGA prototype has been implemented to explore the design with a set of typical irregular kernels. We finally present an analytical model that highlights the benefits of the approach and helps identifying the bottlenecks in the prototype. The experimental evaluation on graph based applications demonstrates the scalability of the architecture for different configurations of the whole system."",""1558-2183"","""",""10.1109/TPDS.2014.2345073"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6871392"",""Computer architecture";irregular applications;high-performance computing;distributed computing;multithreadedarchitectures;parallel architectures;field programmable gate arrays;"prototypes"",""Instruction sets";Hardware;Multithreading;System-on-chip;Data structures;"Context"","""",""4"","""",""42"",""IEEE"",""5 Aug 2014"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Exploring Parallel Data Access Methods in Emerging Non-Volatile Memory Systems,""M. Jung"",""School of Integrated Technology, Yonsei University, Seodaemun-gu, Seoul, Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""746"",""759"",""The exploitation of internal parallelism over hundreds of NAND flash memories is becoming a key design issue in highspeed solid state disks (SSDs). In this study, we simulate a cycle-accurate SSD platform with diverse parallel data access methods and 24 page allocation strategies, which are geared toward exploiting both system-level parallelism and flash-level parallelism, using a variety of design parameters. Our extensive experimental analysis reveals that 1) the previously proposed channel striping-based page allocation strategy is not the best from a performance perspective, 2) as opposed to the common belief that system-level and flash-level concurrency mechanisms are largely orthogonal, the system-level parallel data access methods employed interferes with flash-level parallelism, 3) when most of the current currency controls and page allocation strategies are implemented, the SSD internal resources are significantly underutilized, and 4) while the performance of all the page allocation strategies on read-intensive workloads (reads > 99 percent) is improved by employing a high frequency flash interface, the performance enhancements are significantly limited. Finally, we present several optimization points to extract the maximum internal parallelism by offering comprehensive evaluations with controllable and easy-to-understand micro-benchmarks."",""1558-2183"","""",""10.1109/TPDS.2016.2588491"",""Ministry of Science ICT and Future Planning(grant numbers:IITP-2015-R0346-15-1008)"; National Research Foundation of Korea(grant numbers:2015M3C4A7065645,2016R1C1B2015312); Lawrence Berkeley National Laboratory(grant numbers:DE-AC02-05CH1123);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7514757"",""Memory-level parallelism";memory system design;page allocation;performance model;I/O scheduling;storage management;"solid state disk system"",""Resource management";Pipeline processing;Registers;Solids;Radio spectrum management;"Distributed databases"","""",""15"","""",""42"",""IEEE"",""18 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Eyes in the Dark: Distributed Scene Understanding for Disaster Management,""L. Li"; K. Ota; M. Dong;" W. Borjigin"",""Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran, Japan"; Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran, Japan; Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran, Japan;" Department of Information and Electronic Engineering, Muroran Institute of Technology, Muroran, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3458"",""3471"",""Robotic is a great substitute for human to explore the dangerous areas, and will also be a great help for disaster management. Although the rise of depth sensor technologies gives a huge boost to robotic vision research, traditional approaches cannot be applied to disaster-handling robots directly due to some limitations. In this paper, we focus on the 3D robotic perception, and propose a view-invariant Convolutional Neural Network (CNN) Model for scene understanding in disaster scenarios. The proposed system is highly distributed and parallel, which is of great help to improve the efficiency of network training. In our system, two individual CNNs are used to, respectively, propose objects from input data and classify their categories. We attempt to overcome the difficulties and restrictions caused by disasters using several specially-designed multi-task loss functions. The most significant advantage in our work is that the proposed method can learn a view-invariant feature with no requirement on RGB data, which is essential for harsh, disordered and changeable environments. Additionally, an effective optimization algorithm to accelerate the learning process is also included in our work. Simulations demonstrate that our approach is robust and efficient, and outperforms the state-of-the-art in several related tasks."",""1558-2183"","""",""10.1109/TPDS.2017.2740294"",""JSPS"; KAKENHI(grant numbers:JP16K00117,JP15K15976); KDDI Foundation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8010892"",""Distributed artificial intelligence";scene understanding system;"disaster management"",""Three-dimensional displays";Robot sensing systems;Two dimensional displays;Solid modeling;"Disaster management"","""",""37"","""",""42"",""IEEE"",""15 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Failure Diagnosis for Distributed Systems Using Targeted Fault Injection,""C. Pham"; L. Wang; B. C. Tak; S. Baset; C. Tang; Z. Kalbarczyk;" R. K. Iyer"",""University of Illinois at Urbana-Champaign, Champaign, IL"; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; Facebook Inc., Menlo Park, CA; University of Illinois at Urbana-Champaign, Champaign, IL;" University of Illinois at Urbana-Champaign, Champaign, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""503"",""516"",""This paper introduces a novel approach to automating failure diagnostics in distributed systems by combining fault injection and data analytics. We use fault injection to populate the database of failures for a target distributed system. When a failure is reported from production environment, the database is queried to find “matched” failures generated by fault injections. Relying on the assumption that similar faults generate similar failures, we use information from the matched failures as hints to locate the actual root cause of the reported failures. In order to implement this approach, we introduce techniques for (i) reconstructing end-to-end execution flows of distributed software components, (ii) computing the similarity of the reconstructed flows, and (iii) performing precise fault injection at pre-specified executing points in distributed systems. We have evaluated our approach using an OpenStack cloud platform, a popular cloud infrastructure management system. Our experimental results showed that this approach is effective in determining the root causes, e.g., fault types and affected components, for 71-100 percent of tested failures. Furthermore, it can provide fault locations close to actual ones and can easily be used to find and fix actual root causes. We have also validated this technique by localizing real bugs that occurred in OpenStack."",""1558-2183"","""",""10.1109/TPDS.2016.2575829"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7484300"",""Fault injection";failure diagnosis;processing flow;distributed system;"fault localization"",""Databases";Software;Production systems;Fault location;"Computer bugs"","""",""24"","""",""40"",""IEEE"",""2 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FairGV: Fair and Fast GPU Virtualization,""C. -H. Hong"; I. Spence;" D. S. Nikolopoulos"",""School of Electronics, Queen’s University Belfast, Belfast, United Kingdom"; School of Electronics, Queen’s University Belfast, Belfast, United Kingdom;" School of Electronics, Queen’s University Belfast, Belfast, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3472"",""3485"",""Increasingly high performance computing (HPC) application developers are opting to use cloud resources due to higher availability. Virtualized GPUs would be an obvious and attractive option for HPC application developers using cloud hosting services. Unfortunately, existing GPU virtualization software is not ready to address fairness, utilization, and performance limitations associated with consolidating mixed HPC workloads. This paper presents FairGV, a radically redesigned GPU virtualization system that achieves system-wide weighted fair sharing and strong performance isolation in mixed workloads that use GPUs with variable degrees of intensity. To achieve its objectives, FairGV introduces a trap-less GPU processing architecture, a new fair queuing method integrated with work-conserving and GPU-centric coscheduling polices, and a collaborative scheduling method for non-preemptive GPUs. Our prototype implementation achieves near ideal fairness (≥ 0.97 Min-Max Ratio) with little performance degradation (≤ 1.02 aggregated overhead) in a range of mixed HPC workloads that leverage GPUs."",""1558-2183"","""",""10.1109/TPDS.2017.2717908"",""European Commission’s Horizon 2020 program(grant numbers:644312)"; RAPID(grant numbers:687628); VINEYARD;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954729"",""GPU virtualization";trap-less architecture;fair queuing;"coscheduling and hybrid scheduling strategies"",""Graphics processing units";Virtualization;Virtual machine monitors;Cloud computing;Processor scheduling;"Context awareness"","""",""23"","""",""39"",""IEEE"",""21 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Fast Connected Components Computation in Large Graphs by Vertex Pruning,""A. Lulli"; E. Carlini; P. Dazzi; C. Lucchese;" L. Ricci"",""Department of Computer Science, University of Pisa, Pisa, Italy"; Information Science and Technologies Institute “A. Faedo”, National Research Council of Italy, Rome, Italy; Information Science and Technologies Institute “A. Faedo”, National Research Council of Italy, Rome, Italy; Information Science and Technologies Institute “A. Faedo”, National Research Council of Italy, Rome, Italy;" Department of Computer Science, University of Pisa, Pisa, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""760"",""773"",""Finding connected components is a fundamental task in applications dealing with graph analytics, such as social network analysis, web graph mining and image processing. The exponentially growing size of today's graphs has required the definition of new computational models and algorithms for their efficient processing on highly distributed architectures. In this paper we present CRACKER, an efficient iterative MapReduce-like algorithm to detect connected components in large graphs. The strategy of CRACKER is to transform the input graph in a set of trees, one for each connected component in the graph. Nodes are iteratively removed from the graph and added to the trees, reducing the amount of computation at each iteration. We prove the correctness of the algorithm, evaluate its computational cost and provide an extensive experimental evaluation considering a wide variety of synthetic and real-world graphs. The experimental results show that CRACKER consistently outperforms state-of-the-art approaches both in terms of total computation time and volume of messages exchanged."",""1558-2183"","""",""10.1109/TPDS.2016.2591038"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7515231"",""Distributed computing";distributed algorithms;"graph algorithms"",""Labeling";Heuristic algorithms;Computational modeling;Convergence;Social network services;Algorithm design and analysis;"Skeleton"","""",""19"","""",""36"",""IEEE"",""18 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Feedback-Control & Queueing Theory-Based Resource Management for Streaming Applications,""R. Tolosana-Calasanz"; J. Diaz-Montes; O. F. Rana;" M. Parashar"",""Departamento de Informática e Ingeniería de Sistemas, Universidad de Zaragoza, Zaragoza, Spain"; Rutgers Discovery Informatics Institute, Rutgers University, New Brunswick, NJ; School of Computer Science & Informatics, Cardiff University, Cardiff, United Kingdom;" Rutgers Discovery Informatics Institute, Rutgers University, New Brunswick, NJ"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1061"",""1075"",""Recent advances in sensor technologies and instrumentation have led to an extraordinary growth of data sources and streaming applications. A wide variety of devices, from smart phones to dedicated sensors, have the capability of collecting and streaming large amounts of data at unprecedented rates. A number of distinct streaming data models have been proposed. Typical applications for this include smart cites & built environments for instance, where sensor-based infrastructures continue to increase in scale and variety. Understanding how such streaming content can be processed within some time threshold remains a non-trivial and important research topic. We investigate how a cloud-based computational infrastructure can autonomically respond to such streaming content, offering Quality of Service guarantees. We propose an autonomic controller (based on feedback control and queueing theory) to elastically provision virtual machines to meet performance targets associated with a particular data stream. Evaluation is carried out using a federated Cloud-based infrastructure (implemented using CometCloud)-where the allocation of new resources can be based on: (i) differences between sites, i.e., types of resources supported (e.g., GPU versus CPU only), (ii) cost of execution";" (iii) failure rate and likely resilience, etc. In particular, we demonstrate how Little's Law-a widely used result in queuing theory-can be adapted to support dynamic control in the context of such resource provisioning."",""1558-2183"","""",""10.1109/TPDS.2016.2603510"",""Industry and Innovation department"; Aragonese Government and European Social Funds; COSMOS(grant numbers:T93); Spanish Ministry of Education; CEI; Iberus-Universidad de Zaragoza; Spanish Ministry of Economy; Programa de I+D+i Estatal de Investigación; Desarrollo e innovación Orientada a los Retos de la Sociedad(grant numbers:TIN2013-40809-R); US National Science Foundation(grant numbers:ACI 1339036,ACI 1441376); Rutgers Discovery Informatics Institute(grant numbers:RDI2);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7553568"",""Elastic resource provisioning";autonomic systems;"feedback control"",""Cloud computing";Queueing analysis;Resource management;Distributed databases;Instruments;Feedback control;"Process control"","""",""18"","""",""44"",""IEEE"",""26 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Fence-Free Synchronization with Dynamically Serialized Synchronization Variables,""Y. Hong"; Y. Zheng; H. Guan; B. Zang;" H. Chen"",""Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China"; Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Shanghai Key Laboratory for Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, China; Software School, Shanghai Jiao Tong University, Shanghai, China;" Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3486"",""3500"",""Memory fences are widely used to ensure the correctness for synchronization constructs on machines with relaxed consistency models. However, they are expensive and usually impose over-constrained ordering that causes unnecessary CPU stalls. In this paper, we observe that memory fences in TSO are merely intended to order synchronization variables. Based on this observation, we rethink the hardware-software interface of synchronization constructs on multicore processors and propose a new design called Sync-Order that differentiates synchronization variables (sync-vars) from normal ones. Sync-Order reduces hardware complexity such that the processor only needs to serialize the ordering among sync-vars. Its simplicity makes it easy to be integrated to the directory controller and it supports distributed directory, a missing feature in prior designs. We show that Sync-Order eliminates traditional fences on all sides of synchronization constructs (instead of only one side in prior work) and requires small effort for a programmer or compiler to annotate sync-vars. Our experimental results show that Sync-Order significantly reduces CPU stalls and boosts the performance of a set of synchronization constructs and concurrent data structures by 10 percent";" meanwhile, the fence overhead of full applications from SPLASH-2 and PARSEC is reduced from 42 to 3 percent."",""1558-2183"","""",""10.1109/TPDS.2016.2633353"",""National Key Research and Development Program of China(grant numbers:2016YFB1000104)"; China National Natural Science Foundation(grant numbers:61572314); National Top-notch Youth Talents Program of China; Zhangjiang Hi-Tech program(grant numbers:201501-YP-B108-012); Singapore NRF(grant numbers:CREATE E2S2);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762194"",""Fences";synchronization;memory consistency;sequential consistency;parallel programming;"shared-memory multiprocessors"",""Program processors";Synchronization;Buffer storage;Hardware;Load modeling;"Algorithm design and analysis"","""","""","""",""30"",""IEEE"",""29 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FiDoop-DP: Data Partitioning in Frequent Itemset Mining on Hadoop Clusters,""Y. Xun"; J. Zhang; X. Qin;" X. Zhao"",""Taiyuan University of Science and Technology, Taiyuan, Shanxi, China"; Taiyuan University of Science and Technology, Taiyuan, Shanxi, China; Department of Computer Science and Software Engineering, Samuel Ginn College of Engineering, Auburn University, AL;" Taiyuan University of Science and Technology, Taiyuan, Shanxi, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""101"",""114"",""Traditional parallel algorithms for mining frequent itemsets aim to balance load by equally partitioning data among a group of computing nodes. We start this study by discovering a serious performance problem of the existing parallel Frequent Itemset Mining algorithms. Given a large dataset, data partitioning strategies in the existing solutions suffer high communication and mining overhead induced by redundant transactions transmitted among computing nodes. We address this problem by developing a data partitioning approach called FiDoop-DP using the MapReduce programming model. The overarching goal of FiDoop-DP is to boost the performance of parallel Frequent Itemset Mining on Hadoop clusters. At the heart of FiDoop-DP is the Voronoi diagram-based data partitioning technique, which exploits correlations among transactions. Incorporating the similarity metric and the Locality-Sensitive Hashing technique, FiDoop-DP places highly similar transactions into a data partition to improve locality without creating an excessive number of redundant transactions. We implement FiDoop-DP on a 24-node Hadoop cluster, driven by a wide range of datasets created by IBM Quest Market-Basket Synthetic Data Generator. Experimental results reveal that FiDoop-DP is conducive to reducing network and computing loads by the virtue of eliminating redundant transactions on Hadoop nodes. FiDoop-DP significantly improves the performance of the existing parallel frequent-pattern scheme by up to 31 percent with an average of 18 percent."",""1558-2183"","""",""10.1109/TPDS.2016.2560176"",""National Natural Science Foundation of P.R. China(grant numbers:61272263,61572343)"; U.S. National Science Foundation(grant numbers:CCF-0845257);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7462277"",""Frequent itemset mining";parallel data mining;data partitioning;mapreduce programming model;"hadoop cluster"",""Data mining";Itemsets;Partitioning algorithms;Programming;Computational modeling;Distributed databases;"Correlation"","""",""53"","""",""41"",""IEEE"",""28 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FoToNoC: A Folded Torus-Like Network-on-Chip Based Many-Core Systems-on-Chip in the Dark Silicon Era,""L. Yang"; W. Liu; W. Jiang; M. Li; P. Chen;" E. H. -M. Sha"",""College of Computer Science, Chongqing University, Chongqing, China"; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China;" College of Computer Science, Chongqing University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1905"",""1918"",""Dark silicon refers to the phenomenon that a fraction of a many-core chip has to become “dark” or “dim” in order to guarantee the system to be kept in a safe temperature range and allowable power budget. Techniques have been developed to selectively activate non-adjacent cores on many-core chip to avoid temperature hotspot, while resulting unexpected increase of communication overhead due to the longer average distance between active cores, and in turn affecting application performance and energy efficiency, when Network-on-Chip (NoC) is used as a scalable communication subsystem. To address the brand-new challenges brought by dark silicon, in this paper, we present FoToNoC, a Folded Torus-like NoC, coupled with a hierarchical management strategy for heterogeneous many-core systems. On top of it, objectives of maximizing application performance, energy efficiency and chip reliability are isolated and well achieved by hardware-software co-design in several different phases, including application mapping and scheduling, cluster management and DVFS control. Evaluations on PARSEC benchmark applications demonstrate the significance of the entire strategy. Compared with state-of-the-art approaches, the proposed FoToNoC organization can achieve on average 35.4 and 35.2 percent on communication efficiency and application performance improvement, respectively, when maintaining the safe chip temperature. The hierarchical cluster-based management strategy can further reduce an average 34.6 percent of the total energy consumption with a notable reduction on the chip peak temperature. The significant achievements on system energy efficiency and the reduction on chip temperature of H.264 decoder and DSP-stone benchmarks additionally verify the effectiveness of the proposed methods."",""1558-2183"","""",""10.1109/TPDS.2016.2643669"",""NSFC(grant numbers:61402060)"; National 863 Programs(grant numbers:2013AA013202,2015AA015304); Chongqing High-Tech Research Programs(grant numbers:cstc2015jcyjA40042,cstc2014yykfB40007);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7795229"",""Network-on-chip";dark silicon;heterogeneous systems;performance;energy consumption;"temperature"",""Silicon";Thermal management;Reliability;Computer architecture;Optimization;Scheduling;"Transistors"","""",""28"","""",""41"",""IEEE"",""22 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FPGA-Based Scalable and Power-Efficient Fluid Simulation using Floating-Point DSP Blocks,""K. Sano";" S. Yamamoto"",""Graduate School of Information Sciences, Tohoku University, Sendai, Miyagi, Japan";" Graduate School of Information Sciences, Tohoku University, Sendai, Miyagi, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2823"",""2837"",""High-performance and low-power computation is required for large-scale fluid dynamics simulation. Due to the inefficient architecture and structure of CPUs and GPUs, they now have a difficulty in improving power efficiency for the target application. Although FPGAs become promising alternatives for power-efficient and high-performance computation due to their new architecture having floating-point (FP) DSP blocks, their relatively narrow memory bandwidth requires an appropriate way to fully exploit the advantage. This paper presents an architecture and design for scalable fluid simulation based on data-flow computing with a state-of-the-art FPGA. To exploit available hardware resources including FP DSPs, we introduce spatial and temporal parallelism to further scale the performance by adding more stream processing elements (SPEs) in an array. Performance modeling and prototype implementation allow us to explore the design space for both the existing Altera Arria10 and the upcoming Intel Stratix10 FPGAs. We demonstrate that Arria10 10AX115 FPGA achieves 519 GFlops at 9.67 GFlops/W only with a stream bandwidth of 9.0 GB/s, which is 97.9 percent of the peak performance of 18 implemented SPEs. We also estimate that Stratix10 FPGA can scale up to 6844 GFlops by combining spatial and temporal parallelism adequately."",""1558-2183"","""",""10.1109/TPDS.2017.2691770"",""Grant-in-Aid(grant numbers:26540017)"; Ministry of Education, Culture, Sports, Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893769"",""FPGA";fluid simulation;custom computing machine;stream computing;floating-point;"high-performance computing"",""Field programmable gate arrays";Bandwidth;Computational modeling;Computer architecture;Hardware;Digital signal processing;"Parallel processing"","""",""26"","""",""43"",""IEEE"",""6 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Global EDF Schedulability Analysis for Parallel Tasks on Multi-Core Platforms,""H. S. Chwa"; J. Lee; J. Lee; K. -M. Phan; A. Easwaran;" I. Shin"",""School of Computing, KAIST, Daejeon, South Korea"; Department of Computer Science and Engineering, Sungkyunkwan University (SKKU), Seoul, South Korea; School of Computing, KAIST, Daejeon, South Korea; PRECISE center, University of Pennsylvania, Philadelphia, PA; School of Computer Engineering, Nanyang Technological University, Singapore;" School of Computing, KAIST, Daejeon, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1331"",""1345"",""With the widespread adoption of multi-core architectures, it is becoming more important to develop software in ways that takes advantage of such parallel architectures. This particularly entails a shift in programming paradigms towards fine-grained, thread-parallel computing. Many parallel programming models have been introduced for targeting such intra-task thread-level parallelism. However, most successful results on traditional multi-core real-time scheduling are focused on sequential programming models. For example, thread-level parallelism is not properly captured into the concept of interference, which is key to many schedulability analysis techniques. Thereby, most interference-based analysis techniques are not directly applicable to parallel programming models. Motivated by this, we extend the notion of interference to capture thread-level parallelism more accurately. We then leverage the proposed notion of parallelism-aware interference to derive efficient EDF schedulability tests that are directly applicable to parallel task models, including DAG models, on multi-core platforms, without knowing an optimal schedule. Our evaluation results indicate that the proposed analysis significantly advances the state-of-the-art in global EDF schedulability analysis for parallel tasks. In particular, we identify that our proposed schedulability tests are adaptive to different degrees of thread-level parallelism and scalable to the number of processors, resulting in substantial improvement of schedulability for parallel tasks on multi-core platforms."",""1558-2183"","""",""10.1109/TPDS.2016.2614669"",""IITP(grant numbers:B0101-15-0557)"; BSRP(grant numbers:NRF-2015R1D1A1A01058713,NRF-2014R1A1A1035827); NCRC(grant numbers:2010-0028680); NRF(grant numbers:2015M3A9A7067220,MEST/MSIP/MOTIE); MoE(grant numbers:MOE2013-T2-2-029,ARC9/14);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7580608"",""Real-time scheduling";parallel task;global EDF;"interference"",""Parallel processing";Optimal scheduling;Interference;Yarn;Synchronization;Computational modeling;"Analytical models"","""",""25"","""",""41"",""IEEE"",""30 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GPU Implementation of Bitplane Coding with Parallel Coefficient Processing for High Performance Image Compression,""P. Enfedaque"; F. Aulı-Llinas;" J. C. Moure"",""Department of Information and Communications Engineering, Universitat Autònoma de Barcelona, Bellaterra, Barcelona, Spain"; Department of Information and Communications Engineering, Universitat Autònoma de Barcelona, Bellaterra, Barcelona, Spain;" Department of Computer Architecture and Operating Systems, Universitat Autònoma de Barcelona, Bellaterra, Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2272"",""2284"",""The fast compression of images is a requisite in many applications like TV production, teleconferencing, or digital cinema. Many of the algorithms employed in current image compression standards are inherently sequential. High performance implementations of such algorithms often require specialized hardware like field integrated gate arrays. Graphics Processing Units (GPUs) do not commonly achieve high performance on these algorithms because they do not exhibit fine-grain parallelism. Our previous work introduced a new core algorithm for wavelet-based image coding systems. It is tailored for massive parallel architectures. It is called bitplane coding with parallel coefficient processing (BPC-PaCo). This paper introduces the first high performance, GPUbased implementation of BPC-PaCo. A detailed analysis of the algorithm aids its implementation in the GPU. The main insights behind the proposed codec are an efficient thread-to-data mapping, a smart memory management, and the use of efficient cooperation mechanisms to enable inter-thread communication. Experimental results indicate that the proposed implementation matches the requirements for high resolution (4 K) digital cinema in real time, yielding speedups of 30× with respect to the fastest implementations of current compression standards. Also, a power consumption evaluation shows that our implementation consumes 40× less energy for equivalent performance than state-of-the-art methods."",""1558-2183"","""",""10.1109/TPDS.2017.2657506"",""Universitat Autònoma de Barcelona"; MINECO; Catalan Government(grant numbers:472-02-2/2012,TIN2015-71126-R,TIN2014-53234-C2-1-R,2014SGR-691);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833172"",""Image coding";SIMD computing;graphics processing unit (GPU);"compute unified device architecture (CUDA)"",""Image coding";Encoding;Graphics processing units;Transform coding;Instruction sets;Codecs;"Computer architecture"","""",""12"","""",""27"",""IEEE"",""25 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GroupTrust: Dependable Trust Management,""X. Fan"; L. Liu; M. Li;" Z. Su"",""College of Computing, Georgia Institute of Technology, Atlanta, GA"; College of Computing, Georgia Institute of Technology, Atlanta, GA; School of Software Technology, Dalian University of Technology, Dalian, China;" State Key Laboratory of High-end Server & Storage Technology, Jinan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1076"",""1090"",""As advanced computing and communication technologies penetrate every aspect of our life, we have witnessed the persistent growth of open systems where entities interact with one another without prior knowledge or experiences. Trust becomes an important metric in such open systems. This paper presents a dependable trust management scheme-GroupTrust, and a working system to support GroupTrust. It makes three original contributions. First, we identify a set of vulnerabilities that are common in existing reputation based trust models. We show that reputation trust built solely on direct experiences or by combining direct experiences with uniform trust propagation can be vulnerable. Second, we develop GroupTrust, a dependable trust management scheme to provide reliable trust management in the presence of dishonest ratings, malicious camouflage, and malicious collusive behaviors. The GroupTrust scheme is novel in two aspects: (i) we develop a pairwise similarity based feedback credibility to enhance the resilience of trust computation in the presence of dishonest ratings";" (ii) we propose to propagate trust based on a Susceptible-Infected-Recovered (SIR) model, which defines trust propagation threshold to control how trust should be propagated. Finally, we evaluate the effectiveness of GroupTrust against fourthreat models using both simulated and real world datasets. Our experimental results show that feedback credibility based local trust computation can effectively constrain strategically malicious participants from taking advantages of their dishonest ratings. SIR-based trust propagation control enables safe trust propagation and blocks irrational trust propagation. We show that GroupTrust scheme significantly outperforms other trust models in terms of both performance and attack resilience in the presence of dishonest feedbacks, sparse feedbacks, and strategically malicious participants against four representative threat models."",""1558-2183"","""",""10.1109/TPDS.2016.2611660"",""Georgia Tech"; National Science Foundation(grant numbers:IIS-0905493,CNS-1115375,NSF 1547102,SaTC 1564097); National 863 Plan(grant numbers:SS2015AA010302); National Science Foundation(grant numbers:61272173,61100194);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572209"",""Trust propagation";trust and reputation management;decentralized computing network;"open systems"",""Computational modeling";Resilience;Open systems;Fans;Reliability;Atmospheric measurements;"Particle measurements"","""",""25"","""",""22"",""IEEE"",""20 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"H-PARAFAC: Hierarchical Parallel Factor Analysis of Multidimensional Big Data,""D. Chen"; Y. Hu; L. Wang; A. Y. Zomaya;" X. Li"",""State Key Lab of Software Engineering, Wuhan University, Wuhan, China"; School of Computer Science, University of California Riverside, Riverside, CA; School of Computer Science, China University of Geosciences, Wuhan, China; School of Information Technologies, University of Sydney, Sydney, NSW, Australia;" National Key Laboratory of Cognitive Neuroscience and Learning, Beijing Normal University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1091"",""1104"",""It has long been an important issue in various disciplines to examine massive multidimensional data superimposed by a high level of noises and interferences by extracting the embedded multi-way factors. With the quick increases of data scales and dimensions in the big data era, research challenges arise in order to (1) reflect the dynamics of large tensors while introducing no significant distortions in the factorization procedure and (2) handle influences of the noises in sophisticated applications. A hierarchical parallel processing framework over a GPU cluster, namely H-PARAFAC, has been developed to enable scalable factorization of large tensors upon a “divide-and-conquer” theory for Parallel Factor Analysis (PARAFAC). The H-PARAFAC framework incorporates a coarse-grained model for coordinating the processing of sub-tensors and a fine-grained parallel model for computing each sub-tensor and fusing sub-factors. Experimental results indicate that (1) the proposed method breaks the limitation on the scale of multidimensional data to be factorized and dramatically outperforms the traditional counterparts in terms of both scalability and efficiency, e.g., the runtime increases in the order of n2 when the data volume increases in the order of n3, (2) H-PARAFAC has potentials in refraining the influences of significant noises, and (3) H-PARAFAC is far superior to the conventional window-based counterparts in preserving the features of multiple modes of large tensors."",""1558-2183"","""",""10.1109/TPDS.2016.2613054"",""National Natural Science Foundation of China(grant numbers:61273063,61272314,81230023,61671332)"; National Social Science Foundation(grant numbers:16BSH107); Fundamental Research Funds for the Central Universities(grant numbers:2042015KF1009,211410100028); Science and Technology Supporting Program in Hubei province(grant numbers:2015BAA113); Excellent Youth Foundation of Hubei Scientific Committee(grant numbers:2012FFA025); Major Achievements Transformation Projects for the Central Universities in Beijing;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7575712"",""Multi-dimensional data processing";factorization;big data;parallel factor analysis;"GPGPU"",""Tensile stress";Computational modeling;Graphics processing units;Parallel processing;Mathematical model;Technological innovation;"Acceleration"","""",""49"","""",""35"",""IEEE"",""26 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"High Performance Exact Triangle Counting on GPUs,""M. Bisson";" M. Fatica"",""NVIDIA Corporation, Santa Clara, CA";" NVIDIA Corporation, Santa Clara, CA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3501"",""3510"",""This paper presents a GPU implementation of the graph triangle counting operation based on the set intersection algorithm. The algorithm is implemented in four kernels optimized for different types of graphs in a code delivering performance higher than the current state-of-the-art and without preprocessing the input graph. At runtime, a lightweight heuristic is used to select the kernel to run based on the specific graph taken as input. In contrast to previous works, the presented approach takes advantage of a set intersection operation implemented via bitmaps. Moreover, the simplicity of the approach allows the code to have limited size and engineering complexity."",""1558-2183"","""",""10.1109/TPDS.2017.2735405"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000612"",""Triangle counting";graph processing;GPU computing;parallel computing;big data;"CUDA"",""Graphics processing units";Big Data;Measurement;Sparse matrices;Data preprocessing;"Complexity theory"","""",""23"","""",""17"",""IEEE"",""3 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"HL-PCM: MLC PCM Main Memory with Accelerated Read,""M. Arjomand"; A. Jadidi; M. T. Kandemir; A. Sivasubramaniam;" C. R. Das"",""Department of Electrical Engineering and Computer Science, Pennsylvania State University, PA"; Department of Electrical Engineering and Computer Science, Pennsylvania State University, PA; Department of Electrical Engineering and Computer Science, Pennsylvania State University, PA; Department of Electrical Engineering and Computer Science, Pennsylvania State University, PA;" Department of Electrical Engineering and Computer Science, Pennsylvania State University, PA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3188"",""3200"",""Multi-Level Cell Phase Change Memory (MLC PCM) is a promising candidate technology for DRAM replacement in main memory of modern computers. Despite of its high density and low power advantages, this technology seriously suffers from slow read and write operations. While prior works extensively studied the problem of slow write, this paper targets high read latency problem in MLC PCM and introduces an architecture mechanism to overcome it. To this end, we rely on the fact that reading different bits from an MLC cell takes different latencies, i.e., for a 2-bit MLC, reading its Most-Significant Bit (MSB) is fast, while reading its Least-Significant Bits (LSBs) is slower. We then propose Half-Line PCM (HL-PCM), a novel memory architecture that leverages this non-uniformity in reading MLC PCM's content to send a requested memory block to the processor in different cycles-it sends half of a memory block to the processor ahead of the other half. If the processor requested a word belonging to the first half, it can resume its execution on receiving the first half, while the other half has not sent yet and scheduled to be received by the memory controller later. HL-PCM is easy and simple to implement, i.e., it needs minor modifications at memory controller, the search/evict policies at last level cache, as well as data layout in main memory. Our experimental results show that the proposed design improves the average memory access latency by 33-43 percent and program's execution time by 23 percent, on average, while incurring negligible overhead at memory controller and PCM DIMM, in a 16-core chip multiprocessor (CMP) running memory-intensive benchmarks."",""1558-2183"","""",""10.1109/TPDS.2017.2705125"",""NSF(grant numbers:1302557,1213052,1439021,1302225,1629129,1526750,1629915)"; Intel;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930492"",""Phase change memory";multi-level cell;"read latency"",""Phase change materials";Resistance;Random access memory;Prefetching;Memory management;"Microprocessors"","""",""12"","""",""44"",""USGov"",""17 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Hybrid Energy Storage with Supercapacitor for Cost-Efficient Data Center Power Shaving and Capping,""W. Zheng"; K. Ma;" X. Wang"",""Ohio State University, Columbus, OH"; Ohio State University, Columbus, OH;" Ohio State University, Columbus, OH"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1105"",""1118"",""Recent studies have proposed to dynamically reshape the power demand curve of a data center (i.e., power shaving) with energy storage devices, particularly uninterruptible power supply (UPS) batteries. Power shaving can be used to limit the peak power demand in a data center, in order to reduce both the power infrastructure investment (i.e., cap-ex) and the electricity bills (i.e., op-ex). However, power shaving requires the UPS batteries to be frequently charged/discharged, which is known to compromise the battery lifetime and availability. This paper presents a detailed quantitative study that explores different options to integrate supercapacitor (SC) with batteries for cost-efficient energy storage. Compared with batteries, SC allows more charge/discharge cycles and has a higher power density, which are desirable for fast power shaving. However, SC also has undesirable characteristics (e.g., relatively high self-discharging rate and cost). Therefore, we quantitatively compare three possible energy storage options (i.e., Battery-only, SC-only, and Battery+SC) in detail, with different SC self-discharging rate assumptions. SC options (SC-only and Battery+SC) are shown to be more cost-efficient designs, saving the energy storage cost by 34 percent, on average, compared with Battery-only. For a 10 MW data center in a 10-year period, the savings can be converted to $3 M in total cost of ownership (TCO) reduction by allowing more servers to be deployed. In addition, we also propose the integration of energy storage with dynamic voltage and frequency scaling (DVFS) to cap the peak power demand (i.e., power capping). Specifically, we comparatively studyfour power capping algorithms and discuss their applicable scenarios. Finally, we introduce our proof-of-concept SC physical testbed and present preliminary hardware testing results."",""1558-2183"","""",""10.1109/TPDS.2016.2607715"",""US National Science Foundation(grant numbers:CCF-1143605,CCF-1331712,CNS-1421452,CNS-1143607)"; US National Science Foundation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7563872"",""Data center";power shaving;power capping;supercapacitor;energy storage;server;"cost efficiency"",""Batteries";Uninterruptible power systems;Power demand;Supercapacitors;Hybrid power systems;"Servers"","""",""24"","""",""38"",""IEEE"",""9 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"I/O Stack Optimization for Efficient and Scalable Access in FCoE-Based SAN Storage,""Y. Wu"; F. Wang; Y. Hua; D. Feng; Y. Hu; W. Tong; J. Liu;" D. He"",""Ministry of Education of China, Key Laboratory of Data Storage Systems (School of Computer Science and Technology, Huazhong University of Science and Technology), Wuhan, China"; Ministry of Education of China, Key Laboratory of Data Storage Systems (School of Computer Science and Technology, Huazhong University of Science and Technology), Wuhan, China; Ministry of Education of China, Key Laboratory of Data Storage Systems (School of Computer Science and Technology, Huazhong University of Science and Technology), Wuhan, China; Ministry of Education of China, Key Laboratory of Data Storage Systems (School of Computer Science and Technology, Huazhong University of Science and Technology), Wuhan, China; Ministry of Education of China, Key Laboratory of Data Storage Systems (School of Computer Science and Technology, Huazhong University of Science and Technology), Wuhan, China; Ministry of Education of China, Key Laboratory of Data Storage Systems (School of Computer Science and Technology, Huazhong University of Science and Technology), Wuhan, China; Ministry of Education of China, Key Laboratory of Data Storage Systems (School of Computer Science and Technology, Huazhong University of Science and Technology), Wuhan, China;" Ministry of Education of China, Key Laboratory of Data Storage Systems (School of Computer Science and Technology, Huazhong University of Science and Technology), Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2514"",""2526"",""Due to the high complexity in software hierarchy and the shared queue & lock mechanism for synchronized access, existing I/O stack for accessing the FCoE based SAN storage becomes a performance bottleneck, thus leading to a high I/O overhead and limited scalability in multi-core servers. In order to address this performance bottleneck, we propose a synergetic and efficient solution that consists of three optimization strategies for accessing the FCoE based SAN storage: (1) We use private per-CPU structures and disabling kernel preemption method to process I/Os, which significantly improves the performance of parallel I/O in multi-core servers"; (2) We directly map the requests from the block-layer to the FCoE frames, which efficiently translates I/O requests into network messages;" (3) We adopt a low latency I/O completion scheme, which substantially reduces the I/O completion latency. We have implemented a prototype (called FastFCoE, a protocol stack for accessing the FCoE based SAN storage). Experimental results demonstrate that FastFCoE achieves efficient and scalable I/O throughput, obtaining 1132.1K/836K IOPS (6.6/5.4 times as much as original Linux Open-FCoE stack) for read/write requests."",""1558-2183"","""",""10.1109/TPDS.2017.2685139"",""863 Project(grant numbers:2015AA015301)"; National Key Research and Development Program of China(grant numbers:2016YFB1000202); 863 Project(grant numbers:2015AA016701); NSFC(grant numbers:61502191,61502190,61472153,61402189); State Key Laboratory of Computer Architecture(grant numbers:CARCH201505); Wuhan Applied Basic Research Project(grant numbers:2015010101010004); Hubei Provincial NSFC(grant numbers:2016CFB226);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7882635"",""Storage architecture";fiber channel over ethernet;"multi-core framework"",""Servers";Scalability;Throughput;Synchronization;Multicore processing;Optimization;"Protocols"","""",""4"","""",""36"",""IEEE"",""20 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"iASK: A Distributed Q&A System Incorporating Social Community and Global Collective Intelligence,""G. Liu";" H. Shen"",""Holcombe Department of Electrical and Computer Engineering, Clemson University, Clemson, SC";" Department of Computer Science, Univeristy of Virginia, Charlottesville, VA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1346"",""1360"",""Traditional web-based Question and Answer (Q&A) websites cannot easily solve non-factual questions to match askers' preference. Recent research efforts begin to study social-based Q&A systems that rely on an asker's social friends to provide answers. However, this method cannot find answerers for a question not belonging to the asker's interests. To solve this problem, we propose a distributed Q&A system incorporating both social community intelligence and global collective intelligence, named as iASK. iASK improves the response latency and answer quality in both the social domain and global domain. It uses a neural network based friend ranking method to identify answerer candidates by considering social closeness and Q&A activities. To efficiently identify answerers in the global user base, iASK builds a virtual server tree that embeds the hierarchical structure of interests, and also maps users to the tree based on user interests. To accurately locate the cooperative experts, iASK has a fine-grained reputation system to evaluate user reputation based on their cooperativeness and expertise, and uses a reputation based reward strategy to encourage users to be cooperative. To further improve the performance of iASK, we propose a weak tie assisted social based potential answerer location algorithm and an interest coefficient based uncategorized question forwarding algorithm. To further improve the response quality and cooperativeness, we propose a reputation based reward strategy that motivates users to answer questions from unknown users. Experimental results from large-scale trace-driven simulation and real-world daily usages of the iASK prototype show the superior performance of iASK. It achieves high answer quality with 24 percent higher accuracy, short response latency with 53 percent less delay and effective cooperative incentives with 16 percent more answers compared to other social-based Q&A systems. The results also show the effectiveness of the enhancement algorithms in improving the performance of iASK."",""1558-2183"","""",""10.1109/TPDS.2016.2618864"",""U.S. NSF(grant numbers:NSF-1404981,IIS-1354123,CNS-1254006,CNS-1249603)"; Microsoft Research(grant numbers:8300751);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7600358"",""Distributed systems";question and answer systems;social networks;"information search"",""Social network services";Servers;Delays;Search engines;Neural networks;"Prototypes"","""",""2"","""",""44"",""IEEE"",""19 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improved Carry-in Workload Estimation for Global Multiprocessor Scheduling,""Q. Zhou"; G. Li;" J. Li"",""School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China"; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2527"",""2538"",""As an important and fundamental tool for analyzing the schedulability of a real-time task set on the multiprocessor platform, response time analysis (RTA) has been researched for several years on both Global Fixed Priority (G-FP) and Global Earliest Deadline First (G-EDF) scheduling. This paper proposes a new analysis that improves over current state-of-the-art RTA methods for both G-FP and G-EDF scheduling, by reducing their pessimism. The key observation is that when estimating the carry-in workload, all the existing RTA techniques depend on the worst case scenario in which the carry-in job should execute as late as possible and just finishes execution before its worst case response time (WCRT). But the carry-in workload calculated under this assumption may be over-estimated, and thus the accuracy of the response time analysis may be impacted. To address this problem, we first propose a new method to estimate the carry-in workload more precisely. The proposed method does not depend on any specific scheduling algorithm and can be used for both G-FP and G-EDF scheduling. We then propose a general RTA algorithm that can improve most existing RTA tests by incorporating our carry-in estimation method. To further improve the execution efficiency, we also introduce an optimization technique for our RTA tests. Experiments with randomly generated task sets are conducted and the results show that, compared with the state-of-the-art technologies, the proposed tests exhibit considerable performance improvements, up to 9 and 7.8 percent under G-FP and G-EDF scheduling respectively, in terms of schedulability test precision."",""1558-2183"","""",""10.1109/TPDS.2017.2679195"",""National Natural Science Foundation of China(grant numbers:61332001)"; National Natural Science Foundation of China(grant numbers:61300045,61572215,61672252,61602197); Natural Science Foundation of Hubei Province(grant numbers:2016CFB192); Fundamental Research Funds for the Central Universities(grant numbers:HUST-2016YXMS076,HUST-2016YXMS085);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873231"",""Real-time scheduling";multiprocessor;global scheduling;"response time analysis"",""Processor scheduling";Time factors;Interference;Scheduling;Estimation;Upper bound;"Real-time systems"","""",""14"","""",""34"",""IEEE"",""7 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving Execution Concurrency of Large-Scale Matrix Multiplication on Distributed Data-Parallel Platforms,""R. Gu"; Y. Tang; C. Tian; H. Zhou; G. Li; X. Zheng;" Y. Huang"",""State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu Sheng, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu Sheng, China; State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu Sheng, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Jiangsu Sheng, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2539"",""2552"",""Matrix multiplication is a dominant but very time-consuming operation in many big data analytic applications. Thus its performance optimization is an important and fundamental research issue. The performance of large-scale matrix multiplication on distributed data-parallel platforms is determined by both computation and IO costs. For existing matrix multiplication execution strategies, when the execution concurrency scales up above a threshold, their execution performance deteriorates quickly because the increase of the IO cost outweighs the decrease of the computation cost. This paper presents a novel parallel execution strategy CRMM (Concurrent Replication-based Matrix Multiplication) along with a parallel algorithm, Marlin, for large-scale matrix multiplication on data-parallel platforms. The CRMM strategy exploits higher execution concurrency for sub-block matrix multiplication with the same IO cost. To further improve the performance of Marlin, we also propose a number of novel system-level optimizations, including increasing the concurrency of local data exchange by calling native library in batch, reducing the overhead of block matrix transformation, and reducing disk heavy shuffle operations by exploiting the semantics of matrix computation. We have implemented Marlin as a library along with a set of related matrix operations on Spark and also contributed Marlin to the open-source community. For large-sized matrix multiplication, Marlin outperforms existing systems including Spark MLlib, SystemML and SciDB, with about 1.29×, 3.53× and 2.21× speedup on average, respectively. The evaluation upon a real-world DNN workload also indicates that Marlin outperforms above systems by about 12.8×, 5.1× and 27.2× speedup, respectively."",""1558-2183"","""",""10.1109/TPDS.2017.2686384"",""National Natural Science Foundation of China(grant numbers:61572250,61602194)"; Jiangsu Province Industry Support Program(grant numbers:BE2014131); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884988"",""Parallel matrix multiplication";data-parallel algorithms;"machine learning library"",""Concurrent computing";Libraries;Sparks;Machine learning algorithms;Training;Optimization;"Big Data"","""",""15"","""",""46"",""IEEE"",""22 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving IBM POWER8 Performance Through Symbiotic Job Scheduling,""J. Feliu"; S. Eyerman; J. Sahuquillo; S. Petit;" L. Eeckhout"",""Department of Computer Engineering (DISCA), Universitat Politécnica de Valéncia, Valéncia, Spain"; Intel Belgium, Kontich, Belgium; Department of Computer Engineering (DISCA), Universitat Politécnica de Valéncia, Valéncia, Spain; Department of Computer Engineering (DISCA), Universitat Politécnica de Valéncia, Valéncia, Spain;" Department of Electronics and Information Systems, Ghent University, Gent, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2838"",""2851"",""Symbiotic job scheduling, i.e., scheduling applications that co-run well together on a core, can have a considerable impact on the performance of processors with simultaneous multithreading (SMT) cores. SMTcores share most of their microarchitectural components among the co-running applications, which causes performance interference between them. Therefore, scheduling applications with complementary resource requirements on the same core can greatly improve the throughput of the system. This paper enhances symbiotic job scheduling for the IBM POWER8 processor. We leverage the existing cycle accounting mechanism to build an interference model that predicts symbiosis between applications. The proposed models achieve higher accuracy than previous models by predicting job symbiosis from throttled CPI stacks, i.e., CPI stacks of the applications when running in the same SMT mode to consider the statically partitioned resources, but without interference from other applications. The symbiotic scheduler uses these interference models to decide, at run-time, which applications should run on the same core or on separate cores. We prototype the symbiotic scheduler as a user-level scheduler in the Linux operating system and evaluate it on an IBM POWER8 server running multiprogram workloads. The symbiotic job scheduler significantly improves performance compared to both an agnostic random scheduler and the default Linux scheduler. Across all evaluated workloads in SMT4 mode, throughput improves by 12.4 and 5.1 percent on average over the random and Linux schedulers, respectively."",""1558-2183"","""",""10.1109/TPDS.2017.2691708"",""Spanish Ministerio de Economía y Competitividad (MINECO)(grant numbers:TIN2015-66972-C5-1-R,TIN2014-62246-EXP)"; European Research Council; European Community’s Seventh Framework Programme(grant numbers:FP7/2007-2013); ERC(grant numbers:259295);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7893747"",""Symbiotic job scheduling";performance estimation;interference model;IBM POWER8;"simultaneous multithreading (SMT)"",""Symbiosis";Multicore processing;Interference;Predictive models;Program processors;Message systems;"Throughput"","""",""3"","""",""27"",""IEEE"",""6 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving Performance for Flash-Based Storage Systems through GC-Aware Cache Management,""S. Wu"; B. Mao; Y. Lin;" H. Jiang"",""Computer Science Department of Xiamen University, Xiamen, Fujian, China"; Software School of Xiamen University, Xiamen, Fujian, China; Computer Science Department of Xiamen University, Xiamen, Fujian, China;" Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2852"",""2865"",""Flash-based SSDs have been extensively deployed in modern storage systems to satisfy the increasing demand of storage performance and energy efficiency. However, Garbage Collection (GC) is an important performance concern for flash-based SSDs, because it tends to disrupt the normal operations of an SSD. This problem continues to plague flash-based storage systems, particularly in the high performance computing and enterprise environment. An important root cause for this problem, as revealed by previous studies, is the serious contention for the flash resources and the severe mutually adversary interference between the user I/O requests and GC-induced I/O requests. The on-board buffer cache within SSDs serves to play an essential role in smoothing the gap between the upper-level applications and the lower-level flash chips and alleviating this problem to some extend. Nevertheless, the existing cache replacement algorithms are well optimized to reduce the miss rate of the buffer cache by reducing the I/O traffic to the flash chips as much as possible, but without considering the GC operations within the flash chips. Consequently, they fail to address the root cause of the problem and thus are far from being sufficient and effective in reducing the expensive I/O traffic to the flash chips that are in the GC state. To address this important performance issue in flash-based storage systems, particularly in the HPC and enterprise environment, we propose a Garbage Collection aware Replacement policy, called GCaR, to improve the performance of flash-based SSDs. The basic idea is to give higher priority to caching the data blocks belonging to the flash chips that are in the GC state. This substantially lessens the contentions between the user I/O operations and the GC-induced I/O operations. To verify the effectiveness of GCaR, we have integrated it into the SSD extended Disksim simulator. The simulation results show that GCaR can significantly improve the storage performance by up to 40.7 percent in terms of the average response times."",""1558-2183"","""",""10.1109/TPDS.2017.2692757"",""National Natural Science Foundation of China(grant numbers:61472336,61402385)"; US NSF(grant numbers:NSF-CNS-1116606,NSF-CNS-1016609); Huawei Innovation Research Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7895128"",""Flash storage";solid state drive;garbage collection;cache management;"performance evaluation"",""Performance evaluation";Interference;High performance computing;Time factors;Program processors;Writing;"Computer science"","""",""17"","""",""47"",""IEEE"",""12 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving Performance of Heterogeneous MapReduce Clusters with Adaptive Task Tuning,""D. Cheng"; J. Rao; Y. Guo; C. Jiang;" X. Zhou"",""Department of Computer Science, University of North Carolina at Charlotte, NC"; Department of Computer Science, University of Colorado, Colorado Springs, CO; Postdoc Fellow in the Argonne National Lab, Lemont, IL; Department of Computer Science & Technology, Tongji University, Jiading, Shanghai, China;" Department of Computer Science, University of Colorado, Colorado Springs, CO"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""774"",""786"",""Datacenter-scale clusters are evolving toward heterogeneous hardware architectures due to continuous server replacement. Meanwhile, datacenters are commonly shared by many users for quite different uses. It often exhibits significant performance heterogeneity due to multi-tenant interferences. The deployment of MapReduce on such heterogeneous clusters presents significant challenges in achieving good application performance compared to in-house dedicated clusters. As most MapReduce implementations are originally designed for homogeneous environments, heterogeneity can cause significant performance deterioration in job execution despite existing optimizations on task scheduling and load balancing. In this paper, we observe that the homogeneous configuration of tasks on heterogeneous nodes can be an important source of load imbalance and thus cause poor performance. Tasks should be customized with different configurations to match the capabilities of heterogeneous nodes. To this end, we propose a self-adaptive task tuning approach, Ant, that automatically searches the optimal configurations for individual tasks running on different nodes. In a heterogeneous cluster, Ant first divides nodes into a number of homogeneous subclusters based on their hardware configurations. It then treats each subcluster as a homogeneous cluster and independently applies the self-tuning algorithm to them. Ant finally configures tasks with randomly selected configurations and gradually improves tasks configurations by reproducing the configurations from best performing tasks and discarding poor performing configurations. To accelerate task tuning and avoid trapping in local optimum, Ant uses genetic algorithm during adaptive task configuration. Experimental results on a heterogeneous physical cluster with varying hardware capabilities show that Ant improves the average job completion time by 31, 20, and 14 percent compared to stock Hadoop (Stock), customized Hadoop with industry recommendations (Heuristic), and a profilingbased configuration approach (Starfish), respectively. Furthermore, we extend Ant to virtual MapReduce clusters in a multi-tenant private cloud. Specifically, Ant characterizes a virtual node based on two measured performance statistics: I/O rate and CPU steal time. It uses k-means clustering algorithm to classify virtual nodes into configuration groups based on the measured dynamic interference. Experimental results on virtual clusters with varying interferences show that Ant improves the average job completion time by 20, 15, and 11 percent compared to Stock, Heuristic and Starfish, respectively."",""1558-2183"","""",""10.1109/TPDS.2016.2594765"",""U.S. National Science Foundation(grant numbers:CNS-1422119,CNS-1320122,CNS-1217979)"; NSF(grant numbers:61328203);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7523426"",""MapReduce performance improvement";self-adaptive task tuning;heterogeneous clusters;"genetic algorithm"",""Tuning";Hardware;Cloud computing;Interference;Optimization;Clustering algorithms;"Industries"","""",""66"","""",""25"",""IEEE"",""27 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"iShuffle: Improving Hadoop Performance with Shuffle-on-Write,""Y. Guo"; J. Rao; D. Cheng;" X. Zhou"",""Department of Computer Science, University of Colorado, Colorado Springs, CO"; Department of Computer Science, University of Colorado, Colorado Springs, CO; Department of Computer Science, University of Colorado, Colorado Springs, CO;" Department of Computer Science, University of Colorado, Colorado Springs, CO"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1649"",""1662"",""Hadoop is a popular implementation of the MapReduce framework for running data-intensive jobs on clusters of commodity servers. Shuffle, the all-to-all input data fetching phase between the map and reduce phase can significantly affect job performance. However, the shuffle phase and reduce phase are coupled together in Hadoop and the shuffle can only be performed by running the reduce tasks. This leaves the potential parallelism between multiple waves of map and reduce unexploited and resource wastage in multi-tenant Hadoop clusters, which significantly delays the completion of jobs in a multi-tenant Hadoop cluster. More importantly, Hadoop lacks the ability to schedule task efficiently and mitigate the data distribution skew among reduce tasks, which leads to further degradation of job performance. In this work, we propose to decouple shuffle from reduce tasks and convert it into a platform service provided by Hadoop. We present iShuffle, a user-transparent shuffle service that pro-actively pushes map output data to nodes via a novel shuffle-on-write operation and flexibly schedules reduce tasks considering workload balance. Experimental results with representative workloads and Facebook workload trace show that iShuffle reduces job completion time by as much as 29.6 and 34 percent in single-user and multi-user clusters, respectively."",""1558-2183"","""",""10.1109/TPDS.2016.2587645"",""U.S. National Science Foundation CAREER(grant numbers:CNS-0844983,CNS-1422119,CNS-1320122,CNS-1217979)"; NSF of China(grant numbers:61328203);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506133"",""MapReduce";shuffle;dataskew;"task scheduling"",""Delays";Couplings;Data models;Scheduling;Parallel processing;Schedules;"Facebook"","""",""59"","""",""37"",""IEEE"",""7 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Large-Scale VM Placement with Disk Anti-Colocation Constraints Using Hierarchical Decomposition and Mixed Integer Programming,""Y. Xia"; M. Tsugawa; J. A. B. Fortes;" S. Chen"",""Department of Computer and Information Science and Engineering, University of Florida, Gainesville, FL"; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL;" Department of Computer and Information Science and Engineering, University of Florida, Gainesville, FL"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1361"",""1374"",""As computational clouds offer increasingly sophisticated services, there is a dramatic increase in the variety and complexity of virtual machine (VM) placement problems. In this paper, we consider a VM placement problem with a special type of anti-colocation requirements-disk anti-colocation-which stipulate that, for every VM assigned to a PM (physical machine), its virtual disks should be spread out across the physical disks of the PM. Once such a requirement is met, the users of the VM can expect improved disk I/O performance. There will also be improvement in fault tolerance and availability. For scalable solutions, we propose a method that combines hierarchical decomposition with mixed integer programming (MIP), where the basic building blocks are independent, small MIP subproblems. We provide experimental results to demonstrate the effectiveness of the proposed method. We show that it is scalable and achieves high performance with respect to the optimization objective."",""1558-2183"","""",""10.1109/TPDS.2016.2615933"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586124"",""Cloud computing";datacenter;virtual machine placement;resource management;"mixed integer programming"",""Heuristic algorithms";Resource management;Cloud computing;Servers;Clustering algorithms;Algorithm design and analysis;"Linear programming"","""",""11"","""",""44"",""IEEE"",""7 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Lazy Irrevocability for Best-Effort Transactional Memory Systems,""R. Quislant"; E. Gutierrez; E. L. Zapata;" O. Plata"",""Department of Computer Architecture, University of Málaga, Málaga, Spain"; Department of Computer Architecture, University of Málaga, Málaga, Spain; Department of Computer Architecture, University of Málaga, Málaga, Spain;" Department of Computer Architecture, University of Málaga, Málaga, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1919"",""1932"",""IBM and Intel now offer commercial systems with Transactional Memory (TM), a programming paradigm whose aim is to facilitate concurrent programming while maximizing parallelism. These TM systems are implemented in hardware and provide a software fallback path to overcome the hardware implementation limitations. They are known as best-effort hardware TM (BE-HTM) systems. The software fallback path must be provided by the user to ensure forward progress, which adds programming complexity to the TM paradigm. We propose a new type of irrevocability (a transactional mode that marks transactions as non-abortable) to deal with BE-HTM limitations in a more efficient manner, and to liberate the user from having to program a fallback path. It is based on the concept of lazy subscription used in the context of software fallback paths, where the fallback lock is checked at the end of the transaction instead of at the beginning. We propose a hardware lazy irrevocability mechanism that does not involve changes in the coherence protocol. It solves the unsafe execution problem of premature commits associated with lazy subscription fallbacks, and can be triggered by the user via an ISA extension, for the sake of versatility. It is compared with its software counterpart, which we propose as an enhanced lazy single global lock with escaped spinning at the end of the transaction. We also propose the lazy irrevocability with anticipation, a mechanism that cannot be implemented in software, which significantly improves the performance of codes with multiple cache evictions of transactional data. The evaluation of the proposals is carried out with the Simics/GEMS simulator along with the STAMP benchmark suite, and we obtain speedups from 14 to 28 percent over the fallback path approaches."",""1558-2183"","""",""10.1109/TPDS.2016.2638815"",""Government of Spain(grant numbers:TIN2013-42253-P)"; Junta de Andalucía(grant numbers:P12-TIC-1470);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782383"",""Best-effort hardware transactional memory";irrevocability;lazy subscriptionm;"transaction fallback path"",""Software";Hardware;Protocols;Coherence;Context;Programming;"Concurrent computing"","""",""3"","""",""31"",""IEEE"",""13 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"LazyCtrl: A Scalable Hybrid Network Control Plane Design for Cloud Data Centers,""K. Zheng"; L. Wang; B. Yang; Y. Sun;" S. Uhlig"",""Huawei Technologies, Beijing, China"; SnT, University of Luxembourg, Luxembourg; IBM Research, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China;" Queen Mary University of London, London, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""115"",""127"",""The advent of software defined networking enables flexible, reliable and feature-rich control planes for data center networks. However, the tight coupling of centralized control and complete visibility leads to a wide range of issues among which scalability has risen to prominence due to the excessive workload on the central controller. By analyzing the traffic patterns from a couple of production data centers, we observe that data center traffic is usually highly skewed and thus edge switches can be clustered into a set of communication-intensive groups according to traffic locality. Motivated by this observation, we present LazyCtrl, a novel hybrid control plane design for data center networks where network control is carried out by distributed control mechanisms inside independent groups of switches while complemented with a global controller. LazyCtrl aims at bringing laziness to the global controller by dynamically devolving most of the control tasks to independent switch groups to process frequent intra-group events near the datapath while handling rare inter-group or other specified events by the controller. We implement LazyCtrl and build a prototype based on Open vSwitch and Floodlight. Trace-driven experiments on our prototype show that an effective switch grouping is easy to maintain in multi-tenant clouds and the central controller can be significantly shielded by staying “lazy”, with its workload reduced by up to 82 percent."",""1558-2183"","""",""10.1109/TPDS.2016.2558512"",""National Basic Research Program of China(grant numbers:2012CB315802)"; Natural Science Foundation of China(grant numbers:61520106005,61379133);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7460089"",""Software defined networks";network control;data center;"cloud computing"",""Cloud computing";Scalability;Distributed databases;Decentralized control;"Centralized control"","""",""12"","""",""38"",""IEEE"",""27 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Leader Set Selection for Low-Latency Geo-Replicated State Machine,""S. Liu";" M. Vukolić"",""College of Computer Science, National University of Defense Technology, Changsha, China";" IBM Research-Zurich, Rueschlikon, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1933"",""1946"",""Modern planetary scale distributed systems largely rely on a State Machine Replication protocol to keep their service reliable, yet it comes with a specific challenge: latency, bounded by the speed of light. In particular, clients of a single-leader protocol, such as Paxos, must communicate with the leader which must in turn communicate with other replicas: inappropriate selection of a leader may result in unnecessary round-trips across the globe. To cope with this limitation, several all-leader and leaderless alternatives have been proposed recently. Unfortunately, none of them fits all circumstances. In this article we argue that the “right” choice of the number of leaders depends on a given replica configuration and the workload. Then we present ${\mathsf {Droopy}}$  and  ${\mathsf {Dripple}}$ , two sister approaches built upon state machine replication protocols. ${\mathsf {Droopy}}$  dynamically reconfigures the set of leaders. Whereas, ${\mathsf {Dripple}}$  coordinates state partitions wisely, so that each partition can be reconfigured (by ${\mathsf {Droopy}}$ ) separately. Our experimental evaluation on Amazon EC2 shows that,  ${\mathsf {Droopy}}$  and ${\mathsf {Dripple}}$  reduce latency under imbalanced or localized workloads, compared to their native protocol. When most requests are non-commutative, our approaches do not affect the performance of their native protocol and both outperform a state-of-the-art leaderless protocol."",""1558-2183"","""",""10.1109/TPDS.2016.2636148"",""EU H2020(grant numbers:643964)"; Swiss Secretariat for Education, Research and Innovation(grant numbers:15.0025); National Key Research and Development Program(grant numbers:2016YFB1000101); China Scholarship Council;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7774985"",""State machine replication";geo-replication;latency optimization;"state-partitioning"",""Protocols";Gold;Indexes;Computer crashes;Reliability;Delays;"Commutation"","""",""11"","""",""31"",""IEEE"",""6 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Leaky Buffer: A Novel Abstraction for Relieving Memory Pressure from Cluster Data Processing Frameworks,""Z. Liu";" T. S. E. Ng"",""Computer Science Department, Rice University, Houston, TX";" Computer Science Department, Rice University, Houston, TX"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""128"",""140"",""The shift to the in-memory data processing paradigm has had a major influence on the development of cluster data processing frameworks. Numerous frameworks from the industry, open source community and academia are adopting the in-memory paradigm to achieve functionalities and performance breakthroughs. However, despite the advantages of these in-memory frameworks, in practice they are susceptible to memory-pressure related performance collapse and failures. The contributions of this paper are twofold. First, we conduct a detailed diagnosis of the memory pressure problem and identify three preconditions for the performance collapse. These preconditions not only explain the problem but also shed light on the possible solution strategies. Second, we propose a novel programming abstraction called the leaky bufferthat eliminates one of the preconditions, thereby addressing the underlying problem. We have implemented a leaky buffer enabled hashtable in Spark, and we believe it is also able to substitute the hashtable that performs similar hash aggregation operations in any other programs or data processing frameworks. Experiments on a range of memory intensive aggregation operations show that the leaky buffer abstraction can drastically reduce the occurrence of memoryrelated failures, improve performance by up to 507 percent and reduce memory usage by up to 87.5 percent."",""1558-2183"","""",""10.1109/TPDS.2016.2546909"",""NSF(grant numbers:CNS1422925,CNS1305379,CNS1162270)"; IBM; Microsoft; Rice University Computer Science Graduate Fellowship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442158"",""C.2.4.b Distributed applications";C.2.4 distributed systems;C.2 communication/networking and information technology;C computer systems organization;C.4.a design studies;C.4 performance of systems;C computer systems organization;D.1.0 general;D.1 programming techniques;D software/software engineering;D.2.10.a design concepts;D.2.10 design;D.2 software engineering;D software/software engineering;E.0 general;"E data"",""Data processing";Sparks;Random access memory;Memory management;Industries;Programming;"Servers"","""",""9"","""",""41"",""IEEE"",""25 Mar 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Leveraging Adaptive I/O to Optimize Collective Data Shuffling Patterns for Big Data Analytics,""B. Nicolae"; C. H. A. Costa; C. Misale; K. Katrinis;" Y. Park"",""IBM Research, Dublin, Ireland"; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; University of Torino, Torino, Italy; IBM Research, Dublin, Ireland;" IBM Thomas J. Watson Research Center, Yorktown Heights, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1663"",""1674"",""Big data analytics is an indispensable tool in transforming science, engineering, medicine, health-care, finance and ultimately business itself. With the explosion of data sizes and need for shorter time-to-solution, in-memory platforms such as Apache Spark gain increasing popularity. In this context, data shuffling, a particularly difficult transformation pattern, introduces important challenges. Specifically, data shuffling is a key component of complex computations that has a major impact on the overall performance and scalability. Thus, speeding up data shuffling is a critical goal. To this end, state-of-the-art solutions often rely on overlapping the data transfers with the shuffling phase. However, they employ simple mechanisms to decide how much data and where to fetch it from, which leads to sub-optimal performance and excessive auxiliary memory utilization for the purpose of prefetching. The latter aspect is a growing concern, given evidence that memory per computation unit is continuously decreasing while interconnect bandwidth is increasing. This paper contributes a novel shuffle data transfer strategy that addresses the two aforementioned dimensions by dynamically adapting the prefetching to the computation. We implemented this novel strategy in Spark, a popular inmemory data analytics framework. To demonstrate the benefits of our proposal, we run extensive experiments on an HPC cluster with large core count per node. Compared with the default Spark shuffle strategy, our proposal shows: up to 40 percent better performance with 50 percent less memory utilization for buffering and excellent weak scalability."",""1558-2183"","""",""10.1109/TPDS.2016.2627558"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7740885"",""Distributed systems";big data analytics;Spark;data shuffling;scalable I/O;memory efficient concurrent data transfers;I/O load balancing;"elastic buffering"",""Sparks";Memory management;Data transfer;Big data;Context;Scalability;"Explosions"","""",""19"","""",""31"",""IEEE"",""10 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Leveraging Time Prediction and Error Compensation to Enhance the Scalability of Parallel Multi-Core Simulations,""X. Zhu"; J. Wu;" T. Li"",""College of Computer Science and Engineering, Anhui University of Science and Technology, China"; Suzhou Institute for Advanced Study, University of Science and Technology of China, Hefei, China;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2553"",""2566"",""Due to synchronization overhead, it is challenging to apply the parallel simulation technique of multi-core processors at larger scales. Although the use of lax synchronization schemes could reduce overhead and balance the load between synchronous points, it introduces timing error and deteriorates simulation accuracy. Through observing the propagation paths of errors, we find that these paths always concentrate on some pivotal events. Based on the observation, we design a delay-calibration mechanism to alleviate errors. We decouple the timing and functional processes of the pivotal events, leveraging prediction technique of delays to connect two categories of the processes. Errors are traced throughout the timing processes of the pivotal events, and are deducted from the predicted delays before the delays are consumed by the functional processes. Therefore, through cleaning the errors at the successive pivot events, the mechanism decreases the simulated time deviations efficiently. Since the prediction and error deduction processes do not have any constraint on synchronizations, our approach largely maintains the scalability of lax synchronization schemes. Furthermore, our proposal is orthogonal to other parallel simulation techniques and can be used in conjunction with them. Experimental results show that error compensation improves the accuracy of lax synchronized simulations by 68 percent and achieves 97.8 percent accuracy when combined with an enhanced lax synchronization."",""1558-2183"","""",""10.1109/TPDS.2016.2612633"",""National Natural Science Foundation of China(grant numbers:61272132)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7574321"",""Multi-core architecture modeling";parallel simulation;synchronization;"timing error"",""Synchronization";Delays;Multicore processing;Error compensation;"Load modeling"","""",""1"","""",""24"",""IEEE"",""22 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Lightweight Virtual Memory Support for Zero-Copy Sharing of Pointer-Rich Data Structures in Heterogeneous Embedded SoCs,""P. Vogel"; A. Marongiu;" L. Benini"",""Integrated Systems Laboratory, ETH Zurich, Zurich, Switzerland"; Electrical, Electronic, and Information Engineering Department of the University of Bologna, Bologna, Italy;" Electrical, Electronic, and Information Engineering Department of the University of Bologna, Bologna, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1947"",""1959"",""While high-end heterogeneous systems are increasingly supporting heterogeneous uniform memory access (hUMA), their low-power counterparts still lack basic features like virtual memory support for accelerators. Instead of simply passing pointers, explicit data management involving copies is needed which hampers programmability and performance. In this work, we evaluate a mixed hardware/software solution for lightweight virtual memory support for many-core accelerators in heterogeneous embedded systems-on-chip. Based on an input/output translation lookaside buffer managed by a host kernel-level driver, and compiler extensions protecting the accelerator's accesses to shared data, our solution is non-intrusive to the architecture of the accelerator cores, and enables zero-copy sharing of pointer-rich data structures."",""1558-2183"","""",""10.1109/TPDS.2016.2645219"",""EU 7th Framework Programme(grant numbers:FP7-611016)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797491"",""Heterogeneous embedded systems on chip";programmable many-core accelerators;shared virtual memory;pointer-rich data structures;"graph processing"",""Data structures";Memory management;Support vector machines;Hardware;Acceleration;"Software"","""",""8"","""",""50"",""IEEE"",""26 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Limitations of Highly-Available Eventually-Consistent Data Stores,""H. Attiya"; F. Ellen;" A. Morrison"",""Computer Science Department, Technion—Israel Institute of Technology, Haifa, Israel"; Department of Computer Science, University of Toronto, Toronto, Canada;" Computer Science Department, Technion—Israel Institute of Technology, Haifa, Israel"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""141"",""155"",""Modern replicated data stores aim to provide high availability, by immediately responding to client requests, often by implementing objects that expose concurrency. Such objects, for example, multi-valued registers (MVRs), do not have sequential specifications. This paper explores a recent model for replicated data stores that can be used to precisely specify causalconsistency for such objects, and liveness properties like eventual consistency, without revealing details of the underlying implementation. The model is used to prove the following results: 1) An eventually consistent data store implementing MVRs cannot satisfy a consistency model strictly stronger than observable causal consistency (OCC). OCC is a model somewhat stronger than causal consistency, which captures executions in which client observations can use causality to infer concurrency of operations. This result holds under certain assumptions about the data store. 2) Under the same assumptions, an eventually consistent and causally consistent replicated data store must send messages of size linear in the size of the system: Ifs objects, each Ω(lg k)-bit in size, are supported by n replicas, then there is an execution in which an Ω(min{n, s}lg k)-bit message is sent."",""1558-2183"","""",""10.1109/TPDS.2016.2556669"",""Israel Science Foundation(grant numbers:1227/10,1749/14)"; Yad HaNadiv Foundation; Israel Science Foundation(grant numbers:1227/10,1749/14); Yad HaNadiv Foundation; Natural Science and Engineering Research Council of Canada;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457259"",""Replicated data store";causal consistency;"eventual consistency"",""Data models";Distributed databases;Concurrent computing;Registers;Delays;Safety;"Computer science"","""",""22"","""",""31"",""IEEE"",""20 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Live VM Migration Under Time-Constraints in Share-Nothing IaaS-Clouds,""K. Tsakalozos"; V. Verroios; M. Roussopoulos;" A. Delis"",""Canonical Ltd, London, United Kindom"; Stanford University, Stanford, CA; University of Athens, Athens, Greece;" University of Athens, Athens, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2285"",""2298"",""Live VMmigration helps attain both cloud-wide load balancing and operational consolidation while the migrating VMs remain accessible to users. To avoid periods of high-load for the involved resources, IaaS-cloud operators assign specific time windows for such migrations to occur in an orderly manner. Moreover, providers typically rely on share-nothing architectures to attain scalability. In this paper, we focus on the real-time scheduling of live VM migrations in large share-nothing IaaS clouds, such that migrations are completed on time and without adversely affecting agreed-upon SLAs. We propose a scalable, distributed network of brokers that oversees the progress of all on-going migration operations within the context of a provider. Brokers make use of an underlying special purpose file system, termed MigrateFS, that is capable of both replicating and keeping in sync virtual disks while the hypervisor livemigrates VMs (i.e., RAM and CPU state). By limiting the resources consumed during migration, brokers implement policies to reduce SLA violations while seeking to complete all migration tasks on time. We evaluate two such policies, one based on task prioritization and a second that considers the financial implications set by migration deadline requirements. Using our MigrateFS prototype operating on a real cloud, we demonstrate the feasibility of performing migrations within time windows. By simulating large clouds, we assess the effectiveness of our proposed broker policies in a share-nothing configuration";" we also demonstrate that our approach stresses 24 percent less an already saturated network if compared to an unsupervised set up."",""1558-2183"","""",""10.1109/TPDS.2017.2658572"",""EU"; FP7; ERC(grant numbers:279237);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833184"",""Distributed systems";cloud computing;IaaS  clouds;"virtual machine migration"",""Cloud computing";Synchronization;Resource management;Load management;Scalability;Real-time systems;"Random access memory"","""",""53"","""",""42"",""IEEE"",""25 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Load Balancing at the Edge of Chaos: How Self-Organized Criticality Can Lead to Energy-Efficient Computing,""J. L. J. Jiménez Laredo"; F. Guinand; D. Olivier;" P. Bouvry"",""Normandie Univ, LITIS, Le Havre, France"; Normandie Univ, UNIHAVRE, UNIROUEN, INSA Rouen, LITIS, Le Havre, France; Normandie Univ, LITIS, Le Havre, France;" CSC/SnT, University of Luxembourg, Luxembourg City, Luxembourg"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""517"",""529"",""This paper investigates a self-organized critical approach for dynamically load-balancing computational workloads. The proposed model is based on the Bak-Tang-Wiesenfeld sandpile: a cellular automaton that works in a critical regime at the edge of chaos. In analogy to grains of sand, tasks arrive and pile up on the different processing elements or sites of the system. When a pile exceeds a certain threshold, it collapses and initiates an avalanche of migrating tasks, i.e., producing load-balancing. We show that the frequency of such avalanches is in power-law relation with their sizes, a scale-invariant fingerprint of self-organized criticality that emerges without any tuning of parameters. Such an emergent pattern has organic properties such as the self-organization of tasks into resources or the self-optimization of the computing performance. The conducted experimentation also reveals that the system has a critical attractor in the point in which the arrival rate of tasks equals the processing power of the system. Taking advantage of this fact, we hypothesize that the processing elements can be turned on and off depending on the state of the workload as to maximize the utilization of resources. An interesting side effect is that the overall energy consumption of the system is minimized without compromising the quality of service."",""1558-2183"","""",""10.1109/TPDS.2016.2582160"",""Upper Normandy Region GRR-MRT PROTEC project"; TERA-MRT; Luxembourg INTER/CNRS/11/03 Green@ Cloud project;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7494682"",""Energy efficiency";nonlinear dynamical systems;distributed computing;"scheduling algorithms"",""Chaos";Load modeling;Automata;Quality of service;Energy consumption;Complex systems;"Scheduling"","""",""16"","""",""34"",""IEEE"",""20 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Managing Battery Aging for High Energy Availability in Green Datacenters,""L. Liu"; H. Sun; C. Li; T. Li; J. Xin;" N. Zheng"",""School of Electrical and Information Engineering, Xi'an Jiaotong University, Xi'an, China"; School of Electrical and Information Engineering, Xi'an Jiaotong University, Xi'an, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL; School of Electrical and Information Engineering, Xi'an Jiaotong University, Xi'an, China;" School of Electrical and Information Engineering, Xi'an Jiaotong University, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3521"",""3536"",""Energy storage devices (ESD), such as UPS batteries, have been repurposed in datacenter as a promising tuning knob for peak power shaving and power cost reducing. However, batteries progressively aging due to irregular usage patterns, which result in less effective capacity and even pose serious threat to server availability. Nevertheless, prior proposals largely ignore the aging issues of battery which may lead to low energy availability for datacenter servers. To fill this critical void, we thoroughly investigate battery aging on a heavily instrumented prototype system over an observation period of ten months. We propose Battery Anti-Aging Treatment Plus (BAAT-P), a novel power delivery architecture included aging management algorithms from the perspective of computing system to hide, reduce, mitigate and plan the battery aging effects for high energy availability in datacenter. Our techniques exploit diverse battery aging mechanisms and dynamic aging management algorithms to provide system-level availability guarantee for datacenter. We evaluate the BAAT-P design with a real prototype. Compared with a battery powered datacenter without aging management policies, the results show that BAAT-P can extend battery lifetime by 72 percent, reduce battery cost by 33 percent and effectively improve energy availability for datacenter servers while maintaining workload performance for the performance critical workloads."",""1558-2183"","""",""10.1109/TPDS.2017.2712778"",""NSF(grant numbers:1423090,1320100,1117261)"; NSFC(grant numbers:61602368,61502302); National Key Research and Development Program of China(grant numbers:2016YFB0200501); China Postdoctoral Science Foundation(grant numbers:168236);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7942036"",""Green datacenters";battery aging management;power management;"availability"",""Batteries";Aging;Servers;Green products;Computer architecture;"Heuristic algorithms"","""",""18"","""",""68"",""IEEE"",""7 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Managing Resource Limitation of Best-Effort HTM,""M. Mohamedin"; R. Palmieri; A. Hassan;" B. Ravindran"",""University of Alexandria, Alexandria, Egypt"; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA; University of Alexandria, Alexandria, Egypt;" Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2299"",""2313"",""The first release of hardware transactional memory (HTM) as commodity processor posed the question of how to efficiently handle its best-effort nature. In this paper we present Part-HTM, a hybrid transactional memory protocol that solves the problem of transactions aborted due to the resource limitations (space/time) of current best-effort HTM. The basic idea of Part-HTM is to partition those transactions into multiple sub-transactions, which can likely be committed in hardware. Due to the eager nature of HTM, we designed a low-overhead software framework to preserve transaction's correctness (with and without opacity) and isolation. Part-HTM is effective: our evaluation study confirms that its performance is the best in all tested cases, except for those where HTM cannot be outperformed. However, in such a workload, Part-HTM still performs better than all other software and hybrid competitors."",""1558-2183"","""",""10.1109/TPDS.2017.2668415"",""Air Force Office of Scientific Research (AFOSR)(grant numbers:FA9550-14-1-0187)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852519"",""Transactional memory";hardware transactions;"concurrency"",""Hardware";Software;Protocols;Synchronization;Memory management;Instruments;"Proposals"","""",""3"","""",""38"",""IEEE"",""13 Feb 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Measuring Scale-Up and Scale-Out Hadoop with Remote and Local File Systems and Selecting the Best Platform,""Z. Li";" H. Shen"",""Department of Computer Science, University of Virginia, Charlottesville, VA";" Department of Computer Science, University of Virginia, Charlottesville, VA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3201"",""3214"",""MapReduce is a popular computing model for parallel data processing on large-scale datasets, which can vary from gigabytes to terabytes and petabytes. Though Hadoop MapReduce normally uses Hadoop Distributed File System (HDFS) local file system, it can be configured to use a remote file system. Then, an interesting question is raised: for a given application, which is the best running platform among the different combinations of scale-up and scale-out Hadoop with remote and local file systems. However, there has been no previous research on how different types of applications (e.g., CPU-intensive, data-intensive) with different characteristics (e.g., input data size) can benefit from the different platforms. Thus, in this paper, we conduct a comprehensive performance measurement of different applications on scale-up and scale-out clusters configured with HDFS and a remote file system (i.e., OFS), respectively. We identify and study how different job characteristics (e.g., input data size, the number of file reads/writes, and the amount of computations) affect the performance of different applications on the different platforms. Based on the measurement results, we also propose a performance prediction model to help users select the best platforms that lead to the minimum latency. Our evaluation using a Facebook workload trace demonstrates the effectiveness of our prediction model. This study is expected to provide a guidance for users to choose the best platform to run different applications with different characteristics in the environment that provides both remote and local storage, such as HPC cluster and cloud environment."",""1558-2183"","""",""10.1109/TPDS.2017.2712635"",""U.S. NSF(grant numbers:ACI-1719397,CNS-1733596)"; Microsoft Research(grant numbers:8300751);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7940040"",""MapReduce";Hadoop;scale-up;scale-out;remote file system;local file system;"job characteristics"",""Random access memory";Predictive models;File systems;Distributed databases;Program processors;Computational modeling;"Facebook"","""",""17"","""",""46"",""IEEE"",""6 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Minimizing Delay in Network Function Virtualization with Shared Pipelines,""O. Rottenstreich"; I. Keslassy; Y. Revah;" A. Kadosh"",""Princeton University, Princeton, NJ"; Department of Electrical Engineering, Technion, Haifa, Israel; Marvell Israel, Yokneam, Israel;" Marvell Israel, Yokneam, Israel"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""156"",""169"",""Pipelines are widely used to increase throughput in multi-core chips by parallelizing packet processing while relying on virtualization. Typically, each packet type is served by a dedicated pipeline with several cores, each implementing a network service. However, with the increase in the number of packet types and their number of required services, there are not enough cores for pipelines. In this paper, we study pipeline sharing, such that a single pipeline can be used to serve several packet types. Pipeline sharing decreases the needed total number of cores, but typically increases pipeline lengths and therefore packet delays. We consider two novel optimization problems of allocating cores between different packet types such that the average or the worst-case delay is minimized. We study the two problems and suggest optimal algorithms that apply under different assumptions on the input. We also present greedy algorithms for the general case. Last, we examine our solutions on synthetic examples as well as on real-life applications and demonstrate that they often achieve close-to-optimal delays."",""1558-2183"","""",""10.1109/TPDS.2016.2556670"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457328"",""Network function virtualization";multicore optimization;"network processors"",""Pipelines";Delays;Multicore processing;Pipeline processing;Virtualization;"Program processors"","""",""20"","""",""19"",""IEEE"",""20 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Mitigating Service Variability in MapReduce Clusters via Task Cloning: A Competitive Analysis,""H. Xu"; W. C. Lau; Z. Yang; G. de Veciana;" H. Hou"",""College of Computer Science and Technology, Dongguan University of Technology, Dongguan, China"; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong; Department of Electrical and Computer Engineering, The University of Texas, Austin, TX;" College of Electronic Engineering, Dongguan University of Technology, Dongguan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2866"",""2880"",""Measurement traces from real-world production environment show that the execution time of tasks within a MapReduce job varies widely due to the variability in machine service capacity. This variability issue makes efficient job scheduling over large-scale MapReduce clusters extremely challenging. To tackle this problem, we adopt the task cloning approach to mitigate the effect of machine variability and design corresponding scheduling algorithms so as to minimize the overall job flowtime in different scenarios. For offline scheduling where all jobs arrive at the same time, we design an $O(1)$ -competitive algorithm, which gives priorities to jobs with small effective workload. We then extend this offline algorithm to yield the so-called Smallest Remaining Effective Workload based $\beta$ -fraction Sharing plus Cloning algorithm (SREW+C($\beta$ )) for the online case. We also show that SREW+C($\beta$ ) is  $(1+ 2\beta + \epsilon)$ -speed  $O(\frac{1}{\beta \epsilon })$ -competitive with respect to the sum of job flowtime within a cluster. We demonstrate via trace-driven simulations that SREW+C( $\beta$ ) can significantly reduce the overall job flowtime by cutting down the elapsed time of small jobs substantially. In particular, SREW+C($\beta$ ) reduces the total job flowtime by 14, 10 and 11 percent respectively when comparing to Mantri, Dolly and Grass."",""1558-2183"","""",""10.1109/TPDS.2017.2689767"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7890998"",""MapReduce";job scheduling;cloning;job flowtime;"competitive performance ratio"",""Cloning";Redundancy;Algorithm design and analysis;Scheduling;Electronic mail;"Scheduling algorithms"","""",""3"","""",""38"",""IEEE"",""31 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Model-Based Estimation of the Communication Cost of Hybrid Data-Parallel Applications on Heterogeneous Clusters,""J. -A. Rico-Gallego"; A. L. Lastovetsky;" J. -C. Díaz-Martín"",""University of Extremadura, Avd. Universidad s/n, Cáceres, Spain"; University College Dublin, Belfield, Dublin 4, Ireland;" University of Extremadura, Avd. Universidad s/n, Cáceres, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3215"",""3228"",""Heterogeneous systems composed of CPUs and accelerators sharing communication channels of different performance are getting mainstream in HPC but, at the same time, they show a complexity that makes it difficult to optimize the deployment of a data parallel application. Recent analytical tools such as Functional Performance Models, combined with advanced partitioning algorithms, manage to achieve a balanced configuration by distributing the workload unevenly, according to the performance of the different processing units. Unfortunately, such uneven distribution of the computation load leads to communication unbalances that, very often, render worthless the previous workload balancing efforts. Finding the optimal communication scheme without expensive testing on the executing platform requires an analytical approach to the estimation of the communication cost of different configurations of the application. With this goal in mind, we propose and discuss an extension of the t-Lop communication performance model to cover heterogeneous architectures. In order to provide a quantitative assessment of this extended model, we conduct experiments with two representative computational kernels, the SUMMA algorithm and the 2D wave equation solver. The t-Lop predictions are compared against the HLogGP model and the observed costs for a variety of configurations, hardware resources and problem sizes."",""1558-2183"","""",""10.1109/TPDS.2017.2715809"",""Science Foundation Ireland (SFI)(grant numbers:14/IA/2474)"; COST Program Action(grant numbers:IC1305); Network for Sustainable Ultrascale Computing (NESUS); Extremadura Research Center for Advanced Technologies (CETA-CIEMAT); European Regional Development Fund (ERDF);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949135"",""Communication performance modeling";hybrid algorithms;performance analysis;functional performance models;"heterogeneous platforms"",""Computational modeling";Kernel;Biological system modeling;Load modeling;Analytical models;Mathematical model;"Tools"","""",""18"","""",""41"",""IEEE"",""15 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Model-Based Optimization of EULAG Kernel on Intel Xeon Phi Through Load Imbalancing,""A. Lastovetsky"; L. Szustak;" R. Wyrzykowski"",""School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland"; Czestochowa University of Technology, Dabrowskiego 69, Czestochowa, Poland;" Czestochowa University of Technology, Dabrowskiego 69, Czestochowa, Poland"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""787"",""797"",""Load balancing is a widely accepted technique for performance optimization of scientific applications on parallel architectures. Indeed, balanced applications do not waste processor cycles on waiting at points of synchronization and data exchange, maximizing this way the utilization of processors. In this paper, we challenge the universality of the load-balancing approach to optimization of the performance of parallel applications. First, we formulate conditions that should be satisfied by the performance profile of an application in order for the application to achieve its best performance via load balancing. Then we use a real-life scientific application, EULAG MPDATA kernel, to demonstrate that its performance profile on a modern parallel architecture, Intel Xeon Phi, significantly deviates from these conditions. Based on this observation, we propose a method of performance optimization of scientific applications through load imbalancing. In the case of data parallel application, the method uses functional performance models of the application to find partitioning that minimizes its computation time but not necessarily balances the load of processors. We apply this method to optimization of MPDATA on Intel Xeon Phi. Experimental results demonstrate that the performance of this carefully optimized load-balanced application can be further improved by 15percent using the proposed load-imbalancing technique."",""1558-2183"","""",""10.1109/TPDS.2016.2599527"",""Science Foundation Ireland (SFI)(grant numbers:14/IA/2474)"; NCN(grant numbers:UMO-2011/03/B/ST6/03500,UMO-2015/17/D/ST6/04059); COST(grant numbers:IC1305); Network for Sustainable Ultrascale Computing (NESUS); Czestochowa University of Technology; Intel Xeon Phi coprocessors; MICLAB(grant numbers:POIG.02.03.00.24-093/13);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7542201"",""Functional performance model";data partitioning;Intel Xeon Phi;EULAG;MPDATA;"load imbalancing"",""Optimization";Load management;Heuristic algorithms;Partitioning algorithms;Load modeling;Parallel architectures;"Computational modeling"","""",""34"","""",""43"",""IEEE"",""11 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Modeling and Simulating Multiple Failure Masking Enabled by Local Recovery for Stencil-Based Applications at Extreme Scales,""M. Gamell"; K. Teranishi; J. Mayo; H. Kolla; M. A. Heroux; J. Chen;" M. Parashar"",""Rutgers Discovery Informatics Institute, Rutgers University, Piscataway, NJ"; Sandia National Laboratories, Livermore, CA; Sandia National Laboratories, Livermore, CA; Sandia National Laboratories, Livermore, CA; Sandia National Laboratories, Albuquerque, NM; Sandia National Laboratories, Livermore, CA;" Rutgers Discovery Informatics Institute, Rutgers University, Piscataway, NJ"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2881"",""2895"",""Obtaining multi-process hard failure resilience at the application level is a key challenge that must be overcome before the promise of exascale can be fully realized. Previous work has shown that online global recovery can dramatically reduce the overhead of failures when compared to the more traditional approach of terminating the job and restarting it from the last stored checkpoint. If online recovery is performed in a local manner further scalability is enabled, not only due to the intrinsic lower costs of recovering locally, but also due to derived effects when using some application types. In this paper we model one such effect, namely multiple failure masking, that manifests when running Stencil parallel computations on an environment when failures are recovered locally. First, the delay propagation shape of one or multiple failures recovered locally is modeled to enable several analyses of the probability of different levels of failure masking under certain Stencil application behaviors. Our results indicate that failure masking is an extremely desirable effect at scale which manifestation is more evident and beneficial as the machine size or the failure rate increase."",""1558-2183"","""",""10.1109/TPDS.2017.2696538"",""National Science Foundation (NSF)(grant numbers:ACI 1339036,ACI 1310283,CNS 1305375,DMS 1228203)"; Office of Advanced Scientific Computing Research; Office of Science; US Department of Energy; SciDAC Institute for Scalable Data Management; Analysis and Visualization (SDAV)(grant numbers:DE-SC0007455); RSVP(grant numbers:4000126989); ASCR; FES; EPSI(grant numbers:DE-FG02-06ER54857); ExaCT Combustion Co-Design Center(grant numbers:4000110839); Rutgers Discovery Informatics Institute; Sandia National Laboratories; National Technology and Engineering Solutions of Sandia; Honeywell International; U.S. Department of Energy’s; National Nuclear Security Administration(grant numbers:DE-NA0003525);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7908967"",""Parallel processing";stencil computation;resilience;fault tolerance;failure masking;"modeling"",""Computational modeling";Delays;Protocols;Resilience;Fault tolerance;Fault tolerant systems;"Hardware"","""",""11"","""",""47"",""IEEE"",""24 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Moving Hadoop into the Cloud with Flexible Slot Management and Speculative Execution,""Y. Guo"; J. Rao; C. Jiang;" X. Zhou"",""Department Computer Science, University of Colorado, Colorado Springs, CO"; Department Computer Science, University of Colorado, Colorado Springs, CO; Key Lab of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China;" Department Computer Science, University of Colorado, Colorado Springs, CO"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""798"",""812"",""Load imbalance is a major source of overhead in parallel programs such as MapReduce. Due to the uneven distribution of input data, tasks with more data become stragglers and delay the overall job completion. Running Hadoop in a private cloud opens up opportunities for expediting stragglers with more resources but also introduces problems that often outweigh the performance gain: (1) performance interference from co-running jobs may create new stragglers";" (2) there exists a semantic gap between the Hadoop task management and resource pool-based virtual cluster management preventing tasks from using resources efficiently. In this paper, we strive to make Hadoop more resilient to data skew and more efficient in cloud environments. We present FlexSlot, a user-transparent task slot management scheme that automatically identifies map stragglers and resizes their slots accordingly to accelerate task execution. FlexSlot adaptively changes the number of slots on each virtual node to balance the resource usage so that the pool of resources can be efficiently utilized. FlexSlot further improves mitigation of data skew with an adaptive speculative execution strategy. Experimental results show that FlexSlot effectively reduces job completion time up to $47.2$  percent compared to stock Hadoop and two recently proposed skew mitigation and speculative execution approaches."",""1558-2183"","""",""10.1109/TPDS.2016.2587641"",""US National Science Foundation(grant numbers:CNS-1422119,CNS-1320122,CNS-1217979)"; NSF(grant numbers:61328203);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506079"",""MapReduce, cloud, data skew, stragglers, task slot management, speculative execution"",""Resource management";Cloud computing;Interference;Dynamic scheduling;Containers;Semantics;"Acceleration"","""",""30"","""",""41"",""IEEE"",""7 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"MRCP-RM: A Technique for Resource Allocation and Scheduling of MapReduce Jobs with Deadlines,""N. Lim"; S. Majumdar;" P. Ashwood-Smith"",""Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada"; Department of Systems and Computer Engineering, Carleton University, Ottawa, ON, Canada;" Huawei Technologies Canada, Kanata, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1375"",""1389"",""Resource allocation and scheduling on clouds are required to harness the power of the underlying resource pool such that the service provider can meet the quality of service requirements of users, which are often captured in service level agreements (SLAs). This paper focuses on resource allocation and scheduling on clouds and clusters that process MapReduce jobs with SLAs. The resource allocation and scheduling problem is modelled as an optimization problem using constraint programming, and a novel MapReduce Constraint Programming based Resource Management algorithm (MRCP-RM) is devised that can effectively process an open stream of MapReduce jobs where each job is characterized by an SLA comprising an earliest start time, a required execution time, and an end-to-end deadline. A detailed performance evaluation of MRCP-RM is conducted for an open system subjected to a stream of job arrivals using both simulation and experimentation on a real system. The experiments on a real system are performed on a Hadoop cluster (deployed on Amazon EC2) that runs our new Hadoop Constraint Programming based Resource Management algorithm (HCP-RM) that incorporates a technique for handling data locality. The results of the performance evaluation demonstrate the effectiveness of MRCP-RM/HCP-RM in generating a schedule that leads to a low proportion of jobs missing their deadlines (P) and also provide insights into system behaviour and performance. In the simulation experiments, it is observed that MRCP-RM achieves on average an 82 percent lower P compared to a technique from the existing literature when processing a synthetic workload from Facebook. Furthermore, in the experiments performed on a Hadoop cluster deployed on Amazon EC2, it is observed that HCP-RM achieved on average a 63 percent lower P compared to an EDF-Scheduler for a wide variety of workload and system parameters experimented with."",""1558-2183"","""",""10.1109/TPDS.2016.2617324"",""Huawei Technologies"; Natural Science and Engineering Research Council of Canada;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7590072"",""Resource allocation and scheduling on clusters and clouds";hadoop scheduler;"and constraint programming"",""Resource management";Scheduling;Programming;Quality of service;Processor scheduling;Cloud computing;"Performance evaluation"","""",""24"","""",""28"",""IEEE"",""13 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Nebula: Distributed Edge Cloud for Data Intensive Computing,""A. Jonathan"; M. Ryden; K. Oh; A. Chandra;" J. Weissman"",""University of Minnesota Twin Cities, Computer Science & Engineering, Minneapolis, MN"; University of Minnesota Twin Cities, Computer Science & Engineering, Minneapolis, MN; University of Minnesota Twin Cities, Computer Science & Engineering, Minneapolis, MN; University of Minnesota Twin Cities, Computer Science & Engineering, Minneapolis, MN;" University of Minnesota Twin Cities, Computer Science & Engineering, Minneapolis, MN"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3229"",""3242"",""Centralized cloud infrastructures have become the popular platforms for data-intensive computing today. However, they suffer from inefficient data mobility due to the centralization of cloud resources, and hence, are highly unsuited for geo-distributed data-intensive applications where the data may be spread at multiple geographical locations. In this paper, we present Nebula: a dispersed edge cloud infrastructure that explores the use of voluntary resources for both computation and data storage. We describe the lightweight Nebula architecture that enables distributed data-intensive computing through a number of optimization techniques including location-aware data and computation placement, replication, and recovery. We evaluate Nebula performance on an emulated volunteer platform that spans over 50 PlanetLab nodes distributed across Europe, and show how a common data-intensive computing framework, MapReduce, can be easily deployed and run on Nebula. We show Nebula MapReduce is robust to a wide array of failures and substantially outperforms other wide-area versions based on emulated existing systems."",""1558-2183"","""",""10.1109/TPDS.2017.2717883"",""NSF(grant numbers:CSR-1162405,CNS-1619254)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7954728"",""Distributed Systems";cloud computing;edge cloud;"data intensive computing"",""Cloud computing";Distributed databases;Bandwidth;Computational modeling;Memory;Data models;"Wide area networks"","""",""52"","""",""41"",""IEEE"",""21 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Nessie: A Decoupled, Client-Driven Key-Value Store Using RDMA,""B. Cassell"; T. Szepesi; B. Wong; T. Brecht; J. Ma;" X. Liu"",""David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada"; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada;" David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3537"",""3552"",""Key-value storage systems are an integral part of many data centre applications, but as demand increases so does the need for high performance. This has motivated new designs that use Remote Direct Memory Access (RDMA) to reduce communication overhead. Current RDMA-enabled key-value stores (RKVSes) target workloads involving small values, running on dedicated servers on which no other applications are running. Outside of these domains, however, there may be other RKVS designs that provide better performance. In this paper, we introduce Nessie, an RKVS that is fully client-driven, meaning no server process is involved in servicing requests. Nessie also decouples its index and storage data structures, allowing indices and data to be placed on different servers. This flexibility can decrease the number of network operations required to service a request. These design elements make Nessie well-suited for a different set of workloads than existing RKVSes. Compared to a server-driven RKVS, Nessie more than doubles system throughput when there is CPU contention on the server, improves throughput by 70 percent for PUT-oriented workloads when data value sizes are 128 KB or larger, and reduces power consumption by 18 percent at 80 percent system utilization and 41 percent at 20 percent system utilization compared with idle power consumption."",""1558-2183"","""",""10.1109/TPDS.2017.2729545"",""University of Waterloo"; Natural Sciences and Engineering Research Council of Canada (NSERC); Ontario Graduate Scholarship; University of Waterloo; Canada Foundation for Innovation; Ontario Research Fund;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7987083"",""Computer networks";data storage systems;distributed computing;distributed systems;key-value stores;"RDMA"",""Servers";Throughput;Instruction sets;Data transfer;Indexing;"Distributed databases"","""",""20"","""",""31"",""IEEE"",""20 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"New Model-Based Methods and Algorithms for Performance and Energy Optimization of Data Parallel Applications on Homogeneous Multicore Clusters,""A. Lastovetsky";" R. Reddy Manumachu"",""School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland";" School of Computer Science, University College Dublin, Belfield, Dublin 4, Ireland"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1119"",""1133"",""Modern homogeneous parallel platforms are composed of tightly integrated multicore CPUs. This tight integration has resulted in the cores contending for various shared on-chip resources such as Last Level Cache (LLC) and interconnect, leading to resource contention and non-uniform memory access (NUMA). Due to these newly introduced complexities, the performance and energy profiles of real-life scientific applications on these platforms are not smooth and may deviate significantly from the shapes that allowed traditional and state-of-the-art load balancing algorithms to minimize their computation time. In this paper, we propose new model-based methods and algorithms for minimization of time and energy of computations for the most general shapes of performance and energy profiles of data parallel applications observed on the modern homogeneous multicore clusters. We formulate the performance and energy optimization problems and present efficient algorithms of complexity O(p2) solving these problems where p is the number of processors. It is important to note that the globally optimal solutions found by these algorithms may not load-balance the application. We experimentally study the efficiency and scalability of our algorithms for two data parallel applications, matrix multiplication and fast Fourier transform, on a modern multicore CPU and clusters of such CPUs. We also demonstrate the optimality of solutions determined by our algorithms."",""1558-2183"","""",""10.1109/TPDS.2016.2608824"",""Science Foundation Ireland (SFI)(grant numbers:14/IA/2474)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7565748"",""Performance";energy;power;performance optimization;energy optimization;functional performance models;energy models;data partitioning;load balancing;"homogeneous multicore clusters"",""Servers";Energy consumption;Load management;Multicore processing;Clustering algorithms;Shape;"Program processors"","""",""43"","""",""36"",""IEEE"",""13 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"NVM Way Allocation Scheme to Reduce NVM Writes for Hybrid Cache Architecture in Chip-Multiprocessors,""J. -H. Choi";" G. -H. Park"",""Department of Computer Science and Engineering, Seoul National University, Gwanak-ro, Gwanak-gu, Seoul, Republic of Korea";" Department of Computer Engineering, Sejong University 98, Gunja-Dong, Gwangjin-Gu, Seoul, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2896"",""2910"",""Hybrid cache architectures (HCAs) containing both SRAM and non-volatile memory (NVM) have been proposed to overcome the disadvantages of NVM-based cache architecture. Most previous works have concentrated on managing write-intensive blocks by storing these blocks to SRAM to reduce the number of the write operations to NVM. However, they have not focused on reducing linefill operations which also occupy a large portion of overall NVM write counts in chip-multiprocessor (CMP) environments. This paper proposes an NVM way allocation scheme, taking into account the NVM linefill counts as well as cache miss rate and the NVM write hit counts during victim selection. Three metrics are introduced to estimate the effectiveness of NVM way allocation: Miss counts change (ΔM), write counts change (ΔW), and NVM write counts change (ΔNVMW). An algorithm to minimize the write counts of NVM based on these metrics is proposed as well. Our experimental results show that dynamic energy consumption is reduced by 37.5 percent on average."",""1558-2183"","""",""10.1109/TPDS.2017.2689010"",""National Research Foundation of Korea (NRF)(grant numbers:NRF-2011-220-D00098,2015-R1D1A1A01059711)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7889038"",""Cache memories";emerging technologies;heterogeneous (hybrid) systems low-power design;"multi-core/single-chip multiprocessors"",""Nonvolatile memory";Random access memory;Resource management;Measurement;Energy consumption;Heuristic algorithms;"Computer architecture"","""",""24"","""",""38"",""IEEE"",""29 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Oasis: Scaling Out Datacenter Sustainably and Economically,""C. Li"; Y. Hu; J. Gu; J. Yuan;" T. Li"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL; Department of Electrical and Computer Engineering, University of Michigan, Ann Arbor, MI; School of Computer Science and Technology, Wuhan University of Technology, Wuhan, China;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1960"",""1973"",""As big data applications proliferate, datacenters today are increasingly looking to adopt a scale-out model. Nevertheless, power capacity has become an important bottleneck that restricts horizontal scaling of servers, especially in datacenters that oversubscribe power infrastructure. When a datacenter hits its ceiling for power provisioning, conventionally the owner has to either build another facility or upgrade existing infrastructure-both approaches add huge cost, require significant time, and can further increase carbon footprint. This paper proposes Oasis, a novel datacenter expansion strategy that enables power-/carbon- constrained servers to scale out economically and sustainably. The basic structure of Oasis, called Oasis Node, naturally supports incremental capacity expansion with near-zero environmental impact since it leverages modular solar panels and distributed battery systems to power newly added servers. To optimize the operation of newly added nodes, we further propose a management framework called Ozone. It allows Oasis to jointly perform power supply switching and server speed scaling to improve efficiency locally and globally. We implement a prototype of Oasis and use it as a research platform for evaluating the design tradeoffs of green scale-out datacenters. With Oasis, a green datacenter could gradually double its capacity with near-oracle performance, extended battery lifetime, and 26 percent cost savings."",""1558-2183"","""",""10.1109/TPDS.2016.2615625"",""NSF(grant numbers:1320100,1117261,0937869,0916384,0845721,0834288,0811611,0720476)"; NSFC(grant numbers:61128004,61502302); CCF-Intel Young Faculty Researcher Program; CCF-Tencent Open Fund;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7585018"",""Data center";power management;scalabilty;"energy efficiency"",""Servers";Green products;Batteries;Gases;Power supplies;"Computational modeling"","""",""8"","""",""43"",""IEEE"",""6 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"On Data Integrity Attacks Against Real-Time Pricing in Energy-Based Cyber-Physical Systems,""X. Zhang"; X. Yang; J. Lin; G. Xu;" W. Yu"",""Xi’an Jiaotong University, Shaanxi, P.R., China"; Xi’an Jiaotong University, Shaanxi, P.R., China; Xi’an Jiaotong University, Shaanxi, P.R., China; Frostburg State University, Frostburg, MD;" Towson University, MD"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""170"",""187"",""In this paper, we investigate a novel real-time pricing scheme, which considers both renewable energy resources and traditional power resources and could effectively guide the participants to achieve individual welfare maximization in the system. To be specific, we develop a Lagrangian-based approach to transform the global optimization conducted by the power company into distributed optimization problems to obtain explicit energy consumption, supply, and price decisions for individual participants. Also, we show that these distributed problems derived from the global optimization by the power company are consistent with individual welfare maximization problems for end-users and traditional power plants. We also investigate and formalize the vulnerabilities of the real-time pricing scheme by considering two types of data integrity attacks: Ex-ante attacks and Ex-post attacks, which are launched by the adversary before or after the decision-making process. We systematically analyze the welfare impacts of these attacks on the real-time pricing scheme. Through a combination of theoretical analysis and performance evaluation, our data shows that the real-time pricing scheme could effectively guide the participants to achieve welfare maximization, while cyber-attacks could significantly disrupt the results of real-time pricing decisions, imposing welfare reduction on the participants."",""1558-2183"","""",""10.1109/TPDS.2016.2546259"",""National Science Foundation of China(grant numbers:61373115,61402356,61572398)"; China Post doctoral Science Foundation(grant numbers:2015M572565); US National Science Foundation(grant numbers:CNS 1117175,1350145);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440873"",""Real-time pricing scheme";energy-based cyber-physical system;system modeling and validation;"vulnerabilities analysis"",""Pricing";Real-time systems;Power generation;Renewable energy sources;Companies;Energy consumption;"Cyber-physical systems"","""",""35"","""",""62"",""IEEE"",""24 Mar 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"On Reliability Management of Energy-Aware Real-Time Systems Through Task Replication,""M. A. Haque"; H. Aydin;" D. Zhu"",""Department of Computer Science, George Mason University, Fairfax, VA"; Department of Computer Science, George Mason University, Fairfax, VA;" Department of Computer Science, University of Texas at San Antonio, San Antonio, TX"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""813"",""825"",""On emerging multicore systems, task replication is a powerful way to achieve high reliability targets. In this paper, we consider the problem of achieving a given reliability target for a set of periodic real-time tasks running on a multicore system with minimum energy consumption. Our framework explicitly takes into account the coverage factor of the fault detection techniques and the negative impact of Dynamic Voltage Scaling (DVS) on the rate of transient faults leading to soft errors. We characterize the subtle interplay between the processing frequency, replication level, reliability, fault coverage, and energy consumption on DVS-enabled multicore systems. We first develop static solutions and then propose dynamic adaptation schemes in order to reduce the concurrent execution of the replicas of a given task and to take advantage of early completions. Our simulation results indicate that through our algorithms, a very broad spectrum of reliability targets can be achieved with minimum energy consumption thanks to the judicious task replication and frequency assignment."",""1558-2183"","""",""10.1109/TPDS.2016.2600595"",""US National Science Foundation(grant numbers:0953005,1421855,1422709)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7544521"",""Energy-aware systems";real-time and embedded systems;reliability;multicore systems;"scheduling"",""Reliability";Transient analysis;Multicore processing;Real-time systems;Energy consumption;Voltage control;"Management"","""",""95"","""",""41"",""IEEE"",""16 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"On the Heterogeneity Bias of Cost Matrices for Assessing Scheduling Algorithms,""L. -C. Canon";" L. Philippe"",FEMTO-ST/CNRS/UBFC";" FEMTO-ST/CNRS/UBFC,""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1675"",""1688"",""Assessing the performance of scheduling heuristics through simulation requires one to generate synthetic instances of tasks and machines with well-identified properties. Carefully controlling these properties is mandatory to avoid any bias. We consider the scheduling problem consisting of allocating independent sequential tasks on unrelated machines while minimizing the maximum execution time. In this problem, the instance is a cost matrix that specifies the execution cost of any task on any machine. This article proposes two measures for quantifying the heterogeneity properties of a cost matrix. An analysis of two classical methods used in the literature reveals a bias in previous studies. We propose new methods to generate instances with given heterogeneity properties and we show that heterogeneity has a significant impact on twelve heuristics."",""1558-2183"","""",""10.1109/TPDS.2016.2629503"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7745973"",""Scheduling";cost matrix;heterogeneity;bias;parallelism;unrelated;"measure"",""Standards";Covariance matrices;Weight measurement;Noise measurement;Shape measurement;Random variables;"Correlation"","""",""3"","""",""25"",""IEEE"",""16 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"On the Implication of NTC versus Dark Silicon on Emerging Scale-Out Workloads: The Multi-Core Architecture Perspective,""J. Wang"; X. Fu; W. Zhang; J. Zhang; K. Qiu;" T. Li"",""College of Information Engineering, Capital Normal University, Beijing, China"; Department of Electrical and Computer Engineering, University of Houston, Houston, TX; Beijing Advanced Innovation Center for Imaging Technology, Capital Normal University, Beijing, China; Beijing Engineering Research Center of High Reliable Embedded System, Capital Normal University, Beijing; College of Information Engineering, Capital Normal University, Beijing, China;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2314"",""2327"",""The end of Dennard's scaling poses computer systems, especially the datacenters, in front of both power and utilization walls. One possible solution to combat the power and utilization walls is dark silicon where transistors are under-utilized in the chip, but this will result in a diminishing performance. Another solution is Near-Threshold Voltage Computing (NTC) which operates transistors in the near-threshold region and provides much more flexible tradeoffs between power and performance. However, prior efforts largely focus on a specific design option based on the legacy desktop applications, therefore, lacking comprehensive analysis of emerging scale-out applications with multiple design options when dark silicon and/or NTC are/is applied. In this paper, we characterize different perspectives including performance, energy efficiency and reliability in the context of NTC/dark silicon cloud processors running emerging scale-out workloads on various architecture designs. We find NTC is generally an effective way to alleviate the power challenge over scale-out applications compared with dark silicon, it can improve performance by 1.6X, energy efficiency by 50 percent and the reliability problem can be relieved by ECC. Meanwhile, we also observe tiled-OoO architecture improves the performance by 20~370 percent and energy efficiency by 40~600 percent over alternative architecture designs, making it a preferable design paradigm for scale-out workloads. We believe that our observations will provide insights for the design of cloud processors under dark silicon and/or NTC."",""1558-2183"","""",""10.1109/TPDS.2017.2654243"",""National Natural Science Foundation of China (NSFC)(grant numbers:61402302,61472260)"; Beijing Municipal Commission of Education(grant numbers:KM201610028016); Teacher Career Development for Universities and Colleges Universities; Beijing Municipality(grant numbers:IDHT20150507);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823012"",""Cloud processors";dark silicon;near threshold computing;performance;energy-efficiency;"reliability"",""Silicon";Multicore processing;Energy efficiency;Program processors;Reliability;"Transistors"","""",""1"","""",""36"",""IEEE"",""18 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Online QoS Prediction for Runtime Service Adaptation via Adaptive Matrix Factorization,""J. Zhu"; P. He; Z. Zheng;" M. R. Lyu"",""Shenzhen Research Institute and Department of Computer Science & Engineering, The Chinese University of Hong Kong, Sha Tin, Hong Kong"; Shenzhen Research Institute and Department of Computer Science & Engineering, The Chinese University of Hong Kong, Sha Tin, Hong Kong; School of Data and Computer Science and Guangdong Key Laboratory of Big Data Analysis and Processing, Sun Yat-sen University, Guangzhou, China;" Shenzhen Research Institute and Department of Computer Science & Engineering, The Chinese University of Hong Kong, Sha Tin, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2911"",""2924"",""Cloud applications built on service-oriented architectures generally integrate a number of component services to fulfill certain application logic. The changing cloud environment highlights the need for these applications to keep resilient against QoS variations of their component services so that end-to-end quality-of-service (QoS) can be guaranteed. Runtime service adaptation is a key technique to achieve this goal. To support timely and accurate adaptation decisions, effective and efficient QoS prediction is needed to obtain real-time QoS information of component services. However, current research has focused mostly on QoS prediction of working services that are being used by a cloud application, but little on predicting QoS values of candidate services that are equally important in determining optimal adaptation actions. In this paper, we propose an adaptive matrix factorization (namely AMF) approach to perform online QoS prediction for candidate services. AMF is inspired from the widely-used collaborative filtering techniques in recommender systems, but significantly extends the conventional matrix factorization model with new techniques of data transformation, online learning, and adaptive weights. Comprehensive experiments, as well as a case study, have been conducted based on a real-world QoS dataset of Web services (with over 40 million QoS records). The evaluation results demonstrate AMF's superiority in achieving accuracy, efficiency, and robustness, which are essential to enable optimal runtime service adaptation."",""1558-2183"","""",""10.1109/TPDS.2017.2700796"",""National Natural Science Foundation of China(grant numbers:61332010,61472338)"; National Basic Research Program of China 973 Project(grant numbers:2014CB347701); Research Grants Council of the Hong Kong Special Administrative Region(grant numbers:CUHK 415113); General Research Fund(grant numbers:2015); Microsoft Research Asia Collaborative Research Program(grant numbers:FY16-RESTHEME-005);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7918529"",""Cloud computing";runtime service adaptation;QoS prediction;online learning;"adaptive matrix factorization"",""Quality of service";Runtime;Cloud computing;Collaboration;Service-oriented architecture;"Time factors"","""",""85"","""",""54"",""IEEE"",""3 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Online Scheduling and Interference Alleviation for Low-Latency, High-Throughput Processing of Data Streams,""T. Buddhika"; R. Stern; K. Lindburg; K. Ericson;" S. Pallickara"",""Deptartment of Computer Science, Colorado State University, Fort Collins, CO"; Deptartment of Computer Science, Colorado State University, Fort Collins, CO; Deptartment of Computer Science, Colorado State University, Fort Collins, CO; Deptartment of Computer Science, Colorado State University, Fort Collins, CO;" Deptartment of Computer Science, Colorado State University, Fort Collins, CO"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3553"",""3569"",""Data Streams occur naturally in several observational settings and often need to be processed with a low latency. Streams pose unique challenges: they have no preset lifetimes, the traffic on these streams may be bursty, and data arrival rates on these streams can be quite high. Furthermore, stream processing computations are generally stateful where the outcome of processing a data stream packet depends on the state that builds up within the computation over multiple, successive rounds of execution. As the number of streams increases, stream processing computations need to be orchestrated over a collection of machines. Achieving timeliness and high throughput in such settings is a challenge. Optimal scheduling of stream processing computations is an instance of the resource constrained scheduling problem, and depending on the precise formulation of the problem can be characterized as either NP-Complete or NP-Hard. We have designed an algorithm for online scheduling of stream processing computations. Our algorithm focuses on reducing interference that adversely impacts performance of stream processing computations. Our measure of interference is based on stream packet arrivals at a particular machine, the accompanying resource utilization encompassing CPU, memory and network utilization, and the resource utilization at machines comprising the cluster. Our algorithm performs continuous, incremental detection of interference experienced by computations and performing migrations to alleviate them."",""1558-2183"","""",""10.1109/TPDS.2017.2723403"",""US National Science Foundation"; Computer Systems Research Program(grant numbers:CNS-1253908);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7968325"",""Low-latency stream processing";online scheduling;"data intensive computing"",""Interference";Resource management;Processor scheduling;Throughput;Optimal scheduling;Scheduling;"Clustering algorithms"","""",""26"","""",""44"",""IEEE"",""4 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"OpenCL-Based FPGA-Platform for Stencil Computation and Its Optimization Methodology,""H. M. Waidyasooriya"; Y. Takei; S. Tatsumi;" M. Hariyama"",""Graduate School of Information Sciences, Tohoku University, Aoba 6-6-05, Aramaki, Aoba, Sendai, Miyagi, Japan"; Graduate School of Information Sciences, Tohoku University, Aoba 6-6-05, Aramaki, Aoba, Sendai, Miyagi, Japan; Graduate School of Information Sciences, Tohoku University, Aoba 6-6-05, Aramaki, Aoba, Sendai, Miyagi, Japan;" Graduate School of Information Sciences, Tohoku University, Aoba 6-6-05, Aramaki, Aoba, Sendai, Miyagi, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1390"",""1402"",""Stencil computation is widely used in scientific computations and many accelerators based on multicore CPUs and GPUs have been proposed. Stencil computation has a small operational intensity so that a large external memory bandwidth is usually required for high performance. FPGAs have the potential to solve this problem by utilizing large internal memory efficiently. However, a very large design, testing and debugging time is required to implement an FPGA architecture successfully. To solve this problem, we propose an FPGA-platform using C-like programming language called open computing language (OpenCL). We also propose an optimization methodology to find the optimal architecture for a given application using the proposed FPFA-platform. According to the experimental results, we achieved 119 - 237 Gflop/s of processing power and higher processing speed compared to conventional GPU and multicore CPU implementations."",""1558-2183"","""",""10.1109/TPDS.2016.2614981"",""MEXT"; KAKENHI(grant numbers:15K15958);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582502"",""OpenCL for FPGA";high performance computing;stencil computation;"FDTD"",""Field programmable gate arrays";Kernel;Bandwidth;Multicore processing;Computational modeling;"Optimization"","""",""55"","""",""38"",""IEEE"",""4 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Operation-Level Wait-Free Transactional Memory with Support for Irrevocable Operations,""J. Z. Kończak"; P. T. Wojciechowski;" R. Guerraoui"",""Institute of Computing Science, Poznań, Poland"; Institute of Computing Science, Poznań, Poland;" School of Computer and Communication Sciences, EPFL, Lausanne, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3570"",""3583"",""Transactional memory (TM) aims to be a general purpose concurrency mechanism. However, operations which cause side-effects cannot be easily managed by a TM system, in which transactions are executed optimistically. In particular, networking, I/O, and some system calls cannot be executed within a transaction that may abort and restart (e.g., due to conflicts). Thus, many TM systems let transactions become irrevocable, i.e., they are guaranteed to commit. Supporting this in TM is a challenge, but there exist fast and highly parallel TM systems that allow for irrevocable transactions. However, no such system so far provides guarantees that all transactional operations terminate in a finite time. In this paper, we show that support for irrevocable operations does not entail inherent waiting. We present a TM algorithm that guarantees wait-freedom for any transactional operation. The algorithm is based on the weakest synchronization primitive possible (test-and-set), and guarantees opacity and strong progressiveness. To experimentally evaluate the algorithm, we developed a proof-of-concept TM system and tested it using the STMBench7 benchmark."",""1558-2183"","""",""10.1109/TPDS.2017.2734879"",""National Science Centre(grant numbers:DEC-2012/06/M/ST6/00463)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000384"",""Software transactional memory";irrevocable operations;"operation-level wait-freedom"",""Synchronization";Benchmark testing;Semantics;Algorithm design and analysis;"Concurrent computing"","""",""1"","""",""24"",""IEEE"",""2 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Opportunistic Mobile Data Offloading with Deadline Constraints,""G. Gao"; M. Xiao; J. Wu; K. Han; L. Huang;" Z. Zhao"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, P.R. China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, P.R. China; Department of Computer and Information Sciences, Temple University, 1805 N. Broad Street, Philadelphia, PA; School of Computer Science and Technology, University of Science and Technology of China, Hefei, P.R. China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, P.R. China;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3584"",""3599"",""Due to the explosive proliferation of mobile cloud computing applications, much data needs to be transmitted between mobile users and clouds, incurring a huge traffic demand on cellular networks. Mobile offloading is a promising approach to address this challenge. In this paper, we focus on the problem of offloading many deadline-sensitive data items to some WiFi networks with capacity constraints";" that is, how to schedule each data item to the WiFi networks, so that we can offload as many data items before their deadlines as possible, while taking the constraints of transmission capacity into consideration. This problem involves a probabilistic combination of multiple 0-1 knapsack constraints, which differs from existing problems. To solve this problem, we propose a greedy oFfline Data Offloading (FDO) algorithm, achieving an approximation ratio of 2. Also, we propose an oNline Data Offloading (NDO) algorithm, which has a competitive ratio of 2. Additionally, we extend our problem to a more general scenario where WiFi transmission costs are heterogeneous. We design a Heterogeneous Data Offloading (HDO) algorithm to solve the extended problem, and give its performance analysis. Finally, we demonstrate the significant performances of our algorithms through extensive simulations based on some real-world and synthetic WiFi datasets."",""1558-2183"","""",""10.1109/TPDS.2017.2720741"",""National Natural Science Foundation of China (NSFC)(grant numbers:61572457,61379132,U1301256,61502261,61303206,61572342)"; NSF(grant numbers:CNS 1629746,CNS 1564128,CNS 1449860,CNS 1461932,CNS 1460971,CNS 1439672,CNS 1301774,ECCS 1231461); Natural Science Foundation of Jiangsu Province(grant numbers:BK20131174,BK2009150);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7961189"",""Deadline-sensitive data offloading";mobile data offloading;"opportunistic WiFi offloading"",""Wireless fidelity";Cellular networks;Mobile computing;Cloud computing;Algorithm design and analysis;"Approximation algorithms"","""",""33"","""",""38"",""IEEE"",""28 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimal Functional-Unit Assignment for Heterogeneous Systems Under Timing Constraint,""W. Jiang"; E. H. -M. Sha; X. Chen; L. Yang; L. Zhou;" Q. Zhuge"",""College of Computer Science, Chongqing University, Chongqing, China"; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China;" College of Computer Science, Chongqing University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2567"",""2580"",""In high-level synthesis for real-time systems, it typically employs heterogeneous functional-unit types to achieve high-performance and low-cost designs. In the design phase, it is critical to determine which functional-unit type to be mapped for each operation in a given application such that the total cost is minimized while the deadline can be met. For a path or tree structured application, existing approaches can obtain the minimum-cost assignment, called “optimal assignment”, under which the resultant system satisfies a given timing constraint. However, it is still an open question whether there exist efficient algorithms to obtain the optimal assignment for the directed acyclic graph (DAG), or more generally, the data-flow graph with cycles (cyclic DFG). For DAGs, by analyzing the property of the problem, this paper designs an efficient algorithm to obtain the optimal assignments. For cyclic DFGs, we approach this problem with the combination of retiming technique to thoroughly explore the design space. We formulate a Mixed Integer Linear Programming (MILP) model to give the optimal solution. But because of the high degree of its time complexity, we devise a practical algorithm to obtain near-optimal solutions within a minute. Experimental results show the effectiveness of our algorithms. Specifically, compared with existing techniques, we can achieve 25.70 and 30.23 percent reductions in total cost on DAGs and cyclic DFGs, respectively."",""1558-2183"","""",""10.1109/TPDS.2017.2676764"",""National 863 Program(grant numbers:2015AA015304)"; NSFC(grant numbers:61472052); Chongqing Research Program(grant numbers:cstc2014yykfB40007);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867836"",""Real-time heterogeneous system";data-flow graphs;functional-unit assignment;retiming;"optimal algorithms"",""Algorithm design and analysis";Delays;Signal processing algorithms;Mixed integer linear programming;"Mathematical model"","""",""11"","""",""36"",""IEEE"",""1 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimal Symbiosis and Fair Scheduling in Shared Cache,""X. Hu"; X. Wang; Y. Li; Y. Luo; C. Ding;" Z. Wang"",""Peking University, Beijing, China"; Peking University, Beijing, China; Peking University, Beijing, China; Peking University, Beijing, China; University of Rochester, Rochester, NY;" Michigan Technological University, Houghton, MI"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1134"",""1148"",""On multi-core processors, applications are run sharing the cache. This paper presents optimization theory to co-locate applications to minimize cache interference and maximize performance. The theory precisely specifies MRC-based composition, optimization, and correctness conditions. The paper also presents a new technique called footprint symbiosis to obtain the best shared cache performance underfair CPU allocation as well as a new sampling technique which reduces the cost of locality analysis. When sampling and optimization are combined, the paper shows that it takes less than 0.1 second analysis per program to obtain a co-run that is within 1.5 percent of the best possible performance. In an exhaustive evaluation with 12,870 tests, the best prior work improves co-run performance by 56 percent on average. The new optimization improves it by another 29 percent. Without single co-run test, footprint symbiosis is able to choose co-run choices that are just 8 percent slower than the best co-run solutions found with exhaustive testing."",""1558-2183"","""",""10.1109/TPDS.2016.2611572"",""National Science Foundation of China(grant numbers:61232008,61272158,61328201,61472008,61672053)"; 863 Program of China(grant numbers:2015AA015305); National Science Foundation(grant numbers:CCF-1629376,CNS-1319617,CSR-1618384,CSR-1422342); IBM; CAS; Huawei;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572145"",""Locality";working set;cache;performance;"optimization"",""Symbiosis";Optimization;Linearity;Schedules;Aggregates;Heuristic algorithms;"Bandwidth"","""",""7"","""",""41"",""IEEE"",""20 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimistic Transactional Boosting,""A. Hassan"; R. Palmieri; S. Peluso;" B. Ravindran"",""University of Alexandria, Alexandria, Egypt"; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA; Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA;" Department of Electrical and Computer Engineering, Virginia Tech, Blacksburg, VA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3600"",""3614"",""The last two decades witnessed the success of many efficient designs of concurrent data structures. A large set of them has a common base principle: each operation is split into a read-only traversal phase, which scans the data structure without locking or monitoring, and a read-write commit phase, which atomically validates the output of the traversal phase and applies the needed modifications to the data structure. In this paper we introduce Optimistic Transactional Boosting (OTB), an optimistic methodology for extending those designs in order to support the composition of multiple operations into one atomic execution by building a single traversal phase and a single commitphase for the whole atomic execution. As a result, OTB-based data structures are optimisticand composable. The former because they defer any locking and/or monitoring to the commit phase of the entire atomic execution";" the latter because they allow the execution of multiple operations atomically. Additionally, in this paper we provide a theoretical model for analyzing OTB-based data structures and proving their correctness. In particular, we extended a recent approach that models concurrent data structures by including the two notions of optimism and composition of operations."",""1558-2183"","""",""10.1109/TPDS.2017.2725837"",""Air Force Office of Scientific Research (AFOSR)(grant numbers:FA9550-14-1-0187)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7974764"",""Composability";transactional memory;concurrent data structures;"linearizability"",""Semantics";Boosting;Data models;Monitoring;"Transaction databases"","""",""5"","""",""34"",""IEEE"",""11 Jul 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimization for Speculative Execution in Big Data Processing Clusters,""H. Xu";" W. C. Lau"",""Department of Information Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong";" Department of Information Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""530"",""545"",""A big parallel processing job can be delayed substantially as long as one of its many tasks is being assigned to an unreliable or congested machine. To tackle this so-called straggler problem, most parallel processing frameworks such as MapReduce have adopted various strategies under which the system may speculatively launch additional copies of the same task if its progress is abnormally slow when extra idling resource is available. In this paper, we focus on the design of speculative execution schemes for parallel processing clusters from an optimization perspective under different loading conditions. For the lightly loaded case, we analyze and propose one cloning scheme, namely, the Smart Cloning Algorithm (SCA) which is based on maximizing the overall system utility. We also derive the workload threshold under which SCA should be used for speculative execution. For the heavily loaded case, we propose the Enhanced Speculative Execution (ESE) algorithm which is an extension of the Microsoft Mantri scheme to mitigate stragglers. Our simulation results show SCA reduces the total job flowtime, i.e., the job delay/ response time by nearly $6$  percent comparing to the speculative execution strategy of Microsoft Mantri. In addition, we show that the ESE Algorithm outperforms the Mantri baseline scheme by $71$  percent in terms of the job flowtime while consuming the same amount of computation resource."",""1558-2183"","""",""10.1109/TPDS.2016.2564962"",""Mobile Technologies Centre The Chinese University of Hong Kong";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7466828"",""Job scheduling";speculative execution;cloning;straggler detection;"optimization"",""Cloning";Optimization;Big data;Clustering algorithms;Servers;"Job shop scheduling"","""",""21"","""",""42"",""IEEE"",""9 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimization of Duplication-Based Schedules on Network-on-Chip Based Multi-Processor System-on-Chips,""Q. Tang"; S. -F. Wu; J. -W. Shi;" J. -B. Wei"",""Department of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China"; Department of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China; Department of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China;" Department of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""826"",""837"",""Many applications such as streaming applications are both computation and communication intensive. The Multi-Processor System-on-Chip (MPSoC) based on Network-on-Chip (NoC) outperforms the multiprocessors with bus-based networking architecture in communication bandwidth and scalability, making it a better choice for implementing systems running these applications. It's important to schedule both the computation and communication onto processors and the networking architecture so as to satisfy the stringent timing requirements. To reduce or avoid inter-processor communication, task duplication has been employed in scheduling. Most of the available techniques for the duplication-based scheduling problem use heuristics to solve the problem, and seldom has any work studied further improving the schedule performance, despite the fact that the heuristic cannot provide quality guarantee. To fill in this gap, this paper introduces a duplication and mapping constrained task-communication co-scheduling problem that assumes the duplication strategy and task-to-processor mapping are known a priory, and proposes two Integer Linear Programming (ILP) formulations, i.e., CF-ILP and CA-ILP, to solve two editions of this problem, i.e., the contention-free problem and the contention-aware problem. The proposed ILP formulations optimize the ordering and timing of the communication and computation, thus improving the performance. Both synthesized and real applications are tested on a set of platforms to evaluate the performance of the proposed methods. The experimental results demonstrate the effectiveness of the proposed methods."",""1558-2183"","""",""10.1109/TPDS.2016.2599166"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7539657"",""Network-on-chip";scheduling;integer linear programming;task duplication;"communication contention"",""Schedules";Program processors;Processor scheduling;Scheduling;Timing;Computer architecture;"Scalability"","""",""16"","""",""26"",""IEEE"",""10 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimization of Fusion Kernels on Accelerators with Indirect or Strided Memory Access Patterns,""Y. Asahi"; G. Latu; T. Ina; Y. Idomura; V. Grandgirard;" X. Garbet"",""Japan Atomic Energy Agency, Kashiwa, Chiba, Japan"; CEA Cadarache, IRFM, Saint-Paul-lez-Durance, France; Japan Atomic Energy Agency, Kashiwa, Chiba, Japan; Japan Atomic Energy Agency, Kashiwa, Chiba, Japan; CEA Cadarache, IRFM, Saint-Paul-lez-Durance, France;" CEA Cadarache, IRFM, Saint-Paul-lez-Durance, France"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1974"",""1988"",""This paper describes optimization for high-dimensional stencil computations on accelerators involving complex memory access patterns, which appear in five dimensional fusion plasma turbulence codes, GYSELA and GT5D. They include different types of memory access patterns, the indirect memory access in GYSELA with a Semi-Lagrangian scheme and the strided memory access in GT5D with a Finite-Difference scheme. We focus on the affinity of the memory access patterns to accelerators such as GPGPUs and Xeon Phi coprocessors. On both devices, the Array of Structure of Array (AoSoA) data layout is preferable for contiguous memory accesses. It is shown that the effective local cache usage by improving spatial and temporal data locality is critical on Xeon Phi. On GPGPU, the texture memory usage improves the performance of the indirect memory accesses in the Semi-Lagrangian scheme. The reuse of registers by taking account of the physical symmetry of the Finite-Difference scheme reduces the amount of memory accesses. Through these optimizations, we achieve acceleration of 3.9 (8.1) on Xeon Phi (GPGPU) for the Semi-Lagrangian scheme and of 1.4 (3.9) on Xeon Phi (GPGPU) for the Finite-Different scheme with respect to the fully optimized codes on Sandy Bridge."",""1558-2183"","""",""10.1109/TPDS.2016.2633349"",""MEXT"; Development of Innovative Clean Energy(grant numbers:22686086);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762180"",""GPU";Xeon Phi;semi-lagrangian;"finite-difference"",""Finite difference methods";Kernel;Instruction sets;Optimization;Plasma accelerators;"Coprocessors"","""",""10"","""",""41"",""IEEE"",""29 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Cloud-Service Performance: Efficient Resource Provisioning via Optimal Workload Allocation,""Z. Wang"; M. M. Hayat; N. Ghani;" K. B. Shaban"",""Department of Electrical and Computer Engineering"; Department of Electrical and Computer Engineering; Department of Electrical Engineering, University of South Florida, Tampa, FL;" Computer Science and Engineering Department, Qatar University, Doha, Qatar"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1689"",""1702"",""Cloud computing is being widely accepted and utilized in the business world. From the perspective of businesses utilizing the cloud, it is critical to meet their customers' requirements by achieving service-level-objectives. Hence, the ability to accurately characterize and optimize cloud-service performance is of great importance. In this paper a stochastic multi-tenant framework is proposed to model the service of customer requests in a cloud infrastructure composed of heterogeneous virtual machines. Two cloud-service performance metrics are mathematically characterized, namely the percentile and the mean of the stochastic response time of a customer request, in closed form. Based upon the proposed multi-tenant framework, a workload allocation algorithm, termed maxmin-cloud algorithm, is then devised to optimize the performance of the cloud service. A rigorous optimality proof of the max-min-cloud algorithm is also given. Furthermore, the resource-provisioning problem in the cloud is also studied in light of the max-min-cloud algorithm. In particular, an efficient resource-provisioning strategy is proposed for serving dynamically arriving customer requests. These findings can be used by businesses to build a better understanding of how much virtual resource in the cloud they may need to meet customers' expectations subject to cost constraints."",""1558-2183"","""",""10.1109/TPDS.2016.2628370"",""NPRP(grant numbers:5-137-2-045)"; Qatar National Research Fund; Qatar Foundation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7742971"",""Cloud computing";heterogeneous computing;multi-tenant model;performance analysis;workload allocation;"resource provisioning"",""Cloud computing";Time factors;Computational modeling;Analytical models;Resource management;"Stochastic processes"","""",""42"","""",""37"",""IEEE"",""14 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing End-to-End Big Data Transfers over Terabits Network Infrastructure,""Y. Kim"; S. Atchley; G. R. Vallée; S. Lee;" G. M. Shipman"",""Department of Computer Science and Engineering, Sogang University, Seoul, South Korea"; Oak Ridge National Laboratory, Oak Ridge, TN; Oak Ridge National Laboratory, Oak Ridge, TN; Oak Ridge National Laboratory, Oak Ridge, TN;" Los Alamos National Laboratory, Los Alamos, NM"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""188"",""201"",""While future terabit networks hold the promise of significantly improving big-data motion among geographically distributed data centers, significant challenges must be overcome even on today's 100 gigabit networks to realize end-to-end performance. Multiple bottlenecks exist along the end-to-end path from source to sink, for instance, the data storage infrastructure at both the source and sink and its interplay with the wide-area network are increasingly the bottleneck to achieving high performance. In this paper, we identify the issues that lead to congestion on the path of an end-to-end data transfer in the terabit network environment, and we present a new bulk data movement framework for terabit networks, called LADS. LADS exploits the underlying storage layout at each endpoint to maximize throughput without negatively impacting the performance of shared storage resources for other users. LADS also uses the Common Communication Interface (CCI) in lieu of the sockets interface to benefit from hardware-level zero-copy, and operating system bypass capabilities when available. It can further improve data transfer performance under congestion on the end systems using buffering at the source using flash storage. With our evaluations, we show that LADS can avoid congested storage elements within the shared storage resource, improving input/output bandwidth, and data transfer rates across the high speed networks. We also investigate the performance degradation problems of LADS due to I/O contention on the parallel file system (PFS), when multiple LADS tools share the PFS. We design and evaluate a meta-scheduler to coordinate multiple I/O streams while sharing the PFS, to minimize the I/O contention on the PFS. With our evaluations, we observe that LADS with meta-scheduling can further improve the performance by up to 14 percent relative to LADS without meta-scheduling."",""1558-2183"","""",""10.1109/TPDS.2016.2550439"",""Institute for Information & communications Technology Promotion"; Korea Government; MSIP(grant numbers:R0190-15-2012); National Research Foundation of Korea; Korea Government; MISP(grant numbers:2015R1C1A1A0152105); National Center for Computational Sciences; DOE(grant numbers:DE-AC05-00OR22725);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7447811"",""File and storage systems";parallel file sysetms;networks;"I/O scheduling"",""Data transfer";Servers;Throughput;Data models;Instruction sets;Big data;"Computational modeling"","""",""15"","""",""27"",""IEEE"",""5 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Extended Hodgkin-Huxley Neuron Model Simulations for a Xeon/Xeon Phi Node,""G. Chatzikonstantis"; D. Rodopoulos; C. Strydis; C. I. De Zeeuw;" D. Soudris"",""Department of Electrical and Computer Engineering (ECE), National Technical University of Athens (NTUA), Zografou, Greece"; IMEC, Leuven, Heverlee, Belgium; Erasmus Medical Center Rotterdam (EMC), Rotterdam, CE, Netherlands; Erasmus Medical Center Rotterdam (EMC), Rotterdam, CE, Netherlands;" Department of Electrical and Computer Engineering (ECE), National Technical University of Athens (NTUA), Zografou, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2581"",""2594"",""Brain modeling has been receiving significant attention over the years, both for its neuroscientific potential and for its challenges in the context of high-performance computing. The development of physiologically plausible neuron models comes at the cost of increased complexity. In this work, we have selected a highly computationally demanding model of the Inferior-Olivary Nucleus (InfOli) based on the Hodgkin-Huxley (HH) neuron model. This brain region, functionally coupled with the cerebellum, is of vital importance for motor skills and time-sensitive cognitive functions. The computing fabric of choice is an Intel Xeon/Xeon Phi system, which is a typical node of modern computing infrastructure. The target application is parallelized with various combinations of MPI and OpenMP and performance is measured on the target platform. The different implementations are compared and the best one is chosen. Further optimization of this implementation is presented in detail. Its behaviour is then examined when scaling up to neuron populations representative of realistic, human Inferior-Olivary neuronal networks. The evaluation's results highlight the importance of examining a network's size and density before choosing the best platform for its simulation. All the parallelization and vectorization options presented in the current paper are available on a public repository for further examination."",""1558-2183"","""",""10.1109/TPDS.2017.2686389"",""European Commission(grant numbers:H2020-687628)"; STFC;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7884986"",""MPI";neuron modeling;OpenMP;Xeon Phi;performance;"vectorization"",""Neurons";Computational modeling;Brain modeling;Biological system modeling;Graphics processing units;Fabrics;"Mathematical model"","""",""6"","""",""51"",""IEEE"",""22 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Graph Processing on GPUs,""W. Zhong"; J. Sun; H. Chen; J. Xiao; Z. Chen; C. Cheng;" X. Shi"",""College of Computer Science and Electronic Engineering, Hunan University, China"; College of Computer Science and Electronic Engineering, Hunan University, China; College of Computer Science and Electronic Engineering, Hunan University, China; College of Computer Science and Electronic Engineering, Hunan University, China; College of Computer Science and Electronic Engineering, Hunan University, China; College of Computer Science and Electronic Engineering, Hunan University, China;" School of Computers, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1149"",""1162"",""Distributed vertex-centric model has been recently proposed for large-scale graph processing. Due to the simple but efficient programming abstraction, similar graph computing frameworks based on GPUs are gaining more and more attention. However, prior works of GPU-based graph processing suffer from load imbalance and irregular memory access because of the inherent characteristics of graph applications. In this paper, we propose a generalized graph computing framework for GPUs to simplify existing models but with higher performance. In particular, two novel algorithmic optimizations, lightweight approximate sorting and data layout transformation, are proposed to tackle the performance issues of current systems. With extensive experimental evaluation under a wide range of real world and synthetic workloads, we show that our system can achieve 1.6× to 4.5× speedups over the state-of-the-art."",""1558-2183"","""",""10.1109/TPDS.2016.2611659"",""National Science Foundation of China(grant numbers:61272190,61572179,61173166)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7572196"",""GPGPU";graph computing;pregel;bulk synchronous model;"load imbalance"",""Graphics processing units";Computational modeling;Programming;Message systems;Load modeling;Optimization;"Parallel processing"","""",""17"","""",""37"",""IEEE"",""20 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Parallel Clustering Throughput in Shared Memory,""M. Gowanlock"; D. M. Blair;" V. Pankratius"",""Massachusetts Institute of Technology, Haystack Observatory, Westford, MA"; Massachusetts Institute of Technology, Haystack Observatory, Westford, MA;" Massachusetts Institute of Technology, Haystack Observatory, Westford, MA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2595"",""2607"",""This article studies the optimization of parallel clustering throughput in the context of variant-based parallelism, which exploits commonalities and reuse among variant computations for multithreading scalability. This direction is motivated by challenging scientific applications where scientists have to execute multiple runs of clustering algorithms with different parameters to determine which ones best explain phenomena observed in empirical data. To make this process more efficient, we propose a novel set of optimizations to maximize the throughput of Density-Based Spatial Clustering of Applications with Noise (DBSCAN), a frequently used algorithm for scientific data mining in astronomy, geoscience, and many other fields. Our approach executes multiple algorithm variants in parallel, computes clusters concurrently, and leverages heuristics to maximize the reuse of results from completed variants. As scientific datasets continue to grow, maximizing clustering throughput with our techniques may accelerate the search and identification of natural phenomena of interest with computational support, i.e., Computer-Aided Discovery. We present evaluations on a whole spectrum of datasets, such as geoscience data on space weather phenomena, astronomical data from the Sloan Digital Sky Survey on intermediate-redshift galaxies, as well as synthetic datasets to characterize performance properties. Selected results show a 1,115 percent performance improvement due to indexing tailored for variant-based clustering, and a 2,209 percent performance improvement when applying all of our proposed optimizations."",""1558-2183"","""",""10.1109/TPDS.2017.2675421"",""US National Science Foundation(grant numbers:ACI-1442997)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7865993"",""Parallel clustering";clustering throughput;DBSCAN;data mining;"computer-aided discovery"",""Clustering algorithms";Throughput;Parallel processing;Optimization;Meteorology;"Indexing"","""",""9"","""",""32"",""IEEE"",""1 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallel Continuous Preference Queries over Out-of-Order and Bursty Data Streams,""G. Mencagli"; M. Torquati; M. Danelutto;" T. De Matteis"",""Department of Computer Science, University of Pisa, Pisa, Italy"; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Computer Science, University of Pisa, Pisa, Italy;" Department of Computer Science, University of Pisa, Pisa, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2608"",""2624"",""Techniques to handle traffic bursts and out-of-order arrivals are of paramount importance to provide real-time sensor data analytics in domains like traffic surveillance, transportation management, healthcare and security applications. In these systems the amount of raw data coming from sensors must be analyzed by continuous queries that extract value-added information used to make informed decisions in real-time. To perform this task with timing constraints, parallelism must be exploited in the query execution in order to enable the real-time processing on parallel architectures. In this paper we focus on continuous preference queries, a representative class of continuous queries for decision making, and we propose a parallel query model targeting the efficient processing over out-of-order and bursty data streams. We study how to integrate punctuation mechanisms in order to enable out-of-order processing. Then, we present advanced scheduling strategies targeting scenarios with different burstiness levels, parameterized using the index of dispersion quantity. Extensive experiments have been performed using synthetic datasets and real-world data streams obtained from an existing real-time locating system. The experimental evaluation demonstrates the efficiency of our parallel solution and its effectiveness in handling the out-of-orderness degrees and burstiness levels of real-world applications."",""1558-2183"","""",""10.1109/TPDS.2017.2679197"",""EU(grant numbers:H2020-ICT-2014-1,644235)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7873332"",""Parallelism";multicores;data streams;continuous preference queries;sliding windows;out-of-order arrivals;"burstiness and traffic surges"",""Out of order";Real-time systems;Data models;Parallel processing;Computational modeling;"Multicore processing"","""",""11"","""",""36"",""IEEE"",""7 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallel Deep Neural Network Training for Big Data on Blue Gene/Q,""I. -H. Chung"; T. N. Sainath; B. Ramabhadran; M. Picheny; J. Gunnels; V. Austel; U. Chauhari;" B. Kingsbury"",""IBM Thomas J. Watson Research Center, Yorktown Heights, NY"; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Thomas J. Watson Research Center, Yorktown Heights, NY;" IBM Thomas J. Watson Research Center, Yorktown Heights, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1703"",""1714"",""Deep Neural Networks (DNNs) have recently been shown to significantly outperform existing machine learning techniques in several pattern recognition tasks. DNNs are the state-of-the-art models used in image recognition, object detection, classification and tracking, and speech and language processing applications. The biggest drawback to DNNs has been the enormous cost in computation and time taken to train the parameters of the networks-often a tenfold increase relative to conventional technologies. Such training time costs can be mitigated by the application of parallel computing algorithms and architectures. However, these algorithms often run into difficulties because of the cost of inter-processor communication bottlenecks. In this paper, we describe how to enable Parallel Deep Neural Network Training on the IBM Blue Gene/Q (BG/Q) computer system. Specifically, we explore DNN training using the data-parallel Hessian-free 2nd order optimization algorithm. Such an algorithm is particularly well-suited to parallelization across a large set of loosely coupled processors. BG/Q, with its excellent inter-processor communication characteristics, is an ideal match for this type of algorithm. The paper discusses how issues regarding programming model and data-dependent imbalances are addressed. Results on large-scale speech tasks show that the performance on BG/Q scales linearly up to 4,096 processes with no loss in accuracy. This allows us to train neural networks using billions of training examples in a few hours."",""1558-2183"","""",""10.1109/TPDS.2016.2626289"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7738586"",""Big data";speech recognition;"high performance computing"",""Training";Optimization;Neural networks;Speech;Speech recognition;"Multicore processing"","""",""24"","""",""37"",""IEEE"",""8 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallel Graph Partitioning for Complex Networks,""H. Meyerhenke"; P. Sanders;" C. Schulz"",""Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany"; Karlsruhe Institute of Technology (KIT), Karlsruhe, Germany;" University of Vienna, Vienna, Austria"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2625"",""2638"",""Processing large complex networks like social networks or web graphs has attracted considerable interest. To do this in parallel, we need to partition them into pieces of roughly equal size. Unfortunately, previous parallel graph partitioners originally developed for more regular mesh-like networks do not work well for complex networks. Here we address this problem by parallelizing and adapting the label propagation technique originally developed for graph clustering. By introducing size constraints, label propagation becomes applicable for both the coarsening and the refinement phase of multilevel graph partitioning. This way we exploit the hierarchical cluster structure present in many complex networks. We obtain very high quality by applying a highly parallel evolutionary algorithm to the coarsest graph. The resulting system is both more scalable and achieves higher quality than state-of-theart systems like ParMetis or PT-Scotch. For large complex networks the performance differences are very big. As an example, our algorithm partitions a web graph with 3.3 G edges in 16 seconds using 512 cores of a high-performance cluster while producing a high quality partition-none of the competing systems can handle this graph on our system."",""1558-2183"","""",""10.1109/TPDS.2017.2671868"",""German Research Foundation (DFG)(grant numbers:TEAM (ME 3619/2-1))";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859409"",""Load balancing and task assignment";graph algorithms;combinatorial algorithms;"algorithm design and analysis"",""Partitioning algorithms";Clustering algorithms;Complex networks;Evolutionary computation;Social network services;Sociology;"Statistics"","""",""87"","""",""43"",""IEEE"",""20 Feb 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallel Nonuniform Discrete Fourier Transform (P-NDFT) Over a Random Wireless Sensor Network,""X. Xu"; R. Ansari;" A. Khokhar"",""Department of Electrical and Computer Engineering, University of Illinois at Chicago, Champaign, IL"; Department of Electrical and Computer Engineering, University of Illinois at Chicago, Champaign, IL;" Department of Electrical and Computer Engineering, Iowa State University, Ames, IA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3615"",""3625"",""Reduced execution time and increased power efficiency are important objectives in the distributed execution of collaborative signal processing tasks over wireless sensor networks (WSNs). Meanwhile, Fourier transforms are among the most widely used frequency analysis tools in WSNs for studying the behavior of sensed phenomena. Several energy-efficient in-network Fourier transform computation algorithms have been proposed for WSNs. Most of these works assume that the sensors are equally spaced over a one-dimensional (1D) region. However, in practice, the sensors are usually randomly distributed over a two-dimensional (2D) plane. Consequently, the conventional 2D Fast Fourier Transform (FFT) designed for data sampled on a uniform grid is not applicable in such environments. We address this problem by designing a distributed hybrid structure consisting of local Non-equispaced Discrete Fourier Transform (NDFT) and global FFT computations. First, the NDFT method is applied within suitably selected clusters to obtain the initial uniform Fourier coefficients within allowable estimation error bounds. We investigate both classical linear and generalized interpolation methods for computing the NDFT coefficients within each cluster. Second, a separable 2D FFT is applied over all clusters using our proposed energy-efficient 1D FFT computation method, which reduces communication costs by employing a novel binary representation mapping strategy for data exchanges between sensors. The proposed techniques are implemented on the SIDnet-SWANS platform, and the tradeoffs between communication cost, execution time, and energy consumption are studied."",""1558-2183"","""",""10.1109/TPDS.2017.2706690"",""National Science Foundation(grant numbers:CNS 0910988)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7932188"",""Nonuniform FFT";distributed computing;energy efficiency;"sensor network"",""Sensors";Wireless sensor networks;Discrete Fourier transforms;Algorithm design and analysis;Two dimensional displays;Interpolation;"Data communication"","""",""3"","""",""20"",""IEEE"",""19 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallelizing Exact and Approximate String Matching via Inclusive Scan on a GPU,""Y. Mitani"; F. Ino;" K. Hagihara"",""Development Head Office, DWANGO Corporation, Ltd., Tokyo, Japan"; Graduate School of Information Science and Technology, Osaka University, Osaka, Japan;" Graduate School of Information Science and Technology, Osaka University, Osaka, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""1989"",""2002"",""In this study, to substantially improve the runtimes of exact and approximate string matching algorithms, we propose a tribrid parallel method for bit-parallel algorithms such as the Shift-Or and Wu-Manber algorithms. Our underlying idea is to interpret bit-parallel algorithms as inclusive-scan operations, which allow these bit-parallel algorithms to run efficiently on a graphics processing unit (GPU)";" we achieve this speed-up here because inclusive-scan operations not only eliminate duplicate searches between threads but also realize a GPU-friendly memory access pattern that maximizes memory read/write throughput. To realize our ideas, we first define two binary operators and then present a proof regarding the associativity of these operators, which is necessary for the parallelization of the inclusive-scan operations. Finally, we integrate the inclusive-scan scheme into a previous segmentation-based scheme to maximize search throughput, identifying the best tradeoff point between synchronization cost and duplicate work. Through our experiments, we compared our proposed method with previous segmentation-based methods and indexing-based sequence aligners. For online string matching, our proposed method performed 6.7-16.7 times faster than previous methods, achieving a search throughput of up to 1.88 terabits per second (Tbps) on a GeForce GTX TITAN X GPU. We therefore conclude that our proposed method is quite effective for decreasing the runtimes of online string matching of short patterns."",""1558-2183"","""",""10.1109/TPDS.2016.2645222"",""Japan Society for the Promotion of Science(grant numbers:15K12008,15H01687,16H02801)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7797444"",""String matching";bit-parallel algorithm;inclusive scan;shift-or algorithm;Wu-Manber algorithm;"GPU"",""Graphics processing units";Approximation algorithms;Throughput;Parallel processing;Instruction sets;Pattern matching;"Automata"","""",""19"","""",""43"",""IEEE"",""26 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Path Setup for Hybrid NoC Architectures Exploiting Flooding and Standby,""E. Fusella"; J. Flich;" A. Cilardo"",""Department of Electrical Engineering and Information Technologies, University of Naples Federico II, Naples, Italy"; Department of Computer Engineering (DISCA), Universitat Politècnica de València, Valencia, Spain;" Department of Electrical Engineering and Information Technologies, University of Naples Federico II, Naples, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1403"",""1416"",""Future many-core systems will require energy-efficient, high-throughput and low-latency communication architectures. Silicon Photonics appears today a promising solution towards these goals. The inability of photonics networks to perform inflight buffering and logic computation suggests the use of hybrid photonic-electronic architectures. In order to exploit the full potential of photonics, it is essential to carefully design the path-setup architecture, which is a primary source of performance degradation and power consumption. In this paper, we propose a new path-setup approach which can put allocated circuits in a stand-by state, rapidly restoring them when needed. Path-setup messages are sent using a flooding routing strategy to enhance the possibility of finding free optical paths. We compare the proposed approach with a commonly used path-setup strategy as well as some other alternatives available. The results exhibit encouraging improvements in terms of both performance and energy consumption."",""1558-2183"","""",""10.1109/TPDS.2016.2622265"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7723923"",""Silicon photonics";path-setup;hybrid photonic-electronic networks-on-chip;parallel architectures;"on-chip interconnection networks"",""Photonics";Optical packet switching;Optical buffering;Protocols;Optical ring resonators;Optical fiber networks;"Routing"","""",""7"","""",""44"",""IEEE"",""27 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Path-Diversity-Aware Fault-Tolerant Routing Algorithm for Network-on-Chip Systems,""Y. -Y. Chen"; E. -J. Chang; H. -K. Hsin; K. -C. Chen;" A. -Y. Wu"",""Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan"; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan; Computer Science and Engineering Department, National Sun Yat-Sen University, Kaohsiung City, Taiwan;" Graduate Institute of Electronics Engineering, National Taiwan University, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""838"",""849"",""Network-on-Chip (NoC) is the regular and scalable design architecture for chip multiprocessor (CMP) systems. With the increasing number of cores and the scaling of network in deep submicron (DSM) technology, the NoC systems become subject to manufacturing defects and have low production yield. Due to the fault issues, the reduction in the number of available routing paths for packet delivery may cause severe traffic congestion and even to a system crash. Therefore, the fault-tolerant routing algorithm is desired to maintain the correctness of system functionality. To overcome fault problems, conventional fault-tolerant routing algorithms employ fault information and buffer occupancy information of the local regions. However, the information only provides a limited view of traffic in the network, which still results in heavy traffic congestion. To achieve fault-resilient packet delivery and traffic balancing, this work proposes a Path-Diversity-Aware Fault-Tolerant Routing (PDA-FTR) algorithm, which simultaneously considers path diversity information and buffer information. Compared with other fault-tolerant routing algorithms, the proposed work can improve average saturation throughput by 175 percent with only 8.9 percent average area overhead and 7.1 percent average power overhead."",""1558-2183"","""",""10.1109/TPDS.2016.2588482"",""National Science Council of Taiwan(grant numbers:MOST 102-2220-E-002-001,MOST 104-2220-E-002-003,MOST 104-2218-E-110-011-MY2)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7506215"",""Network-on-Chip (NoC)";fault-tolerant adaptive routing;selection strategy;"path diversity"",""Routing";Fault tolerance;Fault tolerant systems;Algorithm design and analysis;Handheld computers;"Topology"","""",""43"","""",""28"",""IEEE"",""7 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Patron Allocation for Group Services Under Lower Bound Constraints,""H. -J. Hong"; G. -M. Chiu; S. -y. Wu; T. -R. Hsiang;" T. -L. Chin"",""Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan"; Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan; Department of Computer Science and Information Engineering, National Dong Hwa University, Hualien, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan;" Department of Computer Science and Information Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""850"",""862"",""Group services are highly important for a variety of computing application domains. In this paper, we study the fundamental problem of allocating a set of service patrons to a set of service groups in an attempt to maximize the total profit gained by the grouping platform. The problem under consideration is unique in that group service is not provided at all unless its lower bound requirement is satisfied. In addition, we allow each service patron to join multiple groups. In this paper, after proving the hardness property of the problem, we focus first on a special case of the problem. To this end, we propose two approaches. One aims at providing a suboptimal solution using a 1/2-approximation algorithm. The other approach turns to seeking an optimal solution using a branch and bound technique. For this purpose, we introduce a theorem that captures a useful property of an optimal allocation. Based on this theorem, we design an efficient branch and bound algorithm to find an optimal solution. We then extend these methods to solve the general problem. Extensive experiments show that our branch and bound algorithm is able to obtain an optimal solution with a small amount of computation time in many different settings."",""1558-2183"","""",""10.1109/TPDS.2016.2597840"",""Ministry of Science and Technology, Taiwan(grant numbers:MOST 102-2218-E-011-008,103-2221-E-011-093,103-2221-E-259-017,103-2221-E-011-094,103-2221-E-011-095,104-2221-E-011-019,104-2221-E-259-025-MY2,104-2221-E-011-041)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7530888"",""Approximation algorithm";branch and bound;group service;lower bound constraint;"patron allocation"",""Resource management";Dispatching;Crowdsourcing;Linear programming;Seminars;Algorithm design and analysis;"Footwear"","""","""","""",""22"",""IEEE"",""3 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Patterns for Distributed Real-Time Stream Processing,""P. Basanta-Val"; N. Fernández-García; L. Sánchez-Fernández;" J. Arias-Fisteus"",""University Carlos III of Madrid, Leganes, Madrid, Spain"; Universidade de Vigo, Marin, Pontevedra, Spain; University Carlos III of Madrid, Leganes, Madrid, Spain;" University Carlos III of Madrid, Leganes, Madrid, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3243"",""3257"",""In recent years, big data systems have become an active area of research and development. Stream processing is one of the potential application scenarios of big data systems where the goal is to process a continuous, high velocity flow of information items. High frequency trading (HFT) in stock markets or trending topic detection in Twitter are some examples of stream processing applications. In some cases (like, for instance, in HFT), these applications have end-to-end quality-of-service requirements and may benefit from the usage of real-time techniques. Taking this into account, the present article analyzes, from the point of view of real-time systems, a set of patterns that can be used when implementing a stream processing application. For each pattern, we discuss its advantages and disadvantages, as well as its impact in application performance, measured as response time, maximum input frequency and changes in utilization demands due to the pattern."",""1558-2183"","""",""10.1109/TPDS.2017.2716929"",""Java Infrastructure for Real-Time Big Data(grant numbers:CAS14/00118)"; eMadrid(grant numbers:S2013/ICE-2715); HERMES-SMARTDRIVER(grant numbers:TIN2013-46801-C4-2-R); AUDACity(grant numbers:TIN2016-77158-C4-1-R); European Union's 7th Framework Program(grant numbers:FP7-IC6-318763);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7953583"",""Real-time patterns";stream processing;"big data"",""Real-time systems";Computational modeling;Storms;Big Data applications;Twitter;"Time factors"","""",""19"","""",""63"",""IEEE"",""19 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PDFS: Partially Dedupped File System for Primary Workloads,""H. Yu"; X. Zhang; W. Huang;" W. Zheng"",""Computer Science Department, Tsinghua University, Beijing, China"; Facebook Inc., Los Angeles, CA; Computer Science Department, Tsinghua University, Beijing, China;" Computer Science Department, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""863"",""876"",""Primary storage dedup is difficult to be accomplished because of challenges to achieve low IO latency and high throughput while eliminating data redundancy effectively in the critical IO Path. In this paper, we design and implement the PDFS, a partially dedupped file system for primary workloads, which is built on a generalized framework using partial data lookup for efficient searching of redundant data in quickly chosen data subsets instead of the whole data. PDFS improves IO latency and throughput systematically by techniques including write path optimization, data dedup parallelization and write order preserving. Such design choices bring dedup to the masses for general primary workloads. Experimental results show that PDFS achieves 74-99 percent of the theoretical maximum dedup ratio with very small or even negative performance degradations compared with main stream file systems without dedup support. Discussions about varied configuring experiences of PDFS are also carried out."",""1558-2183"","""",""10.1109/TPDS.2016.2594070"",""HTRDP(grant numbers:2012AA012602)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7522088"",""Partially dedup";file system;"primary workloads"",""Throughput";Metadata;Probability density function;Indexes;Redundancy;Memory management;"Optimization"","""",""6"","""",""46"",""IEEE"",""26 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Perfect Hashing Based Parallel Algorithms for Multiple String Matching on Graphic Processing Units,""C. -H. Lin"; J. -C. Li; C. -H. Liu;" S. -C. Chang"",""Department of Electrical Engineering, National Taiwan Normal University, Taipei, Taiwan"; Department of Computer Science, National Tsing-Hua University, Hsinchu, Taiwan; Department of Computer Science, National Tsing-Hua University, Hsinchu, Taiwan;" Department of Computer Science, National Tsing-Hua University, Hsinchu, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2639"",""2650"",""Multiple string matching has a wide range of applications such as network intrusion detection systems, spam filters, information retrieval systems, and bioinformatics. To accelerate multiple string matching, many hardware approaches are proposed to accelerate string matching. Among the hardware approaches, memory architectures have been widely adopted because of their flexibility and scalability. A conventional memory architecture compiles multiple string patterns into a state machine and performs string matching by traversing the corresponding state transition table. Due to the ever-increasing number of attack patterns, the memory used for storing the state transition table increased tremendously. Therefore, memory reduction has become a crucial issue in optimizing memory architectures. In this paper, we propose two parallel string matching algorithms which adopt perfect hashing to compact a state transition table. Different from most state-of-the-art approaches implemented on specific hardware such as TCAM, FPGA, or ASIC, our proposed approaches are easily implemented on commodity DRAM and extremely suitable to be implemented on GPUs. The proposed algorithms reduce up to 99.5 percent memory requirements for storing the state transition table compared to the traditional two-dimensional memory architecture. By studying existing approaches, our results obtain significant improvements in memory efficiency."",""1558-2183"","""",""10.1109/TPDS.2017.2674664"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7864442"",""Perfect hashing";string matching;"deterministic finite automaton"",""Memory architecture";Memory management;Automata;Algorithm design and analysis;Pattern matching;Hardware;"Indexes"","""",""18"","""",""22"",""IEEE"",""24 Feb 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Performance Characterization on Handling Large-Scale Partitionable Workloads on Heterogeneous Networked Compute Platforms,""X. Wang";" B. Veeravalli"",""School of Computer Science and Technology, Xidian University, Xi’an, China";" Department of Electrical and Computer Engineering, National University of Singapore, 4 Engineering Drive 3, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2925"",""2938"",""Multi-installment scheduling (MIS) has shown great effectiveness in minimizing the processing time for large-scale partitionable workloads. To derive an optimal MIS strategy, one has to explicitly determine optimal numbers of installments and processors. Existing studies tend to solve this problem by treating the influence of number of installments (and processors) w.r.t processing time as time-continuous functions and taking the derivative of these functions to determine the optimal values, which may lead to invalid solutions. In this paper, we employ periodic multi-installment scheduling (P-MIS) models for homogeneous and heterogeneous single-level tree networks. Using these models we make the following significant contributions. First, we derive a closed-form solution for an optimal number of installments based on a given network size and a fixed load distribution sequence. Second, we propose a heuristic algorithm for determining an optimal number of processors by first proving several important intermediate lemmas and theorems. Third, for heterogeneous systems, we propose a genetic algorithm to determine an optimal load distribution sequence. Finally, we conduct various experiments to illustrate the effectiveness of the proposed algorithms and perform rigorous analysis on the influence of load distribution sequence on processing time, on the basis of which a practical advice for determining a near-optimal load distribution sequence is given."",""1558-2183"","""",""10.1109/TPDS.2017.2693149"",""Department of Electrical and Computer Engineering"; National University of Singapore; School of Computer Science and Technology; Xidian University; National Natural Science Foundation of China(grant numbers:61402350,61472297,61572391); China Scholarship Council;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7896640"",""Divisible load";multi-installment scheduling;load distribution;processing time;"genetic algorithm"",""Program processors";Processor scheduling;Load modeling;Optimal scheduling;Silicon;Genetic algorithms;"Computational modeling"","""",""12"","""",""21"",""IEEE"",""12 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PIC: Enable Large-Scale Privacy Preserving Content-Based Image Search on Cloud,""L. Zhang"; T. Jung; K. Liu; X. -Y. Li; X. Ding; J. Gu;" Y. Liu"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"; Department of Computer Science, Illinoise Institute of Technology, Chicago, IL; School of Software,Tsinghua University, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Software,Tsinghua University, Beijing, China; School of Computer Science and Technology, Northwestern Polytechnical University, Xi’an, China;" School of Software,Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3258"",""3271"",""Many cloud platforms emerge to meet urgent requirements for large-volume personal image store, sharing and search. Though most would agree that images contain rich sensitive information (e.g., people, location and event) and people's privacy concerns hinder their participation into untrusted services, today's cloud platforms provide little support for image privacy protection. Facing large-scale images from multiple users, it is extremely challenging for the cloud to maintain the index structure and schedule parallel computation without learning anything about the image content and indices. In this work, we introduce a novel system PIC: A Privacy-preserving Image search system on Cloud, which is a step towards feasible cloud services which provide secure content-based large-scale image search with fine-grained access control. Users can search on others' images if they are authorized by the image owners. Majority of the computationally intensive jobs are handled by the cloud, and a querier can now simply send the query and receive the result. Specially, to deal with massive images, we design our system suitable for distributed and parallel computation and introduce several optimizations to further expedite the search process. Our security analysis and prototype system evaluation results show that PIC successfully protects the image privacy at a low cost of computation and communication."",""1558-2183"","""",""10.1109/TPDS.2017.2712148"",""NSF(grant numbers:61572281,61472218,61502271)"; China National Funds for Distinguished Young Scientists(grant numbers:61625205); CAS(grant numbers:QYZDY-SSW-JSC002); NSFC(grant numbers:61520106007); NSF(grant numbers:ECCS-1247944,NSF CMMI 1436786,NSF CNS 1526638);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7938682"",""Large-scale image search";privacy protection;"Map-Reduce"",""Privacy";Encryption;Feature extraction;Indexing;"Image reconstruction"","""",""65"","""",""41"",""IEEE"",""5 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Piccolo: A Fast and Efficient Rollback System for Virtual Machine Clusters,""L. Cui"; Z. Hao; Y. Peng;" X. Yun"",""Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China;" Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2328"",""2341"",""Rollback is an effective technique to resume the system execution from a recorded intermediate state upon failures, without having to restart the entire system. However, in virtualized environments, rollback of a virtual machine cluster (VMC) produces high network traffic and long service disruption, particularly for a large cluster used for scientific computing, thereby imposing significant overhead both on network and applications. This paper proposes Piccolo, a fast and efficient rollback system, to restore a VMC from snapshot files over data center network. First, we exploit the similarity among VMC snapshots and leverage multicast to deliver the identical pages across VMs placed on disperse hosts, thereby bypassing unnecessary transmission of a large number of pages. Second, we analyze the impact on network traffic of varying VM placements in data center network, formulate the traffic aware placement as an optimization problem, and design a two-tier approximation algorithm that efficiently solves the problem. In addition to presenting Piccolo, we detail its implementation, and evaluate it by a set of experiments. The results show that Piccolo could achieve a significant reduction in terms of total sent data, network traffic and rollback latency compared to the existing generic techniques."",""1558-2183"","""",""10.1109/TPDS.2017.2668403"",""National Key Technology(grant numbers:2012BAH46B02)"; National Natural Science Foundation of China(grant numbers:61602465);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7852513"",""Rollback";virtual machine cluster;multicast;"virtual machine placement"",""Switches";Servers;Virtual machining;Algorithm design and analysis;Noise measurement;Optimization;"Distributed databases"","""",""10"","""",""58"",""IEEE"",""13 Feb 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PLAN: Joint Policy- and Network-Aware VM Management for Cloud Data Centers,""L. Cui"; F. P. Tso; D. P. Pezaros; W. Jia;" W. Zhao"",""Department of Computer Science, Jinan University, Guangzhou, China"; Department of Computer Science, Liverpool John Moores University, Liverpool, United Kingdom; School of Computing Science, University of Glasgow, Glasgow, United Kingdom; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China;" Department of Computer and Information Science, University of Macau, Macau, SAR, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1163"",""1175"",""Policies play an important role in network configuration and therefore in offering secure and high performance services especially over multi-tenant Cloud Data Center (DC) environments. At the same time, elastic resource provisioning through virtualization often disregards policy requirements, assuming that the policy implementation is handled by the underlying network infrastructure. This can result in policy violations, performance degradation and security vulnerabilities. In this paper, we define PLAN, a PoLicy-Aware and Network-aware VM management scheme to jointly consider DC communication cost reduction through Virtual Machine (VM) migration while meeting network policy requirements. We show that the problem is NP-hard and derive an efficient approximate algorithm to reduce communication cost while adhering to policy constraints. Through extensive evaluation, we show that PLAN can reduce topology-wide communication cost by 38 percent over diverse aggregate traffic and configuration policies."",""1558-2183"","""",""10.1109/TPDS.2016.2604811"",""Chinese National Research Fund (NSFC)"; Chinese National Research Fund (NSFC)(grant numbers:61402200,61373125); NSFC(grant numbers:61532013); National China 973 Project(grant numbers:2015CB352401); STCSM(grant numbers:15JC1402400); Shanghai Jiao Tong University(grant numbers:WF220103001); Science and Technology Fund of Macau (FDCT)(grant numbers:061/2011/A3,092/2014/A2); University of Macau(grant numbers:MYRG112-FST12-ZW,MYRG2015-00165-FST); US National Science Foundation(grant numbers:0963979); UK Engineering and Physical Sciences Research Council (EPSRC)(grant numbers:EP/N033957/1,EP/L026015/1,EP/L005255/1,EP/P004407/1,EP/P004024/1);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7556964"",""Data centers";policy;virtual machine;"migration"",""Middleboxes";Web servers;Cloud computing;Electronic mail;"Network topology"","""",""24"","""",""45"",""IEEE"",""31 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Preserving Privacy with Probabilistic Indistinguishability in Weighted Social Networks,""Q. Liu"; G. Wang; F. Li; S. Yang;" J. Wu"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan Province, P. R. China"; School of Computer Science and Educational Software, Guangzhou University, Guangzhou, Guangdong Province, P. R. China; Department of Computer and Information Technology, Indiana University-Purdue University Indianapolis, Indianapolis, IN; Department of Math, Computer Science, and Statistics, Purdue University Calumet, Hammond, IN;" Department of Computer and Information Sciences, Temple University, Philadelphia, PA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Apr 2017"",""2017"",""28"",""5"",""1417"",""1429"",""The increasing popularity of social networks has inspired recent research to explore social graphs for marketing and data mining. As social networks often contain sensitive information about individuals, preserving privacy when publishing social graphs becomes an important issue. In this paper, we consider the identity disclosure problem in releasing weighted social graphs. We identify weighted 1*-neighborhood attacks, which assume that an attacker has knowledge about not only a target's one-hop neighbors and connections between them (1-neighborhood graph), but also related node degrees and edge weights. With this information, an attacker may re-identify a target with high confidence, even if any node's 1-neighborhood graph is isomorphic with $k-1$  other nodes’ graphs. To counter this attack while preserving high utility of the published graph, we define a key privacy property,  probabilistic indistinguishability, and propose a heuristic indistinguishable group anonymization (HIGA) scheme to anonymize a weighted social graph with such a property. Extensive experiments on both real and synthetic data sets illustrate the effectiveness and efficiency of the proposed scheme."",""1558-2183"","""",""10.1109/TPDS.2016.2615020"",""US Natural Science Foundation(grant numbers:CNS 1449860,CNS 1461932,CNS 1460971,CNS 1439672,DUE 1431330,CNS 1301774,ECCS 1231461)"; NSFC(grant numbers:61632009,61272151,61472451,61402161); Natural Science Foundation of China(grant numbers:2015JJ3046);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7582550"",""Weighted social networks";weighted 1*-neighborhood attack;probabilistic indistinguishability;"privacy"",""Social network services";Privacy;Probabilistic logic;Publishing;Knowledge engineering;Computer science;"Electronic mail"","""",""134"","""",""27"",""IEEE"",""4 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Privacy-Aware Scheduling SaaS in High Performance Computing Environments,""S. Sharif"; P. Watson; J. Taheri; S. Nepal;" A. Y. Zomaya"",""Center for Distributed and High Performance Computing, The University of Sydney, Armidale, NSW, Australia"; Newcastle University, Newcastle upon Tyne, England; Karlstad University, Karlstad, Sweden; CSIRO, Armidale, NSW, Australia;" University of Sydney, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1176"",""1188"",""Hybrid clouds have gained popularity in recent times in a variety of organizations due to their ability to provide additional capacity in a public cloud, to augment private cloud capacity, when it is needed. However, scheduling distributed applications' jobs (e.g, workflow tasks) on hybrid cloud resources introduces new challenges. One key problem is the danger of exposing private data and jobs in a third-party public cloud infrastructure, for example in healthcare applications. In this article, we tackle the problem of designing workflow scheduling algorithms to meet customers' deadlines, while not compromising data and task privacy requirements. Our work is different from most studies on workflow scheduling where the main goal is to achieve a balance between desirable, yet incompatible constraints, such as meeting the deadline and/or minimizing the execution time. Although many others have addressed the trade-off between cost and time, or privacy and cost, their work still suffers from an insufficient consideration of the trade-off between privacy and time. To address such shortcomings in the literature, we present a new SaaS scheduling broker composed of MPHC-P1, MPHCP2, and MPHC-P3 policies to preserve privacy while scheduling the workflows' tasks under customers' deadlines. We evaluated our approach using real workflows running on a VMware based hybrid cloud. Results demonstrate that under our scheduling policies, MPHC-P2 and MPHC-P3 are promising in time-critical scenarios by reducing the total cost by 10-20 percent compared to alternatives. Overall, results show that our approach is efficient in reducing the cost of executing workflows while satisfying both their privacy and deadline constraints."",""1558-2183"","""",""10.1109/TPDS.2016.2603153"",""Faculty of Engineering & Information Technologies"; University of Sydney; Faculty Research Cluster Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552583"",""Hybrid cloud";SaaS;workflow scheduling;"privacy preserving"",""Cloud computing";Privacy;Data privacy;Scheduling;Processor scheduling;Organizations;"Electronic mail"","""",""25"","""",""44"",""IEEE"",""25 Aug 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Projective Networks: Topologies for Large Parallel Computer Systems,""C. Camarero"; C. Martínez; E. Vallejo;" R. Beivide"",""Department of Computer Science and Electronics, UNICAN, Santander, Spain"; Department of Computer Science and Electronics, UNICAN, Santander, Spain; Department of Computer Science and Electronics, UNICAN, Santander, Spain;" Department of Computer Science and Electronics, UNICAN, Santander, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""2003"",""2016"",""The interconnection network comprises a significant portion of the cost of large parallel computers, both in economic terms and power consumption. Several previous proposals exploit large-radix routers to build scalable low-distance topologies with the aim of minimizing these costs. However, they fail to consider potential unbalance in the network utilization, which in some cases results in suboptimal designs. Based on an appropriate cost model, this paper advocates the use of networks based on incidence graphs of projective planes, broadly denoted as Projective Networks. Projective Networks rely on generalized Moore graphs with uniform link utilization and encompass several proposed direct (PN and demi-PN) and indirect (OFT) topologies under a common mathematical framework. Compared to other proposals with average distance between 2 and 3 hops, these networks provide very high scalability while preserving a balanced network utilization, resulting in low network costs."",""1558-2183"","""",""10.1109/TPDS.2016.2635640"",""Spanish Science and Technology Commission (CICYT)(grant numbers:TIN2013-46957-C2-2-P,TIN2016-76635-C2-2-R)"; FP7(grant numbers:ICT-2013-10-610402); European HiPEAC Network of Excellence;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7769228"",""High-performance computing interconnection networks";generalized Moore graphs;"projective networks"",""Topology";Network topology;Ports (Computers);Multiprocessor interconnection;Computational modeling;Adaptation models;"Routing"","""",""12"","""",""37"",""IEEE"",""5 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Prophet: A Parallel Instruction-Oriented Many-Core Simulator,""W. Zhang"; X. Ji; Y. Lu; H. Wang; H. Chen;" P. -C. Yew"",""Parallel Processing Institute, Fudan University, Shanghai, China"; Parallel Processing Institute, Fudan University, Shanghai, China; Parallel Processing Institute, Shanghai, China; Software School, Shanghai Key Laboratory of Data Science; Institute of Parallel and Distributed Systems, Shanghai Jiaotong University, Shanghai, China;" Department of Computer Science and Engineering, University of Minnesota, Minneapolis, MN"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2939"",""2952"",""Most existing computer architecture simulators are cycle oriented, i.e., they are driven cycle by cycle. However, frequent switches among simulation contexts, excessive buffer accesses and tightly coupled manner often make such an architecture simulator slow, difficult to parallelize and hard to scale to large-scale many-core systems. In this paper, we propose Prophet, a parallel instruction-oriented simulation framework for many-cores. Prophet adopts a general instruction-oriented model to simulate processor cores, in which a simulator is built from the perspective of each simulated instruction impacting a small number of relevant processor components, as opposed to that of a large number of processor components executing many instructions in each cycle as in the cycle-oriented approach. Prophet determines the execution cycle of a simulated instruction based on the states of the relevant components impacted by the instruction, and update the components states after the execution of the instruction. Prophet also adopts a speculative model to decouple private resources from the shared resources (e.g., shared cache), which avoids unnecessary interactions between them and only pays a penalty upon a rare mis-speculation. We have designed and implemented a prototype of Prophet that supports both user-level and full-system simulation. Experimental results show Prophet can scale up to simulate thousands of simulated cores (4,096 cores in the current implementation) with good performance and small accuracy loss. It achieves average simulation speeds of about 98 and 235 MIPS (millions of simulated instructions per second) for full-system and user-level simulation, respectively, with only 3 percent IPC error rate and negligible deviation in cache simulation results. When run on a many-core platform (i.e., Intel Xeon Phi), it achieved an average simulation speed of about 413 MIPS."",""1558-2183"","""",""10.1109/TPDS.2017.2700307"",""National Key Research and Development Program of China(grant numbers:2016YFB0200501)"; National Natural Science Foundation of China(grant numbers:61672160,61370081);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7917344"",""Instruction-oriented";parallel;manycore;simulation;"architecture-simulation"",""Computational modeling";Timing;Data models;Pipelines;Registers;"Clocks"","""",""6"","""",""31"",""IEEE"",""2 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Querying Web-Scale Knowledge Graphs Through Effective Pruning of Search Space,""J. Jin"; J. Luo; S. Khemmarat;" L. Gao"",""School of Computer Science and Engineering, Southeast University, Nanjing, China"; School of Computer Science and Engineering, Southeast University, Nanjing, China; Department of Electrical and Computer Engineering, University of Massachusetts Amherst, Amherst, MA;" Department of Electrical and Computer Engineering, University of Massachusetts Amherst, Amherst, MA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2342"",""2356"",""Web-scale knowledge graphs containing billions of entities are common nowadays. Querying these graphs can be modeled as a subgraph matching problem. Since knowledge graphs are incomplete and noisy in nature, it is important to discover answers matching exactly as well as answers similar to queries. Existing graph matching algorithms usually use graph indices to accelerate query processing. For billion-node graphs, it may be infeasible to build the graph indices due to the amount of work and the memory/ storage required. In this paper, we propose an efficient algorithm for finding the best k answers for a given query without precomputing graph indices. An answer's quality is measured by a matching score that is computed online. To accelerate query processing, we propose a novel technique for bounding the matching scores during the computation. By using bounds, the low quality answers can be efficiently pruned. The bounding technique can be implemented in a distributed environment, allowing our approach to efficiently query web-scale knowledge graphs. We evaluate the effectiveness and the efficiency of our approach on real-world datasets. The result shows that our bounding technique can reduce the running time up to two orders of magnitude comparing to an approach without using bounds."",""1558-2183"","""",""10.1109/TPDS.2017.2665478"",""National Natural Science Foundation of China(grant numbers:61632008,61320106007,61572129,61502097,61370207,61602112,61532013,61572128)"; International S&T Cooperation Program of China(grant numbers:2015DFA10490); National Science Foundation of Jiangsu Province(grant numbers:BK20160695); Jiangsu Provincial Key Laboratory of Network and Information Security(grant numbers:BM2003201); Ministry of Education(grant numbers:93K-9); Collaborative Innovation Center of Novel Software Technology and Industrialization; Collaborative Innovation Center of Wireless Communications Technology; US National Science Foundation(grant numbers:CNS-1217284,CCF-1018114);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7845718"",""Subgraph similarity matching";knowledge graph;billion-node graph;index-free;top-  $k$      query;"distributed system"",""Knowledge engineering";Films;Avatars;Optimization;Acceleration;Query processing;"Resource description framework"","""",""16"","""",""39"",""IEEE"",""7 Feb 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Raccoon: A Novel Network I/O Allocation Framework for Workload-Aware VM Scheduling in Virtual Environments,""L. Zeng"; Y. Wang; X. Fan;" C. Xu"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology (HUST), Wuhan, HuBei, China"; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China;" Department of Electrical and Computer Engineering, Wayne State University, Detroit, MI"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2651"",""2662"",""We present a network I/O allocation framework, called Raccoon, for workload-aware VM scheduling algorithm to facilitate hybrid I/O workloads in virtual environments. Raccoon combines the strengths of paravirtual I/O and SR-IOV techniques to not only minimize the network latency, but also optimize the bandwidth utilization for workload-aware VM scheduling. In Raccoon, a limited number of VFs in SR-IOV are granted to I/O-intensive VMs while the paravirtual Network Interface Cards (vNICs) are allocated to other non-I/O-intensive VMs as the default resources. With this design, Raccoon provides latency reduction and bandwidth guarantee under the premise that I/O-intensive VMs will always be granted the VFs to facilitate their I/O operations. The types of workloads in each VM are identified at runtime by modified XenMon. By leveraging the ACPI Hotplug technique, Raccoon can adaptively plugin and plugout the SR-IOV VFs upon the changes of VM requirements so that an efficient I/O workload-aware VM scheduling algorithm can be implemented based on the bonding driver technique. The experimental results reveal that Raccoon can combine the benefits of para-virtual I/O and SR-IOV techniques to improve the overall performance of virtualized platforms with VMs that have diverse I/O workloads."",""1558-2183"","""",""10.1109/TPDS.2017.2685386"",""China National Basic Research Program 973 Program(grant numbers:2015CB352400)"; National Natural Science Foundation of China (NSFC)(grant numbers:61472153,61672513,61572377); National Science Foundation of Hubei Province(grant numbers:2015CFB192); Science and Technology Planning Project of Guangdong Province(grant numbers:2015B010129011,2016A030313183); Shenzhen Overseas High-Caliber Personnel Innovation Funds(grant numbers:KQCX20140521115045446);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7883963"",""Workload aware";VM scheduling;hypervisor;bandwidth allocation;I/O virtualization;SR-IOV;bonding driver;"hotplug"",""Virtualization";Bandwidth;Virtual machine monitors;Resource management;Hardware;Performance evaluation;"Scheduling algorithms"","""",""13"","""",""40"",""IEEE"",""21 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"RAMSYS: Resource-Aware Asynchronous Data Transfer with Multicore SYStems,""T. Li"; Y. Ren; D. Yu;" S. Jin"",""VMware Inc., Palo Alto, CA"; Stony Brook University, Stony Brook, NY; Brookhaven National Laboratory, Upton, NY;" Stony Brook University, Stony Brook, NY"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1430"",""1444"",""High-speed data transfer is vital to data-intensive computing that often requires moving large data volumes efficiently within a local data center and among geographically dispersed facilities. Effective utilization of the abundant resources in modern multicore environments for data transfer remains a persistent challenge, particularly, for Non-Uniform Memory Access (NUMA) systems wherein the locality of data accessing is an important factor. This requires rethinking how to exploit parallel access to data and to optimize the storage and network I/Os. We address this challenge and present a novel design of asynchronous processing and resource-aware task scheduling in the context of high-throughput data replication. Our software allocates multiple sets of threads to different stages of the processing pipeline, including storage I/O and network communication, based on their capacities. Threads belonging to each stage follow an asynchronous model, and attain high performance via multiple locality-aware and peer-aware mechanisms, such as task grouping, buffer sharing, affinity control and communication protocols. Our design also integrates high performance features to enhance the scalability of data transfer in several scenarios, e.g., file-level sorting, block-level asynchrony, and thread-level pipelining. Our experiments confirm the advantages of our software under different types of workloads and dynamic environments with contention for shared resources, including a 28-160 percent increase in bandwidth for transferring large files, 1.7-66 times speed-up for small files, and up to 108 percent larger throughput for mixed workloads compared with three state of the art alternatives, GridFTP , BBCP and Aspera."",""1558-2183"","""",""10.1109/TPDS.2016.2619344"",""United States Department of Energy(grant numbers:DE-SC0012704,DE-SC0003361)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7600392"",""Multicore systems";input/output;high-speed data transfer;parallelism;asynchronous processing;"pipelining"",""Data transfer";Instruction sets;Protocols;Message systems;Multicore processing;"Cloud computing"","""",""8"","""",""21"",""IEEE"",""19 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Reactive Molecular Dynamics on Massively Parallel Heterogeneous Architectures,""S. B. Kylasa"; H. M. Aktulga;" A. Y. Grama"",""Department of Electrical and Computer Engineering, Purdue University, West Lafayette, IN"; Lawrence Berkeley National Laboratory, 1 Cyclotron Rd, MS 50F-1650 Berkeley, CA, 94720;" Department of Computer Science, Purdue University, West Lafayette, IN"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""202"",""214"",""We present a parallel implementation of the ReaxFF force field on massively parallel heterogeneous architectures, called PuReMD-Hybrid. PuReMD, on which this work is based, along with its integration into LAMMPS, is currently used by a large number of research groups worldwide. Accelerating this important community codebase that implements a complex reactive force field poses a number of algorithmic, design, and optimization challenges, as we discuss in detail. In particular, different computational kernels are best suited to different computing substrates-CPUs or GPUs. Scheduling these computations requires complex resource management, as well as minimizing data movement across CPUs and GPUs. Integrating powerful nodes, each with multiple CPUs and GPUs, into clusters and utilizing the immense compute power of these clusters requires significant optimizations for minimizing communication and, potentially, redundant computations. From a programming model perspective, PuReMD-Hybrid relies on MPI across nodes, pthreads across cores, and CUDA on the GPUs to address these challenges. Using a variety of innovative algorithms and optimizations, we demonstrate that our code can achieve over 565-fold speedup compared to a single core implementation on a cluster of 36 state-of-the-art GPUs for complex systems. In terms of application performance, our code enables simulations of over 1.8M atoms in under 0.68 seconds per simulation time step."",""1558-2183"","""",""10.1109/TPDS.2016.2548462"",""National Science Foundation(grant numbers:CCF 1533795)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7444209"",""Reactive molecular dynamics";parallel GPU implementations;"material simulations"",""Graphics processing units";Force;Kernel;Biological system modeling;Computational modeling;Clustering algorithms;"Numerical models"","""",""14"","""",""45"",""IEEE"",""30 Mar 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Real-Time GPU-Based Image Processing for a 3-D THz Radar,""F. García-Rial"; L. Úbeda-Medina;" J. Grajal"",""Department of Signals, Systems, and Radiocommunications, Technical University of Madrid, Madrid, Spain"; Department of Signals, Systems, and Radiocommunications, Technical University of Madrid, Madrid, Spain;" Department of Signals, Systems, and Radiocommunications, Technical University of Madrid, Madrid, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2953"",""2964"",""A real-time image processing algorithm, using a GPU with NVIDIA's CUDA technology, for a 3-D through-clothes detection radar in the THz band has been developed. Traditional CPU-based architectures lack the computing power and parallelization needed to meet the image refresh rates imposed by the radar's scanning. This solution presents a low-cost and flexible alternative that allows image refresh at a rate of more than 8 fps for images composed of 6,000 pixels, corresponding to a scanned area of 50x90 cm2. The performance of this code has been profiled, with comparison against a previous CPU-executed version, and some optimizations that would enable even faster refresh have been analyzed."",""1558-2183"","""",""10.1109/TPDS.2017.2687927"",""Ministerio de Economía y Competitividad(grant numbers:TEC2014-53815-R)"; Madrid Regional Government(grant numbers:S2013/ICE-3000); Spanish Directorate General of Traffic(grant numbers:SPIP2015-01879); Federico García-Rial; FPU; Ministerio de Economía y Competitividad;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887682"",""Compute unified device architecture (CUDA)";graphics processing unit (GPU);imaging radar;real-time;terahertz;"threat detection"",""Radar imaging";Graphics processing units;Imaging;Signal processing algorithms;Field programmable gate arrays;"Time-frequency analysis"","""",""11"","""",""38"",""IEEE"",""27 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Realistic and Scalable Benchmarking Cloud File Systems: Practices and Lessons from AliCloud,""Z. Ren"; W. Shi; J. Wan; F. Cao;" J. Lin"",""School of Computer Science and Technology, Hangzhou Dianzi University, Hangzhou, China"; Department of Computer Science, Wayne State University, Detroit, MI; Department of Software Engineering, Zhejiang University of Science and Technology, Hangzhou, China; AliCloud Computing Co., Ltd., Hangzhou, China;" AliCloud Computing Co., Ltd., Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3272"",""3285"",""The past decade has witnessed the rapid boom of cloud computing. Many public cloud infrastructures have been implemented and serve millions of tenants. Cloud file systems, which take charge of petabyte-scale data storage, play a crucial role in the performance of cloud infrastructures. Typical cloud file systems, including GFS, HDFS and Ceph, have attracted notable research efforts for performance evaluation and optimization. However, due to the heterogeneity and complexity of I/O workload characteristics in cloud environments, it is still challenging to conduct an accurate and efficient performance evaluation. To address this problem, we collected a two-week I/O workload trace from a 2,500-node production cluster in AliCloud, which is one of the largest cloud providers in Asia. Using the AliCloud trace, we characterized the I/O workload and data distribution, and compared two cloud services in multiple perspectives, including the request arrival pattern, request size, data population and so on. A list of observations and implications were derived and applied to help design a cloud file system benchmarking suite, called Porcupine. Porcupine aims to deploy a scalable and efficient performance evaluation on cloud file systems using realistic I/O workloads. We conducted a group of validation experiments, which demonstrated that Porcupine can achieve high accuracy and scalability. This paper provides our experiences and lessons in generating I/O workloads and deploying performance tests on cloud file systems, which we believe will be insightful to the cloud computing community in general."",""1558-2183"","""",""10.1109/TPDS.2017.2715327"",""NSF(grant numbers:61572163)"; US National Science Foundation (NSF)(grant numbers:CNS-1563728);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7949134"",""Cloud file systems";workload generator;"performance benchmarking"",""Cloud computing";Benchmark testing;Performance evaluation;Throughput;Big Data;Servers;"Distributed databases"","""",""7"","""",""53"",""IEEE"",""15 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Reducing Synchronization Overhead with Computation Replication in Parallel Agent-Based Road Traffic Simulation,""Y. Xu"; V. Viswanathan;" W. Cai"",""AIDA, TUMCREATE Ltd., Singapore"; RP5, TUMCREATE Ltd., Singapore;" School of Computer Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3286"",""3297"",""Road traffic simulation is a useful tool for studying road traffic and evaluating solutions to traffic problems. Large-scale agent-based road traffic simulation is computationally intensive, which triggers the need for conducting parallel simulation. This paper deals with the synchronization problem in parallel agent-based road traffic simulation to reduce the overall simulation execution time. We aim to reduce synchronization operations by introducing some redundant computation to the simulation. There is a trade-off between the benefit of reduced synchronization operations and the overhead of redundant computation. The challenge is to minimize the total overhead of redundant computation and synchronization. First, to determine the amount of redundant computation, we proposed a way to define extended layers of partitions in the road network. The sizes of extended layers are determined by the behavior of agents and the topology of road networks. Second, due to the dynamic nature of road traffic, a heuristic was proposed to adjust the amount of redundant computation according to traffic conditions during simulation run-time to minimize the overall simulation execution time. The efficiency of the proposed method was investigated in a parallel agent-based road traffic simulator using real-world network and trip data. Results have shown that the method can reduce synchronization overhead and improve the overall performance of the parallel simulation significantly."",""1558-2183"","""",""10.1109/TPDS.2017.2714165"",""Singapore National Research Foundation"; Research Excellence And Technological Enterprise (CREATE) programme;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7945281"",""Agent-based traffic simulation";parallel simulation;conservative synchronization;"computation replication"",""Roads";Computational modeling;Synchronization;Sensors;Protocols;Mathematical model;"Data models"","""",""5"","""",""24"",""IEEE"",""9 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Relational Joins on GPUs: A Closer Look,""M. Yabuta"; A. Nguyen; S. Kato; M. Edahiro;" H. Kawashima"",""Graduate School of Information Science, Nagoya University, Aichi Prefecture, Japan"; Graduate School of Information Science, Nagoya University, Aichi Prefecture, Japan; Graduate School of Information Science, Nagoya University, Aichi Prefecture, Japan; Graduate School of Information Science, Nagoya University, Aichi Prefecture, Japan;" Graduate School of System and Information Engineering, University of Tsukuba, Ibaraki Prefecture, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2663"",""2673"",""The problem of scaling out relational join performance for large data sets in the database management system (DBMS) has been studied for years. Although in-memory DBMS engines can reduce load times by storing data in the main memory, join queries still remain computationally expensive. Modern graphics processing units (GPUs) provide massively parallel computing and may enhance the performance of such join queries";" however, it is not clearyet in what condition relational joins perform well on GPUs. In this paper, we identify the performance characteristics of GPU computing for relational joins by implementing several well-known GPU-based join algorithms under various configurations. Experimental results indicate that the speedup ratio of GPU-based relational joins to CPU-based counterparts depends on the number of compute cores, the size of data sets, join conditions, and join algorithms. In the best case, the speedup ratios are up to 6.67 times for non-index joins, 9.41 times for sort index joins, and 2.55 times for hash joins. The execution time of GPU-based implementation for index joins, on the other hand, is only about 0.696 times less than the execution time of the CPU's counterparts."",""1558-2183"","""",""10.1109/TPDS.2017.2677451"",""Core Research for Evolutional Science and Technology"; JST; JSPS; KAKENHI(grant numbers:25280043);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869389"",""Graphics processors";query processing;"parallelism and concurrency"",""Graphics processing units";Instruction sets;Indexes;Parallel processing;Engines;"Query processing"","""",""13"","""",""22"",""IEEE"",""2 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Reliable Virtual Machine Placement and Routing in Clouds,""S. Yang"; P. Wieder; R. Yahyapour; S. Trajanovski;" X. Fu"",""Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG), Göttingen, Germany"; Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG), Göttingen, Germany; Institute of Computer Science, University of Göttingen, Göttingen, Germany; Philips Research and Delft University of Technology, Delft, CD, The Netherlands;" Institute of Computer Science, University of Göttingen, Göttingen, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2965"",""2978"",""In current cloud computing systems, when leveraging virtualization technology, the customer’s requested data computing or storing service is accommodated by a set of communicated virtual machines (VM) in a scalable and elastic manner. These VMs are placed in one or more server nodes according to the node capacities or failure probabilities. The VM placement availability refers to the probability that at least one set of all customer’s requested VMs operates during the requested lifetime. In this paper, we first study the problem of placing at most  $H$  groups of $k$  requested VMs on a minimum number of nodes, such that the VM placement availability is no less than $\delta$ , and that the specified communication delay and connection availability for each VM pair under the same placement group are not violated. We consider this problem with and without Shared-Risk Node Group (SRNG) failures, and prove this problem is NP-hard in both cases. We subsequently propose an exact Integer Nonlinear Program (INLP) and an efficient heuristic to solve this problem. We conduct simulations to compare the proposed algorithms with two existing heuristics in terms of performance. Finally, we study the related reliable routing problem of establishing a connection over at most $w$  link-disjoint paths from a source to a destination, such that the connection availability requirement is satisfied and each path delay is no more than a given value. We devise an exact algorithm and two heuristics to solve this NP-hard problem, and evaluate them via simulations."",""1558-2183"","""",""10.1109/TPDS.2017.2693273"",""EU FP7"; CleanSky(grant numbers:607584); University of Amsterdam; Delft University of Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7896612"",""Virtual machine placement";routing;availability;reliability;cloud computing;"optimization algorithms"",""Reliability";Servers;Routing;Heuristic algorithms;Cloud computing;Delays;"Virtual machining"","""",""29"","""",""31"",""IEEE"",""12 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Renewable Energy Pricing Driven Scheduling in Distributed Smart Community Systems,""Y. Liu";" S. Hu"",""Department of Electrical and Computer Engineering, Michigan Technological University, Houghton, MI";" Department of Electrical and Computer Engineering, Michigan Technological University, Houghton, MI"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1445"",""1456"",""A smart community is a distributed system consisting of a set of smart homes which utilize the smart home scheduling techniques to enable customers to automatically schedule their energy loads targeting various purposes such as electricity bill reduction. Smart home scheduling is usually implemented in a decentralized fashion inside a smart community, where customers compete for the community level renewable energy due to their relatively low prices. Typically there exists an aggregator as a community wide electricity policy maker aiming to minimize the total electricity bill among all customers. This paper develops a new renewable energy aware pricing scheme to achieve this target. We establish the proof that under certain assumptions the optimal solution of decentralized smart home scheduling is equivalent to that of the centralized technique, reaching the theoretical lower bound of the community wide total electricity bill. In addition, an advanced cross entropy optimization technique is proposed to compute the pricing scheme of renewable energy, which is then integrated in smart home scheduling. The simulation results demonstrate that our pricing scheme facilitates the reduction of both the community wide electricity bill and individual electricity bills compared to the uniform pricing. In particular, the community wide electricity bill can be reduced to only 0.06 percent above the theoretic lower bound."",""1558-2183"","""",""10.1109/TPDS.2016.2615936"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586083"",""Smart home";smart community;pricing scheme;cross entropy optimization;"renewable energy"",""Renewable energy sources";Smart homes;Pricing;Processor scheduling;Home appliances;Energy consumption;"Schedules"","""",""18"","""",""26"",""IEEE"",""7 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Repair Tree: Fast Repair for Single Failure in Erasure-Coded Distributed Storage Systems,""H. Zhang"; H. Li;" S. -Y. R. Li"",""Shenzhen Key Lab of Information Theory and Future Network Arch, Peking University, Beijing, P.R. China"; Shenzhen Key Lab of Information Theory and Future Network Arch, Peking University, Beijing, P.R. China;" The Chinese University of Hong Kong, Sha Tin, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1728"",""1739"",""In order to guarantee data reliability, erasure codes have been used in distributed storage systems. Nevertheless, this mechanism suffers from the repair problem that excess data are needed to repair a single failure, causing both high bandwidth consuming for the network and heavy computing load on the replacement node. To reduce repair traffic, researchers pointed out the tradeoff between storage and repair traffic and proposed regenerating codes by combining network coding. However, the combination only focuses on the storage terminal and the construction of the codes is quite complicated. Therefore, this paper further combines network coding with network structure and proposes a repair tree model based on general erasure codes to simplify the repair procedure. By decomposing repair computing and distributing it among the tree nodes, our model can mitigate the computing tension. The performance of repair tree is analyzed and evaluated by preliminary emulation. The result shows it can make about three times faster computing than conventional measure and the repair throughput is doubled if there are network bottlenecks. For proper topology, it can significantly reduce the repair traffic. We present algorithms to generate trees across the network topology. At last, we present the idea of extending repair tree to repair multiple failures."",""1558-2183"","""",""10.1109/TPDS.2016.2628024"",""National Keystone R&D Program of China(grant numbers:2016YFB0800101)"; Natural Science Foundation of China (NSFC)(grant numbers:61671001,61521003); Guangdong Research Programs(grant numbers:2016B030305005); Shenzhen Research Programs(grant numbers:ZDSYS201603311739428,JCYJ20150331100723974,20140509093817684);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7742352"",""Distributed storage system";erasure codes;decomposed repair computing;single failure;repair tree;transitive vector;network coding;"intermediate node"",""Maintenance engineering";Strips;Network coding;Bandwidth;Computational modeling;Encoding;"Distributed databases"","""",""10"","""",""40"",""IEEE"",""11 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Resource Availability Prediction in Distributed Systems: An Approach for Modeling Non-Stationary Transition Probabilities,""S. Kianpisheh"; M. Kargahi;" N. M. Charkari"",""Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran"; School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran;" Faculty of Electrical and Computer Engineering, Tarbiat Modares University, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2357"",""2372"",""Large scale distributed systems employ thousands of resources which inevitably suffer from the unavailability issue. Serious side effects like unexpected delay or failure in the application execution are probable in case of such an issue. The imposed outcome might then be catastrophic consequences for real time applications or penalties for the service providers. Better prediction of the resource unavailability helps diminishing the undesired outcomes. This paper proposes a resource availability prediction algorithm for the mentioned goal. The resource availability variation is modeled as a stochastic process. By analyzing the availability information of NDU resources and both physical and virtual machines of the PlantLab, we found that the transition probabilities among the availability levels are non-stationary. To cope with this characteristic, we introduce Availability Transition Patterns (ATPs)";" the ATPs are dynamically constructed and the transitions between them are modeled by a Markov chain. The future ATP is then predicted based on the constructed Markov chain, according to which the resource availability-level is predicted. Experimental results confirm the efficiency of the proposed prediction algorithm."",""1558-2183"","""",""10.1109/TPDS.2017.2659746"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7835279"",""Availability";prediction;stochastic process;Markov chain;"transient analysis"",""Prediction algorithms";Markov processes;Hidden Markov models;Computational modeling;Predictive models;"History"","""",""7"","""",""45"",""IEEE"",""26 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Resource Sharing in Multicore Mixed-Criticality Systems: Utilization Bound and Blocking Overhead,""J. -J. Han"; X. Tao; D. Zhu;" L. T. Yang"",""School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, University of Texas at San Antonio, San Antonio, TX;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3626"",""3641"",""In mixed-criticality (MC) system, diverse application activities with various certification requirements (different criticality) can share a computing platform, where multicore processors have emerged as the prevailing computing engines. Focusing on the problem of resource access contention in multicore MC systems, we analyze the synchronization issues and blocking characteristics of the Multiprocessor Stack Resource Policy (MSRP) with both priority and criticality inversions among MC tasks being considered. We develop the first criticality-aware utilization bound under partitioned Earliest Deadline First (EDF) and MSRP by taking the worst case synchronization overheads of tasks into account. The non-monotonicityof the bound where it may decrease when more cores are deployed is identified, which can cause anomalies in the feasibility tests. With the objective to improve system schedulability, a novel criticality-cognizant and resource-oriented analysis approach is further studied to tighten the bound on the synchronization overheads for MC tasks scheduled under partitioned EDF and MSRP. The simulation results show that the new analysis approach can effectively reduce the blocking times for tasks (up to 30 percent) and thus improve the schedulability ratio (e.g., 10 percent more). The actual implementation in Linux kernel further shows the practicability of partitioned-EDF with MSRP (with run-time overhead being about 3 to 7 percent of the overall execution time) for MC tasks running on multicores with shared resources."",""1558-2183"","""",""10.1109/TPDS.2017.2677442"",""National Natural Science Foundation of China (NSFC)(grant numbers:61472150,61173045)"; Fundamental Research Funds for the Central Universities(grant numbers:2016YXMS081,2015TS072); US National Science Foundation(grant numbers:CNS-1422709,CNS-1421855);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7869415"",""Mixed-criticality systems";multicore;shared resources;resource access contention;utilization bound;"run-time overhead"",""Multicore processing";Synchronization;Program processors;Processor scheduling;Resource management;"Access protocols"","""",""18"","""",""38"",""IEEE"",""2 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Response-Time Analysis in Hierarchically-Scheduled Time-Partitioned Distributed Systems,""J. C. Palencia"; M. G. Harbour; J. J. Gutiérrez;" J. M. Rivas"",""Software Engineering and Real-Time Group, Facultad de Ciencias, Universidad de Cantabria, Santander, Spain"; Software Engineering and Real-Time Group, Facultad de Ciencias, Universidad de Cantabria, Santander, Spain; Software Engineering and Real-Time Group, Facultad de Ciencias, Universidad de Cantabria, Santander, Spain;" Software Engineering and Real-Time Group, Facultad de Ciencias, Universidad de Cantabria, Santander, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""2017"",""2030"",""This paper develops an offset-based response-time analysis technique for analyzing complex distributed real-time systems where processing and communication resources use the time-partitioning strategy to isolate the operation of separate software components. Time partitioning may be provided in the processors by an ARINC 653 compliant operating system, and in the networks via the TTP communication protocol. The software components executed by the system may themselves be distributed and complex, composed of many concurrent tasks and with one or more end-to-end flows that may have end-to-end timing requirements. The developed analysis supports hierarchical scheduling where a primary scheduler performs time partitioning into separate partitions, and secondary fixed-priority schedulers dispatch the different concurrent tasks inside each partition. It also supports end-to-end flows that are either synchronized with the partition schedule or not. This is the first time that this kind of analysis is developed. An evaluation of an improvement introduced in the analysis is discussed. Two representative case studies are described."",""1558-2183"","""",""10.1109/TPDS.2016.2642960"",""Spanish Government(grant numbers:TIN2014-56158-C4-2-P (M2C2))"; Juan Antonio de la Puente; Technical University of Madrid;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792746"",""Real-time";distributed systems;scheduling;task partitioning;clock synchronization;embedded systems;modeling techniques;worst-case analysis;"response-time analysis"",""Synchronization";Program processors;Jitter;Real-time systems;Analytical models;"Schedules"","""",""17"","""",""24"",""IEEE"",""21 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Routing and Fault Tolerance in Z-Fat Tree,""M. Adda";" A. Peratikou"",""School of Computing, University of Portsmouth, Portsmouth, United Kingdom";" School of Computing, University of Portsmouth, Portsmouth, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2373"",""2386"",""Fat tree topologies have been extensively used as interconnection networks for high performance computing, cluster and data center systems, with their most recent variants able to fairly extend and scale to accommodate higher processing power. While each progressive and evolved fat-tree topology includes some extra advancements, these networks do not fully address all the issues of large scale HPC. We propose a topology called Zoned-Fat tree (Z-Fat tree,) which is a further extension to the fat trees. The extension relates to the provision of extra degree of connectivity to utilize the extra ports per switches (routing nodes), that are, in some cases, not utilized by the architectural constraints of other variants of fat trees, and hence increases the bisection bandwidth, reduces the latency and supplies additional paths for fault tolerance. To support and profit from the extra links, we propose an adaptive low latency routing for up traffic which is based on a series of leading direction bits predefined at the source";" furthermore we suggest a deterministic routing by implementing a dynamic round robin algorithm that overtakes D-mod-K in same cases and guarantees the utilization of all the extra links. We also propose a fault tolerance algorithm, named recoil-and-reroute which makes use of the extra links to ensure higher message delivery even in the presence of faulty links and switches."",""1558-2183"","""",""10.1109/TPDS.2017.2666807"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7850986"",""Adaptive routing";connectivity;deterministic routing;fault tolerance;"fat-tree"",""Fats";Routing;Topology;Network topology;Fault tolerance;Fault tolerant systems;"Ports (Computers)"","""",""18"","""",""38"",""IEEE"",""13 Feb 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"SACAT: Streaming-Aware Conflict-Avoiding Thrashing-Resistant GPGPU Cache Management Scheme,""M. Khairy"; M. Zahran;" A. Wassal"",""Department of Computer Engineering, Cairo University, Giza, Egypt"; Department of Computer Science, New York University, New York, NY;" Department of Computer Engineering, Cairo University, Giza, Egypt"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1740"",""1753"",""Modern graphical processing units (GPUs) are equipped with general-purpose L1 and L2 caches to reduce the memory bandwidth demand and improve the performance of some irregular general-purpose GPU (GPGPU) applications. However, due to the massive multithreading, GPGPU caches suffer from severe resource contention and low data-sharing which may lead to performance degradation instead. This paper proposes a low-cost streaming-aware conflict-avoiding thrashing-resistant (SACAT) GPGPU cache management scheme that efficiently utilizes the GPGPU cache resources and addresses all the problems associated with GPGPU caches. The proposed scheme employs three orthogonal techniques. First, it dynamically detects and bypasses streaming applications at fine granularity. Second, a dynamic warp throttling via cores sampling (DWT-CS) is proposed to alleviate cache thrashing. DWT-CS runs an exhaustive search over cores to find the best number of warps that achieves the highest performance. Third, it employs pseudo random interleaving cache (PRIC), which is an improved cache indexing function based on polynomial modulus mapping, to mitigate associativity stalls and eliminate conflict misses. Experimental results demonstrate that the proposed scheme achieves a 1.87× and a 1.5× performance improvement over the cache-conscious wavefront scheduler (CCWS) and the memory request prioritization buffer (MRPB), respectively."",""1558-2183"","""",""10.1109/TPDS.2016.2627560"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7744628"",""Cache management";"GPGPU"",""Instruction sets";Graphics processing units;Message systems;Programming;Discrete wavelet transforms;System-on-chip;"Registers"","""",""5"","""",""43"",""IEEE"",""15 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scalable Adaptive NUMA-Aware Lock,""M. Zhang"; H. Chen; L. Cheng; F. C. M. Lau;" C. -L. Wang"",""Department of Computer Science, University of Hong Kong, Hong Kong"; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Minhang, Qu, China; Facebook, Menlo Park, CA; Department of Computer Science, University of Hong Kong, Hong Kong;" Department of Computer Science, University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1754"",""1769"",""Scalable locking is a key building block for scalable multi-threaded software. Its performance is especially critical in multi-socket, multi-core machines with non-uniform memory access (NUMA). Previous schemes such as in-place locks and delegation locks only perform well under a certain level of contention, and often require non-trivial tuning for a particular configuration. Besides, in large NUMA systems, current delegation locks cannot perform satisfactorily due to lack of optimized NUMA policies. In this work, we propose SANL, a locking scheme that can deliver high performance under various contention levels by adaptively switching between in-place locks and delegation locks. To optimize the performance of delegation locks, we introduce a new NUMA policy that jointly considers node distances and server utilization when choosing lock servers. We have implemented SANL and evaluated it with four popular multi-threaded applications (Memcached, Berkeley DB, Phoenix2 and SPLASH-2), on a 40-core Intel machine and a 64-core AMD machine. The comparison results with seven other representative locking schemes show that SANL outperforms them in most contention situations. For example, in one group test, SANL is 3.7 times faster than RCL lock and 17 times faster than POSIX mutex."",""1558-2183"","""",""10.1109/TPDS.2016.2630695"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7748539"",""Delegation lock";"adaptive synchronization"",""Servers";Synchronization;Message systems;Switches;Hardware;"Instruction sets"","""",""7"","""",""41"",""IEEE"",""18 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scalable Single Source Shortest Path Algorithms for Massively Parallel Systems,""V. T. Chakaravarthy"; F. Checconi; P. Murali; F. Petrini;" Y. Sabharwal"",""IBM Research, Vasant Kunj, New Delhi, India"; IBM Thomas J. Watson Research Center, Yorktown Heights, NY; IBM Research, Vasant Kunj, New Delhi, India; Intel Parallel Computing Labs, Santa Clara, CA;" IBM Research, Vasant Kunj, New Delhi, India"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""2031"",""2045"",""We consider the single-source shortest path (SSSP) problem: given an undirected graph with integer edge weights and a source vertex $v$ , find the shortest paths from $v$  to all other vertices. In this paper, we introduce a novel parallel algorithm, derived from the Bellman-Ford and Delta-stepping algorithms. We employ various pruning techniques, such as edge classification and direction-optimization, to dramatically reduce inter-node communication traffic, and we propose load balancing strategies to handle higher-degree vertices. These techniques are particularly effective on power-law graphs, as demonstrated by our extensive performance analysis. In the largest tested configuration, an R-MAT graph with $2^{38}$  vertices and $2^{42}$  edges on 32,768 Blue Gene/Q nodes, we have achieved a processing rate of three Trillion Edges Per Second (TTEPS), a four orders of magnitude improvement over the best published results."",""1558-2183"","""",""10.1109/TPDS.2016.2634535"",""Argonne National Laboratory"; Office of Science; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7763835"",""Shortest path";parallel algorithm;delta stepping;graph 500 benchmark;"distributed system"",""Algorithm design and analysis";Benchmark testing;Parallel algorithms;Load management;Blogs;"Very large scale integration"","""",""27"","""",""29"",""IEEE"",""1 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scheduling for Workflows with Security-Sensitive Intermediate Data by Selective Tasks Duplication in Clouds,""H. Chen"; X. Zhu; D. Qiu; L. Liu;" Z. Du"",""College of Information System and Management, National University of Defense Technology, Changsha, Hunan, P. R. China"; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, P. R. China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, P. R. China; College of Computing, Georgia Institute of Technology, 266 Ferst Drive, Atlanta, GA;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2674"",""2688"",""With the wide deployment of cloud computing in many business enterprises as well as science and engineering domains, high quality security services are increasingly critical for processing workflow applications with sensitive intermediate data. Unfortunately, most existing worklfow scheduling approaches disregard the security requirements of the intermediate data produced by workflows, and overlook the performance impact of encryption time of intermediate data on the start of subsequent workflow tasks. Furthermore, the idle time slots on resources, resulting from data dependencies among workflow tasks, have not been adequately exploited to mitigate the impact of data encryption time on workflows' makespans and monetary cost. To address these issues, this paper presents a novel task-scheduling framework for security sensitive workflows with three novel features. First, we provide comprehensive theoretical analyses on how selectively duplicating a task's predecessor tasks is helpful for preventing both the data transmission time and encryption time from delaying task's start time. Then, we define workflow tasks' latest finish time, and prove that tasks can be completed before tasks' latest finish time by using cheapest resources to reduce monetary cost without delaying tasks' successors' start time and workflows' makespans. Based on these analyses, we devise a novel scheduling approach with selective tasks duplication, named SOLID, incorporating two important phases: 1) task scheduling with selectively duplicating predecessor tasks to idle time slots on resources";" and 2) intermediate data encrypting by effectively exploiting tasks' laxity time. We evaluate our solution approach through rigorous performance evaluation study using both randomly generated workflows and some real-world workflow traces. Our results show that the proposed SOLID approach prevails over existing algorithms in terms of makespan, monetary costs and resource efficiency."",""1558-2183"","""",""10.1109/TPDS.2017.2678507"",""National Natural Science Foundation of China(grant numbers:61572511,61603404)"; National University of Defense Technology(grant numbers:ZK16-03-57,ZK16-03-09,ZK16-03-30); China Postdoctoral Science Foundation(grant numbers:2016M602960); Southwest Electron and Telecom Technology Institute(grant numbers:2015014); Georgia Tech; National Science Foundation(grant numbers:NSF 1547102,SaTC 1564097);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7872483"",""Cloud computing";data security;security-sensitive;data encryption;workflow;task duplication;"scheduling"",""Cloud computing";Encryption;Scheduling algorithms;"Solids"","""",""86"","""",""41"",""IEEE"",""6 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scheduling Independent Moldable Tasks on Multi-Cores with GPUs,""R. Bleuse"; S. Hunold; S. Kedad-Sidhoum; F. Monna; G. Mounié;" D. Trystram"",""Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, Grenoble, France"; Faculty of Informatics, Institute of Information Systems, Favoritenstraße 16/184-5, Vienna, Austria; Sorbonne Universités, UPMC University Paris 06, UMR 7606, LIP6, Paris, France; Sorbonne Universités, UPMC University Paris 06, UMR 7606, LIP6, Paris, France; Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, Grenoble, France;" Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, LIG, Grenoble, France"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2689"",""2702"",""We present a new approach for scheduling independent tasks on multiple CPUs and multiple GPUs. The tasks are assumed to be parallelizable on CPUs using the moldable model: the final number of cores allotted to a task can be decided and set by the scheduler. More precisely, we design an algorithm aiming at minimizing the makespan-the maximum completion time of all tasks-for this scheduling problem. The proposed algorithm combines a dual approximation scheme with a fast integer linear program (ILP). It determines both the partitioning of the tasks, i.e., whether a task should be mapped to CPUs or a GPU, and the number of CPUs allotted to a moldable task if mapped to the CPUs. A worst-case analysis shows that the algorithm has an approximation ratio of 3/2 + ε. Since the time complexity of the ILP-based algorithm could be non-polynomial, we also present a polynomial-time algorithm with an approximation ratio of 2 + ε. We complement the theoretical analysis of our two novel algorithms with a simulation study. In these simulations, we compare our algorithms to a modified version of the classical HEFT algorithm, which we adapted to handle moldable tasks. The simulation results show that our algorithm with the (3/2 + ε)-approximation ratio produces significantly shorter schedules than the modified HEFT for most of the instances. In addition, our results provide evidence that our ILP-based algorithm can solve larger problem instances in a reasonable amount of time."",""1558-2183"","""",""10.1109/TPDS.2017.2675891"",""DGA-MRIS"; GDR-RO;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867044"",""Scheduling";heterogeneous computing;moldable tasks;dual approximation scheme;"integer linear programming"",""Approximation algorithms";Algorithm design and analysis;Scheduling;Graphics processing units;"Scheduling algorithms"","""",""21"","""",""32"",""IEEE"",""1 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scheduling of Electric Vehicle Charging via Multi-Server Fair Queueing,""X. Wang"; Y. Pi;" A. Tang"",""University of Michigan-Shanghai Jiao Tong University Joint Institute, Shanghai, China"; University of Michigan, Ann Arbor, MI;" University of Michigan-Shanghai Jiao Tong University Joint Institute, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3298"",""3312"",""Charging electric vehicles (EVs) at home is attractive to EV users. However, when the penetration level of EVs becomes high, a distribution grid suffers from problems such as under-voltage and transformer overloading. EV users also experience a fairness problem, i.e., the limited capacity is unfairly shared among EVs. To solve these problems, a physical fair-queueing framework is established for EV charging. In this framework, a distribution sub-grid is first mapped to a multi-server queueing system, and then a fluid-model based queueing scheme called physical multi-server generalized processor sharing (pMGPS) is designed. pMGPS ensures perfect fairness but cannot be used practically due to its nature of fluid model. To this end, a packetized scheme called physical start-time fair queueing (pSTFQ) is developed to schedule tasks of EV charging. The fairness performance of the pSTFQ scheduling scheme is characterized by the ratio of energy difference between pSTFQ and pMGPS. This critical performance metric is studied through theoretical analysis and is also evaluated via simulations. Performance results show that the pSTFQ scheduling scheme achieves an energy difference ratio of less than 4 percent in various scenarios without causing under-voltage and transformer overloading problems."",""1558-2183"","""",""10.1109/TPDS.2017.2710197"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7937889"",""Electric vehicle charging";distribution grid;"multi-server fair queueing"",""Batteries";Capacitors;Switches;Voltage control;Safety;Schedules;"Charging stations"","""",""17"","""",""24"",""IEEE"",""1 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scientific Workflow Mining in Clouds,""W. Song"; F. Chen; H. -A. Jacobsen; X. Xia; C. Ye;" X. Ma"",""School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China"; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; Middleware Systems Research Group, University of Toronto, Toronto, ON, Canada; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; College of Information Science and Technology, Hainan University, Haikou, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2979"",""2992"",""Computing clouds have become the platform of choice for the deployment and execution of scientific workflows. Due to the uncertainty and unpredictability of scientific exploration, the execution plan for a scientific workflow may vary from the definition. It is therefore of great significance to be able to discover actual workflows from execution histories (event logs) to reproduce experimental results and to establish provenance. However, most existing process mining techniques focus on discovering control flow-oriented business processes in a centralized environment, and thus, they are mostly inapplicable to the discovery of data flow-oriented, unstructured scientific workflows in distributed cloud environments. In this paper, we present Scientific Workflow Mining as a Service (SWMaaS) to support both intra-cloud and inter-cloud scientific workflow mining. The approach is implemented as a ProM plug-in and is evaluated on event logs derived from real-world scientific workflows. Through experimental results, we demonstrate the effectiveness and efficiency of our approach."",""1558-2183"","""",""10.1109/TPDS.2017.2696942"",""National Basic Research Program of China(grant numbers:2015CB352202)"; National Natural Science Foundation of China(grant numbers:61690204,61379047,61202003); Fundamental Research Funds for the Central Universities(grant numbers:30917011322); China Scholarship Council(grant numbers:201606845006); NSERC; Alexander von Humboldt Foundation(grant numbers:5090551); University of Toronto; Technische Universität München;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7907335"",""Scientific workflow";inter-cloud;workflow mining;event log;"direct precedence"",""Business";Process control;Data mining;Petri nets;Electronic mail;Optimization;"PROM"","""",""29"","""",""51"",""IEEE"",""24 Apr 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Securing Coding-Based Cloud Storage Against Pollution Attacks,""C. Anglano"; R. Gaeta;" M. Grangetto"",""DiSIT-Computer Science Institute, Università degli Studi del Piemonte Orientale, Alessandria, Italia"; Dipartimento di Informatica, Università degli Studi di Torino, Torino, Italia;" Dipartimento di Informatica, Università degli Studi di Torino, Torino, Italia"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1457"",""1469"",""The widespread diffusion of distributed and cloud storage solutions has changed dramatically the way users, system designers, and service providers manage their data. Outsourcing data on remote storage provides indeed many advantages in terms of both capital and operational costs. The security of data outsourced to the cloud, however, still represents one of the major concerns for all stakeholders. Pollution attacks, whereby a set of malicious entities attempt to corrupt stored data, are one of the many risks that affect cloud data security. In this paper we deal with pollution attacks in coding-based block-level cloud storage systems, i.e., systems that use linear codes to fragment, encode, and disperse virtual disk sectors across a set of storage nodes to achieve desired levels of redundancy, and to improve reliability and availability without sacrificing performance. Unfortunately, the effects of a pollution attack on linear coding can be disastrous, since a single polluted fragment can propagate pervasively in the decoding phase, thus hampering the whole sector. In this work we show that, using rateless codes, we can design an early pollution detection algorithm able to spot the presence of an attack while fetching the data from cloud storage during the normal disk reading operations. The alarm triggers a procedure that locates the polluting nodes using the proposed detection mechanism along with statistical inference. The performance of the proposed solution is analyzed under several aspects using both analytical modelling and accurate simulation using real disk traces. Our results show that the proposed approach is very robust and is able to effectively isolate the polluters, even in harsh conditions, provided that enough data redundancy is used."",""1558-2183"","""",""10.1109/TPDS.2016.2619686"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7604064"",""Cloud storage";coding;security;integrity;performance;"pollution attack"",""Pollution";Encoding;Cloud computing;Decoding;Detection algorithms;Maintenance engineering;"Redundancy"","""",""15"","""",""25"",""IEEE"",""20 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Seek-Efficient I/O Optimization in Single Failure Recovery for XOR-Coded Storage Systems,""Z. Shen"; J. Shu; P. P. C. Lee;" Y. Fu"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""877"",""890"",""Erasure coding provides an effective means for storage systems to protect against disk failures with low redundancy. One important objective for erasure-coded storage systems is to speed up single disk failure recovery. Previous approaches reduce the amount of read data for recovery by reading only a small subset of data. However, they often incur high disk seeks, which may negate the resulting recovery performance. We propose SIOR, a seek-efficient I/O recovery algorithm for improving the performance of single disk failure recovery. SIOR carefully balances the trade-off between the amount of read data and the number of disk seeks by considering the data layout at the multi-stripe level. It then greedily determines the data to read for recovery using Tabu search. Experiments show that SIOR achieves similar performance to the brute-force enumeration method while keeping high search efficiency. Also, SIOR reduces $31.8\sim 65.1$  percent of disk seeks during recovery and provides up to 150.0 percent recovery speed improvement, when compared to a state-of-the-art greedy recovery approach."",""1558-2183"","""",""10.1109/TPDS.2016.2591040"",""National Natural Science Foundation of China(grant numbers:61232003,61327902,61433008)"; University Grants Committee of Hong Kong(grant numbers:AoE/E-02/08); CUHK(grant numbers:VCF2014007); CUHK;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7511742"",""XOR-Coded storage systems, single failure fecovery, disk seeks, repair traffic, greedy algorithm"",""Encoding";Strips;Distributed databases;Maintenance engineering;Generators;Optimization;"Layout"","""",""6"","""",""38"",""IEEE"",""13 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Seer Grid: Privacy and Utility Implications of Two-Level Load Prediction in Smart Grids,""A. Boustani"; A. Maiti; S. Y. Jazi; M. Jadliwala;" V. Namboodiri"",""Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, KS"; Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, KS; Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, KS; Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, KS;" Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, KS"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""546"",""557"",""We propose “Seer Grid”, a novel two-level energy consumption prediction framework for smart grids, aimed to decrease the trade-off between privacy requirements (of the customer) and data utility requirements (of the energy company (EC)). The first-level prediction at the household level is performed by each smart meter (SM), and the predicted energy consumption pattern (instead of the actual energy usage data) is reported to a cluster head (CH). Then, a second-level prediction at the neighborhood level is done by the CH which predicts the energy spikes in the neighborhood or cluster and shares it with the EC. Our two-level prediction mechanism is designed such that it preserves the correlation between the predicted and actual energy consumption patterns at the cluster level and removes this correlation in the predicted data communicated by each SM to the CH. This maintains the usefulness of the cluster-level energy consumption data communicated to the EC, while preserving the privacy of the household-level energy consumption data against the CH (and thus the EC). Our evaluation results show that Seer Grid is successful in hiding private consumption patterns at the household-level while still being able to accurately predict energy consumption at the neighborhood-level."",""1558-2183"","""",""10.1109/TPDS.2016.2564399"",""Power Systems Engineering Research Center (PSERC)(grant numbers:S-54)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7466137"",""Smart grid";smart meter;load prediction;privacy;"data utility"",""Energy consumption";Data privacy;Batteries;Smart grids;Privacy;Smart meters;"Correlation"","""",""7"","""",""53"",""IEEE"",""6 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Semi-External Memory Sparse Matrix Multiplication for Billion-Node Graphs,""D. Zheng"; D. Mhembere; V. Lyzinski; J. T. Vogelstein; C. E. Priebe;" R. Burns"",""Department of Computer Science, Johns Hopkins University, Baltimore, MD"; Department of Computer Science, Johns Hopkins University, Baltimore, MD; Department of Applied Mathematics and Statistics, Johns Hopkins University, Baltimore, MD; Department of Biomedical Engineering, Johns Hopkins University, Baltimore, MD; Department of Applied Mathematics and Statistics, Johns Hopkins University, Baltimore, MD;" Department of Computer Science, Johns Hopkins University, Baltimore, MD"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1470"",""1483"",""Sparse matrix multiplication is traditionally performed in memory and scales to large matrices using the distributed memory of multiple nodes. In contrast, we scale sparse matrix multiplication beyond memory capacity by implementing sparse matrix dense matrix multiplication (SpMM) in a semi-external memory (SEM) fashion"; i.e., we keep the sparse matrix on commodity SSDs and dense matrices in memory. Our SEM-SpMM incorporates many in-memory optimizations for large power-law graphs. It outperforms the in-memory implementations of Trilinos and Intel MKL and scales to billion-node graphs, far beyond the limitations of memory. Furthermore, on a single large parallel machine, our SEM-SpMM operates as fast as the distributed implementations of Trilinos using five times as much processing power. We also run our implementation in memory (IM-SpMM) to quantify the overhead of keeping data on SSDs. SEM-SpMM achieves almost 100 percent performance of IM-SpMM on graphs when the dense matrix has more than four columns;" it achieves at least 65 percent performance of IM-SpMM on all inputs. We apply our SpMM to three important data analysis tasks-PageRank, eigensolving, and non-negative matrix factorization-and show that our SEM implementations significantly advance the state of the art."",""1558-2183"","""",""10.1109/TPDS.2016.2618791"",""US National Science Foundation(grant numbers:ACI-1261715)"; DARPA; GRAPHS(grant numbers:N66001-14-1-4028); DARPA; SIMPLEX; SPAWAR(grant numbers:N66001-15-C-4041);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7593270"",""Sparse matrix multiplication";semi-external memory;billion-node graphs;"SSDs"",""Sparse matrices";Memory management;Optimization;Matrix decomposition;Algorithm design and analysis;Registers;"Engines"","""",""14"","""",""57"",""IEEE"",""19 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Semi-Online Algorithms for Computational Task Offloading with Communication Delay,""J. P. Champati";" B. Liang"",""Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada";" Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1189"",""1201"",""We study the scheduling of computational tasks on one local processor and one remote processor with communication delay. This problem has important application in cloud computing. Although the communication time to transmit a task can be inferred from the known data size of the task and the transmission bandwidth, the processing time of the task is generally unknown until it is processed to completion. Given a set of independent tasks with unknown processing times, our objective is to minimize makespan. We study the problem under two scenarios: (1) the communication times of the tasks to the remote processor are smaller than their corresponding processing times on the remote processor, and (2) the communication times of the tasks to the remote processor are larger than their corresponding processing times on the remote processor. For the first scenario we propose the Semi-online Partitioning and Communication (SPaC) algorithm, and for the second scenario we propose the SPaC-Restart (SPaC-R) algorithm. Even though the offline version of this problem, with a priori known processing times, is NP-hard, we show that the proposed semionline algorithms achieve O(1) competitive ratios for their intended scenarios. We also provide competitive ratios for both algorithms for more general communication times. We use simulation to demonstrate that SPaC and SPaC-R outperform online list scheduling and performs comparably well with the best known offline heuristics."",""1558-2183"","""",""10.1109/TPDS.2016.2605684"",""Natural Sciences and Engineering Research Council of Canada(grant numbers:STPGP-447497,RGPIN-2015-05506)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7559817"",""Computational offloading";mobile cloud computing;computation with communication;"semi-online algorithms"",""Delays";Processor scheduling;Cloud computing;Scheduling;Computational modeling;Optimal scheduling;"Partitioning algorithms"","""",""20"","""",""31"",""IEEE"",""2 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Service-Oriented Architecture on FPGA-Based MPSoC,""C. Wang"; X. Li; Y. Chen; Y. Zhang; O. Diessel;" X. Zhou"",""University of Science and Technology of China, Hefei, Anhui, China"; University of Science and Technology of China, Hefei, Anhui, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, Tsinghua University, Beijing, China; University of New South Wales, Sydney, Australia;" University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""2993"",""3006"",""The integration of software services-oriented architecture (SOA) and hardware multiprocessor system-on-chip (MPSoC) has been pursued for several years. However, designing and implementing a service-oriented system for diverse applications on a single chip has posed significant challenges due to the heterogeneous architectures, programming interfaces, and software tool chains. To solve the problem, this paper proposes SoSoC, a service-oriented system-on-chip framework that integrates both embedded processors and software defined hardware accelerators s as computing services on a single chip. Modeling and realizing the SOA design principles, SoSoC provides well-defined programming interfaces for programmers to utilize diverse computing resources efficiently. Furthermore, SoSoC can provide task level parallelization and significant speedup to MPSoC chip design paradigms by providing out-of-order execution scheme with hardware accelerators. To evaluate the performance of SoSoC, we implemented a hardware prototype on Xilinx Virtex5 FPGA board with EEMBC benchmarks. Experimental results demonstrate that the service componentization over original version is less than 3 percent, while the speedup for typical software Benchmarks is up to 372x. To show the portability of SoSoC, we implement the convolutional neural network as a case study on both Xilinx Zynq and Altera DE5 FPGA boards. Results show the SoSoC outperforms state-of-the-art literature with great flexibility."",""1558-2183"","""",""10.1109/TPDS.2017.2701828"",""National Science Foundation of China(grant numbers:61379040)"; Anhui Provincial Natural Science Foundation(grant numbers:1608085QF12); CCF(grant numbers:CCF-VenustechRP1026002); Suzhou Research Foundation(grant numbers:SYG201625); Youth Innovation Promotion Association CAS(grant numbers:2017497); Fundamental Research Funds for the Central Universities(grant numbers:WK2150110003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7920399"",""Service-oriented architecture";multiprocessor;"system on chip"",""Service-oriented architecture";Computer architecture;Hardware;Field programmable gate arrays;Semiconductor optical amplifiers;"Programming"","""",""35"","""",""34"",""IEEE"",""5 May 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Sharing-Aware Online Virtual Machine Packing in Heterogeneous Resource Clouds,""S. Rampersaud";" D. Grosu"",""Department of Computer Science, Wayne State University, 5057 Woodward Avenue, Detroit, MI";" Department of Computer Science, Wayne State University, 5057 Woodward Avenue, Detroit, MI"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""2046"",""2059"",""One of the key problems that cloud providers need to efficiently solve when offering on-demand virtual machine (VM) instances to a large number of users is the VM Packing problem, a variant of Bin Packing. The VM Packing problem requires determining the assignment of user requested VM instances to physical servers such that the number of physical servers is minimized. In this paper, we consider a more general variant of the VM Packing problem, called the Sharing-Aware VM Packing problem, that has the same objective as the standard VM Packing problem, but allows the VM instances collocated on the same physical server to share memory pages, thus reducing the amount of cloud resources required to satisfy the users' demand. Our main contributions consist of designing several online algorithms for solving the Sharing-Aware VM Packing problem, and performing an extensive set of experiments to compare their performance against that of several existing sharing-oblivious online algorithms. For small problem instances, we also compare the performance of the proposed online algorithms against the optimal solution obtained by solving the offline variant of the Sharing-Aware VM Packing problem (i.e., the version of the problem that assumes that the set of VM requests are known a priori). The experimental results show that our proposed sharing-aware online algorithms activate a smaller average number of physical servers relative to the sharing-oblivious algorithms, directly reduce the amount of required memory, and thus, require fewer physical servers to instantiate the VM instances requested by users."",""1558-2183"","""",""10.1109/TPDS.2016.2641937"",""National Science Foundation(grant numbers:DGE-0654014,CNS-1116787)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792170"",""Clouds";heterogeneous resource;multilinear;online algorithm;sharing-aware;vector bin packing;"virtual machine"",""Servers";Algorithm design and analysis;Cloud computing;Virtual machining;Memory management;Resource management;"Virtualization"","""",""32"","""",""35"",""IEEE"",""20 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Simulating MPI Applications: The SMPI Approach,""A. Degomme"; A. Legrand; G. S. Markomanolis; M. Quinson; M. Stillwell;" F. Suter"",""Basel University, Basel, Switzerland"; Univ. Grenoble Alpes, CNRS, Inria, Grenoble INP, Grenoble, France; Supercomputing Laboratory of King Abdullah University of Science and Technology, Jeddah, Saudi Arabia; IRISA laboratory (ENS Rennes/Université de Rennes 1/Inria/CNRS), Rennes, France; Imperial College London, London, United Kindom;" Inria, ENS Lyon, Lyon, France"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2387"",""2400"",""This article summarizes our recent work and developments on SMPI, a flexible simulator of MPI applications. In this tool, we took a particular care to ensure our simulator could be used to produce fast and accurate predictions in a wide variety of situations. Although we did build SMPI on SimGrid whose speed and accuracy had already been assessed in other contexts, moving such techniques to a HPC workload required significant additional effort. Obviously, an accurate modeling of communications and network topology was one of the key to such achievements. Another less obvious key was the choice to combine in a single tool the possibility to do both offline and online simulation."",""1558-2183"","""",""10.1109/TPDS.2017.2669305"",""ANR(grant numbers:11-ANR-INFR-13)"; CNRS(grant numbers:5473); EC(grant numbers:288777); PRACE; EC(grant numbers:RI-261557,RI-283493); CNRS; RENATER;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7855780"",""Simulation";MPI runtime and applications;"performance prediction and extrapolation"",""Computational modeling";Hardware;Adaptation models;Runtime;Extrapolation;Standards;"Network topology"","""",""35"","""",""49"",""IEEE"",""14 Feb 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Solving k-Set Agreement Using Failure Detectors in Unknown Dynamic Networks,""É. Jeanneau"; T. Rieutord; L. Arantes;" P. Sens"",""UPMC Univ Paris 06, Sorbonne Universités, Paris, France"; INFRES, Telecom ParisTech, Paris, France; UPMC Univ Paris 06, Sorbonne Universités, Paris, France;" UPMC Univ Paris 06, Sorbonne Universités, Paris, France"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1484"",""1499"",""The failure detector abstraction has been used to solve agreement problems in asynchronous systems prone to crash failures, but so far it has mostly been used in static and complete networks. This paper aims to adapt existing failure detectors in order to solve agreement problems in unknown, dynamic systems. We are specifically interested in the k-set agreement problem. The problem of k-set agreement is a generalization of consensus where processes can decide up to k different values. Although some solutions to this problem have been proposed in dynamic networks, they rely on communication synchrony or make strong assumptions on the number of process failures. In this paper we consider unknown dynamic systems modeled using the formalism of Time-Varying Graphs, and extend the definition of the existing $\Pi \Sigma _{x,y}$  failure detector to obtain the $\Pi \Sigma _{\bot, x,y}$  failure detector, which is sufficient to solve k-set agreement in our model. We then provide an implementation of this new failure detector using connectivity and message pattern assumptions. Finally, we present an algorithm using  $\Pi \Sigma _{\bot, x,y}$  to solve k-set agreement."",""1558-2183"","""",""10.1109/TPDS.2016.2608829"",""Labex"; SMART; ANR; Investissements d’Avenir(grant numbers:ANR-11-LABX-65); ANR; DISCMAT(grant numbers:N ANR-14-CE35-0010-01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7565628"",""Distributed systems";dynamic networks;failure detectors;"k-set agreement"",""Detectors";Computer crashes;Adaptation models;Heuristic algorithms;Time-varying systems;Protocols;"Indexes"","""",""2"","""",""30"",""IEEE"",""13 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"State-Machine and Deferred-Update Replication: Analysis and Comparison,""P. T. Wojciechowski"; T. Kobus;" M. Kokociński"",""Institute of Computing Science, Poznań University of Technology, Poznań, Poland"; Institute of Computing Science, Poznań University of Technology, Poznań, Poland;" Institute of Computing Science, Poznań University of Technology, Poznań, Poland"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""891"",""904"",""In the paper, we analyze and experimentally compare two popular replication schemes relying on atomic broadcast: state machine replication (SMR) and deferred update replication (DUR). We estimate the lower bounds on the time of executing requests by the SMR and DUR systems running on multi-core servers. We also consider variants of systems that can process read-only requests with a lower overhead. In the analysis of DUR, we consider conflict patterns. We then formally show the scalability of SMR and DUR, which reflects the capacity of systems to effectively utilize an increasing number of processor cores. Next, we compare SMR and DUR experimentally under different levels of contention, using several benchmarks. We show throughput, abort rate (in DUR), and network congestion. The key results of our work are that neither system is superior in all cases, and that the theoretical and experimental results are heavily influenced by the dominance of either the CPU execution time or atomic broadcast time. We therefore propose to combine both replication schemes and gain the best of both worlds."",""1558-2183"","""",""10.1109/TPDS.2016.2590422"",""Pozna Supercomputing and Networking Center (PSNC)"; National Science Centre(grant numbers:DEC-2012/06/M/ST6/00463,DEC-2011/01/N/ST6/06762);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7508994"",""State machine replication";deferred update replication;"distributed transactional memory"",""Protocols";Servers;Analytical models;Scalability;Multicore processing;Databases;"Algorithm design and analysis"","""",""5"","""",""45"",""IEEE"",""11 Jul 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Stochastic Resource Provisioning for Containerized Multi-Tier Web Services in Clouds,""O. Adam"; Y. C. Lee;" A. Y. Zomaya"",""School of Information Technologies, J12, University of Sydney, Camperdown, Australia"; Department of Computing, Macquarie University, Sydney, Australia;" School of Information Technologies, J12, University of Sydney, Camperdown, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""2060"",""2073"",""Under today's bursty web traffic, the fine-grained per-container control promises more efficient resource provisioning for web services and better resource utilization in cloud datacenters. In this paper, we present Two-stage Stochastic Programming Resource Allocator (2SPRA). It optimizes resource provisioning for containerized n-tier web services in accordance with fluctuations of incoming workload to accommodate predefined SLOs on response latency. In particular, 2SPRA is capable of minimizing resource over-provisioning by addressing dynamics of web traffic as workload uncertainty in a native stochastic optimization model. Using special-purpose OpenOpt optimization framework, we fully implement 2SPRA in Python and evaluate it against three other existing allocation schemes, in a Docker-based CoreOS Linux VMs on Amazon EC2. We generate workloads based on four real-world web traces of various traffic variations: AOL, WorldCup98, ClarkNet, and NASA. Our experimental results demonstrate that 2SPRA achieves the minimum resource over-provisioning outperforming other schemes. In particular, 2SPRA allocates only 6.16 percent more than application's actual demand on average and at most 7.75 percent in the worst case. It achieves 3x further reduction in total resources provisioned compared to other schemes delivering overall cost-savings of 53.6 percent on average and up to 66.8 percent. Furthermore, 2SPRA demonstrates consistency in its provisioning decisions and robust responsiveness against workload fluctuations."",""1558-2183"","""",""10.1109/TPDS.2016.2639009"",""Amazon Web Services(grant numbers:AWS in Education Research Grants)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7782339"",""Cloud resources provisioning";workload uncertainty;stochastic optimization;multi-tier web applications;"containers"",""Containers";Stochastic processes;Web services;Servers;Optimization;Uncertainty;"Linux"","""",""37"","""",""48"",""IEEE"",""13 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Survey on Real-Time Networks-on-Chip,""S. Hesham"; J. Rettkowski; D. Goehringer;" M. A. Abd El Ghany"",""German University in Cairo, Cairo, Egypt"; Ruhr-University Bochum, Bochum, Germany; Ruhr-University Bochum, Bochum, Germany;" German University in Cairo, Cairo, Egypt"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Apr 2017"",""2017"",""28"",""5"",""1500"",""1517"",""Multi-Processor Systems-on-Chip (MPSoCs) have emerged as an evolution trend to meet the growing complexity of embedded applications with increasing computation parallelism. Particularly, real-time applications make out a significant portion of the embedded field. Networks-on-Chip (NoCs) are the backbone of communications in an MPSoC platform. However, the use of NoCs in real-time systems imposes complex constraints on the overall design. This paper discusses the challenges faced, when designing NoCs for real-time applications. Contributions in this area are surveyed on the level of guaranteed Quality-of-Service (QoS) support, adaptivity, and energy efficient techniques. Furthermore, the evaluation methodologies and experimental performance measurements of real-time NoCs are examined. This survey provides a comprehensive overview of existing endeavors in real-time NoCs and gives an insight towards future promising research points in this field."",""1558-2183"","""",""10.1109/TPDS.2016.2623619"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728147"",""Multi-core/single-chip multiprocessors";on-chip interconnection networks;power management;"real-time and embedded systems"",""Real-time systems";Nickel;Quality of service;Energy efficiency;IP networks;Computer architecture;"Delays"","""",""56"","""",""104"",""IEEE"",""1 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Task Scheduling Techniques for Asymmetric Multi-Core Systems,""K. Chronaki"; A. Rico; M. Casas; M. Moretó; R. M. Badia; E. Ayguadé; J. Labarta;" M. Valero"",""Barcelona Supercomputing Center and Universitat Polytéchnica de Catalunya, Barcelona, Spain"; ARM, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center and Artificial Intelligence Research Institute (IIIA) - Spanish National Research Council (CSIC), Barcelona, Spain; Barcelona Supercomputing Center and Universitat Polytéchnica de Catalunya, Barcelona, Spain; Barcelona Supercomputing Center and Universitat Polytéchnica de Catalunya, Barcelona, Spain;" Barcelona Supercomputing Center and Universitat Polytéchnica de Catalunya, Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""2074"",""2087"",""As performance and energy efficiency have become the main challenges for next-generation high-performance computing, asymmetric multi-core architectures can provide solutions to tackle these issues. Parallel programming models need to be able to suit the needs of such systems and keep on increasing the application’s portability and efficiency. This paper proposes two task scheduling approaches that target asymmetric systems. These dynamic scheduling policies reduce total execution time either by detecting the longest or the critical path of the dynamic task dependency graph of the application, or by finding the earliest executor of a task. They use dynamic scheduling and information discoverable during execution, fact that makes them implementable and functional without the need of off-line profiling. In our evaluation we compare these scheduling approaches with two existing state-of the art heterogeneous schedulers and we track their improvement over a FIFO baseline scheduler. We show that the heterogeneous schedulers improve the baseline by up to 1.45$\times$  in a real 8-core asymmetric system and up to 2.1$\times$  in a simulated 32-core asymmetric chip."",""1558-2183"","""",""10.1109/TPDS.2016.2633347"",""Spanish Government(grant numbers:SEV2015-0493)"; Spanish Ministry of Science and Innovation(grant numbers:TIN2015-65316-P); Generalitat de Catalunya(grant numbers:2014-SGR-1051,2014-SGR-1272); ERC(grant numbers:GA 321253); European HiPEAC Network of Excellence; EU’s Seventh Framework Programme(grant numbers:FP7/2007-2013,610402); EU’s H2020 Framework Programme(grant numbers:H2020/2014-2020,671697); Ministry of Economy and Competitiveness; Juan de la Cierva postdoctoral fellowship(grant numbers:JCI-2012-15047); Secretary for Universities and Research; Ministry of Economy and Knowledge; Government of Catalonia; 7th R&D Framework Programme of the European Union(grant numbers:2013 BP_B 00243);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7762236"",""Scheduling";heterogeneous;"multi-core"",""Dynamic scheduling";Cats;Programming;Runtime;Processor scheduling;Computer architecture;"Electronic mail"","""",""32"","""",""33"",""IEEE"",""29 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"TC-Release++: An Efficient Timestamp-Based Coherence Protocol for Many-Core Architectures,""Y. Yao"; W. Chen; T. Mitra;" Y. Xiang"",""School of Computer Science and Technology, Zhejiang University, Hangzhou, P.R. China"; School of Computer Science and Technology, Zhejiang University, Hangzhou, P.R. China; School of Computing, National University of Singapore, Singapore;" Swinburne Research, Swinburne University of Technology, Hawthorn, Victoria, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2017"",""2017"",""28"",""11"",""3313"",""3327"",""As we enter the era of many-core, providing the shared memory abstraction through cache coherence has become progressively difficult. The standard directory-based coherence does not scale well with increasing core count. Timestamp-based hardware coherence protocols introduced recently offer an attractive alternative solution. This paper proposes a timestamp-based coherence protocol, called TC-Release++, that efficiently supports cache coherence in large-scale systems. Our approach is inspired by TC-Weak, a recently proposed timestamp-based coherence protocol targeting GPU architectures. We first design TC-Release in an attempt to straightforwardly port TC-Weak to general-purpose many-cores. But re-purposing TC-Weak for general-purpose many-core architectures is challenging due to significant differences both in architecture and the programming model. Indeed the performance of TC-Release turns out to be worse than conventional directory protocols. We overcome the limitations and overheads of TC-Release by exploiting simple hardware support to eliminate frequent memory stalls, and an optimized lifetime prediction mechanism to improve cache performance. The resulting optimized coherence protocol TC-Release++ is highly scalable (storage scales logarithmically with core count) and shows better performance (3.0 percent) and comparable network traffic (within 1.3 percent) relative to the baseline MESI directory protocol. We use Murphi to formally verify that TC-Release++ is error-free and imposes small verification cost."",""1558-2183"","""",""10.1109/TPDS.2017.2719679"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7959101"",""Cache coherence";many-core architecture;timestamp-based system;"memory consistency model"",""Coherence";Protocols;Graphics processing units;Hardware;Memory management;Programming;"Electronic mail"","""",""2"","""",""43"",""IEEE"",""26 Jun 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"The Importance of Worker Reputation Information in Microtask-Based Crowd Work Systems,""A. Tarable"; A. Nordio; E. Leonardi;" M. A. Marsan"",""CNR-IEIIT, Torino, Italy"; CNR-IEIIT, Torino, Italy; Research Associates with CNR-IEIIT, Torino, Italy;" Research Associates with CNR-IEIIT, Torino, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""558"",""571"",""This paper presents the first systematic investigation of the potential performance gains for crowd work systems, deriving from available information at the requester about individual worker reputation. In particular, we first formalize the optimal task assignment problem when workers' reputation estimates are available, as the maximization of a monotone (submodular) function subject to Matroid constraints. Then, being the optimal problem NP-hard, we propose a simple but efficient greedy heuristic task allocation algorithm. We also propose a simple “maximum a-posteriori” decision rule and a decision algorithm based on message passing. Finally, we test and compare different solutions, showing that system performance can greatly benefit from information about workers' reputation. Our main findings are that: i) even largely inaccurate estimates of workers' reputation can be effectively exploited in the task assignment to greatly improve system performance"; ii) the performance of the maximum a-posteriori decision rule quickly degrades as worker reputation estimates become inaccurate;" iii) when workers' reputation estimates are significantly inaccurate, the best performance can be obtained by combining our proposed task assignment algorithm with the message-passing decision algorithm."",""1558-2183"","""",""10.1109/TPDS.2016.2572078"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7478121"",""Human-centered computing";human information processing;"systems and information theory"",""Reliability";Crowdsourcing;Resource management;Maximum a posteriori estimation;System performance;Systematics;"Throughput"","""",""3"","""",""40"",""IEEE"",""24 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"The Unicorn Runtime: Efficient Distributed Shared Memory Programming for Hybrid CPU-GPU Clusters,""T. Beri"; S. Bansal;" S. Kumar"",""Department of Computer Science and Engineering, Indian Institute of Technology, Delhi, India"; Department of Computer Science and Engineering, Indian Institute of Technology, Delhi, India;" Department of Computer Science and Engineering, Indian Institute of Technology, Delhi, India"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Apr 2017"",""2017"",""28"",""5"",""1518"",""1534"",""Programming hybrid CPU-GPU clusters is hard. This paper addresses this difficulty and presents the design and runtime implementation of Unicorn-a parallel programming model for hybrid CPU-GPU clusters. In particular, this paper proves that efficient distributed shared memory style programing is possible and its simplicity can be retained across CPUs and GPUs in a cluster, minus the frustration of dealing with race conditions. Further, this can be done with a unified abstraction, avoiding much of the complication of dealing with hybrid architectures. This is achieved with the help of transactional semantics (on shared global address spaces), deferred bulk data synchronization, workload pipelining and various communication and computation scheduling optimizations. We describe the said abstraction, our computation and communication scheduling system and report its performance on a few benchmarks like Matrix Multiplication, LU Decomposition and 2D FFT. We find that parallelization of coarse-grained applications like matrix multiplication or 2D FFT using our system requires only about 30 lines of C code to set up the runtime. The rest of the application code is regular single CPU/GPU implementation. This indicates the ease of extending parallel code to a distributed environment. The execution is efficient as well. When multiplying two square matrices of size 65, 536 χ 65,536, Unicornachieves a peak performance of 7.88 TFlop/s when run over a cluster of 14 nodes with each node equipped with two Tesla M2070 GPUs and two 6-core Intel Xeon 2.67 GHz CPUs, connected over a 32Gbps Infiniband network. In this paper, we also demonstrate that the Unicorn programming model can be efficiently used to implement high level abstractions like MapReduce. We use such an extension to implement PageRank and report its performance. For a sample web of 500 million web pages, our implementation completes a page rank iteration in about 18 seconds (on average) on a 14-node cluster."",""1558-2183"","""",""10.1109/TPDS.2016.2616314"",""Ministry of Earth Sciences";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7588161"",""Unicorn runtime";distributed system design;scheduling;load balancing;accelerators;"bulk synchronous parallelism"",""Runtime";Programming;Optimization;Processor scheduling;Kernel;Synchronization;"Graphics processing units"","""",""9"","""",""40"",""IEEE"",""11 Oct 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Time Series-Oriented Load Prediction Model and Migration Policies for Distributed Simulation Systems,""R. E. De Grande"; A. Boukerche;" R. Alkharboush"",""School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada"; School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada;" School of Electrical Engineering and Computer Science, University of Ottawa, Ottawa, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""215"",""229"",""HLA-based simulation systems are prone to load imbalances due to lack management of shared resources in distributed environments. Such imbalances lead these simulations to exhibit performance loss in terms of execution time. As a result, many dynamic load balancing systems have been introduced to manage distributed load. These systems use specific methods, depending on load or application characteristics, to perform the required balancing. Load prediction is a technique that has been used extensively to enhance load redistribution heuristics towards preventing load imbalances. In this paper, several efficient Time Series model variants are presented and used to enhance prediction precision for large-scale distributed simulation-based systems. These variants are proposed to extend and correct the issues originating from the implementation of Holt's model for time series in the predictive module of a dynamic load balancing system for HLA-based distributed simulations. A set of migration decision-making techniques is also proposed to enable a prediction-based load balancing system to be independent of any prediction model, promoting a more modular construction."",""1558-2183"","""",""10.1109/TPDS.2016.2552174"",""Canada Research Chairs Program"; NSERC; DIVA Strategic Networks;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7450171"",""Load balancing";prediction;holt’s model;"high level architecture"",""Load modeling";Predictive models;Load management;Computational modeling;Data models;Time series analysis;"Atmospheric modeling"","""",""14"","""",""35"",""IEEE"",""8 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Time-Reversibility for Real-Time Scheduling on Multiprocessor Systems,""J. Lee"",""Department of Computer Science and Engineering, Sungkyunkwan University, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""230"",""243"",""The real-time systems community has widely studied real-time scheduling, focusing on how to guarantee schedulability (i.e., timely execution) of a set of real-time tasks. However, there still exist a number of task sets that are actually schedulable by a target scheduling algorithm, but proven schedulable by none of existing schedulability tests, especially on a multiprocessor. In this paper, we propose a new paradigm for real-time scheduling, called time-reversibility, which views real-time scheduling under a change in the sign of time, and present how to utilize the paradigm for schedulability improvement. To this end, we first define the notion of a time-reversed scheduling algorithm and a time-reversible schedulability test";" for example, the time-reversed scheduling algorithm against Earliest Deadline First (EDF) is Latest Release-time First (LRF). Then, we develop time-reversibility theories for schedulability improvement, which utilizes the definitions so as to compose schedulability. Finally, we generalize the definitions and theories to job-level dynamic-priority scheduling in which the priority of a job may vary with time, such as Earliest Deadline first until Zero Laxity (EDZL). Specifically, we accommodate time-varying job parameters to the time-reversibility definitions, and adapt the time-reversibility theories for the additional necessary deadline-miss conditions specialized for a class of job-level dynamic-priority scheduling algorithms. As case studies, we demonstrate that the time-reversibility theories help to find up to 13.6 percent additional EDFand EDZL-schedulable task sets."",""1558-2183"","""",""10.1109/TPDS.2016.2533615"",""Basic Science Research Program"; National Research Foundation of Korea; Ministry of Science, ICT & Future Planning(grant numbers:NRF-2014R1A1A1035827);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7416238"",""Real-time scheduling";schedulability analysis;"time-reversibility"",""Scheduling algorithms";Real-time systems;Dynamic scheduling;Complexity theory;"Program processors"","""",""6"","""",""23"",""IEEE"",""23 Feb 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TLB-Based Temporality-Aware Classification in CMPs with Multilevel TLBs,""A. Esteve"; A. Ros; M. E. Gómez; A. Robles;" J. Duato"",""Department of Computer Engineering, Universitat Politècnica de València, València, Spain"; Departamento de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Department of Computer Engineering, Universitat Politècnica de València, València, Spain; Department of Computer Engineering, Universitat Politècnica de València, València, Spain;" Department of Computer Engineering, Universitat Politècnica de València, València, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2017"",""2017"",""28"",""8"",""2401"",""2413"",""Recent proposals are based on classifying memory accesses into private or shared in order to process private accesses more efficiently and reduce coherence overhead. The classification mechanisms previously proposed are either not able to adapt to the dynamic sharing behavior of the applications or require frequent broadcast messages. Additionally, most of these classification approaches assume single-level translation lookaside buffers (TLBs). However, deeper and more efficient TLB hierarchies, such as the ones implemented in current commodity processors, have not been appropriately explored. This paper analyzes accurate classification mechanisms in multilevel TLB hierarchies. In particular, we propose an efficient data classification strategy for systems with distributed shared last-level TLBs. Our approach classifies data accounting for temporal private accesses and constrains TLB-related traffic by issuing unicast messages on first-level TLB misses. When our classification is employed to deactivate coherence for private data in directory-based protocols, it improves the directory efficiency and, consequently, reduces coherence traffic to merely 53.0 percent, on average. Additionally, it avoids some of the overheads of previous classification approaches for purely private TLBs, improving average execution time by nearly 9 percent for large-scale systems."",""1558-2183"","""",""10.1109/TPDS.2017.2658576"",""Ministerio de Economía y Competitividad"; European Commission(grant numbers:TIN2015-66972-C5-1-R,TIN2015-66972-C5-3-R); Fundación Seneca-Agencia de Ciencia y Tecnología de la Región de Murcia; Jóvenes Líderes en Investigación(grant numbers:18956/JLI/13);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7833158"",""Distributed shared TLB";data classification;TLB usage predictor;"coherence deactivation"",""Coherence";Program processors;Protocols;Proposals;Maintenance engineering;Electronic mail;"Organizations"","""",""7"","""",""45"",""IEEE"",""25 Jan 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Toward an Optimal Online Checkpoint Solution under a Two-Level HPC Checkpoint Model,""S. Di"; Y. Robert; F. Vivien;" F. Cappello"",""Mathematics and Computer Science (MCS) Division at Argonne National Laboratory, Chicago, IL"; University of Tennessee Knoxville; Laboratoire LIP, CNRS, ENS Lyon, INRIA, UCB Lyon, Lyon, France;" Mathematics and Computer Science (MCS) Division at Argonne National Laboratory, Chicago, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Dec 2016"",""2017"",""28"",""1"",""244"",""259"",""The traditional single-level checkpointing method suffers from significant overhead on large-scale platforms. Hence, multilevel checkpointing protocols have been studied extensively in recent years. The multilevel checkpoint approach allows different levels of checkpoints to be set (each with different checkpoint overheads and recovery abilities), in order to further improve the fault tolerance performance of extreme-scale HPC applications. How to optimize the checkpoint intervals for each level, however, is an extremely difficult problem. In this paper, we construct an easy-to-use two-level checkpoint model. Checkpoint level 1 deals with errors with low checkpoint/recovery overheads such as transient memory errors, while checkpoint level 2 deals with hardware crashes such as node failures. Compared with previous optimization work, our new optimal checkpoint solution offers two improvements: (1) it is an online solution without requiring knowledge of the job length in advance, and (2) it shows that periodic patterns are optimal and determines the best pattern. We evaluate the proposed solution and compare it with the most up-to-date related approaches on an extreme-scale simulation testbed constructed based on a real HPC application execution. Simulation results show that our proposed solution outperforms other optimized solutions and can improve the performance significantly in some cases. Specifically, with the new solution the wall-clock time can be reduced by up to 25.3 percent over that of other state-of-the-art approaches. Finally, a brute-force comparison with all possible patterns shows that our solution is always within 1 percent of the best pattern in the experiments."",""1558-2183"","""",""10.1109/TPDS.2016.2546248"",""European project SCoRPiO"; Institut Universitaire de France; US Department of Energy; Office of Science; Advanced Scientific Computing Research Program(grant numbers:DE-AC02-06CH11357); US Department of Energy Office of Science laboratory(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7442859"",""High-performance computing";fault tolerance;optimization;"multilevel checkpoint"",""Checkpointing";Fault tolerance;Fault tolerant systems;Optimization;Computational modeling;Transient analysis;"Computer crashes"","""",""34"","""",""29"",""IEEE"",""29 Mar 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Toward General Software Level Silent Data Corruption Detection for Parallel Applications,""E. Berrocal"; L. Bautista-Gomez; S. Di; Z. Lan;" F. Cappello"",""Department of Computer Science, Illinois Institute of Technology, Chicago, IL"; Barcelona Supercomputing Center, Barcelona, Spain; Argonne National Laboratory, Lemont, IL; Department of Computer Science, Illinois Institute of Technology, Chicago, IL;" Argonne National Laboratory, Lemont, IL"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3642"",""3655"",""Silent data corruption (SDC) poses a great challenge for high-performance computing (HPC) applications as we move to extreme-scale systems. Mechanisms have been proposed that are able to detect SDC in HPC applications by using the peculiarities of the data (more specifically, its “smoothness” in time and space) to make predictions. However, these data-analytic solutions are still far from fully protecting applications to a level comparable with more expensive solutions such as full replication. In this work, we propose partial replication to overcome this limitation. More specifically, we have observed that not all processes of an MPI application experience the same level of data variability at exactly the same time. Thus, we can smartly choose and replicate only those processes for which the lightweight data-analytic detectors would perform poorly. In addition, we propose a new evaluation method based on the probability that a corruption will pass unnoticed by a particular detector (instead of just reporting overall single-bit precision and recall). In our experiments, we use four applications dealing with different explosions. Our results indicate that our new approach can protect the MPI applications analyzed with 7-70 percent less overhead (depending on the application) than that of full duplication with similar detection recall."",""1558-2183"","""",""10.1109/TPDS.2017.2735971"",""Illinois Institute of Technology"; U.S. National Science Foundation(grant numbers:CNS-1320125,CCF-1422009); U.S. Department of Energy; Office of Science; Advanced Scientific Computing Research Program(grant numbers:DE-AC02-06CH11357); ANR; RESCUE(grant numbers:INRIA-Illinois-ANL- BSC-JSC-Riken);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8002625"",""Silent data corruption detection";partial replication;data analysis;high-performance computing;"parallel applications"",""High performance computing";Parallel processing;Random access memory;Electronic mail;"Measurement"","""",""10"","""",""42"",""IEEE"",""4 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Towards Long-View Computing Load Balancing in Cluster Storage Systems,""G. Liu"; H. Shen;" H. Wang"",""Department of ECE, Clemson University, Clemson, SC"; Department of Computer Science, University of Virginia, Charlottesville, VA;" Department of Computer Science, University of Virginia, Charlottesville, VA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1770"",""1784"",""In large-scale computing clusters, when the server storing a task's requested data does not have sufficient computing capacity for the task, current job schedulers either schedule the task to the closest server and transmit to it the requested data, or let the task wait until the server has sufficient computing capacity. The former solution generates network load while the latter solution increases task delay. To handle this problem, load balancing methods are needed to reduce the number of overloaded servers due to computing workloads. However, current load balancing methods do not aim to balance the computing load for the long term. Through trace analysis, we demonstrate the diversity of computing workloads of different tasks and the necessity of balancing the computing workloads among servers. Then, we propose a cost-efficient Computing load Aware and Long-View load balancing approach (CALV). CALVis novel in that it achieves long-term computing load balance by migrating out an overloaded server data blocks contributing more computing workloads when the server is more overloaded and contribute less computing workloads when the server is more underloaded at different epochs during a time period. Based upon the task schedules, we further propose a task reassignment algorithm that reassigns tasks from an overloaded server to other data servers of the tasks to make it non-overloaded before CALVis conducted. The above methods are for the tasks whose submission times and execution latencies can be predicted. To handle unexpected tasks or insufficiently accurate predictions, we propose a dynamic load balancing method, in which an overloaded server dynamically redirects tasks to other data servers of the tasks, or replicates the tasks' requested data to other servers and redirects the tasks to those servers in order to become non-overloaded. Finally, we propose a proximity-aware tree based distributed load balancing method to reduce the reallocation cost and improve the scalability of CALV. Trace-driven experiments in simulation and a real computing cluster show that CALV outperforms other methods in terms of balancing the computing workloads and cost efficiency."",""1558-2183"","""",""10.1109/TPDS.2016.2632713"",""US National Science Foundation(grant numbers:NSF-1404981,IIS-1354123,CNS-1254006)"; IBM Faculty(grant numbers:5501145); Microsoft Research Faculty Fellowship(grant numbers:8300751);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7756366"",""Computing cluster";data allocation;load balancing;"data locality"",""Servers";Load management;Delays;Schedules;Distributed databases;Scalability;"Computational modeling"","""",""6"","""",""39"",""IEEE"",""24 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Towards the Tradeoffs in Designing Data Center Network Architectures,""D. Li"; J. Wu; Z. Liu;" F. Zhang"",""Department of Computer and Information Sciences, Temple University, Philadelphia, PA"; Department of Computer and Information Sciences, Temple University, Philadelphia, PA; State Key Laboratory for Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China;" Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""260"",""273"",""Existing Data Center Network (DCN) architectures are classified into two categories: switch-centric and server-centric architectures. In switch-centric DCNs, routing intelligence is placed on switches"; each server usually uses only one port of the Network Interface Card (NIC) to connect to the network. In server-centric DCNs, switches are only used as cross-bars, and routing intelligence is placed on servers, where multiple NIC ports may be used. In this paper, we formally introduce a new category of DCN architectures: the dual-centric DCN architectures, where routing intelligence can be placed on both switches and servers. The dual-centric philosophy can achieve various tradeoffs in designing DCN architectures. We propose three novel dual-centric DCN architectures: FCell, FRectangle, and FSquare, all of which are based on the folded Clos topology. FCell is a power-efficient DCN architecture, with a larger diameter and lower bisection bandwidth than FSquare and FRectangle. FSquare is a high performance DCN architecture, in which the diameter is small and the bisection bandwidth is large;" however, the DCN power consumption per server in FSquare is high. FRectangle significantly reduces the DCN power consumption per server, compared to FSquare, at the sacrifice of some networking performances. By investigating FCell, FRectangle and FSquare, and by comparing them with existing architectures, we demonstrate that, the three novel dual-centric architectures enjoy the advantages of both switch-centric designs and server-centric designs, have various nice properties for practical data centers, and provide flexible tradeoff choices in designing DCN architectures."",""1558-2183"","""",""10.1109/TPDS.2016.2610970"",""US National Science Foundation(grant numbers:CNS 1449860,CNS 1461932,CNS 1460971,CNS 1439672,CNS 1301774,ECCS 1231461)"; NSFC(grant numbers:61520106005,61521092);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7779314"",""Data center network (DCN)";power consumption;end-to-end delay;bisection bandwidth;"dual-centric design"",""Servers";Delays;Computer architecture;Power demand;Switches;Routing;"Bandwidth"","""",""22"","""",""32"",""IEEE"",""9 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Traffic At-a-Glance: Time-Bounded Analytics on Large Visual Traffic Data,""G. Li"; X. Li; F. Yang; J. Teng; S. Ding; Y. F. Zheng; D. Xuan; B. Chen;" W. Zhao"",""Department of Computer Science & Engineering, The Ohio State University, OH"; Department of Computer Science & Engineering, The Ohio State University, OH; Department of Computer Science & Engineering, The Ohio State University, OH; Department of Computer Science & Engineering, The Ohio State University, OH; Department of Electrical & Computer Engineering, The Ohio State University, OH; Department of Electrical & Computer Engineering, The Ohio State University, OH; Department of Computer Science & Engineering, The Ohio State University, OH; Department of Computer & Information Science, University of Macau, Macau;" Department of Computer & Information Science, University of Macau, Macau"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2017"",""2017"",""28"",""9"",""2703"",""2717"",""Massive visual traffic data have become available recently. Though it opens the realm of intelligent traffic analysis, processing the data in a timely manner is difficult yet critical to time sensitive decisions, which are typical to traffic related management. In this paper, we study time-bounded aggregation analytics on large visual traffic data including traffic images and videos. We first find that current MapReduce framework can not work well due to two challenges: first, significant dual diversities exist on data distributions and processing time";" second, apriori knowledge on these distributions and time costs are not always available. However, we also observe spatial and temporal locality on data values and processing time. Based on the examination, we design Traffic At-a-Glance (TaG), an augmented MapReduce framework for time-bounded traffic analytics jobs. Particularly, we propose a novel sampling algorithm that exploits traffic data localities and stratifies samples based on data distributions and processing time. It runs in an iterative, adaptive manner without apriori knowledge. Moreover, we propose a heuristic scheduling algorithm with considerations of batch processing overhead. Further, we refine the load balancing mechanism based on data processing time locality to respect job time bounds. In addition, we extend TaG to well handle traffic videos by sampling video data based on motion information encoded in the videos. We implement TaG on Hadoop and conduct extensive experiments on a large visual traffic dataset. The evaluations on different data sizes show TaG is able to achieve high accuracy within time bounds."",""1558-2183"","""",""10.1109/TPDS.2017.2684158"",""Natural Science Foundation of China (NSFC)(grant numbers:61373115,61402356,61572398)"; FDCT(grant numbers:009/2010/A1,061/2011/A3); University of Macau(grant numbers:MYRG112); China Post doctoral Science Foundation(grant numbers:2015M572565); Fundamental Research Funds for the Central Universities(grant numbers:xkjc2015010); U.S. National Science Foundation (NSF)(grant numbers:CNS1065136,CNS1218876);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7880553"",""Traffic analysis";visual data;big data;MapReduce;"load balance"",""Visualization";Videos;Load management;Big Data;Surveillance;"Scheduling algorithms"","""",""5"","""",""39"",""IEEE"",""17 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Traffic Load Balancing Schemes for Devolved Controllers in Mega Data Centers,""X. Gao"; L. Kong; W. Li; W. Liang; Y. Chen;" G. Chen"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""572"",""585"",""In most existing cloud services, a centralized controller is used for resource management and coordination. However, such infrastructure is gradually not sufficient to meet the rapid growth of mega data centers. In recent literature, a new approach named devolved controller was proposed for scalability concern. This approach splits the whole network into several regions, each with one controller to monitor and reroute a portion of the flows. This technique alleviates the problem of an overloaded single controller, but brings other problems such as unbalanced work load among controllers and reconfiguration complexities. In this paper, we make an exploration on the usage of devolved controllers for mega data centers, and design some new schemes to overcome these shortcomings and improve the performance of the system. We first formulate Load Balancing problem for Devolved Controllers (LBDC) in data centers, and prove that it is NP-complete. We then design an f-approximation for LBDC, where f is the largest number of potential controllers for a switch in the network. Furthermore, we propose both centralized and distributed greedy approaches to solve the LBDC problem effectively. The numerical results validate the efficiency of our schemes, which can become a solution to monitoring, managing, and coordinating mega data centers with multiple controllers working together."",""1558-2183"","""",""10.1109/TPDS.2016.2579622"",""China 973 project(grant numbers:2014CB340303)"; Opening Project of Key Lab of Information Network Security of Ministry of Public Security; The Third Research Institute of Ministry of Public Security(grant numbers:C15602); Opening Project of Baidu(grant numbers:181515P005267); National Natural Science Foundation of China(grant numbers:61472252,61133006,61303202); Open Project Program of Shanghai Key Laboratory of Data Science(grant numbers:201609060001); China Postdoctoral Science Foundation(grant numbers:2014M560334,2015T80433);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488213"","""",""Switches";Monitoring;Load management;Routing;Servers;"Scalability"","""",""24"","""",""42"",""IEEE"",""9 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Traffic-Aware Geo-Distributed Big Data Analytics with Predictable Job Completion Time,""P. Li"; S. Guo; T. Miyazaki; X. Liao; H. Jin; A. Y. Zomaya;" K. Wang"",""School of Computer Science and Engineering, University of Aizu, Japan"; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; School of Computer Science and Engineering, University of Aizu, Japan; Service Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; Service Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China; School of Information Technologies, University of Sydney, Camperdown, NSW, Australia;" Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing University of Posts and Telecommunications, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1785"",""1796"",""Big data analytics has attracted close attention from both industry and academic because of its great benefits in cost reduction and better decision making. As the fast growth of various global services, there is an increasing need for big data analytics across multiple data centers (DCs) located in different countries or regions. It asks for the support of a cross-DC data processing platform optimized for the geo-distributed computing environment. Although some recent efforts have been made for geo-distributed big data analytics, they cannot guarantee predictable job completion time, and would incur excessive traffic overthe inter-DC network that is a scarce resource shared by many applications. In this paper, we study to minimize the inter-DC traffic generated by MapReduce jobs targeting on geo-distributed big data, while providing predicted job completion time. To achieve this goal, we formulate an optimization problem by jointly considering input data movement and task placement. Furthermore, we guarantee predictable job completion time by applying the chance-constrained optimization technique, such that the MapReduce job can finish within a predefined job completion time with high probability. To evaluate the performance of our proposal, we conduct extensive simulations using real traces generated by a set of queries on Hive. The results show that our proposal can reduce 55 percent inter-DC traffic compared with centralized processing by aggregating all data to a single data center."",""1558-2183"","""",""10.1109/TPDS.2016.2626285"",""JSPS KAKENHI(grant numbers:16K16038)"; NSFC(grant numbers:61572262); NSF of Jiangsu Province(grant numbers:BK20141427); Australian Research Council Discovery(grant numbers:A7921);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7738559"",""Big data";geo-distributed;MapReduce;"traffic-aware"",""Big data";Optimization;Distributed databases;Scheduling;Data models;"Proposals"","""",""52"","""",""36"",""IEEE"",""8 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Trajectory Pattern Mining for Urban Computing in the Cloud,""A. Altomare"; E. Cesario; C. Comito; F. Marozzo;" D. Talia"",""Institute of High Performance Computing and Networks (ICAR-CNR), Via P. Bucci, 7/11 C, Rende, (CS), Italy"; Institute of High Performance Computing and Networks (ICAR-CNR), Via P. Bucci, 7/11 C, Rende, (CS), Italy; Institute of High Performance Computing and Networks (ICAR-CNR), Via P. Bucci, 7/11 C, Rende, (CS), Italy; DIMES Department, University of Calabria, Rende, (CS), Italy;" DIMES Department, University of Calabria, Rende, (CS), Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Jan 2017"",""2017"",""28"",""2"",""586"",""599"",""The increasing pervasiveness of mobile devices along with the use of technologies like GPS, Wifi networks, RFID, and sensors, allows for the collections of large amounts of movement data. This amount of data can be analyzed to extract descriptive and predictive models that can be properly exploited to improve urban life. From a technological viewpoint, Cloud computing can play an essential role by helping city administrators to quickly acquire new capabilities and reducing initial capital costs by means of a comprehensive pay-as-you-go solution. This paper presents a workflow-based parallel approach for discovering patterns and rules from trajectory data, in a Cloud-based framework. Experimental evaluation has been carried out on both real-world and synthetic trajectory data, up to one million of trajectories. The results show that, due to the high complexity and large volumes of data involved in the application scenario, the trajectory pattern mining process takes advantage from the scalable execution environment offered by a Cloud architecture in terms of both execution time, speed-up and scale-up."",""1558-2183"","""",""10.1109/TPDS.2016.2565480"",""MIUR project"; DICET-INMOTO(grant numbers:PON04a2_D);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467530"",""Trajectory mining";urban computing;cloud computing;"distributed data mining"",""Cloud computing";Trajectory;Urban areas;Data mining;Computer architecture;"Sensors"","""",""17"","""",""23"",""IEEE"",""10 May 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Transport-Support Workflow Composition and Optimization for Big Data Movement in High-Performance Networks,""D. Yun"; C. Q. Wu;" M. M. Zhu"",""Computer and Information Sciences Program, Harrisburg University of Science and Technology, Harrisburg, PA"; Department of Computer Science, New Jersey Institute of Technology, Newark, NJ;" Department of Computer Science, Montclair State University, Montclair, NJ"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3656"",""3670"",""High-performance networks (HPNs) are being increasingly developed and deployed to support the transfer of big data. However, such HPN-based technologies and services have not been fully utilized as their use often requires considerable networking and system domain knowledge and many application users are even not aware of their existence. This work develops an integrated solution to discover system and network resources and compose end-to-end paths for big data movement. We first develop profiling and modeling approaches to characterize various types of resources distributed in end systems, edge segments, and backbone networks. A comprehensive set of performance metrics and network parameters are considered in different phases including device deployment, circuit setup, and data transfer. Based on these profiles and models, we then formulate a class of transport-support workflow optimization problems to compose the best end-to-end path that meets various performance requirements. We prove this problem to be NP-complete and design pseudo-polynomial optimal algorithms. We conduct extensive simulations to evaluate the proposed algorithms in comparison with a greedy approach, and also carry out real-life experiments across different network segments in production HPNs to evaluate the validity of the constructed cost models and illustrate the efficacy of the proposed transport solution."",""1558-2183"","""",""10.1109/TPDS.2017.2732987"",""Harrisburg University of Science and Technology’s Presidential Research"; U.S. Department of Energy’s; Office of Science(grant numbers:DE-SC0015892); National Science Foundation(grant numbers:CNS-1560698); New Jersey Institute of Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8000332"",""Big data transfer";performance modeling;workflow optimization;"high-performance networks"",""Data transfer";Bandwidth;Optimization;Big Data;Algorithm design and analysis;"Heuristic algorithms"","""",""7"","""",""46"",""IEEE"",""2 Aug 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Trends in Data Locality Abstractions for HPC Systems,""D. Unat"; A. Dubey; T. Hoefler; J. Shalf; M. Abraham; M. Bianco; B. L. Chamberlain; R. Cledat; H. C. Edwards; H. Finkel; K. Fuerlinger; F. Hannig; E. Jeannot; A. Kamil; J. Keasler; P. H. J. Kelly; V. Leung; H. Ltaief; N. Maruyama; C. J. Newburn;" M. Pericás"",""Department of Computer Engineering, Koç University, Istanbul, Turkey"; Argonne National Laboratory, Lemont, IL; ETH Zürich, Zürich, Switzerland; Lawrence Berkeley National Laboratory, Berkeley, CA; KTH Royal Institute of Technology, Solna, Sweden; Swiss National Supercomputer Centre, Lugano, Switzerland; Cray Inc., Seattle, WA; Intel Cooperation, Santa Clara, CA; Sandia National Laboratories, Albuquerque, NM; Argonne National Laboratory, Argonne, IL; Ludwig-Maximilians-Universität München, Munich, Germany; University of Erlangen-Nuremberg, Erlangen, Germany; INRIA Bordeaux Sud-Ouest, Talence, France; Lawrence Berkeley National Laboratory, Berkeley, CA; Lawrence Livermore National Laboratory, Livermore, CA; Imperial College London, London, United Kingdom; Sandia National Laboratories, Albuquerque, NM; King Abdullah University of Science and Technology, Thuwal, Kingdom of Saudi Arabia; RIKEN, Kobe, Hyogo, Japan; Nvidia Corporation, Santa Clara, CA;" Chalmers University of Technology, Göteborg, Sweden"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Sep 2017"",""2017"",""28"",""10"",""3007"",""3020"",""The cost of data movement has always been an important concern in high performance computing (HPC) systems. It has now become the dominant factor in terms of both energy consumption and performance. Support for expression of data locality has been explored in the past, but those efforts have had only modest success in being adopted in HPC applications for various reasons. them However, with the increasing complexity of the memory hierarchy and higher parallelism in emerging HPC systems, locality management has acquired a new urgency. Developers can no longer limit themselves to low-level solutions and ignore the potential for productivity and performance portability obtained by using locality abstractions. Fortunately, the trend emerging in recent literature on the topic alleviates many of the concerns that got in the way of their adoption by application developers. Data locality abstractions are available in the forms of libraries, data structures, languages and runtime systems";" a common theme is increasing productivity without sacrificing performance. This paper examines these trends and identifies commonalities that can combine various locality concepts to develop a comprehensive approach to expressing and managing data locality on future large-scale high-performance computing systems."",""1558-2183"","""",""10.1109/TPDS.2017.2703149"",""German Research Foundation(grant numbers:TE 163/17-1)"; European Commission(grant numbers:655965);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7924389"",""Data locality";programming abstractions;high-performance computing;data layout;"locality-aware runtimes"",""Layout";Arrays;Market research;Distributed databases;Parallel processing;"Libraries"","""",""53"","""",""77"",""IEEE"",""10 May 2017"","""","""",""IEEE"",""IEEE Journals"""
"Understanding Big Data Analytics Workloads on Modern Processors,""Z. Jia"; J. Zhan; L. Wang; C. Luo; W. Gao; Y. Jin; R. Han;" L. Zhang"",""University of Chinese Academy of Sciences, Beijing, China"; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; Beijing Computing Center, Beijing Academy of Science and Technology, Beijing, China; University of Chinese Academy of Sciences, Beijing, China;" University of Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 May 2017"",""2017"",""28"",""6"",""1797"",""1810"",""Big data analytics workloads are very significant ones in modern data centers, and it is more and more important to characterize their representative workloads and understand their behaviors so as to improve the performance of data center computer systems. In this paper, we embark on a comprehensive study to understand the impacts and performance implications of the big data analytics workloads on the systems equipped with modern superscalar out-of-order processors. After investigating three most important application domains in Internet services in terms of page views and daily visitors, we choose 11 representative data analytics workloads and characterize their micro-architectural behaviors by using hardware performance counters. Our study reveals that the big data analytics workloads share many inherent characteristics, which place them in a different class from the traditional workloads and the scale-out services. To further understand the characteristics of big data analytics workloads, we perform correlation analysis to identify the most key factors that affect cycles per instruction (CPI). Also, we reveal that the increasing complexity of the big data software stacks will put higher pressures on the modern processor pipelines."",""1558-2183"","""",""10.1109/TPDS.2016.2625244"",""The National Key Research and Development Program of China(grant numbers:2016YFB1000600,2016YFB1000601)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7736117"",""Big data analytics";workload characterization;micro-architectural characteristics;"performance optimization"",""Big data";Social network services;Program processors;Search engines;Electronic commerce;"Data analysis"","""",""23"","""",""45"",""IEEE"",""4 Nov 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Understanding Co-Running Behaviors on Integrated CPU/GPU Architectures,""F. Zhang"; J. Zhai; B. He; S. Zhang;" W. Chen"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computing, National University of Singapore, Singapore, Singapore; School of Computing, National University of Singapore, Singapore, Singapore;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Feb 2017"",""2017"",""28"",""3"",""905"",""918"",""Architecture designers tend to integrate both CPUs and GPUs on the same chip to deliver energy-efficient designs. It is still an open problem to effectively leverage the advantages of both CPUs and GPUs on integrated architectures. In this work, we port 42 programs in Rodinia, Parboil, and Polybench benchmark suites and analyze the co-running behaviors of these programs on both AMD and Intel integrated architectures. We find that co-running performance is not always better than running the program only with CPUs or GPUs. Among these programs, only eight programs can benefit from the co-running, while 24 programs only using GPUs and seven programs only using CPUs achieve the best performance. The remaining three programs show little performance preference for different devices. Through extensive workload characterization analysis, we find that architecture differences between CPUs and GPUs and limited shared memory bandwidth are two main factors affecting current co-running performance. Since not all the programs can benefit from integrated architectures, we build an automatic decision-tree-based model to help application developers predict the co-running performance for a given CPU-only or GPU-only program. Results show that our model correctly predicts 14 programs out of 15 for evaluated programs. For a co-run friendly program, we further propose a profiling-based method to predict the optimal workload partition ratio between CPUs and GPUs. Results show that our model can achieve 87.7 percent of the optimal performance relative to the best partition. The co-running programs acquired with our method outperform the original CPU-only and GPU-only programs by 34.5 and 20.9 percent respectively."",""1558-2183"","""",""10.1109/TPDS.2016.2586074"",""Chinese 863 project(grant numbers:2012AA010901)"; NSFC(grant numbers:61472201); Tsinghua University; Initiative Scientific Research Program; Tsinghua-Tencent Joint Laboratory for Internet Innovation Technology; Huawei Innovation Research Program (HIRP) in China; MoE; AcRF(grant numbers:MOE2012-T2-2-067);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7501903"",""Heterogeneous computing";integrated architecture;performance prediction;performance tuning;"workload characterization"",""Computer architecture";Performance evaluation;Graphics processing units;Predictive models;Bandwidth;Kernel;"Ports (Computers)"","""",""62"","""",""38"",""IEEE"",""29 Jun 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Using Imbalance Characteristic for Fault-Tolerant Workflow Scheduling in Cloud Systems,""G. Yao"; Y. Ding;" K. Hao"",""Engineering Research Center of Digitized Textile and Apparel Technology, Donghua University, Shanghai, China"; Engineering Research Center of Digitized Textile and Apparel Technology, Donghua University, Shanghai, China;" Engineering Research Center of Digitized Textile and Apparel Technology, Donghua University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Nov 2017"",""2017"",""28"",""12"",""3671"",""3683"",""Resubmission and replication are two fundamental and widely recognized techniques in distributed computing systems for fault tolerance. The resubmission based strategy has an advantage in resource utilization, while the replication based strategy can reduce the task completed time in the context of fault. However, few researches take these two techniques together for fault-tolerant workflow scheduling, especially in Cloud systems. In this paper, we present a novel fault-tolerant workflow scheduling (ICFWS) algorithm for Cloud systems by combining the aforementioned two strategies together to play their respective advantages for fault tolerance while trying to meet the soft deadline of workflow. First, it divides the soft deadline of workflow into multiple sub-deadlines for all tasks. Then, it selects a reasonable fault-tolerant strategy and reserves suitable resource for each task by taking the imbalance sub-deadlines among tasks and on-demand resource provisioning of Cloud systems into consideration. Finally, an online scheduling and reservation adjustment scheme is designed to select a suitable resource for the task with resubmission strategy and adjust the sub-deadlines as well as fault-tolerant strategies of some unexecuted tasks during the task execution process, respectively. The proposed algorithm is evaluated on both real-world and randomly generated workflows. The results demonstrate that the ICFWS outperforms some well-known approaches on corresponding metrics."",""1558-2183"","""",""10.1109/TPDS.2017.2687923"",""National Natural Science Foundation of China(grant numbers:61134009)"; National Natural Science Foundation of China(grant numbers:61473078,61473077,61702068); Changjiang Scholars from the Ministry of Education(grant numbers:2015-2019); Shanghai Committee of Science and Technology(grant numbers:16510711100); Anhui University(grant numbers:KJ2017A418);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7887706"",""Fault-tolerant";workflow;Cloud systems;resubmission;replication;"imbalance"",""Fault tolerance";Fault tolerant systems;Cloud computing;Context awareness;Algorithm design and analysis;Scheduling algorithms;"Computational modeling"","""",""51"","""",""36"",""IEEE"",""27 Mar 2017"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Using MinMax-Memory Claims to Improve In-Memory Workflow Computations in the Cloud,""S. He"; Y. Wang; X. -H. Sun;" C. Xu"",""State Key Laboratory of Software Engineering, Wuhan University, Luojiashan, Wuhan, Hubei, China"; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Xueyuan Blvd. 1068, Shenzhen, China; Department of Computer Science, Illinois Institute of Technology, Chicago, IL;" Department of Electrical and Computer Engineering, Wayne State University, Detroit, MI"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1202"",""1214"",""In this paper, we consider to improve scientific workflows in cloud environments where data transfers between tasks are performed via provisioned in-memory caching as a service, instead of relying entirely on slower disk-based file systems. However, this improvement is not free since services in the cloud are usually charged in a “pay-as-you-go” model. As a consequence, the workflow tenants have to estimate the amount of memory that they would like to pay. Given the intrinsic complexity of the workflows, it would be very hard to make an accurate prediction, which would lead to either oversubscription or undersubscription, resulting in unproductive spending or performance degradation. To address this problem, we propose a concept of minmax memory claim (MMC) to achieve cost-effective workflow computations in in-memory cloud computing environments. The minmax-memory claim is defined as the minimum amount of memory required to finish the workflow without compromising its maximum concurrency. With the concept of MMC, the workflow tenants can achieve the best performance via in-memory computing while minimizing the cost. In this paper, we present the procedure of how to find the MMCs for those workflows with arbitrary graphs in general and develop optimal efficient algorithms for some well-structured workflows in particular. To further show the values of this concept, we also implement these algorithms and apply them, through a simulation study, to improve deadlock resolutions in workflow-based workloads when memory resources are constrained."",""1558-2183"","""",""10.1109/TPDS.2016.2614294"",""China National Basic Research Program 973 Program(grant numbers:2015CB352400)"; National Science Foundation of China(grant numbers:61672513,61572377,U1401258,61550110250); Science and Technology Planning Project of Guangdong Province(grant numbers:2015B010129011,2016A030313183); SKLSE(grant numbers:2015-A-06); Natural Science Foundation of Hubei Province of China(grant numbers:2014CFB239); HPCL(grant numbers:201512-02);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7579169"",""Minmax memory claim";in-memory caching;deadlock avoidance;memory constraints;"workflow scheduling"",""Cloud computing";System recovery;Memory management;Computational modeling;Heuristic algorithms;Concurrent computing;"Cache memory"","""",""6"","""",""39"",""IEEE"",""28 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Using Switchable Pins to Increase Off-Chip Bandwidth in Chip-Multiprocessors,""S. Chen"; S. Irving; L. Peng; Y. Hu; Y. Zhang;" A. Srivastava"",""Division of Electrical & Computer Engineering, School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, LA"; Division of Electrical & Computer Engineering, School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, LA; Division of Electrical & Computer Engineering, School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, LA; Division of Electrical & Computer Engineering, School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, LA; Intel Corporation, Santa Clara, CA;" Division of Electrical & Computer Engineering, School of Electrical Engineering and Computer Science, Louisiana State University, Baton Rouge, LA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""274"",""289"",""Off-chip memory bandwidth has been considered as one of the major limiting factors of processor performance, especially for multi-cores and many-cores. Conventional processor design allocates a large portion of off-chip pins to deliver power, leaving a small number of pins for processor signal communication. We observe that a processor requires much less power during memory intensive stages than is available. This is due to the fact that the frequencies of processor cores waiting for data to be fetched from offchip memories can be scaled down in order to save power without degrading performance. Motivated by this observation, we propose a dynamic pin switching technique to alleviate this bandwidth limitation. This technique is introduced to dynamically exploit surplus power delivery pins to provide extra bandwidth during memory intensive program phases, thereby significantly boosting performance. This work is extended to compare two approaches for increasing off chip bandwidths using switchable pins. Additionally, it shows significant performance improvements for memory intensive workloads on a memory subsystem using Phase Change Memory."",""1558-2183"","""",""10.1109/TPDS.2016.2546246"",""US National Science Foundation(grant numbers:CCF-1017961,CCF-1422408)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7440872"",""Multiprocessors";"reconfigurable hardware"",""Switches";Bandwidth;Pins;Random access memory;Memory management;Power grids;"Hardware"","""",""3"","""",""45"",""IEEE"",""24 Mar 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"VarCatcher: A Framework for Tackling Performance Variability of Parallel Workloads on Multi-Core,""W. Zhang"; X. Ji; B. Song; S. Yu; H. Chen; T. Li; P. -C. Yew;" W. Zhao"",""Parallel Processing Institute, Fudan University, Shanghai, China"; Parallel Processing Institute, Fudan University, Shanghai, China; Parallel Processing Institute, Fudan University, Shanghai, China; Parallel Processing Institute, Fudan University, Shanghai, China; Institute of Parallel and Distributed Systems, Shanghai Jiaotong University, Shanghai, China; Department of Electrical and Computer Engineering, University of Florida, FL; Department of Computer Science and Engineering, University of Minnesota, Twin Cities, MN;" Parallel Processing Institute, Fudan University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2017"",""2017"",""28"",""4"",""1215"",""1228"",""The non-deterministic nature of multi-threaded workloads running on multi-core platforms often leads to notable performance variability from run to run. Such variability makes experimental results prone to misinterpretations or misguided claims. To deal with such variability, statistical inference methods are usually used to summarize the experimental results with certain confidence levels by running the experiments or measurements a large number of times. However, such statistical results are often too vague or too simplistic. They are not sufficient to help users understand the causes of such variability, and allow more in-depth analysis on the results or reproduce the results for validation during design space exploration. To allow better analyzability and reproducibility, we propose a framework to tackle such variability, called VarCatcher. The key to VarCatcher is to characterize a parallel execution using Parallel Characteristics Vector (PCV). A clustering-based approach is then used to group runs with similar execution characteristics that can later be used to analyze results in-depth, to customize different evaluation strategies, reproduce the result for variability, to determine the impact of features, or to assist performance diagnosis. We have built a prototype of VarCatcher that includes a user-level toolset for runtime monitoring and measurements using the Intel Processor Trace feature on commodity Intel processors as well as an architecture extension with very low runtime overheads (around 3 and 0.01 percent accordingly). Several case studies confirm that VarCatcher enables several appealing features such as in-depth result analysis, customized evaluation strategies, and reproducibility."",""1558-2183"","""",""10.1109/TPDS.2016.2613524"",""National Key Research and Development Program of China(grant numbers:2016YFB0800104)"; National Natural Science Foundation of China(grant numbers:61672160); Shanghai Science and Technology Development Funds(grant numbers:16JC1400801);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7576653"",""Variability";parallel application;evaluation;"multi core"",""Space exploration";Runtime;Electronic mail;Visualization;Optical wavelength conversion;"Benchmark testing"","""",""10"","""",""44"",""IEEE"",""26 Sep 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"VLCcube: A VLC Enabled Hybrid Network Structure for Data Centers,""L. Luo"; D. Guo; J. Wu; T. Qu; T. Chen;" X. Luo"",""College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China"; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China; Department of Computer and Information Science, Temple University, Philadelphia, Pennsylvania, USA; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China; College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China;" College of Information System and Management, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""2088"",""2102"",""Recent results have made a promising case for offering oversubscribed wired data center networks (DCN) with extreme costs. Inter-rack wireless networks are drawing intensive attention to augment such wired DCNs with a few wireless links. Inspired by the promise of easy deployment and plug-and-play, we present VLCcube, a novel inter-rack wireless solution that extends the design of wireless DCN into three further dimensions: (1) all inter-rack links are wireless"; (2) there is no imposition of any infrastructure-level alteration on wired production data centers;" and (3) it should be plug-and-play, without any need of additional mechanical or electronic control operations. This vision, if realized, will lead to increased flexibility, reduced reconstructing cost, simplified configuration and usage, and outstanding compatibility with existing wired DCNs. Previous proposals, however, are opposed to the last two design rationales. To achieve this vision, the proposed VLCcube augments Fat-Tree, a representative DCN in production data centers, by organizing all racks into a wireless Torus structure via the emerging visible light links. We further present the topology design, hybrid routing, and flow scheduling schemes for VLCcube. Extensive evaluations indicate that VLCcube outperforms Fat-Tree significantly under the existing ECMP flow scheduling scheme, irrespective of the undergoing traffic pattern. Moreover, the performance of VLCcube can be significantly promoted by our congestion-aware flow scheduling scheme. More precisely, compared to ECMP, our flow scheduling scheme makes VLCcube achieve $\times 1.50$  throughput under batched flows, $\mathrm{\times}2.21$  and  $\times 2.59$  throughput under two different kinds of online flows."",""1558-2183"","""",""10.1109/TPDS.2016.2646366"",""National Natural Science Foundation"; Outstanding Excellent Young Scholars of China(grant numbers:61422214); National Basic Research Program (973 program)(grant numbers:2014CB347800); Program for New Century Excellent Talents in University; Hunan Provincial Natural Science Fund; Distinguished Young Scholars(grant numbers:2016JJ1002); NUDT(grant numbers:JQ14-05-02,ZDYYJCYJ20140601);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7801878"",""Data center networks";inter-rack network;visible light communication;throughput;"packet loss rate"",""Servers";Topology;Wireless networks;Scheduling;Network topology;"Proposals"","""",""17"","""",""38"",""IEEE"",""29 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Workflow Scheduling in Multi-Tenant Cloud Computing Environments,""B. P. Rimal";" M. Maier"",""Optical Zeitgeist Laboratory, Institut National de la Recherche Scientifique, Montréal, QC, Canada";" Optical Zeitgeist Laboratory, Institut National de la Recherche Scientifique, Montréal, QC, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Dec 2016"",""2017"",""28"",""1"",""290"",""304"",""Multi-tenancy is one of the key features of cloud computing, which provides scalability and economic benefits to the end-users and service providers by sharing the same cloud platform and its underlying infrastructure with the isolation of shared network and compute resources. However, resource management in the context of multi-tenant cloud computing is becoming one of the most complex task due to the inherent heterogeneity and resource isolation. This paper proposes a novel cloud-based workflow scheduling (CWSA) policy for compute-intensive workflow applications in multi-tenant cloud computing environments, which helps minimize the overall workflow completion time, tardiness, cost of execution of the workflows, and utilize idle resources of cloud effectively. The proposed algorithm is compared with the state-of-the-art algorithms, i.e., First Come First Served (FCFS), EASY Backfilling, and Minimum Completion Time (MCT) scheduling policies to evaluate the performance. Further, a proof-of-concept experiment of real-world scientific workflow applications is performed to demonstrate the scalability of the CWSA, which verifies the effectiveness of the proposed solution. The simulation results show that the proposed scheduling policy improves the workflow performance and outperforms the aforementioned alternative scheduling policies under typical deployment scenarios."",""1558-2183"","""",""10.1109/TPDS.2016.2556668"",""Fonds de recherche du Québec—Nature et Technologies"; MERIT Doctoral Research Scholarship Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7457258"",""Cloud computing";direct acyclic graph;multi-tenancy;resource management;"scientific workflow applications"",""Cloud computing";Processor scheduling;Resource management;Computer architecture;Scalability;"Dynamic scheduling"","""",""114"","""",""47"",""IEEE"",""20 Apr 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Workload Consolidation for Cloud Data Centers with Guaranteed QoS Using Request Reneging,""S. Homsi"; S. Liu; G. A. Chaparro-Baquero; O. Bai; S. Ren;" G. Quan"",""Department of Electrical and Computer Engineering, Florida International University, Miami, FL"; Department of Electrical and Computer Engineering, Florida International University, Miami, FL; Department of Electrical and Computer Engineering, Florida International University, Miami, FL; Department of Electrical and Computer Engineering, Florida International University, Miami, FL; Department of Electrical and Computer Engineering, University of California, CA;" Department of Electrical and Computer Engineering, Florida International University, Miami, FL"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2017"",""2017"",""28"",""7"",""2103"",""2116"",""Cloud data centers are widely employed to offer reliable cloud services. However, low resource utilization and high power consumption have been great challenges for cloud providers. Moreover, the rapid increase in demand for affordable cloud services magnifies the obstacles for proficient resource management policies. In this paper, we investigate how to improve resource utilization and power consumption in cloud data centers when delivering services with statistically guaranteed Quality of Service (QoS). We assume that the service provider hosts different types of services, each of which has request classes with different QoS requirements. Different from the traditional approaches that distribute workloads with different QoS levels on different Virtual Machines (VMs), we introduce an approach to pack requests of the same service type, even with different QoS requirements, into the same VM, and to remove potential failure requests in time to improve resource usage and energy cost. We formally prove that our algorithm can statistically guarantee QoS conditions in terms of deadline miss ratios. We develop a cloud prototype to empirically validate our proposed methods and algorithm. Our experimental results demonstrate that our approach can significantly outperform other traditional approaches in terms of QoS guarantees, power consumption, resource demand and electricity cost."",""1558-2183"","""",""10.1109/TPDS.2016.2642941"",""NSF(grant numbers:CNS-1423137,CNS-1551661,CNS-1565474,ECCS-1610471)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7792712"",""Cloud computing";virtualization;workload consolidation;power efficiency;utilization;reneging;"guaranteed QoS"",""Quality of service";Cloud computing;Servers;Power demand;Resource management;Distributed databases;"Virtualization"","""",""26"","""",""46"",""IEEE"",""21 Dec 2016"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;