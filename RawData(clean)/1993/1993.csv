"A compile-time scheduling heuristic for interconnection-constrained heterogeneous processor architectures,""G. C. Sih";" E. A. Lee"",""Qualcomm, Inc., San Diego, CA, USA";" Electrical Engineering and Computer Science Department, University of California, Berkeley, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""175"",""187"",""The authors present a compile-time scheduling heuristic called dynamic level scheduling, which accounts for interprocessor communication overhead when mapping precedence-constrained, communicating tasks onto heterogeneous processor architectures with limited or possibly irregular interconnection structures. This technique uses dynamically-changing priorities to match tasks with processors at each step, and schedules over both spatial and temporal dimensions to eliminate shared resource contention. This method is fast, flexible, widely targetable, and displays promising performance.<>"",""1558-2183"","""",""10.1109/71.207593"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207593"","""",""Processor scheduling";Dynamic scheduling;Integrated circuit interconnections;Signal processing algorithms;Parallel processing;Hardware;Partitioning algorithms;Delay;Scheduling algorithm;"Computer architecture"","""",""600"",""16"",""18"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A comprehensive performance evaluation of crossbar networks,""H. Y. Youn";" C. C. . -Y. Chen"",""Department of Computer Science Engineering, University of Texas, Arlington, Arlington, TX, USA";" Department of Computer Science, Tamkang University, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""481"",""489"",""A comprehensive model for evaluating crossbar networks in which the memory bandwidth and processor acceptance probability are primary measures considered is presented. This analytical model includes all important network control policies, such as the bus arbitration and rejected request handling policies, as well as the home memory concept. Computer simulation validates the correctness of the model. It is confirmed that the home memory and dynamic bus arbitration policy improve the network performance.<>"",""1558-2183"","""",""10.1109/71.224212"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224212"","""",""Multiprocessor interconnection networks";Multiprocessing systems;Bandwidth;Analytical models;Computer science;Computer simulation;Distributed processing;Resource management;Government;"Propagation delay"","""",""5"",""1"",""16"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A distributed recovery block approach to fault-tolerant execution of application tasks in hypercubes,""K. H. Kim";" A. Kavianpour"",""Department of Electrical and Computer Engineering, University of California, Irvine, CA, USA";" Department of Electrical and Computer Engineering, University of California, Irvine, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""104"",""111"",""An approach to fault-tolerant execution of real-time application tasks in hypercubes is proposed. The approach is based on the distributed recovery block (DRB) scheme and does not require special hardware mechanisms in support of fault tolerance. Each task is assigned to a pair of processors forming a DRB computing station for execution in a dual-redundant and self-checking mode. Assignment of all tasks in an application in such a form is called the full DRB mapping. The DRB scheme was developed as an approach to uniform treatment of hardware and software faults with the effect of fast forward recovery. However, if the system developer is concerned with hardware fault possibilities only, then forming DRB stations becomes a mechanical process not burdening the application software designer in any way. A procedure for converting an efficient nonredundant task-to-processor mapping into an efficient full DRB mapping is presented.<>"",""1558-2183"","""",""10.1109/71.205657"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205657"","""",""Fault tolerance";Hypercubes;Hardware;Application software;Real time systems;Concurrent computing;Computer architecture;Software design;Binary trees;"Costs"","""",""4"","""",""16"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A family of parallel prefix algorithms embedded in networks,""M. Takesue"",""Software Research Laboratory, NTT Software Research Laboratories, Musashino, Tokyo, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1179"",""1184"",""This paper presents a family of algorithms for producing, from ( upsilon /sub 0/, upsilon /sub 1/, ..., upsilon /sub n-1/), all initial prefixes x/sub i/= upsilon /sub 0/ theta upsilon /sub 1/ theta ... theta upsilon /sub i/ (i=0, 1, ..., n-1) in parallel in interconnection networks such as the omega network and the hypercube, where theta is an associative binary operator. Each algorithm can be embedded in the switches and interconnections of the network, and can be executed in O((log/sub 2/ r+1) log/sub r/ n) time steps provided that the network connecting n processors is constructed by using an r*r switch, and that parallelism within as well as among individual switches is exploited. The objective of these algorithms is to attain a communication pattern that fits the topology of the network. One type of network can be made equivalent to, or can be embedded in, another type of network, so a family of algorithms can be derived from one basic algorithm. In the basic algorithm, every processor p/sub i/ upward multicasts upsilon /sub i/ to processors p/sub k/ (k=i+1, i+2, ..., n - 1). En route to p/sub i/, upsilon /sub j/ (j=0, 1, ..., i - 1) are combined in the switches to produce the (i - 1)th initial prefix x/sub i-1/ that is received by p/sub i/, which can then compute the ith initial prefix x/sub i/=x/sub i-1/ theta upsilon /sub i/.<>"",""1558-2183"","""",""10.1109/71.246079"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246079"","""",""Intelligent networks";Switches;Multicast algorithms;Hypercubes;Communication switching;Computer networks;Polynomials;Classification tree analysis;Multiprocessor interconnection networks;"Joining processes"","""",""1"","""",""15"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"A generalized scheme for mapping parallel algorithms,""V. Chaudhary";" J. K. Aggarwal"",""Parallel and Distributed Computing Laboratory, Department of Electrical and Computer Engineering, Wayne State University, Detroit, MI, USA";" NA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""328"",""346"",""A generalized mapping strategy that uses a combination of graph theory, mathematical programming, and heuristics is proposed. The authors use the knowledge from the given algorithm and the architecture to guide the mapping. The approach begins with a graphical representation of the parallel algorithm (problem graph) and the parallel computer (host graph). Using these representations, the authors generate a new graphical representation (extended host graph) on which the problem graph is mapped. An accurate characterization of the communication overhead is used in the objective functions to evaluate the optimality of the mapping. An efficient mapping scheme is developed which uses two levels of optimization procedures. The objective functions include minimizing the communication overhead and minimizing the total execution time which includes both computation and communication times. The mapping scheme is tested by simulation and further confirmed by mapping a real world application onto actual distributed environments.<>"",""1558-2183"","""",""10.1109/71.210815"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210815"","""",""Parallel algorithms";Concurrent computing;Computer architecture;Processor scheduling;Resource management;Distributed computing;Computer vision;Graph theory;Mathematical programming;"Testing"","""",""70"",""1"",""87"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A network flow model for load balancing in circuit-switched multicomputers,""S. H. Bokhari"",""Department of Electrical Engineering, University of Engineering and Technology, Lahore, Pakistan"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""649"",""657"",""In multicomputers that utilize circuit switching or wormhole routing, communication overhead depends largely on link contention-the variation due to distance between nodes is negligible. This has a major impact on the load balancing problem. In this case there are some nodes with an excess load (sources) and other with a deficit load (sinks). A matching of sources to sinks is required to avoid contention. The problem is made complex by the hardwired routing on currently available machines: The user can control only which nodes communicate but not how the messages are routed. Network flow models of message flow in the mesh and the hypercube have been developed to solve this problem. The crucial property of these models is the correspondence between minimum cost flows and correctly routed messages. To solve a given load balancing problem, a minimum cost flow algorithm is applied to the network. This permits the efficient determination of a maximum contention free matching of sources to sinks that, in turn, tells how much of the given imbalance can be eliminated without contention.<>"",""1558-2183"","""",""10.1109/71.242158"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242158"","""",""Load modeling";Load management;Intelligent networks;Hypercubes;Routing;Costs;Integrated circuit interconnections;NASA;Switching circuits;"Communication switching"","""",""9"",""1"",""21"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"A new graph approach to minimizing processor fragmentation in hypercube multiprocessors,""Q. Yang";" H. Wang"",""Department of Electrical Engineering, University of Rhode Island, Kingston, RI, USA";" Department of Electrical Engineering, University of Rhode Island, Kingston, RI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1165"",""1171"",""The authors propose a new approach for subcube and noncubic processor allocations for hypercube multiprocessors. The main idea is to represent available processors in the system by means of a prime cube graph (PC-graph). The PC-graph maintains the inter-relationships between free subcubes and hence reduces both internal and external processor fragmentations. Their simulation results show that the PC-graph approach outperforms the existing allocation strategies by 25% to 50% under certain load conditions.<>"",""1558-2183"","""",""10.1109/71.246077"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246077"","""",""Hypercubes";Binary trees;Computational modeling;Concurrent computing;Nonlinear equations;High performance computing;Application software;Topology;Costs;"Availability"","""",""9"","""",""11"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A new theory of deadlock-free adaptive routing in wormhole networks,""J. Duato"",""Departamento de Ingenieria de Sistemas, Computadores y Automática, Universidad Politécnica de Valencia, Valencia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""12"",""1320"",""1331"",""The theoretical background for the design of deadlock-free adaptive routing algorithms for wormhole networks is developed. The author proposes some basic definitions and two theorems. These create the conditions to verify that an adaptive algorithm is deadlock-free, even when there are cycles in the channel dependency graph. Two design methodologies are also proposed. The first supplies algorithms with a high degree of freedom, without increasing the number of physical channels. The second methodology is intended for the design of fault-tolerant algorithms. Some examples are given to show the application of the methodologies. Simulations show the performance improvement that can be achieved by designing the routing algorithms with the new theory.<>"",""1558-2183"","""",""10.1109/71.250114"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250114"","""",""System recovery";Routing;Intelligent networks;Delay;Hardware;Network topology;Algorithm design and analysis;Design methodology;Fault tolerance;"Pipelines"","""",""597"",""36"",""25"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"A novel concurrent error detection scheme for FFT networks,""D. L. Tao";" C. R. P. Hartmann"",""Department of Electrical Engineering, State University of New York, Stony Brook, Stony Brook, NY, USA";" School of Computer and Information Science, Syracuse University, Syracuse, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""198"",""221"",""The algorithm-based fault tolerance techniques have been proposed to obtain reliable results at very low hardware overhead. Even though 100% fault coverage can be theoretically obtained by using these techniques, the system performance, i.e., fault coverage and throughput, can be drastically reduced due to many practical problems, e.g., round-off errors. A novel algorithm-based fault tolerance scheme is proposed for fast Fourier transform (FFT) networks. It is shown that the proposed scheme achieves 100% fault coverage theoretically. An accurate measure of the fault coverage for FFT networks is provided by taking the round-off error into account. The proposed scheme is shown to provide concurrent error detection capability to FFT networks with low hardware overhead, high throughput, and high fault coverage.<>"",""1558-2183"","""",""10.1109/71.207595"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207595"","""",""Fault tolerance";Fault detection;Hardware;Throughput;Very large scale integration;Matrix decomposition;Fast Fourier transforms;Signal processing algorithms;Roundoff errors;"Discrete Fourier transforms"","""",""33"","""",""16"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A parallel algorithm for an efficient mapping of grids in hypercubes,""M. Y. Chan";" F. Chin"",""University of Texas, Dallas, Richardson, TX, USA";" University of Texas, Dallas, Richardson, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""933"",""946"",""The authors parallelize the embedding strategy for mapping any two-dimensional grid into its optimal hypercube with minimal dilation. The parallelization allows each hypercube node to independently determine, in constant time, which grid node it will simulate and the communication paths it will take to reach the hypercube nodes that simulate its grid-neighbors. The paths between grid-neighbors are chosen in such a way as to curb the congestion at each hypercube node and across each hypercube edge. Explicity, the node congestion for the embedding is at most 6, one above optimal, while the edge congestion is at most 5.<>"",""1558-2183"","""",""10.1109/71.238627"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238627"","""",""Parallel algorithms";Hypercubes;Computer science;Network topology;Labeling;Embedded computing;Grid computing;"Broadcasting"","""",""4"","""",""17"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"A parallel algorithm for random walk construction with application to the Monte Carlo solution of partial differential equations,""A. Youssef"",""Department of Electrical Engineering and Computer Science, George Washington University, Washington D.C., DC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""355"",""360"",""Random walks are widely applicable in statistical and scientific computations. In particular, they are used in the Monte Carlo method to solve elliptic and parabolic partial differential equations (PDEs). This method holds several advantages over other methods for PDEs as it solves problems with irregular boundaries and/or discontinuities, gives solutions at individual points, and exhibits great parallelism. However, the generation of each random walk in the Monte Carlo method has been done sequentially because each point in the walk is derived from the preceding point by moving one grid step along a randomly selected direction. A parallel algorithm for random walk generation in regular as well as irregular regions is presented. The algorithm is based on parallel prefix computations. The communication structure of the algorithm is shown to ideally fit on a hypercube of n nodes, where n is the number of processors.<>"",""1558-2183"","""",""10.1109/71.210818"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210818"","""",""Parallel algorithms";Monte Carlo methods;Concurrent computing;Partial differential equations;Parallel processing;Mesh generation;Hypercubes;Multidimensional systems;Computer architecture;"Grid computing"","""",""3"","""",""15"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"A parallel hash join algorithm for managing data skew,""J. L. Wolf"; P. S. Yu; J. Turek;" D. M. Dias"",""IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA"; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA;" IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""12"",""1355"",""1371"",""Presents a parallel hash join algorithm that is based on the concept of hierarchical hashing, to address the problem of data skew. The proposed algorithm splits the usual hash phase into a hash phase and an explicit transfer phase, and adds an extra scheduling phase between these two. During the scheduling phase, a heuristic optimization algorithm, using the output of the hash phase, attempts to balance the load across the multiple processors in the subsequent join phase. The algorithm naturally identifies the hash partitions with the largest skew values and splits them as necessary, assigning each of them to an optimal number of processors. Assuming for concreteness a Zipf-like distribution of the values in the join column, a join phase which is CPU-bound, and a shared nothing environment, the algorithm is shown to achieve good join phase load balancing, and to be robust relative to the degree of data skew and the total number of processors. The overall speedup due to this algorithm is compared to some existing parallel hash join methods. The proposed method does considerably better in high skew situations.<>"",""1558-2183"","""",""10.1109/71.250117"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250117"","""",""Scheduling algorithm";Processor scheduling;Partitioning algorithms;Relational databases;Load management;Heuristic algorithms;Robustness;Parallel processing;Delay;"Proposals"","""",""31"",""2"",""48"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"A parallel sort merge join algorithm for managing data skew,""J. L. Wolf"; D. M. Dias;" P. S. Yu"",""IBM Research Division, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA"; IBM Research Division, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA;" IBM Research Division, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""70"",""86"",""A parallel sort-merge-join algorithm which uses a divide-and-conquer approach to address the data skew problem is proposed. The proposed algorithm adds an extra, low-cost scheduling phase to the usual sort, transfer, and join phases. During the scheduling phase, a parallelizable optimization algorithm, using the output of the sort phase, attempts to balance the load across the multiple processors in the subsequent join phase. The algorithm naturally identifies the largest skew elements, and assigns each of them to an optimal number of processors. Assuming a Zipf-like distribution of data skew, the algorithm is demonstrated to achieve very good load balancing for the join phase, and is shown to be very robust relative, among other things, to the degree of data skew and the total number of processors.<>"",""1558-2183"","""",""10.1109/71.205654"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205654"","""",""Scheduling algorithm";Processor scheduling;Relational databases;Parallel processing;Costs;Load management;Robustness;Delay;Parallel architectures;"Proposals"","""",""25"",""3"",""41"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"A simple distributed loop-free routing strategy for computer communication networks,""K. G. Shin";" Chih-Che Chou"",""Real-Time Computing Laboratory, Computer Science and Engineering Division, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA";" Real-Time Computing Laboratory, Computer Science and Engineering Division, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""12"",""1308"",""1319"",""The loops resulting from either component failures or load changes in a computer communication network degrade the performance and the adaptability of conventional distributed adaptive routing strategies, such as ARPANET's previous routing strategy (APRS). The authors develop distributed loop-free routing strategy by adding only one additional piece of information-the total number of minimum-delay paths-to the commonly used routing messages and tables. The proposed routing strategy requires only easily obtainable information, yet removes loops completely. It is far more efficient in both time and space than its conventional counterparts, especially for sparse computer networks. The authors prove the correctness of the proposed strategy, and give several illustrative examples. The performance of this strategy is shown to be better than, or at least as good as, that of APRS and any multiorder routing strategies, in which the order of a routing strategy is determined by the amount of routing information carried in each routing message.<>"",""1558-2183"","""",""10.1109/71.250113"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250113"","""",""Routing";Computer networks;Distributed computing;Communication networks;Degradation;Adaptive systems;Fault tolerance;Telecommunication network reliability;"ARPANET"","""",""4"",""5"",""15"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"A sliding memory plane array processor,""M. H. Sunwoo";" J. K. Aggarwal"",""Department of Electronic Engineering, Ajou University, Suwon, South Korea";" NA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""601"",""612"",""A mesh-connected single-input multiple-data (SIMD) architecture called a sliding memory plane (SliM) array processor is proposed. Differing from existing mesh-connected SIMD architectures, SliM has several salient features such as a sliding memory plane that provides inter-PE communication during computation. Two I/O planes provide an I/O overlapping capability. Thus, inter-PE communication and I/O overhead can be overlapped with computation. Inter-PE communication time is invisible in most image processing tasks because the computation time is larger than the communication time on SliM. The ability to overlap inter-PE communication with computation, regardless of window size and shape and without using a coprocessor or an on-chip DMA controller is unique to SliM.<>"",""1558-2183"","""",""10.1109/71.242162"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242162"","""",""Image processing";Computer architecture;Communication system control;Shape control;Computer vision;Very large scale integration;Streaming media;Pixel;Random access memory;"Coprocessors"","""",""20"",""5"",""41"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A study of achievable speedup in distributed simulation via NULL messages,""D. Kumar";" S. Harous"",""Department of Computer Engineering and Science, Case Western Reserve University, Cleveland, OH, USA";" Department of Mathematics and Computing, Sultan Qaboos University, Oman"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""347"",""354"",""The results of an experimental study on distributed simulation of three open queuing networks are reported. The distributed simulation scheme considered is a simple variation of the scheme given by K.M. Chandy and J. Misra (1979) using NULL messages. A new approach is used to study the relationship between the overhead and performance of a distributed simulator, and the approach is illustrated by studying these three example networks. Two measures of ideal speedup of distributed simulation over sequential simulation are defined and measured. These values of ideal speedup are much less than simply the number of processors, and hence provide a more realistic value for the ideal speedup.<>"",""1558-2183"","""",""10.1109/71.210817"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210817"","""",""Computational modeling";Computer simulation;Velocity measurement;System recovery;Military computing;Distributed computing;"Mathematics"","""",""12"","""",""17"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"A symmetric fragment and replicate algorithm for distributed joins,""J. W. Stamos";" H. C. Young"",""IBM Research Division, Almaden Research Center, San Jose, CA, USA";" IBM Research Division, Almaden Research Center, San Jose, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""12"",""1345"",""1354"",""It is shown that the fragment and replicate (FR) distributed join algorithm is a special case of the symmetric fragment and replicate (SFR) algorithm, which improves the FR algorithm by reducing its communication. The SFR algorithm, like the FR algorithm, is applicable to N-way joins and nonequijoins and does tuple balancing automatically. The authors derive formulae that show how to minimize the communication in the SFR algorithm, discuss its performance on a parallel database prototype, and evaluate its practicality under various conditions. It is claimed that SFR improves the worst-case cost for a distributed join, but it will not displace specialized distributed join algorithms when the later are applicable.<>"",""1558-2183"","""",""10.1109/71.250116"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250116"","""",""Partitioning algorithms";Multicast algorithms;Costs;Computational efficiency;Distributed algorithms;Broadcasting;Unicast;Database systems;Relational databases;"Prototypes"","""",""32"",""2"",""19"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A theory of coteries: mutual exclusion in distributed systems,""T. Ibaraki";" T. Kameda"",""Department of Applied Mathematics and Physics, Faculty of Engineering, Kyoto University, Kyoto, Japan";" School of Computing Science, Simon Fraser University, Burnaby, BC, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""7"",""779"",""794"",""A coterie under a ground set U consists of subsets (called quorums) of U such that any pair of quorums intersect with each other. Nondominated (ND) coteries are of particular interest, since they are optimal in some sense. By assigning a Boolean variable to each element in U, a family of subsets of U is represented by a Boolean function of these variables. The authors characterize the ND coteries as exactly those families which can be represented by positive, self-dual functions. In this Boolean framework, it is proved that any function representing an ND coterie can be decomposed into copies of the three-majority function, and this decomposition is representable as a binary tree. It is also shown that the class of ND coteries proposed by D. Agrawal and A. El Abbadi (1989) is related to a special case of the above binary decomposition, and that the composition proposed by M.L. Neilsen and M. Mizuno (1992) is closely related to the classical Ashenhurst decomposition of Boolean functions. A number of other results are also obtained. The compactness of the proofs of most of these results indicates the suitability of Boolean algebra for the analysis of coteries.<>"",""1558-2183"","""",""10.1109/71.238300"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238300"","""",""Boolean functions";Neodymium;Boolean algebra;Binary trees;History;Mathematical model;Database systems;Physics education;Systems engineering education;"Councils"","""",""74"","""",""33"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A unified formalization of four shared-memory models,""S. V. Adve";" M. D. Hill"",""Department of Computer Sciences, University of Wisconsin, Madison, WI, USA";" Department of Computer Sciences, University of Wisconsin, Madison, WI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""613"",""624"",""The authors present a data-race-free-1, shared-memory model that unifies four earlier models: weak ordering, release consistency (with sequentially consistent special operations), the VAX memory model, and data-race-free-0. Data-race-free-1 unifies the models of weak ordering, release consistency, the VAX, and data-race-free-0 by formalizing the intuition that if programs synchronize explicitly and correctly, then sequential consistency can be guaranteed with high performance in a manner that retains the advantages of each of the four models. Data-race-free-1 expresses the programmer's interface more explicitly and formally than weak ordering and the VAX, and allows an implementation not allowed by weak ordering, release consistency, or data-race-free-0. The implementation proposal for data-race-free-1 differs from earlier implementations by permitting the execution of all synchronization operations of a processor even while previous data operations of the processor are in progress. To ensure sequential consistency, two sychronizing processors exchange information to delay later operations of the second processor that conflict with an incomplete data operation of the first processor.<>"",""1558-2183"","""",""10.1109/71.242161"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242161"","""",""Multiprocessing systems";Programming profession;Proposals;Delay;Formal specifications;Logic;Optimization;Hardware;"Out of order"","""",""107"",""2"",""28"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"A virtual bus architecture for dynamic parallel processing,""K. C. Lee"",""Bellcore, Morristown, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""121"",""130"",""To support parallel processing of data-intensive applications, the interconnection network of a parallel/distributed machine must provide high end-to-end communication bandwidth and handle the bursty and concentrated communication patterns generated by dynamic load balancing and data collection operations. A large-scale interconnection network architecture called a virtual bus is proposed. The virtual bus can scale to terabits-per-second end-to-end communication bandwidth with low queuing delay for nonuniform traffic. A terabit virtual bus architecture can be efficiently implemented for less than 5% of the total cost of an eight-thousand-node system. In addition, the virtual bus has an open system parallel interface that is flexible enough to support up to gigabytes per second data transfer rates, different grades of services, and broadcast operation. Such flexibility makes the virtual bus a plausible open system communication backbone for a broad range of applications.<>"",""1558-2183"","""",""10.1109/71.207588"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207588"","""",""Parallel processing";Multiprocessor interconnection networks;Bandwidth;Open systems;Load management;Large-scale systems;Delay;Telecommunication traffic;Traffic control;"Costs"","""",""7"",""9"",""40"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Algorithms and bounds for shortest paths and diameter in faulty hypercubes,""S. . -B. Tien";" C. S. Raghavendra"",""Department of Electrical Engineering, Southem Illinois University, Carbondale, IL, USA";" School of Electrical Engineering and Computer Science, Washington State University, Pullman, WA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""713"",""718"",""In an n-dimensional hypercube Qn, with the fault set mod F mod <2/sub n-2/, assuming S and D are not isolated, it is shown that there exists a path of length equal to at most their Hamming distance plus 4. An algorithm with complexity O( mod F mod logn) is given to find such a path. A bound for the diameter of the faulty hypercube Qn-F, when mod F mod <2/sub n-2/, as n+2 is obtained. This improves the previously known bound of n+6 obtained by A.-H. Esfahanian (1989). Worst case scenarios are constructed to show that these bounds for shortest paths and diameter are tight. It is also shown that when mod F mod <2n-2, the diameter bound is reduced to n+1 if every node has at least 2 nonfaulty neighbors and reduced to n if every node has at least 3 nonfaulty neighbors.<>"",""1558-2183"","""",""10.1109/71.242151"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242151"","""",""Hypercubes";Fault tolerance;Routing;Hardware;Algorithm design and analysis;Hamming distance;Multiprocessing systems;Costs;Multiprocessor interconnection networks;"Network topology"","""",""29"","""",""12"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"An availability model for MIN-based multiprocessors,""C. R. Das"; P. Mohapatra; L. Tien;" L. N. Bhuyan"",""Department of Computer Science and Engineering, Pennsylvania State University, University Park, PA, USA"; Department of Electrical and Computer Engineering, Iowa State University, Ames, IA, USA; AT and T Bell Laboratories, Inc., Holmdel, NJ, USA;" Department of Computer Science, Texas A and M University, College Station, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1118"",""1129"",""System decomposition is a novel technique for modeling the dependability of complex systems without constructing a single-level Markov Chain (MC). This is demonstrated in this paper for the availability computation of a class of multiprocessors that uses 4*4 switching elements for the multistage interconnection network (MIN). The availability model is known as task-based availability, where a system is considered operational as long as the task requirements are satisfied. The authors develop two simple MC's for the processors and memories and solve them using a software package, called HARP. The probabilities of i processing elements (PE's) and j memory modules (MM's) working at any time t, denoted as Pi(t) and Pj(t), are obtained from their corresponding MC's. The effect of the MIN is captured in the model by finding the number of switches required for the connection of i PE's and j MM's. A third MC is then developed for the switches to find the probability that the MIN provides the required (i*j) connection. Multiplying this term with Pi(t) and Pj(t), the probability of an (i*j) working group is obtained. The methodology is generalized to model arbitrary as well as larger size systems. Transient and steady state availabilities are computed for a variety of MIN configurations and the results are validated through simulation.<>"",""1558-2183"","""",""10.1109/71.246073"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246073"","""",""Switches";Availability;Multiprocessor interconnection networks;Fault tolerance;Computer science;Application software;Computer networks;Software packages;Steady-state;"Computational modeling"","""",""11"","""",""20"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"An efficient heuristic for permutation packet routing on meshes with low buffer requirements,""F. Makedon";" A. Symvonis"",""Department of Mathematics and Computer Science, Dartmouth College, Hanover, NH, USA";" Basser Department of Computer Science, University of Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""270"",""276"",""Even though exact algorithms exist for permutation routine of n/sup 2/ messages on a n*n mesh of processors which require constant size queues, the constants are very large and the algorithms very complicated to implement. A novel, simple heuristic for the above problem is presented. It uses constant and very small size queues (size=2). For all the simulations run on randomly generated data, the number of routing steps that is required by the algorithm is almost equal to the maximum distance a packet has to travel. A pathological case is demonstrated where the routing takes more than the optimal, and it is proved that the upper bound on the number of required steps is O(n/sup 2/). Furthermore, it is shown that the heuristic routes in optimal time inversion, transposition, and rotations, three special routing problems that appear very often in the design of parallel algorithms.<>"",""1558-2183"","""",""10.1109/71.210810"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210810"","""",""Routing";Random number generation;Algorithm design and analysis;Parallel architectures;Computer science;Sorting;Radio access networks;Pathology;Upper bound;"Parallel algorithms"","""",""4"","""",""9"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"An efficient protocol for checkpointing recovery in distributed systems,""J. L. Kim";" T. Park"",""Department of Computer Science, Texas A and M University, College Station, TX, USA";" Department of Computer Science, Texas A and M University, College Station, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""955"",""960"",""The authors present an efficient synchronized checkpointing protocol that exploits the dependency relation between processes in distributed systems. In this protocol, a process takes a checkpoint when it knows that all processes on which it computationally depends took their checkpoints, hence the process need not always wait for the decision made by the checkpointing coordinator as in the conventional synchronized protocols. As a result, the checkpointing coordination time is substantially reduced and the possibility of total abort of the checkpointing coordination is reduced.<>"",""1558-2183"","""",""10.1109/71.238629"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238629"","""",""Protocols";Checkpointing;Fault tolerant systems;Resumes;Distributed computing;Computer science;Delay effects;"Propagation delay"","""",""69"",""1"",""21"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"An implementation of F-channels,""M. Ahuja"",""Computer Science and Engineering Department, University of California, San Diego, La Jolla, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""658"",""667"",""An F-channel can permit as much concurrency as a non-first-in-first-out (FIFO) communication channel and yet retain the properties of a FIFO channel that lead to simplicity of reasoning in design and proofs of the correctness of distributed algorithms. The author presents an implementation of an F-channel on top of a non-FIFO channel that derives its non-FIFO nature from a message taking any of the alternate paths from the source to the destination in the underlying network in which each channel is either an F-channel implemented using some other implementation or recursively using the implementation presented or a FIFO channel. The correctness of the implementation is proven.<>"",""1558-2183"","""",""10.1109/71.242157"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242157"","""",""Algorithm design and analysis";Concurrent computing;Distributed algorithms;Communication channels;Computer architecture;Computer networks;Protocols;Distributed computing;Multimedia systems;"Systems engineering and theory"","""",""8"",""1"",""25"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"An optimal implementation of broadcasting with selective reduction,""L. F. Lindon";" S. G. Akl"",""Department of Vomputing and Information Science, Queen's University, Kingston, ONT, Canada";" Department of Computer and Information Science, Queen's University, Kingston, ONT, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""256"",""269"",""A model of parallel computation called broadcasting with selective reduction (BSR) can be viewed as a concurrent-read concurrent-write (CRCW) parallel random access machine (PRAM) with one extension. An additional type of concurrent memory access is permitted in BSR, namely the BROADCAST instruction by means of which all N processors may gain access to all M memory locations simultaneously for the purpose of writing. At each memory location, a subset of the incoming broadcast data is selected and reduced to one value finally stored in that location. For several problems, BSR algorithms are known which require fewer steps than the corresponding best-known PRAM algorithms, using the same number of processors. A circuit is introduced to implement the BSR model, and it is shown that, in size and depth, the circuit presented is of the same order as an optimal circuit implementing the PRAM. Thus, if it is reasonable to assume that CRCW PRAM instructions execute in constant time, the assumption of a constant time BROADCAST instruction is no less reasonable.<>"",""1558-2183"","""",""10.1109/71.210809"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210809"","""",""Broadcasting";Phase change random access memory;Writing;Concurrent computing;Read-write memory;Computational modeling;Integrated circuit interconnections;Casting;"Parallel algorithms"","""",""23"","""",""34"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Analysis of macro-dataflow dynamic scheduling on nonuniform memory access architectures,""M. Al-Mouhamed"",""Department of Computer Engineering, King Fahd University of Petroleum and Minerals, Dhahran, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""875"",""888"",""The author studies dynamic scheduling of computational tasks with communication costs using nonuniform memory access architecture. The computing model assumes that data transfer can be partitioned into parallel and sequential parts with respect to the task execution. A scheduling heuristic, called least-communication (LC), together with a two-level scheduler is proposed in an attempt to minimize the finish time. The LC selects the task that removes the largest amount of remaining data transfer, if no such tasks are available the task that has been ready to run at the earliest is selected first. The time complexity of LC is O(n/sub 2/). Testing the finish time of LC and first-come first-served scheduling (FCFS) shows that LC is useful for tasks having moderate granularity and whose computation and communication requirements vary widely for different data sets.<>"",""1558-2183"","""",""10.1109/71.238623"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238623"","""",""Dynamic scheduling";Memory architecture;Processor scheduling;Samarium;Switches;Computer architecture;Costs;Computer interfaces;Time sharing computer systems;"Scheduling algorithm"","""",""7"",""2"",""26"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Analytic models of adaptive load sharing schemes in distributed real-time systems,""K. G. Shin";" C. . -J. Hou"",""Real-Time Computing Laboratory, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA";" Real-Time Computing Laboratory, Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""7"",""740"",""761"",""In a distributed real-time system, nonuniform task arrivals may temporarily overload some nodes while leaving some other nodes idle. As a result, some of the tasks on an overloaded node may miss their deadlines even if the overall system has the capacity to meet the deadlines of all tasks. A decentralized, dynamic load sharing (LS) scheme has been proposed as a solution to this problem. Analytic queuing models to comparatively evaluate this LS scheme as well as three other schemes-no LS, LS with random selection of a receiver node, and LS with perfect information- are developed. The evolution of a node's load state is modeled as a continuous-time semi-Markov process, where cumulative execution time (CET), rather than the commonly-used queue length (QL), is employed to describe the workload of a node. The proposed scheme is compared against other LS schemes. The validity of analytic models is checked with simulations. Both analytic and simulation results indicate that by using judicious exchange/use of state information and Bayesian decision mechanism, the proposed scheme makes a significant improvement over other existing LS schemes in minimizing the probability of dynamic failure.<>"",""1558-2183"","""",""10.1109/71.238298"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238298"","""",""Load modeling";Real time systems;Information analysis;Analytical models;Bayesian methods;Processor scheduling;Chaos;Queueing analysis;Measurement;"Failure analysis"","""",""21"",""3"",""42"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Application-dependent dynamic monitoring of distributed and parallel systems,""D. M. Ogle"; K. Schwan;" R. Snodgrass"",""Experimental Systems Department, IBM, Corporation, NC, USA"; College of Computing, Georgia Institute of Technology, Atlanta, GA, USA;" Department of Computer Science, University of Arizona Tucson, Tucson, AZ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""7"",""762"",""778"",""Achieving high performance for parallel or distributed programs often requires substantial amounts of information about the programs themselves, about the systems on which they are executing, and about specific program runs. The monitoring system that collects, analyzes, and makes application-dependent monitoring information available to the programmer and to the executing program is presented. The system may be used for off-line program analysis, for on-line debugging, and for making on-line, dynamic changes to parallel or distributed programs to enhance their performance. The authors use a high-level, uniform data model for the representation of program information and monitoring data. They show how this model may be used for the specification of program views and attributes for monitoring, and demonstrate how such specifications can be translated into efficient, program-specific monitoring code that uses alternative mechanisms for the distributed analysis and collection to be performed for the specified views. The model's utility has been demonstrated on a wide variety of parallel machines.<>"",""1558-2183"","""",""10.1109/71.238299"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238299"","""",""Monitoring";Information analysis;Programming profession;Performance analysis;Debugging;Data models;Parallel machines;Local area networks;Writing;"Operating systems"","""",""36"",""6"",""79"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Balanced parallel sort on hypercube multiprocessors,""B. Abali"; F. Ozguner;" A. Bataineh"",""Bilkent University, Ankara, Turkey"; Department of Electrical Engineering, Ohio State Uinversity, Columbus, OH, USA;" Cray Research, Inc., Eagan, MN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""572"",""581"",""A parallel sorting algorithm for sorting n elements evenly distributed over 2/sup d/ p nodes of a d-dimensional hypercube is presented. The average running time of the algorithm is O((n log n)/p+p log 2n). The algorithm maintains a perfect load balance in the nodes by determining the (kn/p)th elements (k1,. . ., (p-1)) of the final sorted list in advance. These p-1 keys are used to partition the sorted sublists in each node to redistribute data to the nodes to be merged in parallel. The nodes finish the sort with an equal number of elements (n/p) regardless of the data distribution. A parallel selection algorithm for determining the balanced partition keys in O(p log2n) time is presented. The speed of the sorting algorithm is further enhanced by the distance-d communication capability of the iPSC/2 hypercube computer and a novel conflict-free routing algorithm. Experimental results on a 16-node hypercube computer show that the sorting algorithm is competitive with the previous algorithms and faster for skewed data distributions.<>"",""1558-2183"","""",""10.1109/71.224220"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224220"","""",""Hypercubes";Partitioning algorithms;Sorting;Routing;Distributed computing;Parallel algorithms;Databases;Parallel processing;Delay effects;"Message passing"","""",""16"",""2"",""21"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Benchmarking parallel processing platforms: an applications perspective,""R. B. Mueller-Thuns"; D. G. Saab; R. F. Damiano;" J. A. Abraham"",""Cadence Design Systems, Inc., San Jose, CA, USA"; Reliable and High Performance Computing, University of Illinois, Urbana-Champaign, Urbana, IL, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA;" Computer Engineering Research Center, University of Texas, Austin, Austin, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""947"",""954"",""Given the increased availability of general purpose parallel computers two issues arise: One needs to compare the performance of the different available platforms using realistic examples, and it is necessary to write application software that can be ported easily in order to take advantage of different platforms. The authors address these issues from an applications point of view. They are interested in the use of general purpose parallel computers for simulation tasks needed during the design of very large scale integrated (VLSI) circuits. They characterize the simulation task as a useful benchmark and introduce a high level process view of parallel simulation that is helpful for deriving portable parallel programs. Details of the partitioning strategy and the simulation algorithm used in the application are given. They discuss their implementation on different parallel machines and give statistics of various experiments.<>"",""1558-2183"","""",""10.1109/71.238628"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238628"","""",""Parallel processing";Computational modeling;Circuit simulation;Application software;Concurrent computing;Very large scale integration;Availability;Computer simulation;Partitioning algorithms;"Parallel machines"","""",""6"","""",""23"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"Block scheduling of iterative algorithms and graph-level priority scheduling in a simulated data-flow multiprocessor,""P. Evripidou";" J. . -L. Gaudiot"",""USC Information Sciences Institute, Marina del Rey, CA, USA";" Department of Electrical Engineering-Systems, University of Southern California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""398"",""413"",""Iterative methods for solving linear systems are discussed. Although these methods are inherently highly sequential, it is shown that much parallelism could be exploited in a data-flow system by scheduling the iterative part of the algorithms in blocks and by looking ahead across several iterations. This approach is general and will apply to other iterative and loop-based problems. It is also demonstrated by simulation that relying solely on data-driven scheduling of parallel and unrolled loops results in low resource utilization and poor performance. A graph-level priority scheduling mechanism has been developed that greatly improves resource utilization and yields higher performance.<>"",""1558-2183"","""",""10.1109/71.219755"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219755"","""",""Iterative algorithms";Iterative methods;Large-scale systems;Resource management;Parallel processing;Multiprocessing systems;Runtime;Linear systems;Scheduling algorithm;"Upper bound"","""",""7"","""",""22"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Capsules: a shared memory access mechanism for Concurrent C/C++,""N. H. Gehani"",""AT and T Bell Laboratories, Inc., Murray Hill, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""7"",""795"",""811"",""Concurrent C/C++ is a superset of C and C++ that provides parallel programming facilities based on message passing. Upon porting Concurrent C/C++to a shared memory multiprocessor, the authors believed it would be appropriate to supplement Concurrent C/C++ with explicit facilities for synchronizing accesses to shared data structures. The capsule, which is a shared memory access mechanism designed especially for Concurrent C/C++ to match the C++data abstraction facility called the class, is discussed. Capsules are like monitors but they have significant advantages. Capsules satisfy T. Bloom's (1979) criteria for expressiveness of synchronization conditions, support inheritance, allow operations to execute in parallel, and permit them to time out. The design of capsules is reviewed. The author evaluates existing shared memory mechanisms, describes capsules, gives examples of capsules, compares capsules with monitors, and discusses how capsules are implemented by the Concurrent C compiler.<>"",""1558-2183"","""",""10.1109/71.238301"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238301"","""",""Message passing";Parallel programming;Data structures;Memory architecture;Computer languages;Strontium;Specification languages;"Programming profession"","""",""13"",""5"",""46"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Clarifications and corrections to 'performance analysis of a generalized class of m-level hierarchical multiprocessor systems' (Mar 1992 129-138),""I. O. Mahgoub";" A. K. Elmagarmid"",""Department of Computer Science and Engineering, Florida Atlantic University, Boca Raton, FL, USA";" Department of Computer Sciences, Purdue University, West Lafayette, IN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""719"",""720"",""Several items in the above-titled work (ibid., vol.3, no.2, pp.129-138, Mar. 1992) are clarified and corrected.<>"",""1558-2183"","""",""10.1109/71.242150"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242150"","""",""Performance analysis";Multiprocessing systems;Analytical models;Computer science;Intelligent networks;Multiprocessor interconnection networks;Inspection;"Bandwidth"","""","""","""",""1"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Clocking arbitrarily large computing structures under constant skew bound,""A. El-Amawy"",""Electrical and Computcr Engineering Department, Louisiana State University, Baton Rouge, LA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""241"",""255"",""A scheme for global synchronization of arbitrarily large computing structures such that clock skew between any two communicating cells is bounded above by a constant is described. The scheme utilizes clock nodes that perform simple processing on clock signals to maintain a constant skew bound irrespective of the size of the computing structure. Among the salient features of the scheme is the interdependence between network topology, skew upper bound, and maximum clocking rate achievable. A 2-D mesh framework is used to present the concepts, introduce three network designs, and to prove some basic results. For each network the (constant) upper bound on clock skew between any two communicating processors, is established, and its independence of network size is shown. Simulations were carried out to verify correctness and to check the workability of the scheme. A 4*4 network was built and successfully tested for stability. Such issues as node design, clocking of nonplanar structures such as hypercubes, and the concept of fuse programmed clock networks are addressed.<>"",""1558-2183"","""",""10.1109/71.210808"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210808"","""",""Clocks";Upper bound;Synchronization;Signal processing;Network topology;Workability;Testing;Stability;Hypercubes;"Fuses"","""",""7"",""4"",""15"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Collecting unused processing capacity: an analysis of transient distributed systems,""L. Kleinrock";" W. Korfhage"",""Department of Computer Science, University of California, Los Angeles, USA";" Department of Computer Science, Polytechnic University, Brooklyn, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""535"",""546"",""It is suggested that if the large numbers of idle computers and workstations in distributed systems could be used then considerable computing power could be harnessed at low cost. Such systems are analyzed using Brownian motion with drift to model the execution of a program distributed over the idle computers in a network of idle and busy processors. The ways in which the use of these transient processors affects a program's execution time is determined. The probability density of a program's finishing time on both single and multiple transient processors is found. These results are explored for qualitative insight. Some approximations for the finishing time probability density are suggested.<>"",""1558-2183"","""",""10.1109/71.224216"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224216"","""",""Transient analysis";Workstations;Finishing;Computer networks;Network servers;Distributed computing;Costs;Power system modeling;Distributed processing;"Laboratories"","""",""16"","""",""33"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Concurrent iterative algorithm for Toeplitz-like linear systems,""V. Y. Pan"",""Department of Computer Science, State University of New York, Albany, Albany, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""592"",""600"",""A nonsingular n*n matrix A is given with its short displacement generator. It has small displacement rank bounded by a fixed constant. The class of such matrices generalizes Toeplitz matrices. A good initial approximation to a short displacement generator for A/sup -1/ is readily available. Ways to refine this approximation and numerically compute a displacement generator of A/sup -1/ and the solution vector x=A/sup -1/b to a linear system Ax=b by using O(log/sup 2/n) parallel arithmetic steps and n processors are presented. These results are extended to some other important classes of dense structure matrices.<>"",""1558-2183"","""",""10.1109/71.224221"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224221"","""",""Iterative algorithms";Linear systems;Arithmetic;Real time systems;Computer science;Phase change random access memory;Concurrent computing;Vectors;Equations;"Parallel algorithms"","""",""17"","""",""56"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Conservative parallel simulation of continuous time Markov chains using uniformization,""P. Heidelberger";" D. M. Nicol"",""IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA";" Department of Computer Science, College of William and Mary, Williamsburg, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""906"",""921"",""The authors describe parallel algorithms for simulating certain continuous time Markov chains, such as those arising in queueing network models of distributed computing systems or communications systems. The algorithms are based on the technique of uniformization. Two variations of a conservative parallel simulation algorithm are presented. In each algorithm, a relatively short presimulation is performed to identify those times, and only those times, at which the simulation algorithm requires processor pairs to synchronize. Speedup studies of the algorithms, performed on a 16-node Intel iPSC/2 hypercube, are presented and discussed.<>"",""1558-2183"","""",""10.1109/71.238625"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238625"","""",""Computational modeling";Distributed computing;Time warp simulation;Discrete event simulation;Computer aided manufacturing;Clustering algorithms;Predictive models;Parallel algorithms;Hypercubes;"Virtual manufacturing"","""",""34"","""",""43"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Control versus data flow in parallel database machines,""W. B. Teeuw";" H. M. Blanken"",""Department of Computer Science, Information Systems-Databases, University of Twente, Enschede, Netherlands";" Department of Computer Science, Information Systems-Databases, University of Twente, Enschede, Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""11"",""1265"",""1279"",""The execution of a query in a parallel database machine can be controlled in either a control flow way, or in a data flow way. In the former case a single system node controls the entire query execution. In the latter case the processes that execute the query, although possibly running on different nodes of the system, trigger each other. Lately, many database research projects focus on data flow control since it should enhance response times and throughput. The authors study control versus data flow with regard to controlling the execution of database queries. An analytical model is used to compare control and data flow in order to gain insights into the question which mechanism is better under which circumstances. Also, some systems using data flow techniques are described, and the authors investigate to which degree they are really data flow. The results show that for particular types of queries data flow is very attractive, since it reduces the number of control messages and balances these messages over the nodes.<>"",""1558-2183"","""",""10.1109/71.250104"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250104"","""",""Database machines";Database systems;Parallel processing;Local area networks;Distributed databases;Concurrent computing;Computer science;Transaction databases;Bandwidth;"Disk drives"","""",""5"",""1"",""19"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Cost and time-cost effectiveness of multiprocessing,""D. Sarkar"",""Department of Mathematics and Computer Science, University of Miami, Coral Gables, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""704"",""712"",""Speedup and efficiency, two measures for performance of pipelined computers, are now used to evaluate performance of parallel algorithms for multiprocessor systems. However, these measures consider only the computation time and number of processors used and do not include the number of the communication links in the system. The author defines two new measures, cost effectiveness and time-cost effectiveness, for evaluating performance of a parallel algorithm for a multiprocessor system. From these two measures two characterization factors for multiprocessor systems are defined and used to analyze some well-known multiprocessor systems. It is found that for a given penalty function, every multiprocessor architecture has an optimal number of processors that produces maximum profit. If too many processors are used, the higher cost of the system reduces the profit obtained from the faster solution. On the other hand, if too few processors are used, the penalty paid for taking a longer time to obtain the solution reduces the profit.<>"",""1558-2183"","""",""10.1109/71.242152"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242152"","""",""Costs";Parallel algorithms;Multiprocessing systems;Hypercubes;Computer architecture;Parallel processing;Concurrent computing;Hardware;Phase change random access memory;"Velocity measurement"","""",""16"","""",""20"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Cube connected Mobius ladders: an inherently deadlock-free fixed degree network,""D. J. Pritchard";" D. A. Nicole"",""Department of Computer Science, University of Liverpool, Liverpool, UK";" Department of Electronics and Computer Science, University of Southampton, UK"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""111"",""117"",""The authors introduce a multiprocessor interconnection network, known as cube-connected Mobius ladders, which has an inherently deadlock-free routing strategy and hence has none of the buffering and computational overhead required by deadlock-avoidance message passing algorithms. The basic network has a diameter phi of 4n-1 for n2/sup n+2 /nodes and has a fixed node degree of 4. The network can be interval routed in two stages and can be represented as a Cayley graph. This is the only practical fixed degree topology of size O(2/sup phi /) which has an inherently deadlock-free routing strategy, making it ideally suited for medium and large sized transputer networks.<>"",""1558-2183"","""",""10.1109/71.205658"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205658"","""",""System recovery";Fault tolerance;Parallel processing;Hardware;Hypercubes;Distributed computing;Real time systems;Fault tolerant systems;Computer architecture;"Safety"","""",""7"","""",""18"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Data management and control-flow aspects of an SIMD/SPMD parallel language/compiler,""M. A. Nichols"; H. J. Siegel;" H. G. Dietz"",""NCR Limited, San Diego, CA, USA"; Parallel Processing Laboratory, School of Electrical Engineering, Purdue University, West Lafayette, IN, USA;" Parallel Processing Laboratory, School of Electrical Engineering, Purdue University, West Lafayette, IN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""222"",""234"",""Features of an explicitly parallel programming language targeted for reconfigurable parallel processing systems, where the machine's N processing elements (PEs) are capable of operating in both the SIMD and SPMD modes of parallelism, are described. The SPMD (single program-multiple data) mode of parallelism is a subset of the MIMD mode where all processors execute the same program. By providing all aspects of the language with an SIMD mode version and an SPMD mode version that are syntactically and semantically equivalent, the language facilitates experimentation with and exploitation of hybrid SIMD/SPMD machines. Language constructs (and their implementations) for data management, data-dependent control-flow, and PE-address-dependent control-flow are presented. These constructs are based on experience gained from programming a parallel machine prototype and are being incorporated into a compiler under development. Much of the research presented is applicable to general SIMD machines and MIMD machines.<>"",""1558-2183"","""",""10.1109/71.207596"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207596"","""",""Parallel languages";Parallel processing;Program processors;Parallel programming;Parallel machines;Concurrent computing;Control systems;Prototypes;Fault tolerance;"Broadcasting"","""",""18"",""9"",""55"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"dBCube: a new class of hierarchical multiprocessor interconnection networks with area efficient layout,""Chienhua Chen";" D. P. Agrawal"",""Dept. of Comput. Sci. & Eng., Tatung Inst. of Technol., Taipei, Taiwan";" NA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""12"",""1332"",""1344"",""Introduces a class of hierarchical networks that is suitable for implementation of large multi-computers in VLSI with wafer scale integration (VLSI/WSI) technology. These networks, which are termed dBCube, employ the hypercube topology as a basic cluster, connect many of these clusters using a de Bruijn graph, and maintain the node connectivity to be the same for all nodes product graph. The size of this class of regular networks can be easily extended by increments of a cluster size. Local communication, to be satisfied by the hypercube topology, allows easy embedding of existing parallel algorithms, while the de Bruijn graph, which was chosen for JPL's 8096-node multiprocessor, provides the shortest distance between clusters running different parts of an application. A scheme for obtaining WSI layout is introduced and used to estimate the number of tracks needed and the required area of the wafer. The exact number of tracks in the hypercube and an approximation for the de Bruijn graph are also obtained. Tradeoffs of area versus static parameters and the size of the hypercube versus that of the de Bruijn graph are also discussed.<>"",""1558-2183"","""",""10.1109/71.250115"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250115"","""",""Multiprocessor interconnection networks";Hypercubes;Very large scale integration;Wafer scale integration;Network topology;Multiprocessing systems;Message passing;Nearest neighbor searches;Computer networks;"Concurrent computing"","""",""62"","""",""30"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Deadlock-free adaptive routing in multicomputer networks using virtual channels,""W. J. Dally";" H. Aoki"",""Artificial Intelligence Laboratory and Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA";" Artificial Intelligence Laboratory and Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""466"",""475"",""The use of adaptive routing in a multicomputer interconnection network improves network performance by using all available paths and provides fault tolerance by allowing messages to be routed around failed channels and nodes. Two deadlock-free adaptive routing algorithms are described. Both algorithms allocate virtual channels using a count of the number of dimension reversals a packet has performed to eliminate cycles in resource dependency graphs. The static algorithm eliminates cycles in the network channel dependency graph. The dynamic algorithm improves virtual channel utilization by permitting dependency cycles and instead eliminating cycles in the packet wait-for graph. It is proved that these algorithms are deadlock-free. Experimental measurements of their performance are presented.<>"",""1558-2183"","""",""10.1109/71.219761"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219761"","""",""System recovery";Routing;Intelligent networks;Multiprocessor interconnection networks;Throughput;Network topology;Communication system traffic control;Concurrent computing;Resource management;"Heuristic algorithms"","""",""374"",""51"",""46"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Declustering: a new multiprocessor scheduling technique,""G. C. Sih";" E. A. Lee"",""Qualcomm, Inc., San Diego, CA, USA";" Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""625"",""637"",""The authors present a new compile-time scheduling heuristic called declustering, which schedules acyclic precedence graphs that fit the synchronous data flow (SDF) model onto multiprocessor architectures. This technique accounts for interprocessor communication (IPC) overheads and considers interconnection constraints in the architecture so that shared resource contention can be avoided. The algorithm initially invokes a new clustering method that uses graph-analysis techniques to isolate parallelism instances. When constructing an initial set of clusters, this procedure explicitly addresses the tradeoff between exploiting parallelism and incurring communication cost. By hierarchically combining these clusters and then systematically decomposing this hierarchy, the declustering method exposes parallelism instances in order of importance and attains a cluster granularity that fits the characteristics of the architecture. It is shown that declustering retains the clustering advantage of avoiding IPC, yet overcomes the inflexibility associated with traditional clustering approaches.<>"",""1558-2183"","""",""10.1109/71.242160"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242160"","""",""Processor scheduling";Parallel processing;Signal processing algorithms;Scheduling algorithm;Computer architecture;Flow graphs;Clustering algorithms;Clustering methods;Costs;"Hardware"","""",""49"","""",""15"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Dependence uniformization: a loop parallelization technique,""T. H. Tzen";" L. M. Ni"",""Convex Computer Corporation, Richardson, TX, USA";" Department of Computer Science, Michigan State University, East Lansing, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""547"",""558"",""Data dependence uniformization, a method for overcoming the difficulties in parallelizing a doubly nested loop with irregular dependence constraints is proposed. This approach is based on the concept of vector decomposition. A simple set of basic dependences is developed from which all dependence constraints can be composed. The set of basic dependences is added to every iteration to replace all original dependences so that the dependence constraints become uniform. An efficient synchronization method is presented to obey the uniform dependence constraints in every iteration.<>"",""1558-2183"","""",""10.1109/71.224217"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224217"","""",""Parallel processing";Processor scheduling;Parallel architectures;Computer science;Program processors;Pattern analysis;"Degradation"","""",""44"",""1"",""23"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Dependency analysis-a Petri-net-based technique for synthesizing large concurrent systems,""Y. Chen"; W. T. Tsai;" D. Chao"",""Hitachi Software Engineering America Limited, San Bruno, CA, USA"; Department of Computer Science, University of Minnesota, Minneapolis, MN, USA;" Department of Computer Science, New Jersey Institute of Technology, Newark, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""414"",""426"",""Petri nets (PNs) are frequently used in modeling, designing, and analyzing concurrent systems. A problem with PNs, in the general case, is that they require high computational complexity to analyze their properties, such as reachability, liveness, and boundedness. To avoid this problem, synthesis techniques for constructing large PNs are presented. Using these techniques, the behavior of the constructed PN can be determined by local analysis that uses known properties of the given nets. Thus, the high computational complexity of global analysis is bypassed. A synthesis technique that explores dependency relations in PNs is presented. It synthesizes large PNs by combining smaller PNs of arbitrary topology structures, and the combination is verified efficiently by dependency analysis. A large system based on a PN can be built up by repeated applications of the technique.<>"",""1558-2183"","""",""10.1109/71.219756"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219756"","""",""Protocols";Computer science;Performance analysis;Chaos;Computational complexity;Topology;Petri nets;Explosions;System recovery;"Software engineering"","""",""17"","""",""19"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Determining possible event orders by analyzing sequential traces,""D. P. Helmbold"; C. E. McDowell;" J. . -Z. Wang"",""Computer and Information Sciences Department, University of California, Santa Cruz, CA, USA"; Computer and Information Sciences Department, University of California, Santa Cruz, CA, USA;" Sun Microsystems, Inc., Mountain View, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""7"",""827"",""840"",""One of the fundamental problems encountered when debugging a parallel program is determining the possible orders in which events could have occurred. Various problems, such as data races and intermittent deadlock, arise when there is insufficient synchronization between the tasks in a parallel program. A sequential trace of an execution can be misleading, as it implies additional event orderings, distorting the concurrent nature of the computation. Algorithms to generate, from the trace of an execution, those event orderings that can be relied on by the programmer are described. By its very nature, the information in an execution trace pertains only to that execution of the program, and may not generalize to other executions. This difficulty is mitigated by defining an inferred program based on the trace and original program, analyzing this inferred program, and showing how the inferred program relates to the original. The results of the algorithms can be used by other automated tools such as a data race detector or constraint checker.<>"",""1558-2183"","""",""10.1109/71.238303"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238303"","""",""Debugging";System recovery;Concurrent computing;Programming profession;Detectors;Algorithm design and analysis;Writing;Sun;Protection;"Data analysis"","""",""15"",""13"",""20"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Determining the number of remote sites accessed in distributed transaction processing,""A. Thomasian"",""IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""99"",""103"",""The number of remote accesses and the number of distinct remote sites accessed by global transactions in a distributed database environment are characterized. Both measures are important in determining the overhead of distributed transaction processing and are affected by the distribution of transaction size (number of distinct objects accessed by a transaction) and the distribution of remote accesses made by a transaction. Keeping the mean transaction size fixed, it is shown that in the case of some commonly used distributions for the number of remote accesses variable size transactions access fewer distinct remote sites on the average than fixed size transactions. Also, for these and some other distributions the mean number of remote accesses is independent of the distribution of transaction size, but this is not generally true.<>"",""1558-2183"","""",""10.1109/71.205656"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205656"","""",""Transaction databases";Distributed databases;Relational databases;Performance analysis;Protocols;Size measurement;Database machines;Delay;Telecommunication traffic;"Costs"","""",""5"","""",""10"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Distributed concurrency control based on limited wait-depth,""P. A. Franaszek"; J. R. Haritsa; J. T. Robinson;" A. Thomasian"",""IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA"; Systems Research Center, University of Maryland, College Park, MD, USA; IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA;" IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""11"",""1246"",""1264"",""The performance of high-volume transaction processing systems for business applications is determined by the degree of contention for hardware resources as well as for data. Hardware resource requirements may be met cost-effectively with a data-partitioned or shared-nothing architecture. However, the two-phase locking (2PL) concurrency control method may restrict the performance of a shared-nothing system more severely than that of a centralized system due to increased lock holding times. Deadlock detection and resolution are an added complicating factor in shared-nothing systems. The authors describe distributed Wait-Depth Limited (WDL) concurrency control (CC), a locking-based distributed CC method that limits the wait-depth of blocked transactions to one, thus preventing the occurrence of deadlocks. Several implementations of distributed WDL which vary in the number of messages and the amount of information available for decision making are discussed. The performance of a generic implementation of distributed WDL is compared with distributed 2PL (with general-waiting policy) and the Wound-Wait CC method through a detailed simulation. It is shown that distributed WDL behaves similarly to 2PL for low lock contention levels, but for substantial lock contention levels (caused by higher degrees of transaction concurrency), distributed WDL outperforms the other methods to a significant degree.<>"",""1558-2183"","""",""10.1109/71.250103"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250103"","""",""Concurrency control";Tin;Costs;Hardware;System recovery;Distributed databases;Transaction databases;Bandwidth;Decision making;"Concurrent computing"","""",""17"",""1"",""23"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"Dynamic synchrony among atomic actions,""G. . -C. Roman"; Y. J. Plun;" C. D. Wilcox"",""Department of Computer Science, Washington University, Saint Louis, MO, USA"; Department of Computer Science, Washington University, Saint Louis, MO, USA;" Department of Computer Science, Washington University, Saint Louis, MO, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""677"",""685"",""Synchrony continues to be an important concern in concurrent programming. Existing languages and models have introduced a great diversity of constructs for expressing and managing synchronization among sequential processes or atomic actions. The authors put forth a model in which synchrony is viewed as a relation among atomic actions, a relation which may evolve with time. The model is shown to be convenient for expressing formally the semantics of synchrony as it appears in many of the languages and models proposed to date. Among such models Swarm is singled out for its use of dynamic synchrony. The Swarm notation is briefly reviewed. A new concurrent algorithm for the leader election problem illustrates the use of dynamic synchrony in Swarm.<>"",""1558-2183"","""",""10.1109/71.242155"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242155"","""",""Distributed computing";Vehicle dynamics;Nominations and elections;Computer languages;Vehicles;History;Concurrent computing;Computer architecture;Systolic arrays;"Very large scale integration"","""",""2"","""",""19"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Efficient algorithms for system diagnosis with both processor and comparator faults,""Y. Chen"; W. Bucken;" K. Echtle"",""Inst. fuer Rechnerentwurf und Fehlertoleranz, Karlsruhe Univ., Germany"; Inst. fuer Rechnerentwurf und Fehlertoleranz, Karlsruhe Univ., Germany;" NA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""371"",""381"",""For the comparison-based self-diagnosis of multiprocessor systems, an extended model that considers both processor and comparator faults is presented. It is shown that in this model the system diagnosability is t<or=Z delta /2Z, where delta is the minimum vertex degree of the system graph. However, if the number of faulty comparators is assumed not to exceed the number of faulty processors, the diagnosability of the model reaches t<or= delta . An optimal O( mod E mod ) algorithm, where E is the set of comparators, is given for identifying all faulty processors and comparators, provided that the total number of faulty components does not exceed the system diagnosability, and an O( mod E mod )/sup 2/ algorithm for the case t<or= delta is also presented. These efficient algorithms determine the faulty processors by calculating each processor's weight, which is mainly defined by the number of adjacent relative tests stating 'agreement'. After sorting the processors according to their weights, the algorithms determine all faulty components by separating the sorted processor list.<<ETX>>"",""1558-2183"","""",""10.1109/71.219748"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219748"","""",""Fault diagnosis";System testing;Automatic testing;Sorting;Fault tolerant systems;Multiprocessing systems;Microprocessors;Performance evaluation;"Hardware"","""",""6"","""",""16"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Efficient mapping algorithms for a class of hierarchical systems,""S. G. Ziavras"",""Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""11"",""1230"",""1245"",""Proposes techniques for mapping application algorithms onto a class of hierarchically structured parallel computing systems. Multiprocessors of this type are capable of efficiently solving a variety of scientific problems because they can efficiently implement both local and global operations for data in a two-dimensional array format. Among the set of candidate application domains, low-level and intermediate-level image processing and computer vision (IPCV) are characterized by high-performance requirements. Emphasis is given to IPCV algorithms. The importance of the mapping techniques stems from the fact that the current technology cannot be used to build cost-effective and efficient systems composed of very large numbers of processors, so the performance of various systems of lower cost should be investigated. Both analytical and simulation results prove the effectiveness and efficiency of the proposed mapping techniques.<>"",""1558-2183"","""",""10.1109/71.250102"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250102"","""",""Hierarchical systems";Multilevel systems;Image processing;Computer vision;Topology;Hypercubes;Parallel processing;Application software;Costs;"Computational modeling"","""",""6"",""1"",""28"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Efficient routing schemes for multiple broadcasts in hypercubes,""G. D. Stamoulis";" J. N. Tsitsiklis"",""Lab. for Inf. & Decision Syst., MIT, Cambridge, MA, USA";" Lab. for Inf. & Decision Syst., MIT, Cambridge, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""7"",""725"",""739"",""The authors analyze the problem in which each node of the binary hypercube independently generates packets according to a Poisson process with rate lambda ";" each of the packets is to be broadcast to all other nodes. Assuming unit packet length and no other communications taking place, it is observed that the system can be stable in steady-state only if the load factor rho identical to lambda (2/sup d/-1)/d satisfies rho <1 where d is the dimensionality (diameter) of the hypercube. Moreover, the authors establish some lower bounds for the steady-state average delay D per packet and devise and analyze two distributed routing schemes that are efficient in the sense that stability is maintained for all rho < rho * where rho * does not depend on the dimensionality d of the network, while the average delay D per packet satisfies D<or=Kd(1+ rho ) for small values of rho (with constant K). The performance evaluation is rigorous for one scheme, while for the other the authors resort to approximations and simulations.<<ETX>>"",""1558-2183"","""",""10.1109/71.238297"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238297"","""",""Routing";Broadcasting;Hypercubes;Stability;Steady-state;Delay;Performance analysis;Computational modeling;Analytical models;"Concurrent computing"","""",""23"",""5"",""16"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Evaluation of parallel copying garbage collection on a shared-memory multiprocessor,""A. Imai";" E. Tick"",""New Generation Computer Technology (ICOT)";" Department of Computer Science, University of Oregon, Eugene, OR, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""9"",""1030"",""1040"",""A parallel copying garbage collection algorithm for symbolic languages executing on shared-memory multiprocessors is proposed. The algorithm is an extension of Baker's sequential algorithm with a novel method of heap allocation to prevent fragmentation and facilitate load distribution during garbage collection. An implementation of the algorithm within a concurrent logic programming system, VPIM, has been evaluated and the results, for a wide selection of benchmarks, are analyzed here. The authors show 1) how much the algorithm reduces the contention for critical sections during garbage collection, 2) how well the load-balancing strategy works and its expected overheads, and 3) the expected speedup achieved by the algorithm.<>"",""1558-2183"","""",""10.1109/71.243529"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=243529"","""",""Logic programming";Algorithm design and analysis;Clustering algorithms;Production;Storage automation;Logic testing;Computer science;System performance;Parallel algorithms;"Performance analysis"","""",""15"",""30"",""22"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Fast algorithms for distributed resource allocation,""I. Page"; T. Jacob;" E. Chern"",""Department of Computer Science, University of Texas, Dallas, Richardson, TX, USA"; Department of Computer Science, University of North Texas, Denton, TX, USA;" Data Communications and Networks Division, Northern Telecom, Inc., Richardson, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""188"",""197"",""Two new algorithms for the distributed static resource allocation problem are presented. The first algorithm, which shows excellent average case behavior in simulation requires the maintenance of a global queue. The second, which needs only local communication, has polynomial waiting time and exhibits better average case behavior than the only other known polynomial time algorithm. Both algorithms have polynomial message complexity.<>"",""1558-2183"","""",""10.1109/71.207594"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207594"","""",""Resource management";Automatic control;Polynomials;System recovery;Protection;Jacobian matrices;Computer science;Algorithm design and analysis;Data communication;"Telecommunications"","""",""13"","""",""10"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Fast allocation of processes in distributed and parallel systems,""C. M. Woodside";" G. G. Monforton"",""Real-Time and Distributed Systems Group, Department of Systems and Computer Engineering, Carleton University, Ottawa, ONT, Canada";" Bell Northern Research Limited, Ottawa, ONT, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""164"",""174"",""MULTIFIT-COM, a static task allocator which could be incorporated into an automated compiler/linker/loader for distributed processing systems, is presented. The allocator uses performance information for the processes making up the system in order to determine an appropriate mapping of tasks onto processors. It uses several heuristic extensions of the MULTIFIT bin-packing algorithm to find an allocation that will offer a high system throughput, taking into account the expected execution and interprocessor communication requirements of the software on the given hardware architecture. Throughput is evaluated by an asymptotic bound for saturated conditions and under an assumption that only processing resources are required. A set of options are proposed for each of the allocator's major steps. An evaluation was made on 680 small randomly generated examples. Using all the search options, an average performance difference of just over 1% was obtained. Using a carefully chosen small subset of only four options, a further degradation of just over 1.5% was obtained. The allocator is also applied to a digital signal processing system consisting of 119 tasks to illustrate its clustering and load balancing properties on a large system.<>"",""1558-2183"","""",""10.1109/71.207592"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207592"","""",""Throughput";Distributed processing;Software algorithms;Signal processing algorithms;Communication system software;Hardware;Computer architecture;Degradation;Digital signal processing;"Load management"","""",""88"","""",""13"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Fault-tolerant embedding of complete binary trees in hypercubes,""M. Y. Chan";" S. . -J. Lee"",""Texas Univ., Dallas, TX, USA";" Texas Univ., Dallas, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""277"",""288"",""The focus is on the following graph-theoretic question associated with the simulation of complete binary trees by faulty hypercubes: if a certain number of nodes or links are removed from an n-cube, will an (n-1)-tree still exists as a subgraph? While the general problem of determining whether a k-tree, k<n, still exists when an arbitrary number of nodes/links are removed from the n-cube is found to be NP-complete, an upper bound is found on how many nodes/links can be removed and an (n-1)-tree still be guaranteed to exist. In fact, as a corollary of this, it is found that if no more than n-3 nodes/links are removed from an (n-1)-subcube of the n-cube, an (n-1)-tree is also guaranteed to exist.<<ETX>>"",""1558-2183"","""",""10.1109/71.210811"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210811"","""",""Fault tolerance";Binary trees;Hypercubes;Computer science;Tree graphs;"Robustness"","""",""35"","""",""9"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Fibonacci cubes-a new interconnection Topology,""W. . -J. Hsu"",""Department of Computer Technology, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""3"",""12"",""A novel interconnection topology called the Fibonacci cube is shown to possess attractive recurrent structures in spite of its asymmetric and relatively sparse interconnections. Since it can be embedded as a subgraph in the Boolean cube (hypercube) and it is also a supergraph of other structures, the Fibonacci cube may find applications in fault-tolerant computing. For a graph with N nodes, the diameter, the edge connectivity, and the node connectivity of the Fibonacci cube are in the logarithmic order of N. It is also shown that common system communication primitives can be implemented efficiently.<>"",""1558-2183"","""",""10.1109/71.205649"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205649"","""",""Topology";Broadcasting;Scattering;Parallel architectures;Parallel algorithms;"Terminology"","""",""164"","""",""24"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Functional and topological relations among banyan multistage networks of differing switch sizes,""A. Youssef";" B. Arden"",""Department of Electrical Engineering and Computer Science, George Washington University, Washington D.C., DC, USA";" College of Engineering and Applied Scicnce, University of Rochester, Rochester, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""235"",""240"",""Relations among banyan multistage interconnection networks (MINs) of differing switch sizes are studied. If two N*N networks W and W' have switch sizes r and s, respectively, and if r>s, then W realizes a larger number of permutations than W'. Consequently, the two networks can never be equivalent. However, W may realize all the permutations of W', in which case W is said to functionally cover W' in the strict sense. More generally, W is said to functionally cover W' in the wide sense if the terminals of W can be relabeled so that W realizes all the permutations of W'. Functional covering is topologically characterized, and an optimal algorithm to decide strict functional covering is developed.<>"",""1558-2183"","""",""10.1109/71.207599"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207599"","""",""Switches";Multiprocessor interconnection networks;"Parallel processing"","""","""","""",""13"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Generalized measures of fault tolerance in n-cube networks,""A. D. Oh";" H. . -A. Choi"",""Dept. of Math., St Mary's Coll. of Mayland, St. Mary's City, MD, USA";" NA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""702"",""703"",""It is shown that for a given p (1<p<or=n), the n-cube network can tolerate up to p2/sup (n-p)/-1 processor failures and remains connected provided that at most p neighbors of any nonfaulty processor are allowed to fail. This generalizes the result for p=n-1, obtained by A.-M Esfahanian (1989). It is also shown that the n-cube network with n>or=5 remains connected provided that at most two neighbors of any processor are allowed to fail.<<ETX>>"",""1558-2183"","""",""10.1109/71.242153"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242153"","""",""Fault tolerance";Intelligent networks;Failure analysis;Network topology;Time measurement;Optical computing;Fault tolerant systems;Context modeling;Mathematics;"Cities and towns"","""",""54"","""",""5"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"HARP: an open architecture for parallel matrix and signal processing,""E. M. Dowling"; Z. Fu;" R. S. Drafz"",""Erik Jonsson School of Engineering and Computer Science, University of Texas, Dallas, Richardson, TX, USA"; Erik Jonsson School of Engineering and Computer Science, University of Texas, Dallas, Richardson, TX, USA;" Texas Instruments, Inc., Dallas, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1081"",""1091"",""Describes and analyzes the Hybrid Array Ring Processor (HARP) architecture. The HARP is an application specific architecture built around a host processor, shared memory, and a set of memory mapped processing cells that are connected both into an open backplane and a bidirectional systolic ring. The architecture is analyzed through detailed simulation of a system implementation based on the Texas Instruments TMS34082 floating point RISC. A bus controller is designed that provides a tightly coupled DMA function that accelerates systolic communication and supports new interleaved transparent communications and reduced overhead message passing. The architecture is benchmarked with the matrix multiplication, FFT, QRD, and SVD algorithms.<>"",""1558-2183"","""",""10.1109/71.246070"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246070"","""",""Signal processing";Signal processing algorithms;Computer architecture;Very large scale integration;Hardware;Algorithm design and analysis;Instruments;Reduced instruction set computing;Array signal processing;"Partitioning algorithms"","""","""","""",""30"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Image shrinking and expanding on a pyramid,""J. . -F. Jenq";" S. Sahni"",""Departments of Physics, Mathematics, and Computer Science, Tennessee State University, Nashville, TN, USA";" Department of Computer and Information Science, University of Florida, Gainesville, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""11"",""1291"",""1296"",""Develops two algorithms to perform the q step shrinking and expanding of an N*N binary image on a pyramid computer with an N*N base. The time complexity of both algorithms is O( square root q). However, one uses O( square root q) space per processor, while the per-processor space requirement of the other is O(1).<>"",""1558-2183"","""",""10.1109/71.250106"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250106"","""",""Pixel";Physics;Mathematics;Computer science;Tree data structures;Hypercubes;"Equations"","""",""16"","""",""5"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Improving memory utilization in cache coherence directories,""D. J. Lilja";" P. . -C. Yew"",""Dept. of Electr. Eng., Minnesota Univ., Minneapolis, MN, USA";" Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1130"",""1146"",""Efficiently maintaining cache coherence is a major problem in large-scale shared memory multiprocessors. Hardware directory coherence schemes have very high memory requirements, while software-directed schemes must rely on imprecise compile-time memory disambiguation. Recently proposed dynamically tagged directory schemes allocate pointers to blocks only as they are referenced, which significantly reduces their memory requirements, but they still allocate pointers to blocks that do not need them. The authors present two compiler optimizations that exploit the high-level sharing information available to the compiler to further reduce the size of a tagged directory by allocating pointers only when necessary. Trace-driven simulations are used to show that the performance of this combined hardware-software approach is comparable to other coherence schemes, but with significantly lower memory requirements. In addition, these simulations suggest that this approach is less sensitive to the quality of the memory disambiguation and interprocedural analysis performed by the compiler than software-only coherence schemes.<>"",""1558-2183"","""",""10.1109/71.246074"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246074"","""",""Hardware";Optimizing compilers;Multiprocessor interconnection networks;NASA;Large-scale systems;Software performance;Analytical models;Performance analysis;Research and development;"US Department of Energy"","""",""6"",""8"",""36"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Language portability across shared memory multiprocessors,""G. Alaghband"; M. S. Benten; R. Jakob; H. F. Jordan;" A. V. Ramanan"",""Department of Computer Science and Engineering, University of Colorado, Denver, CO, USA"; Department of Computer Engineering, King Fahd University, Dhahran, Saudi Arabia; Department of Electrical and Computer Engineering, University of Colorado, Boulder, CO, USA; Department of Electrical and Computer Engineering, University of Colorado, Boulder, CO, USA;" Department of Electrical and Computer Engineering, University of Colorado, Boulder, CO, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""9"",""1064"",""1072"",""Explains why the Force parallel programming language has been easily portable between eight different shared memory multiprocessors. The authors show how a two-layer macro processor allows them to hide machine dependencies and to build machine-independent high-level language constructs. The importance of packaging low-level synchronization operations is demonstrated by a proof of mutual exclusion for asynchronous variable operations. The Force constructs enable one to write portable parallel programs largely independent of the number of processes executing them.<>"",""1558-2183"","""",""10.1109/71.243532"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=243532"","""",""Parallel processing";Processor scheduling;Timing;Testing;Scheduling algorithm;Prototypes;Difference equations;Decision trees;"Parallel programming"","""","""","""",""24"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;
"Latin squares for parallel array access,""K. Kim";" V. K. Prasanna"",""Computer Research Department, Electronics and Telecommunications Research Institute, Daejeon, South Korea";" Department of Electrical Engineering-Systems, University of Southern California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""361"",""370"",""A parallel memory system for efficient parallel array access using perfect latin squares as skewing functions is discussed. Simple construction methods for building perfect latin squares are presented. The resulting skewing scheme provides conflict free access to several important subsets of an array. The address generation can be performed in constant time with simple circuitry. The skewing scheme can provide constant time access to rows, columns, diagonals, and N/sup 1/2/*N/sup 1/2/ subarrays of an N*N array with maximum memory utilization. Self-routing Benes networks can be used to realize the permutations needed between the processing elements and the memory modules. Two skewing schemes that provide conflict free access to three-dimensional arrays are also discussed. Combined with self-routing Benes networks, these schemes provide efficient access to frequently used subsets of three-dimensional arrays.<>"",""1558-2183"","""",""10.1109/71.219753"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219753"","""",""Bandwidth";Multiprocessor interconnection networks;Concurrent computing;High performance computing;Buildings;Routing;Computer architecture;"Cities and towns"","""",""46"",""1"",""30"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Load balancing requirements in parallel implementations of image feature extraction tasks,""D. Gerogiannis";" S. C. Orphanoudakis"",""Daimler Benz Systems Technology Research, Berlin, Germany";" Department of Computer Science, Institute of Computer Science, University of Crete, Heraklion, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""9"",""994"",""1013"",""Load balancing requirements in parallel image analysis are considered and results on the performance of parallel implementations of two image feature extraction tasks on the Connection Machine and the iPSC/2 hypercube are reported and discussed. A load redistribution algorithm, which makes use of parallel prefix operations and one-to-one permutations among the processors, is described and has been used. The expected improvement in performance resulting from load balancing has been determined analytically and is compared to actual performance results obtained from the above implementations. The analytical results demonstrate the specific dependence of the expected improvement in performance on the computational and communication requirements of each task, characteristic machine parameters, a characterization of prior load distribution in terms of parameters which can be computed dynamically at the start of task execution, and the overhead incurred by load redistribution.<>"",""1558-2183"","""",""10.1109/71.243527"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=243527"","""",""Load management";Feature extraction;Image analysis;Parallel machines;Distributed computing;Computer vision;Concurrent computing;Performance analysis;Machine vision;"Data structures"","""",""34"","""",""37"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Loop coalescing and scheduling for barrier MIMD architectures,""M. T. O'Keefe";" H. G. Dietz"",""Department of Electrical Engineering, University of Minnesota, Minneapolis, MN, USA";" School of Electrical Engineering, Purdue University, West Lafayette, IN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""9"",""1060"",""1064"",""Barrier MIMD's are asynchronous multiple instruction stream, multiple data stream architectures capable of parallel execution of variable execution time instructions and arbitrary control flow (e.g., while loops and calls)";" however, they differ from conventional MIMD's in that the need for run-time synchronization is significantly reduced. The authors consider the problem of scheduling nested loop structures on a barrier MIMD. The basic approach employs loop coalescing, a technique for transforming a multiply-nested loop into a single loop. Loop coalescing is extended to nested triangular loops, in which inner loop bounds are functions of outer loop indices. In addition, a more efficient scheme to generate the original loop indices from the coalesced index is proposed for the case of constant loop bounds. These results are general, and can be applied to extend previous work using loop coalescing techniques. The authors concentrate on using loop coalescing for scheduling barrier MIMDs, and show how previous work in loop transformations and linear scheduling theory can be applied to this problem.<>"",""1558-2183"","""",""10.1109/71.243531"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=243531"","""",""Processor scheduling";Synchronization;Hardware;Runtime;Optimizing compilers;Parallel machines;Clocks;Resumes;Scheduling algorithm;"Dynamic scheduling"","""",""7"","""",""17"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Loop-level parallelism in numeric and symbolic programs,""J. R. Larus"",""Computer Sciences Department, University of Wisconsin, Madison, Madison, WI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""7"",""812"",""826"",""A new technique for estimating and understanding the speed improvement that can result from executing a program on a parallel computer is described. The technique requires no additional programming and minimal effort by a program's author. The analysis begins by tracing a sequential program. A parallelism analyzer uses information from the trace to simulate parallel execution of the program. In addition to predicting parallel performance, the parallelism analyzer measures many aspects of a program's dynamic behavior. Measurements of six substantial programs are presented. These results indicate that the three symbolic programs differ substantially from the numeric programs and, as a consequence, cannot be automatically parallelized with the same compilation techniques.<>"",""1558-2183"","""",""10.1109/71.238302"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238302"","""",""Parallel processing";Concurrent computing;Program processors;Size measurement;Computational modeling;Parallel programming;Writing;Data structures;Associate members;"Information analysis"","""",""28"",""3"",""24"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Making compaction-based parallelization affordable,""T. Nakatani";" K. Ebcioglu"",""IBM Tokyo Research Laboratory, Yamato, Kanagawa, Japan";" IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""9"",""1014"",""1029"",""Compaction-based parallelization suffers from long compile time and large code size because of its inherent code explosion problem. If software pipelining is performed for loop parallelization along with compaction, as in the authors' compiler, the code explosion problem becomes more serious. The authors propose the software lookahead heuristic for use in software pipelining, which allows inter-basic-block movement of code within a prespecified number of operations, called the software lookahead window, on any path emanating from the currently processed instruction at compile time. Software lookahead enables instruction-level parallelism to be exploited in a much greater code area than a single basic block, but the lookahead region is still limited to a constant depth by means of a user-specifiable window, and thus code explosion is restricted. The proposed scheme has been implemented in the authors' VLIW parallelizing compiler. To study the code explosion problem and instruction-level parallelism for branch-intensive code, they compiled five AIX utilities: sort, fgrep, sed, yacc, and compress. It is demonstrated that the software lookahead heuristic effectively alleviates the code explosion problem while successfully extracting a substantial amount of inter-basic-block parallelism.<>"",""1558-2183"","""",""10.1109/71.243528"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=243528"","""",""Explosions";Pipeline processing;VLIW;Scheduling;Hardware;Software performance;Compaction;Prefetching;Laboratories;"Trademarks"","""",""15"",""1"",""25"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Mtool: an integrated system for performance debugging shared memory multiprocessor applications,""A. J. Goldberg";" J. L. Hennessy"",""AT and T Bell Laboratories, Inc., Murray Hill, NJ, USA";" Computer Systems Laboratory, University of Stanford, Stanford, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""28"",""40"",""The authors describe Mtool, a software tool for analyzing performance losses in shared memory parallel programs. Mtool augments a program with low overhead instrumentation which perturbs the program's execution as little as possible while generating enough information to isolate memory and synchronization bottlenecks. After running the instrumented version of the parallel program, the programmer can use Mtool's window-based user interface to view compute time, memory, and synchronization objects. The authors describe Mtool's low overhead instrumentation methods, memory bottleneck detection technique, and attention focusing mechanisms, contrast Mtool with other approaches, and offer a case study to demonstrate its effectiveness.<>"",""1558-2183"","""",""10.1109/71.205651"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205651"","""",""Debugging";Instruments;Performance loss;Programming profession;Application software;Concurrent computing;Taxonomy;Parallel programming;Probes;"Time measurement"","""",""45"",""10"",""23"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Multicast communication in multicomputer networks,""X. Lin";" L. M. Ni"",""Department of Computer Science, Michigan State University, East Lansing, MI, USA";" Department of Computer Science, Michigan State University, East Lansing, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1105"",""1117"",""Efficient routing of messages is a key to the performance of multicomputers. Multicast communication refers to the delivery of the same message from a source node to an arbitrary number of destination nodes. While multicast communication is highly demanded in many applications, most of the existing multicomputers do not directly support this service";" rather it is indirectly supported by multiple one-to-one or broadcast communications, which result in more network traffic and a waste of system resources. The authors study routing evaluation criteria for multicast communication under different switching technologies. Multicast communication in multicomputers is formulated as a graph theoretical problem. Depending on the evaluation criteria and switching technologies, they study three optimal multicast communication problems, which are equivalent to the finding of the following three subgraphs: optimal multicast path, optimal multicast cycle, and minimal Steiner tree, where the interconnection of a multicomputer defines a host graph. They show that all these optimization problems are NP-complete for the popular 2D-mesh and hypercube host graphs. Heuristic multicast algorithms for these routing problems are proposed.<>"",""1558-2183"","""",""10.1109/71.246072"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246072"","""",""Multicast communication";Intelligent networks;Routing;Multicast algorithms;Communication switching;Topology;Telecommunication traffic;Hypercubes;Broadcasting;"Heuristic algorithms"","""",""139"","""",""38"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Multinode broadcast in hypercubes and rings with randomly distributed length of packets,""E. A. Varvarigos";" D. P. Bertsekas"",""Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA";" Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""144"",""154"",""Multinode broadcast (MNB) in a hypercube and in a ring network of processors is considered. It is assumed that the lengths of the packets that are broadcast are not fixed, but are distributed according to some probabilistic rule, and the optimal times required to execute the MNB are compared for variable and for fixed packet lengths. For large hypercubes, it is shown, under very general probabilistic assumptions on the packet lengths, that the MNB is completed in essentially the same time as when the packet lengths are fixed. In particular, the MNB is completed by time (1+ delta )T/sub s/ with probability at least 1- epsilon , for any positive epsilon and delta , where T/sub s /is the optimal time required to execute the MNB when the packet lengths are fixed at their mean, provided that the size of the hypercube is large enough. In the case of the ring, it is proved that the average time required to execute a MNB when the packet lengths are exponentially distributed exceeds by a factor of ln n the corresponding time for the case there the packet lengths are fixed at their mean, where n is the number of nodes of the ring.<>"",""1558-2183"","""",""10.1109/71.207590"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207590"","""",""Broadcasting";Hypercubes;Intelligent networks;Routing;Multiprocessing systems;Communication networks;"Joining processes"","""",""13"","""",""22"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"NETRA: a hierarchical and partitionable architecture for computer vision systems,""A. N. Choudhary"; J. H. Patel;" N. Ahuja"",""Department of Electrical and Computer Engineering, Syracuse University, Syracuse, NY, USA"; Center for Reliable and High Performance Computing, University of Illinois, Urbana-Champaign, Urbana, IL, USA;" Center for Reliable and High Performance Computing, University of Illinois, Urbana-Champaign, Urbana, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1092"",""1104"",""Computer vision is regarded as one of the most complex and computationally intensive problems. In general, a Computer Vision System (CVS) attempts to relate scene(s) in terms of model(s). A typical CVS employs algorithms from a very broad spectrum such as numerical, image processing, graph algorithms, symbolic processing, and artificial intelligence. The authors present a multiprocessor architecture, called """"NETRA,"""" for computer vision systems. NETRA is a highly flexible architecture. The topology of NETRA is recursively defined, and hence, is easily scalable from small to large systems. It is a hierarchical architecture with a tree-type control hierarchy. Its leaf nodes consists of a cluster of processors connected with a programmable crossbar with selective broadcast capability to provide the desired flexibility. The processors in clusters can operate in SIMD-, MIMD- or Systolic-like modes. Other features of the architecture include integration of limited data-driven computation within a primarily control flow mechanism, block-level control and data flow, decentralization of memory management functions, and hierarchical load balancing and scheduling capabilities. The paper also presents a qualitative evaluation and preliminary performance results of a cluster of NETRA.<>"",""1558-2183"","""",""10.1109/71.246071"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246071"","""",""Computer architecture";Computer vision;Partitioning algorithms;Image processing;Artificial intelligence;Topology;Tree graphs;Broadcasting;Data flow computing;"Memory management"","""",""16"",""2"",""40"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Network communication in edge-colored graphs: gossiping,""A. L. Liestman";" D. Richards"",""School of Computing Science, Simon Fraser University, Burnaby, BC, Canada";" Department of Computer Science, University of Virginia, Charlottesville, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""438"",""445"",""A mechanism for scheduling communications in a network in which individuals exchange information periodically according to a fixed schedule is presented. A proper k edge-coloring of the network is considered to be a schedule of allowed communications such that an edge of color i can be used only at times i modulo k. Within this communication scheduling mechanism, the information exchange problem known as gossiping is considered. It is proved that there is a proper k edge-coloring such that gossip can be completed in a path of n edges in a certain time for n>or=k>or=1. Gossip can not be completed in such a path any earlier under any proper k edge-coloring. In any tree of bounded degree Delta and diameter d, gossip can be completed under a proper Delta edge-coloring in time ( Delta -1)d+1. In a k edge-colored cycle of n vertices, other time requirements of gossip are determined.<>"",""1558-2183"","""",""10.1109/71.219758"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219758"","""",""Intelligent networks";Communication system control;Councils;"Computer science"","""",""19"","""",""12"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"On balancing sorting on a linear array,""Lin Yen-Chun"",""Department of Electronic Engineering, National Taiwan Institute of Technology, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""566"",""571"",""A balanced parallel algorithm to sort a sequence of items on a linear array of processors is presented. The length of the sequence may be small to arbitrarily large. For a short sequence, the output of the sorted sequence begins at the step following the last input of the whole sequence. For an arbitrarily long sequence, the time complexity is optimal under realistic hardware conditions. A variation of the algorithm is also introduced. Both algorithms require far less local memory than that required by a different approach of balanced computation. Any number of balanced processors can be connected to deliver more computing power without increasing the memory size of each processor.<>"",""1558-2183"","""",""10.1109/71.224219"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224219"","""",""Sorting";Bandwidth;Concurrent computing;Parallel processing;Hardware;Parallel algorithms;Velocity measurement;Time measurement;Algorithm design and analysis;"Councils"","""",""23"","""",""13"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"On job scheduling on a hypercube,""Y. Zhu";" M. Ahuja"",""Dept. of Comput. Sci., North Dakota State Univ., Fargo, ND, USA";" NA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""62"",""69"",""The problem of scheduling n independent jobs on an m-dimensional hypercube system to minimize the finish time is studied. Each job J/sub i/, where 1<or=i<or=n, is associated with a dimension d/sub i/ and a processing time t/sub i/, meaning that J/sub i/ needs a d/sub i/-dimensional subcube for t/sub i/ units of time. When job preemption is allowed, an O(n/sup 2/ log/sup 2/ n) time algorithm which can generate a minimum finish time schedule with at most min(n-2,2/sup m/-1) preemptions is obtained. When job preemption is not allowed, the problem is NP-complete. It is shown that a simple list scheduling algorithm called LDF can perform asymptotically optimally and has an absolute bound no worse than 2-1/2/sup m/. For the absolute bound, it is also shown that there is a lower bound (1+ square root 6)/2 approximately=1.7247 for a class of scheduling algorithms including LDF.<<ETX>>"",""1558-2183"","""",""10.1109/71.205653"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205653"","""",""Hypercubes";Optimal scheduling;Scheduling algorithm;Processor scheduling;Computer science;Computer networks;Communication channels;"Neodymium"","""",""20"","""",""25"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"On process migration and load balancing in Time Warp,""D. W. Glazer";" C. Tropper"",""School of Computer Science, McGill University, Montreal, QUE, Canada";" School of Computer Science, McGill University, Montreal, QUE, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""318"",""327"",""A load balancing algorithm for a discrete event simulation executed under Time Warp is presented. The algorithm rests upon recent developments in active process migration, which permit the use of dynamic strategies. Dynamic load balancing allows for readjustments when resource requirements vary during simulation. It is also useful when initial resource predictions are unknown or incorrect. A simulated multiprocessor environment (PARALLEX) was developed in order to evaluate the algorithm. The results indicate that substantial performance gains may be realized with the algorithm.<>"",""1558-2183"","""",""10.1109/71.210814"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210814"","""",""Load management";Clocks;Discrete event simulation;Time warp simulation;Computational modeling;Frequency;Performance gain;Optimization methods;Protocols;"Random processes"","""",""96"",""1"",""19"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"On the efficiency of parallel backtracking,""V. N. Rao";" V. Kumar"",""Department of Computer Sciences, University of Central Florida, Orlando, FL, USA";" Computer Science Department, University of Minnesota, Minneapolis, MN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""427"",""437"",""Analytical models and experimental results concerning the average case behavior of parallel backtracking are presented. Two types of backtrack search algorithms are considered: simple backtracking, which does not use heuristics to order and prune search, and heuristic backtracking, which does. Analytical models are used to compare the average number of nodes visited in sequential and parallel search for each case. For simple backtracking, it is shown that the average speedup obtained is linear when the distribution of solutions is uniform and superlinear when the distribution of solutions is nonuniform. For heuristic backtracking, the average speedup obtained is at least linear, and the speedup obtained on a subset of instances is superlinear. Experimental results for many synthetic and practical problems run on various parallel machines that validate the theoretical analysis are presented.<>"",""1558-2183"","""",""10.1109/71.219757"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219757"","""",""Analytical models";Algorithm design and analysis;Parallel machines;Artificial intelligence;Laboratories;High performance computing;Military computing;Computer science;"Heuristic algorithms"","""",""55"",""1"",""35"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"On the granularity and clustering of directed acyclic task graphs,""A. Gerasoulis";" T. Yang"",""Department of Computer Science, Rutgers University, New Brunswick, NJ, USA";" Department of Computer Science, Rutgers University, New Brunswick, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""686"",""701"",""The authors consider the impact of the granularity on scheduling task graphs. Scheduling consists of two parts, the processors assignment of tasks, also called clustering, and the ordering of tasks for execution in each processor. The authors introduce two types of clusterings: nonlinear and linear clusterings. A clustering is nonlinear if two parallel tasks are mapped in the same cluster otherwise it is linear. Linear clustering fully exploits the natural parallelism of a given directed acyclic task graph (DAG) while nonlinear clustering sequentializes independent tasks to reduce parallelism. The authors also introduce a new quantification of the granularity of a DAG and define a coarse grain DAG as the one whose granularity is greater than one. It is proved that every nonlinear clustering of a coarse grain DAG can be transformed into a linear clustering that has less or equal parallel time than the nonlinear one. This result is used to prove the optimality of some important linear clusterings used in parallel numerical computing.<>"",""1558-2183"","""",""10.1109/71.242154"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242154"","""",""Processor scheduling";Gaussian processes;Clustering algorithms;Parallel processing;Computer architecture;Parallel architectures;Very large scale integration;Process design;Concurrent computing;"Partitioning algorithms"","""",""203"",""3"",""27"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"On-line control and deadlock-avoidance in a page-parallel multiprocessor rasterizer,""Y. Birk"",""Electrical Engineering Department, Technion-Israel Institute of Technology, Haifa, Israel"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""155"",""163"",""A rasterizer converts a document described in some page-description language into a sequence of full-page bitmaps (pagemaps), which can then be printed or displayed. The page-parallel rasterizer harnesses multiple processors to work on the same document, thereby permitting cost-effective high-speed rasterization of complex documents. Any given page is processed by a single processor, hence the name. For performance reasons, it is desirable to permit out-of-order rasterization as well as to share memory and computation results among the processors. However, this can result in deadlock. Online algorithms are presented for controlling the rasterizer so as to avoid deadlock without being overly restrictive. It is shown that previously proposed approaches for deadlock avoidance cannot be applied directly due to a special form of nonexclusive allocation of shared resources. A solution is given, thereby extending the applicability of deadlock avoidance. The approach should be useful in a variety of similar situations that may occur in other applications.<>"",""1558-2183"","""",""10.1109/71.207591"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207591"","""",""System recovery";Image converters;Microprocessors;Out of order;Resource management;Parallel processing;Printers;Printing;Computer architecture;"Throughput"","""","""","""",""14"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Optimal algorithms on the pipelined hypercube and related networks,""J. JaJa";" K. W. Ryu"",""Maryland Univ., College Park, MD, USA";" Maryland Univ., College Park, MD, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""582"",""591"",""Parallel algorithms for several important combinatorial problems such as the all nearest smaller values problem, triangulating a monotone polygon, and line packing are presented. These algorithms achieve linear speedups on the pipelined hypercube, and provably optimal speedups on the shuffle-exchange and the cube-connected-cycles for any number p of processors satisfying 1<or=p<or=n/((log/sup 3/n)(loglog n)/sup 2/), where n is the input size. The lower bound results are established under no restriction on how the input is mapped into the local memories of the different processors.<<ETX>>"",""1558-2183"","""",""10.1109/71.224210"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224210"","""",""Hypercubes";Parallel algorithms;Particle separators;Associate members;Parallel processing;NIST;Computer science;Merging;Network topology;"Joining processes"","""",""3"","""",""22"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Optimal architectures and algorithms for mesh-connected parallel computers with separable row/column buses,""M. J. Serrano";" B. Parhami"",""Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA, USA";" Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1073"",""1080"",""A two-dimensional mesh of processing elements (PE's) with separable row and column buses (i.e., broadcast mechanisms for rows and columns that can be logically divided into a number of local buses through the use of PE-controlled switches) has been shown to be quite effective for semigroup computation, prefix computation, and a wide class of other computations that do not require excessive communication or data routing. For meshes with separable row/column buses, the authors show how semigroup and prefix computations can be performed with the same asymptotic time complexity without the provision of buses for every row and every column and discuss the VLSI implications of this new architecture.<>"",""1558-2183"","""",""10.1109/71.246069"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246069"","""",""Computer architecture";Concurrent computing;Broadcasting;Computer networks;Very large scale integration;Switches;Delay;Communication switching;Routing;"Buildings"","""",""27"","""",""24"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Optimal resilient distributed algorithms for ring election,""M. Y. Chan";" F. Y. L. Chin"",""Department of Computer Science, University of Hong Kong, Hong Kong, China";" Department of Computer Science, University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""475"",""480"",""The problem of electing a leader in a dynamic ring in which processors are permitted to fail and recover during election is discussed. It is shown that theta (n log n+k/sub r/) messages, counting only messages sent by functional processors, are necessary and sufficient for dynamic ring election, where k/sub r/ is the number of processor recoveries experienced.<>"",""1558-2183"","""",""10.1109/71.219762"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219762"","""",""Distributed algorithms";Nominations and elections;Relays;Switches;Computer science;Telecommunication network reliability;"Protocols"","""",""5"","""",""18"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Optimal routing algorithm and the diameter of the cube-connected cycles,""D. S. Meliksetian";" C. Y. R. Chen"",""Department of Electrical Engineering, South Dakota School of Mines and Technology, Rapid City, SD, USA";" Department of Electrical and Computer Engineering, Syracuse University, Syracuse, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1172"",""1178"",""Communication between processors is one of the most important issues in parallel and distributed systems. The authors study the communication aspects of a well known multiprocessor structure, the cube-connected cycles (CCC). Only nonoptimal routing algorithms and bounds on the diameter of restricted subclasses of the CCC have been presented in earlier work. The authors present an optimal routing algorithm for the general CCC, with a formal proof of its optimality. Based on this routing algorithm, they derive the exact network diameter for the general CCC.<>"",""1558-2183"","""",""10.1109/71.246078"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246078"","""",""Routing";Hypercubes;Multiprocessing systems;Algorithm design and analysis;Distributed algorithms;Multiprocessor interconnection networks;Performance analysis;Hardware;Very large scale integration;"Systolic arrays"","""",""21"","""",""11"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Parallel algorithms for the classes of +or-2/sup b/ DESCEND and ASCEND computations on a SIMD hypercube,""D. Nassimi"",""Department of Computer and Information Science, New Jersey Institute of Technology, Newark, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""12"",""1372"",""1381"",""Derives a simple lower bound for performing a 2/sup b/ permutation on an N-PE SIMD hypercube, proving that log N-b routing steps are needed even if one allows an arbitrary mapping of elements to processors. An algorithm for performing a 2/sup b/ permutation using exactly log N-b full-duplex routing steps that is slightly more efficient than previously known O(log N-b) algorithms, which perform the permutation as an Omega or Omega /sup -1/ mapping, is presented. The author has also identified a general class of parallel computations called +or-2/sup b/ descend, which includes Batcher's odd-even merge and many other algorithms. An efficient algorithm for performing any computation in this class in O(log N) steps on an N-PE SIMD hypercube is given. A related class of parallel computations called +or-2/sup b/ ascend is also defined. This class appears to be more difficult than +or-2/sup b/ descend. A simple O(log/sup 2/ N/log log) N algorithm for this class on a SIMD hypercube, requiring Theta (log log N) space per processor is developed.<>"",""1558-2183"","""",""10.1109/71.250118"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250118"","""",""Parallel algorithms";Concurrent computing;Hypercubes;Routing;Distributed computing;Genetic mutations;Topology;Information science;"Multiprocessor interconnection networks"","""",""4"","""",""14"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Parallel implementation of the extended square-root covariance filter for tracking applications,""E. K. B. Lee";" S. Haykin"",""Motorola, Inc., Fort Lauderdale, FL, USA";" Communication Applied Research Laboratory, McMaster University, Hamilton, ONT, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""446"",""457"",""Parallel implementations of the extended square-root covariance filter (ESRCF) for tracking applications are developed. The decoupling technique and special properties used in the tracking Kalman filter (KF) are employed to reduce computational requirements and to increase parallelism. The application of the decoupling technique to the ESRCF results in the time and measurement updates of m decoupled (n/m)-dimensional matrices instead of one coupled n-dimensional matrix, where m denotes the tracking dimension and n denotes the number of state elements. The updates of m decoupled matrices are found to require approximately m fewer processing elements and clock cycles than the updates of one coupled matrix. The transformation of the Kalman gain which accounts for the decoupling is found to be straightforward to implement. The sparse nature of the measurement matrix and the sparse, band nature of the transition matrix are explored to simplify matrix multiplications.<>"",""1558-2183"","""",""10.1109/71.219759"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219759"","""",""Sparse matrices";Covariance matrix;Parallel architectures;Nonlinear filters;Equations;Concurrent computing;Parallel processing;Target tracking;Very large scale integration;"Measurement standards"","""",""6"","""",""24"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Parallel median splitting and k-splitting with application to merging and sorting,""R. Xiong";" T. Brown"",""Department of Math and Computer Science, Edinboro University of Pennsylvania, Edinboro, PA, USA";" Department of Computer Science, CUNY, Flushing, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""559"",""565"",""Multiple-instruction multiple-data (MIMD) algorithms that use multiple processors to do median splitting, k-splitting and parallel splitting into t equal sections are presented. Both concurrent read, exclusive write (CREW) and exclusive read, exclusive write (EREW) versions of the algorithms are given. It is shown that a k-splitting problem can be easily converted into a median-splitting problem. Methods for finding multiple split points quickly and application of k-splitting to merging and sorting are discussed.<>"",""1558-2183"","""",""10.1109/71.224218"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224218"","""",""Merging";Sorting;Partitioning algorithms;Parallel algorithms;Computer science;Concurrent computing;Operating systems;"Read-write memory"","""",""3"","""",""11"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"PCBN: a high-performance partitionable circular bus network for distributed systems,""Tai-Kuo Woo";" S. Y. W. Su"",""Database Systems Research and Development Center, Department of Computer and Information Science, University of Florida, Gainesville, FL, USA";" Database Systems Research and Development Center, Department of Computer and Information Science, University of Florida, Gainesville, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""12"",""1298"",""1307"",""The authors present a dynamically partitionable circular bus network (PCBN) and efficient algorithms for maximizing its utilization. In their approach, a distributed network is transformed into a graph, in which a vertex represents a communication request and an edge denotes the conflict between a pair of communication requests. A graph traversal algorithm is applied to the graph to identify some maximal independent sets of vertices. The communication requests corresponding to the vertices of a maximum independent set call proceed in parallel. By computing the expected size of the maximal independent sets of a graph, the improvement ratio of the network can be obtained. The network control and synchronization techniques of PCBN are described in detail. The idling problem in the execution of nonconflicting requests is also discussed.<>"",""1558-2183"","""",""10.1109/71.250112"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250112"","""",""Partitioning algorithms";Distributed computing;Communication networks;Communication system control;Computer networks;Hardware;Costs;Parallel processing;"Communication channels"","""",""3"","""",""13"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Performance analysis and scheduling of stochastic fork-join jobs in a multicomputer system,""A. Kumar";" R. Shorey"",""Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India";" Department of Electrical Communication Engineering, Indian Institute of Science, Bangalore, India"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""10"",""1147"",""1164"",""The authors model a parallel processing system comprising several homogeneous computers interconnected by a communication network. Jobs arriving to this system have a linear fork-join structure. Each fork of the job gives rise to a random number of tasks that can be processed independently on any of the computers. Since exact analysis of fork-join models is known to be intractable, the authors resort to obtaining analytical bounds to the mean job response time of the fork-join job. For jobs with a single fork-join and, probabilistic allocation of tasks of the job to the N processors, they obtain upper and lower bounds to the mean job response time. Upper bounds are obtained using the concept of associated random variables and are found to be a good approximation to the mean job response time. A simple lower bound is obtained by neglecting queueing delays. They also find two lower bounds that include queueing delays. For multiple fork-join jobs, they study an approximation based on associated random variables. Finally, two versions of the join-the-shortest-queue (JSQ) allocation policy (i.e., JSQ by batch and JSQ by task) are studied and compared, via simulations and diffusion limits.<>"",""1558-2183"","""",""10.1109/71.246075"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=246075"","""",""Performance analysis";Stochastic processes;Delay;Random variables;Processor scheduling;Parallel processing;Computer networks;Concurrent computing;Communication networks;"Upper bound"","""",""38"",""1"",""34"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Performance analysis of buffer coherency policies in a multisystem data sharing environment,""A. Dan";" P. S. Yu"",""IBM Thomson J.Watson Research Center, Yorktown Heights, NY, USA";" IBM Thomson J.Watson Research Center, Yorktown Heights, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""289"",""305"",""Six buffer coherency policies for a multisystem transaction processing environment are compared. These policies differ in their basic approaches on how and when the invalidated pages are identified or if the updated pages are propagated to the buffers of the remote nodes. They can be classified as detection, notification (of invalid pages), and (update) propagation oriented approaches. The policies trade off CPU overhead of coherency messages with buffer hit probability in different ways, resulting in a tradeoff of response time and maximum throughput. The main contribution is to develop analytical models to predict buffer hit probabilities under various buffer coherency policies assuming the LRU replacement policy and the independent reference model (IRM). The buffer models are validated using simulation models and show excellent agreement. Integrated analytic models capturing buffer hit probability and CPU overhead are developed to predict the overall response times under these coherency policies. The difference in buffer hit probabilities amongst various policies are found to be very sensitive to the skewness of the data access.<>"",""1558-2183"","""",""10.1109/71.210812"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210812"","""",""Performance analysis";Delay;Predictive models;Analytical models;Throughput;Environmental management;Distributed databases;Transaction databases;"File servers"","""",""20"",""2"",""43"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Performance evaluation of client-server systems,""O. C. Ibe"; H. Choi;" K. S. Trivedi"",""GTE Laboratories, Inc., Waltham, MA, USA"; Department of Computer Science, Duke University, Durham, NC, USA;" Department of Computer Science, Duke University, Durham, NC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""11"",""1217"",""1229"",""A client-server system is a distributed system where a server station receives requests from its client stations, processes the requests and returns replies to the requesting stations. The authors consider client-server systems in which a set of workstations access a file server over a local area network. The systems are modelled by a class of stochastic Petri nets. The mean response time, the throughput and the parametric sensitivities are evaluated for a client-server system based on token ring network and a system based on CSMA/CD network. These models are different from the prevalent performance models of token ring or CSMA/CD network systems because of the message interdependencies introduced by the clients-server structure. An approximate analytic-numeric method rather than simulation is used to solve the models. The solution method and the accuracy of approximation are also discussed.<>"",""1558-2183"","""",""10.1109/71.250101"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250101"","""",""Client-server systems";Token networks;Multiaccess communication;Network servers;Workstations;File servers;Local area networks;Stochastic systems;Petri nets;"Delay"","""",""41"",""3"",""28"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Performance evaluation of dynamic sharing of processors in two-stage parallel processing systems,""Jau-Hsiung Huang";" L. Kleinrock"",""Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan";" Computer Science Department, Loos Angeles, University of California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""3"",""306"",""317"",""The performance of job scheduling is studied in a large parallel processing system where a job is modeled as a concatenation of two stages which must be processed in sequence. P/sub i/ is the number of processors required by stage P as the total number of processors in the system. A large parallel computing system is considered where Max(P/sub 1/, P/sub 2/)>or=P>>1 and Max(P/sub 1/, P/sub 2/)>>Min(P/sub 1/, P/sub 2/). For such systems, exact expressions for the mean system delay are obtained for various job models and disciplines. The results show that the priority should be given to jobs working on the stage which requires fewer processors. The large parallel system (i.e. P>>1) condition is then relaxed to obtain the mean system time for two job models when the priority is given to the second stage. Moreover, a scale-up rule is introduced to obtain the approximated delay performance when the system provides more processors than the maximum number of processors required by both stages (i.e. P>Max(P/sub 1/, P/sub 2/)). An approximation model is given for jobs with more than two stages.<>"",""1558-2183"","""",""10.1109/71.210813"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=210813"","""",""Parallel processing";Computer science;Processor scheduling;Delay systems;Terrorism;"Concurrent computing"","""",""4"","""",""20"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Performance evaluation of the time-stamp ordering algorithm in a distributed database,""S. Varma"",""IBM, Corporation, NC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""668"",""676"",""Time-stamp ordering is one of the consistency preserving algorithms that is used in distributed databases. F. Baccelli (1987) has introduced a queueing model that incorporates the fork-join and resequencing synchronization constraints to analyze the algorithm's performance. The power of interpolation approximation technique is illustrated by obtaining extremely good approximations for this rather complex model. The heavy traffic approximations are obtained by showing that this model has the same diffusion limit as a system of parallel fork-join queues. The light traffic limits are obtained by applying the light traffic theory developed by M.I. Reiman and B. Simon (1989). The heavy traffic limits are computed for general arrival and service distributions, but the light traffic limits are restricted to Markovian systems.<>"",""1558-2183"","""",""10.1109/71.242156"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242156"","""",""Distributed databases";Traffic control;Transaction databases;Delay;Power system modeling;Queueing analysis;Performance analysis;Interpolation;Distributed computing;"Broadcasting"","""",""3"","""",""16"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Performance issues in distributed query processing,""C. Liu";" C. Yu"",""Department of Computer Science and Information Systems, De Paul University, Chicago, IL, USA";" Department of Electrical Engineering and Computer Science, University of Illinois, Chicago, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""889"",""905"",""The authors discuss various performance issues in distributed query processing. They validate and evaluate the performance of the local reduction (LR) the fragment and replicate strategy (FRS) and the partition and replicate strategy (PRS) optimization algorithms. The experimental results reveal that the choices made by these algorithms concerning which local operations should be performed, which relation should remain fragmented or which relation should be partitioned are valid. It is shown using experimental results that various parameters, such as the number of processing sites, partitioning speed relative to join speed, and sizes of the join relations, affect the performance of PRS significantly. It is also shown that the response times of query execution are affected significantly by the degree of site autonomy, interferences among processes, interface with the local database management systems (DBMSs) and communications facilities. Pipeline strategies for processing queries in an environment where relations are fragmented are studied.<>"",""1558-2183"","""",""10.1109/71.238624"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238624"","""",""Query processing";Partitioning algorithms;Pipeline processing;Database systems;Distributed databases;Data communication;Cost function;Delay;Interference;"Local area networks"","""",""17"",""1"",""68"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Performance of pruning-cache directories for large-scale multiprocessors,""S. L. Scott";" J. R. Goodman"",""Cray Research, Inc., Chippewa Falls, WI, USA";" Department of Computer Sciences, University of Wisconsin, Madison, Madison, WI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""520"",""534"",""Multis, shared-memory multiprocessors that are implemented with single buses and snooping cache protocols are inherently limited to a small number of processors, and, as systems grow beyond a single bus, the bandwidth requirements of broadcast operations limit scalability. Hardware support to provide cache coherence without the use of broadcast can become very expensive. An approach to maintaining coherence using approximate information held in special-purpose caches called pruning-caches that provides robust performance over a wide range of workloads is presented. The pruning-cache approach is compared to the more conventional inclusion cache for providing multilevel inclusion (MLI) in the cache hierarchy. It is shown that pruning-caches are more cost-effective and more robust. Using both analysis and simulation, it is also shown that the k-ary n-cube topology provides scalable, bottleneck-free communication for uniform, point-to-point traffic.<>"",""1558-2183"","""",""10.1109/71.224215"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224215"","""",""Large-scale systems";Broadcasting;Robustness;Protocols;Bandwidth;Scalability;Hardware;Analytical models;Topology;"Traffic control"","""",""7"",""19"",""23"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Polymorphic processor arrays,""M. Maresca"",""Dipartimento di Informatica Sistemistica e Telematica, Università di Genova, Genoa, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""490"",""506"",""Polymorphic processor arrays (PPAs), two-dimensional mesh-connected arrays of processors in which each processor is equipped with a switch able to interconnect its four NEWS ports, are discussed. The main features of PPA are that it models a realistic class of parallel computers, it supports the definition of high level programming models, it supports virtual parallelism, and it supports low complexity algorithms in a number of application fields. Both the PPA computation model and the PPA programming model are presented. It is shown that the PPA computation model is realistic by relating it to the design of the polymorphic torus (PT) chip. It is also shown that the PPA programming model is scalable by demonstrating that any algorithm having O(p) complexity on a virtual PPA of size square root m* square root m, has O(k p) complexity on a PPA of size square root n* square root n, with m k n and k integers. Some application algorithms in the area of numerical analysis and graph processing are presented.<>"",""1558-2183"","""",""10.1109/71.224213"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224213"","""",""Concurrent computing";Computational modeling;Computer architecture;Switches;Parallel programming;Application software;Multiprocessor interconnection networks;Wiring;Very large scale integration;"Hardware"","""",""58"","""",""21"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Prediction of performance and processor requirements in real-time data flow architectures,""S. Som"; R. R. Mielke;" J. W. Stoughton"",""NASA Langley Research Center Mail Stop 473, Lockheed Martin Engineering and Science Company, Hampton, VA, USA"; Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA;" Department of Electrical and Computer Engineering, Old Dominion University, Norfolk, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""11"",""1205"",""1216"",""Presents a new data flow graph model for describing the real-time execution of iterative control and signal processing algorithms on multiprocessor data flow architectures. Identified by the acronym ATAMM, for Algorithm to Architecture Mapping Model, the model is important because it specifies criteria for a multiprocessor operating system to achieve predictable and reliable performance. Algorithm performance is characterized by execution time and iteration period. For a given data flow graph representation, the model facilitates calculation of greatest lower bounds for these performance measures. When sufficient processors are available, the system executes algorithms with minimum execution time and minimum iteration period, and the number of processors required is calculated. When only limited processors are available or when processors fail, performance is made to degrade gracefully and predictably. The user off-line is able to specify tradeoffs between increasing execution time or increasing iteration period. The approach to achieving predictable performance is to control the injection rate of input data and to modify the data flow graph precedence relations so that a processor is always available to execute an enabled graph node. An implementation of the ATAMM model in a four-processor architecture based on Westinghouse's VHSIC 1750A Instruction Set Processor is described.<>"",""1558-2183"","""",""10.1109/71.250100"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250100"","""",""Signal processing algorithms";Process control;Flow graphs;Concurrent computing;Predictive models;Computer architecture;Data flow computing;Real time systems;Aerospace control;"Iterative algorithms"","""",""8"",""4"",""18"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Prediction-based dynamic load-sharing heuristics,""K. K. Goswami"; M. Devarakonda;" R. K. Iyer"",""Center for Reliable and High Performance Computing, University of Illinois, Urbana, IL, USA"; IBM Research Division, IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA;" Center for Reliable and High Performance Computing, University of Illinois, Urbana, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""6"",""638"",""648"",""Presents dynamic load-sharing heuristics that use predicted resource requirements of processes to manage workloads in a distributed system. A previously developed statistical pattern-recognition method is employed for resource prediction. While nonprediction-based heuristics depend on a rapidly changing system status, the new heuristics depend on slowly changing program resource usage patterns. Furthermore, prediction-based heuristics can be more effective since they use future requirements rather than just the current system state. Four prediction-based heuristics, two centralized and two distributed, are presented. Using trace driven simulations, they are compared against random scheduling and two effective nonprediction based heuristics. Results show that the prediction-based centralized heuristics achieve up to 30% better response times than the nonprediction centralized heuristic, and that the prediction-based distributed heuristics achieve up to 50% improvements relative to their nonpredictive counterpart.<>"",""1558-2183"","""",""10.1109/71.242159"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=242159"","""",""Delay";Predictive models;Aerodynamics;Resource management;Dynamic scheduling;Prediction methods;NASA;High performance computing;Length measurement;"Filters"","""",""48"",""24"",""25"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Program structuring for effective parallel portability,""G. A. Alverson";" D. Notkin"",""Tera Computer Company, Seattle, WA, USA";" Department of Computer Science and Engineering. FR-35, University of Washington, Seattle, WA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""9"",""1041"",""1059"",""The tension between software development costs and efficiency is especially high when considering parallel programs intended to run on a variety of architectures. In the domain of shared memory architectures and explicitly parallel programs, the authors have addressed this problem by defining a programming structure that eases the development of effectively portable programs. On each target multiprocessor, an effectively portable program runs almost as efficiently as a program fine-tuned for that machine. Additionally, its software development cost is close to that of a single program that is portable across the targets. Using this model, programs are defined in terms of data structure and partitioning-scheduling abstractions. Low software development cost is attained by writing source programs in terms of abstract interfaces and thereby requiring minimal modification to port";" high performance is attained by matching (often dynamically) the interfaces to implementations that are most appropriate to the execution environment. The authors include results of a prototype used to evaluate the benefits and costs of this approach.<>"",""1558-2183"","""",""10.1109/71.243530"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=243530"","""",""Costs";Writing;Computer architecture;Partitioning algorithms;Programming profession;Memory architecture;Parallel programming;Runtime;Software prototyping;"Prototypes"","""",""7"","""",""51"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Reconfiguration and analysis of a fault-tolerant circular butterfly parallel system,""N. . -F. Tzeng"",""The Center for Advanced Computer Studies, University of Southwestern Louisiana, Lafayette, LA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""855"",""863"",""The butterfly parallel system has a regular and simple interconnection pattern, making it suitable for VLSI or WSI implementation. The authors propose an effective fault-tolerant technique for the circular butterfly parallel system to ensure its rigid full butterfly structure even in the presence of failures, addressing reconfiguration in detail. The resulting butterfly system has L levels, involves (1/log/sub 2/ L)% spare processing elements (PEs), and approximately 50% additional links. The reconfiguration process of the design in response to any operational fault is easy and can be performed in a distributed manner. The reliability and layout of this proposed design are evaluated analytically. This design, due to its specific configuration, exhibits significant improvement in reliability while taking only moderately more layout area.<>"",""1558-2183"","""",""10.1109/71.238621"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238621"","""",""Fault tolerant systems";Topology;Very large scale integration;Process design;Wafer scale integration;Reliability;Environmental economics;Hardware;Degradation;"Parallel architectures"","""",""3"",""1"",""16"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Replication algorithms in a remote caching architecture,""A. Leff"; J. L. Wolf;" P. S. Yu"",""T. J. Watson Center, IBM Research Division, Yorktown Heights, NY, USA"; T. J. Watson Center, IBM Research Division, Yorktown Heights, NY, USA;" T. J. Watson Center, IBM Research Division, Yorktown Heights, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""11"",""1185"",""1204"",""Studies the cache performance in a remote caching architecture. The authors develop a set of distributed object replication policies that are designed to implement different optimization goals. Each site is responsible for local cache decisions, and modifies cache contents in response to decisions made by other sites. The authors use the optimal and greedy policies as upper and lower bounds, respectively, for performance in this environment. Critical system parameters are identified, and their effect on system performance studied. Performance of the distributed algorithms is found to be close to optimal, while that of the greedy algorithms is far from optimal.<>"",""1558-2183"","""",""10.1109/71.250099"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250099"","""",""Intelligent networks";Decision making;Design optimization;System performance;Distributed algorithms;Greedy algorithms;Computer networks;Distributed computing;Resource management;"Ethernet networks"","""",""88"",""4"",""32"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Resource reclaiming in multiprocessor real-time systems,""C. Shen"; K. Ramamritham;" J. A. Stankovic"",""Mitsubishi Electric Research Laboratories, Inc., Cambridge, MA, USA"; Department of Computer Science, University of Massachusetts, Amherst, MA, USA;" Department of Computer Science, University of Massachusetts, Amherst, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""382"",""397"",""Most real-time scheduling algorithms schedule tasks with regard to their worst case computation times. Resources reclaiming refers to the problem of utilizing the resources left unused by a task when it executes in less than its worst case computation time, or when a task is deleted from the current schedule. Dynamic resource reclaiming algorithms that are effective, avoid any run time anomalies, and have bounded overhead costs that are independent of the number of tasks in the schedule are presented. Each task is assumed to have a worst case computation time, a deadline, and a set of resource requirements. The algorithms utilize the information given in a multiprocessor task schedule and perform online local optimization. The effectiveness of the algorithms is demonstrated through simulation studies.<>"",""1558-2183"","""",""10.1109/71.219754"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219754"","""",""Real time systems";Processor scheduling;Scheduling algorithm;Dynamic scheduling;Heuristic algorithms;Springs;Timing;Upper bound;Costs;"Computational modeling"","""",""53"",""4"",""16"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Strategies for dynamic load balancing on highly parallel computers,""M. H. Willebeek-LeMair";" A. P. Reeves"",""IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA";" School of Electrical Engineering, Cornell University, Ithaca, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""9"",""979"",""993"",""Dynamic load balancing strategies for minimizing the execution time of single applications running in parallel on multicomputer systems are discussed. Dynamic load balancing (DLB) is essential for the efficient use of highly parallel systems when solving non-uniform problems with unpredictable load estimates. With the evolution of more highly parallel systems, centralized DLB approaches which make use of a high degree of knowledge become less feasible due to the load balancing communication overhead. Five DLB strategies are presented which illustrate the tradeoff between 1) knowledge - the accuracy of each balancing decision, and 2) overhead - the amount of added processing and communication incurred by the balancing process. All five strategies have been implemented on an Inter iPSC/2 hypercube.<>"",""1558-2183"","""",""10.1109/71.243526"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=243526"","""",""Load management";Concurrent computing;Hypercubes;Costs;Application software;Distributed control;Multiprocessing systems;Local area networks;Scholarships;"Load modeling"","""",""350"",""9"",""22"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Symmetric crossbar arbiters for VLSI communication switches,""Y. Tamir";" H. . -C. Chi"",""Computer Science Department, University of California, Los Angeles, CA, USA";" Computer Science Department, University of California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""13"",""27"",""The design and implementation of symmetric crossbar arbiters are addressed. Several arbiter designs are compared based on simulations of a multistage interconnection network. These simulations demonstrate the influence of the switch arbitration policy on network throughput, average latency, and worst-case latency. It is shown that some natural designs result in poor system performance and/or slow implementations. Two efficient arbiter implementations are proposed. Based on network simulations, VLSI implementation, and circuit simulation, it is shown that these arbiters achieve nearly optimal system performance without becoming the critical path that limits the system clock.<>"",""1558-2183"","""",""10.1109/71.205650"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205650"","""",""Very large scale integration";Communication switching;Switches;Packet switching;Buffer storage;Multiprocessor interconnection networks;Delay;Circuit simulation;Telecommunication traffic;"Throughput"","""",""163"",""42"",""28"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Synthesis of algorithm-based fault-tolerant systems from dependence graphs,""B. Vinnakota";" N. K. Jha"",""Department of Electrical Engineering, University of Minnesota, Minneapolis, MN, USA";" Department of Electrical Engineering, Princeton University, Princeton, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""864"",""874"",""Algorithm-based fault tolerance (ABFT) is a method for improving the reliability of parallel architectures used for computation-intensive tasks. A two-stage approach to the synthesis of ABFT systems is proposed. In the first stage, a system-level code is chosen to encode the data used in the algorithm. In the second stage, the optimal architecture to implement the scheme is chosen using dependence graphs. Dependence graphs are a graph-theoretic form of algorithm representation. The authors demonstrate that not all architectures are ideal for the implementation of a particular ABFT scheme. They propose new measures to characterize the fault tolerance capability of a system to better exploit the proposed synthesis method. Dependence graphs can also be used for the synthesis of ABFT schemes for non-linear problems. An example of a fault-tolerant median filter is provided to illustrate their utility for such problems.<>"",""1558-2183"","""",""10.1109/71.238622"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238622"","""",""Fault tolerant systems";Fault detection;Parallel architectures;Concurrent computing;Filters;Network synthesis;Computer applications;Computer architecture;Production;"Fault tolerance"","""",""13"","""",""20"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"The DASH prototype: Logic overhead and performance,""D. Lenoski"; J. Laudon; T. Joe; D. Nakahira; L. Stevens; A. Gupta;" J. Hennessy"",""Computer Systems Laboratory, University of Stanford, CA, USA"; Computer Systems Laboratory, University of Stanford, CA, USA; Computer Systems Laboratory, University of Stanford, CA, USA; Computer Systems Laboratory, University of Stanford, CA, USA; Computer Systems Laboratory, University of Stanford, CA, USA; Computer Systems Laboratory, University of Stanford, CA, USA;" Computer Systems Laboratory, University of Stanford, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""41"",""61"",""The fundamental premise behind the DASH project is that it is feasible to build large-scale shared-memory multiprocessors with hardware cache coherence. The hardware overhead of directory-based cache coherence in a 48-processor is examined. The data show that the overhead is only about 10-15%, which appears to be a small cost for the ease of programming offered by coherent caches and the potential for higher performance. The performance of the system is discussed, and the speedups obtained by a variety of parallel applications running on the prototype are shown. Using a sophisticated hardware performance monitor, the effectiveness of coherent caches and the relationship between an application's reference behavior and its speedup are characterized. The optimizations incorporated in the DASH protocol are evaluated in terms of their effectiveness on parallel applications and on atomic tests that stress the memory system.<>"",""1558-2183"","""",""10.1109/71.205652"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205652"","""",""Prototypes";Logic;Large-scale systems;Hardware;Parallel architectures;Software prototyping;Costs;Protocols;Parallel programming;"Scalability"","""",""101"",""16"",""31"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals"""
"The direction vector I test,""K. Psarris"; X. Kong;" D. Klappholz"",""Department of Computer Science, Ohio University, Athens, OH, USA"; Sun Microsystems, Inc., Mountain View, CA, USA;" Department of Electrical Engineering and Computer Science, Stevens Institute of Technology, Hoboken, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""11"",""1280"",""1290"",""The GCD and Banerjee tests are the standard data dependence tests used to determine whether a loop may be parallelized/vectorized. In an earlier work, (1991) the authors presented a new data dependence test, the I test, which extends the accuracy of the GCD and the Banerjee tests. In the original presentation, only the case of general dependence was considered, i.e., the case of dependence with a direction vector of the form (*,*,...,*). In the present work, the authors generalize the I test to check for data dependence subject to an arbitrary direction vector.<>"",""1558-2183"","""",""10.1109/71.250105"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250105"","""",""Testing";Performance evaluation;Costs;Humans;Computer science;Sun;"Equations"","""",""33"","""",""15"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"The hyper-deBruijn networks: scalable versatile architecture,""E. Ganesan";" D. K. Pradhan"",""Department of Electrical Engineering, University of Massachusetts, Amherst, MA, USA";" Department of Computer Scicnce, Texas A and M University, College Station, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""9"",""962"",""978"",""Both Hypercube and deBruijn networks possess desirable properties. It should be understood, though, that some of the attractive features of one are not found in the other. The architecture proposed in this paper is a combination of these architectures, providing some of the desirable properties of both the networks such as admitting many computationally important networks, flexibility in terms of connections per node as well as level of fault-tolerance. Also the network allows a simple VLSI layout, scalability as well as decomposability. Thus, these networks can be a potential candidate for VLSI multiprocessor networks. The proposed network possesses logarithmic diameter, optimal connectivity, and simple routing algorithms amendable to networks with faults. Importantly, in addition to being pancyclic, these hyper-deBruijn networks admit most computationally important subnetworks including rings, multidimensional meshes, complete binary trees, and mesh of trees with perfect dilation. Techniques for optimal one-to-all (OTA) broadcasting in these networks are presented. As an intermediate result, this technique provides the fastest OTA broadcasting in binary deBruijn networks as well. The recent renewed interest in binary deBruijn networks makes this later result valuable.<>"",""1558-2183"","""",""10.1109/71.243525"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=243525"","""",""Computer architecture";Computer networks;Very large scale integration;Broadcasting;Hypercubes;Fault tolerance;Scalability;Routing;Multidimensional systems;"Binary trees"","""",""56"","""",""37"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"The M/sup 3/ multiprocessor laboratory,""H. Burkhart"; R. Eigenmann; H. Kindlimann; M. Moser;" H. Scholian"",""Informatik, University of Basel, Basel, Switzerland"; Center for Supercomputing Research and Development, University of Illinois, Urbana-Champaign, Urbana, IL, USA; Ergosoft AG, Altnau, Switzerland; IBM Zurich Research Laboratory, Zurich, Switzerland;" Institut for Elektronik ETH, Zurich, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""5"",""507"",""519"",""An integrated programming environment for the M/sup 3/ multiprocessor is discussed. Three tools support the software development cycle of a parallel program, including the programming, configuration, and debugging/performance measurement phases. Programmer support for performance analysis has been a primary motivation for the system. The sources of performance loss are identified and the ways in which this information is gathered and analyzed are described. As a case study, a fast maze router algorithm is used to show the functionality of the different tools. The M/sup 3/ environment is compared with other state-of-the-art projects.<>"",""1558-2183"","""",""10.1109/71.224214"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=224214"","""",""Laboratories";Performance analysis;Parallel programming;Hardware;Costs;Operating systems;Resource management;Kernel;Memory management;"Processor scheduling"","""","""","""",""28"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;
"The parallel complexity of embedding algorithms for the solution of systems of nonlinear equations,""A. Chakraborty"; D. C. S. Allison; C. J. Ribbens;" L. T. Watson"",""Citibank Global Finance, New York, NY, USA"; Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA; Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA;" Department of Computer Science, Virginia Polytechnic Institute and State University, Blacksburg, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""4"",""458"",""465"",""Embedding algorithms used to solve nonlinear systems of equations do so by constructing a continuous family of systems and solving the given system by tracking the continuous curve of solutions to the family. Solving nonlinear equations by a globally convergent embedding algorithm requires the evaluation and factoring of a Jacobian matrix at many points along the embedding curve. Ways to optimize the Jacobian matrix on a hypercube are described. Several static and dynamical strategies for assigning components of the Jacobian to processors on the hypercube are investigated. It is found that a static rectangular grid mapping is the preferred choice for inclusion in a robust parallel mathematical software package. The static linear mapping is a viable alternative when there are many common subexpressions in the component evaluation, and the dynamic assignment strategy should only be considered when there is large variation in the evaluation times for the components, leading to a load imbalance on the processors.<>"",""1558-2183"","""",""10.1109/71.219760"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=219760"","""",""Nonlinear equations";Jacobian matrices;Hypercubes;Linear algebra;Nonlinear systems;Software packages;Newton method;Concurrent computing;Robustness;"Algorithm design and analysis"","""",""9"","""",""27"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"The scalability of FFT on parallel computers,""A. Gupta";" V. Kumar"",""Department of Computer Science, University of Minnesota, Minneapolis, MN, USA";" Department of Computer Science, University of Minnesota, Minneapolis, MN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""922"",""932"",""The authors present the scalability analysis of a parallel fast Fourier transform (FFT) algorithm on mesh and hypercube connected multicomputers using the isoefficiency metric. The isoefficiency function of an algorithm architecture combination is defined as the rate at which the problem size should grow with the number of processors to maintain a fixed efficiency. It is shown that it is more cost-effective to implement the FFT algorithm on a hypercube rather than a mesh despite the fact that large scale meshes are cheaper to construct than large hypercubes. Although the scope of this work is limited to the Cooley-Tukey FFT algorithm on a few classes of architectures, the methodology can be used to study the performance of various FFT algorithms on a variety of architectures such as SIMD hypercube and mesh architectures and shared memory architecture.<>"",""1558-2183"","""",""10.1109/71.238626"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238626"","""",""Scalability";Concurrent computing;Hypercubes;Signal processing algorithms;Algorithm design and analysis;Parallel algorithms;Parallel architectures;Performance analysis;Bandwidth;"Large-scale systems"","""",""66"","""",""46"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"The SNAP-1 parallel AI prototype,""R. F. DeMara";" D. I. Moldovan"",""Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA";" Department of Electrical Engineering-Systems, University of Southern California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""8"",""841"",""854"",""The Semantic Network Array Processor (SNAP) is a parallel architecture for knowledge representation and reasoning that uses the marker-propagation paradigm. The primary application areas of SNAP are natural language understanding and speech processing. A first-generation SNAP-1 system has been designed and constructed using an array of 144 digital signal processors organized as 32 multiprocessing clusters with dedicated communication units, a tiered synchronization scheme, and multiported memory network. Issues in the design, performance, and scalability of a marker-propagation architecture are addressed.<>"",""1558-2183"","""",""10.1109/71.238620"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=238620"","""",""Artificial intelligence";Prototypes;Application software;Natural languages;Computer architecture;Speech processing;Parallel processing;Parallel architectures;Knowledge representation;"Signal design"","""",""8"","""",""14"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Trapezoid self-scheduling: a practical scheduling scheme for parallel compilers,""T. H. Tzen";" L. M. Ni"",""Department of Computer Science, Michigan State University, East Lansing, MI, USA";" Michigan State University, East Lansing, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""1"",""87"",""98"",""A practical processor self-scheduling scheme, trapezoid self-scheduling, is proposed for arbitrary parallel nested loops in shared-memory multiprocessors. Generally, loops are the richest source of parallelism in parallel programs. To dynamically allocate loop iterations to processors, one may achieve load balancing among processors at the expense of run-time scheduling overhead. By linearly decreasing the chunk size at run time, the best tradeoff between the scheduling overhead and balanced workload can be obtained in the proposed trapezoid self-scheduling approach. Due to its simplicity and flexibility, this approach can be efficiently implemented in any parallel compiler. The small and predictable number of chores also allow efficient management of memory in a static fashion. The experiments conducted in a 96-node Butterfly GP-1000 clearly show the advantage of the trapezoid self-scheduling over other well-known self-scheduling approaches.<>"",""1558-2183"","""",""10.1109/71.205655"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=205655"","""",""Processor scheduling";Load management;Dynamic scheduling;Multiprocessing systems;Memory management;Parallel processing;Runtime;Parallel languages;Programming profession;"Computer science"","""",""185"",""2"",""15"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Unifying and optimizing parallel linear algebra algorithms,""M. Angelaccio";" M. Colajanni"",""Dipartimento di Ingegneria Elettronica, Universitià di Roma Tor Vergata, Rome, Italy";" Dipartimento di Ingegneria Elettronica, Universitià di Roma Tor Vergata, Rome, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""12"",""1382"",""1397"",""Two issues in linear algebra algorithms for multicomputers are addressed. First, how to unify parallel implementations of the same algorithm in a decomposition-independent way. Second, how to optimize naive parallel programs maintaining the decomposition independence. Several matrix decompositions are viewed as instances of a more general allocation function called subcube matrix decomposition. By this meta-decomposition, a programming environment characterized by general primitives that allow one to design meta-algorithms independently of a particular decomposition. The authors apply such a framework to the parallel solution of dense matrices. This demonstrates that most of the existing algorithms can be derived by suitably setting the primitives used in the meta-algorithm. A further application of this programming style concerns the optimization of parallel algorithms. The idea to overlap communication and computation has been extended from 1-D decompositions to 2-D decompositions. Thus, a first attempt towards a decomposition-independent definition of such optimization strategies is provided.<>"",""1558-2183"","""",""10.1109/71.250119"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=250119"","""",""Linear algebra";Matrix decomposition;Programming environments;Parallel programming;Hypercubes;Algorithm design and analysis;Parallel algorithms;Concurrent computing;Performance analysis;"Linear programming"","""",""12"","""",""19"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Using processor-cache affinity information in shared-memory multiprocessor scheduling,""M. S. Squillante";" E. D. Lazowska"",""IBM Thomas J. Watson Research Center, Yorktown Heights, NY, USA";" Department of Computer Science and Engineering, FR-35, University of Washington, Seattle, WA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1993"",""4"",""2"",""131"",""143"",""In a shared-memory multiprocessor system, it may be more efficient to schedule a task on one processor than on another if relevant data already reside in a particular processor's cache. The effects of this type of processor affinity are examined. It is observed that tasks continuously alternate between executing at a processor and releasing this processor due to I/O, synchronization, quantum expiration, or preemption. Queuing network models of different abstract scheduling policies are formulated, spanning the range from ignoring affinity to fixing tasks on processors. These models are solved via mean value analysis, where possible, and by simulation otherwise. An analytic cache model is developed and used in these scheduling models to include the effects of an initial burst of cache misses experienced by tasks when they return to a processor for execution. A mean-value technique is also developed and used in the scheduling models to include the effects of increased bus traffic due to these bursts of cache misses. Only a small amount of affinity information needs to be maintained for each task. The importance of having a policy that adapts its behavior to changes in system load is demonstrated.<>"",""1558-2183"","""",""10.1109/71.207589"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=207589"","""",""Processor scheduling";Multiprocessing systems;Queueing analysis;Measurement;Operating systems;Degradation;Analytical models;Traffic control;Information analysis;"Performance analysis"","""",""95"",""2"",""28"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;