"A class of low complexity high concurrence algorithms,""J. L. Aravena";" A. O. Barbir"",""Department of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA, USA";" Department of Mathematics and Computer Science, Western Carolina University, Cullowhee, NC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""495"",""502"",""A nonconventional approach to the analysis of dedicated computing structures in which the number of compute cycles is used as a design parameter to determine families of transformations implementable in the structure is presented. Using this approach, a single architecture can be used to implement a family of transformations with varying degrees of complexity. The transformations generated by a matrix multiplication array are considered in detail. It is shown that, for some real-time applications it becomes possible to incorporate the compute time as a constraint for designs based in optimality criteria. In particular, a least square approximation problem is discussed.<>"",""1558-2183"","""",""10.1109/71.97905"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97905"","""",""Signal processing algorithms";Computer architecture;Very large scale integration;Arithmetic;Systolic arrays;Parallel processing;Array signal processing;Time factors;Least squares approximation;"Parallel algorithms"","""",""7"","""",""19"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"A class of randomized strategies for low-cost comparison of file copies,""D. Barbara";" R. J. Lipton"",""Department of Computer Science, Princeton University, Princeton, NJ, USA";" Department of Computer Science, Princeton University, Princeton, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""160"",""170"",""A class of algorithms that use randomized signatures to compare remotely located file copies is presented. A simple technique that sends on the order of 4/sup f/log(n) bits, where f is the number of differing pages that are to be diagnosed and n is the number of pages in the file, is described. A method to improve the bound in the number of bits sent, making them grow with f as flog(f) and with n as log(n)log(log(n)), and a class of algorithms in which the number of signatures grows with f as fr/sup f/, where r can be made to approach 1, are also presented. A comparison of these techniques is discussed.<>"",""1558-2183"","""",""10.1109/71.89062"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89062"","""",""Transaction databases";Costs;Distributed algorithms;Fault tolerance;Humans;Hardware;Database systems;Sun;Broadcasting;"Voting"","""",""19"",""7"",""8"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"A generalized simultaneous access dictionary machine,""Z. Fan";" K. . -H. Cheng"",""Advanced Computing Solutions, Inc., Houston, TX, USA";" Department of Computer Science, University of Houston, Houston, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""149"",""159"",""A simultaneous access design of a dictionary machine which supports insert, delete, and search operations is presented. The design is able to handle p accesses simultaneously and allows redundant accesses to occur. In the design, processors performing insert or delete operations are free to perform other tasks after submitting their accesses to the design"; processors that perform search operations get their response in O(log N) time. Compared to all sequential access designs of a dictionary which require O(p) time to process p accesses, the presented design provides much higher throughput;" specifically, O(p/log p) times better. It also provides a fast mechanism to avoid the sequential access bottleneck in any large multiprocessor system.<>"",""1558-2183"","""",""10.1109/71.89061"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89061"","""",""Dictionaries";Data structures;Throughput;Multiprocessing systems;Process design;Parallel processing;Natural language processing;Hardware;Pipeline processing;"Computer science"","""",""2"","""",""23"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"A loop transformation theory and an algorithm to maximize parallelism,""M. E. Wolf";" M. S. Lam"",""Computer Systems Laboratory, University of Stanford, Stanford, CA, USA";" Computer Systems Laboratory, University of Stanford, Stanford, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""452"",""471"",""An approach to transformations for general loops in which dependence vectors represent precedence constraints on the iterations of a loop is presented. Therefore, dependences extracted from a loop nest must be lexicographically positive. This leads to a simple test for legality of compound transformations: any code transformation that leaves the dependences lexicographically positive is legal. The loop transformation theory is applied to the problem of maximizing the degree of coarse- or fine-grain parallelism in a loop nest. It is shown that the maximum degree of parallelism can be achieved by transforming the loops into a nest of coarsest fully permutable loop nests and wavefronting the fully permutable nests. The canonical form of coarsest fully permutable nests can be transformed mechanically to yield maximum degrees of coarse- and/or fine-grain parallelism. The efficient heuristics can find the maximum degrees of parallelism for loops whose nesting level is less than five.<>"",""1558-2183"","""",""10.1109/71.97902"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97902"","""",""Testing";Parallel processing;Vectors;Parallel machines;Law;Legal factors;Conferences;Computer languages;Program processors;"Space exploration"","""",""411"",""11"",""26"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"A nonblocking quorum consensus protocol for replicated data,""D. Agrawal";" A. J. Bernstein"",""Department of Computer Science, University of California, Santa Barbara, CA, USA";" Department of Computer Science, State University of New York, Stony Brook, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""171"",""179"",""A nonblocking quorum protocol for replica control which guarantees one-copy serializability is developed. The effects of a nonblocking protocol are analyzed, and it is shown that the gains can be substantial under certain conditions. It is demonstrated that in order for the protocol to be useful, it must be integrated with a propagation mechanism. It is also shown that the access latency can be reduced significantly in a replicated environment. An interesting aspect of the quorum protocol is that it essentially uses a read quorom/write-quorom approach for concurrency control but uses a read-one/write-all approach for replica control. It is shown that the nonblocking quorom protocol provides the same level of availability and fault tolerance as the quorum protocol proposed by D.K. Gifford (1979).<>"",""1558-2183"","""",""10.1109/71.89063"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89063"","""",""Access protocols";Delay;Fault tolerance;Database systems;Writing;Concurrency control;Computer science;Availability;Distributed databases;"Fault tolerant systems"","""",""9"","""",""40"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"A parallel execution model of logic programs,""A. C. Chen";" C. . -I. Wu"",""Department of Electrical and Computer Engineering, University of Texas, Austin, USA";" Department of Electrical and Computer Engineering, University of Texas, Austin, Austin, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""79"",""92"",""A parallel-execution model that can concurrently exploit AND and OR parallelism in logic programs is presented. This model employs a combination of techniques in an approach to executing logic problems in parallel, making tradeoffs among number of processes, degree of parallelism, and combination bandwidth. For interpreting a nondeterministic logic program, this model (1) performs frame inheritance for newly created goals, (2) creates data-dependency graphs (DDGs) that represent relationships among the goals, and (3) constructs appropriate process structures based on the DDGs. (1) The use of frame inheritance serves to increase modularity. In contrast to most previous parallel models that have a large single process structure, frame inheritance facilitates the dynamic construction of multiple independent process structures, and thus permits further manipulation of each process structure. (2) The dynamic determination of data dependency serves to reduce computational complexity. In comparison to models that exploit brute-force parallelism and models that have fixed execution sequences, this model can reduce the number of unification and/or merging steps substantially. In comparison to models that exploit only AND parallelism, this model can selectively exploit demand-driven computation, according to the binding of the query and optional annotations. (3) The construction of appropriate process structures serves to reduce communication complexity. Unlike other methods that map DDGs directly onto process structures, this model can significantly reduce the number of data sent to a process and/or the number of communication channels connected to a process.<>"",""1558-2183"","""",""10.1109/71.80191"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80191"","""",""Parallel processing";Logic programming;Manipulator dynamics;Concurrent computing;Logic design;Bandwidth;Employment;Computational complexity;Merging;"Complexity theory"","""",""5"","""",""43"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"A template-based approach to the generation of distributed applications using a network of workstations,""A. Singh"; J. Schaeffer;" M. Green"",""Distributed Systems Research Laboratory, Department of Computing Science, University of Alberta, Edmonton, AB, Canada"; Distributed Systems Research Laboratory, Department of Computing Science, University of Alberta, Edmonton, AB, Canada;" Distributed Systems Research Laboratory, Department of Computing Science, University of Alberta, Edmonton, AB, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""52"",""67"",""A computational model and system for the generation of distributed applications in a workstation environment are presented. The well-known RPC model is modified by a novel concept known as template attachment. A computation consists of a network of sequential procedures which have been encapsulated in templates. A small selection of templates is available from which a distributed application with the desired communication behavior can be rapidly built. The system generates all the required low-level code for correct synchronization, communication, and scheduling. This results in a system that is easy to use and flexible and can provide a programmer with the desired amount of control in using idle processing power over a network of workstations. The practical feasibility of the model has been demonstrated by implementing it for Unix-based workstation environments.<>"",""1558-2183"","""",""10.1109/71.80189"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80189"","""",""Workstations";Computer networks;Distributed computing;Application software;Parallel processing;Power system modeling;Concurrent computing;Parallel programming;Power engineering computing;"Computational modeling"","""",""32"",""4"",""37"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"A top-down processor allocation scheme for hypercube computers,""J. Kim"; C. R. Das;" W. Lin"",""Department of Electrical and Computer Engineering, Pennsylvania State University, University Park, PA, USA"; Department of Electrical and Computer Engineering, Pennsylvania State University, University Park, PA, USA;" Department of Electrical Engineering, University of Hawai, Honolulu, HI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""20"",""30"",""An efficient processor allocation policy is presented for hypercube computers. The allocation policy is called free list since it maintains a list of free subcubes available in the system. An incoming request of dimension k (2/sup k/ nodes) is allocated by finding a free subcube of dimension k or by decomposing an available subcube of dimension greater than k. This free list policy uses a top-down allocation rule in contrast to the bottom-up approach used by the previous bit-map allocation algorithms. This allocation scheme is compared to the buddy, gray code (GC), and modified buddy allocation policies reported for the hypercubes. It is shown that the free list policy is optimal in a static environment, as are the other policies, and it also gives better subcube recognition ability compared to the previous schemes in a dynamic environment. The performance of this policy, in terms of parameters such as average delay, system utilization, and time complexity, is compared to the other schemes to demonstrate its effectiveness. The extension of the algorithm for parallel implementation, noncubic allocation, and inclusion/exclusion allocation is also given.<>"",""1558-2183"","""",""10.1109/71.80186"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80186"","""",""Hypercubes";Concurrent computing;Reflective binary codes;Resource management;Topology;Delay systems;Delay effects;Computer architecture;Parallel machines;"Flow graphs"","""",""79"","""",""17"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"An efficient modular spare allocation scheme and its application to fault tolerant binary hypercubes,""M. S. Alam";" R. G. Melhem"",""Department of Computer Science, University of Pittsburgh, Pittsburgh, PA, USA";" Department of Computer Science, University of Pittsburgh, Pittsburgh, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""117"",""126"",""Consideration is given to fault tolerant systems that are built from modules called fault tolerant basic blocks (FTBBs), where each module contains some primary nodes and some spare nodes. Full spare utilization is achieved when each spare within an FTBB can replace any other primary or spare node in that FTBB. This, however, may be prohibitively expensive for larger FTBBs. Therefore, it is shown that for a given hardware overhead more reliable systems can be designed using bigger FTBBs without full spare utilization than using smaller FTBBs with full spare utilization. Sufficient conditions for maximizing the reliability of a spare allocation strategy in an FTBB for a given hardware overhead are presented. The proposed spare allocation strategy is applied to two fault tolerant reconfiguration schemes for binary hypercubes. One scheme uses hardware switches to replace a faulty node, and the other scheme uses fault tolerant routing to bypass faulty nodes in the system and deliver messages to the destination node.<>"",""1558-2183"","""",""10.1109/71.80194"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80194"","""",""Fault tolerance";Hypercubes;Hardware;Switches;Fault tolerant systems;Routing;Redundancy;Multiprocessing systems;Sufficient conditions;"Binary trees"","""",""32"","""",""11"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"An implementation of interprocedural bounded regular section analysis,""P. Havlak";" K. Kennedy"",""Department of Computer Science, Rice University, Houston, TX, USA";" Department of Computer Science, Rice University, Houston, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""350"",""360"",""Regular section analysis, which summarizes interprocedural side effects on subarrays in a form useful to dependence analysis, while avoiding the complexity of prior solutions, is shown to be a practical addition to a production compiler. Optimizing compilers should produce efficient code even in the presence of high-level language constructs. However, current programming support systems are significantly lacking in their ability to analyze procedure calls. This deficiency complicates parallel programming, because loops with calls can be a significant source of parallelism. The performance of regular section analysis is compared to two benchmarks: the LINPACK library of linear algebra subroutines and the Rice Compiler Evaluation Program Suite (RiCEPS), a set of complete application codes from a variety of scientific disciplines. The experimental results demonstrate that regular section analysis is an effective means of discovering parallelism, given programs written in an appropriately modular programming style.<>"",""1558-2183"","""",""10.1109/71.86110"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86110"","""",""Algorithm design and analysis";Parallel programming;High level languages;Parallel processing;Libraries;Optimizing compilers;Program processors;Design optimization;Read-write memory;"Computer science"","""",""139"",""1"",""41"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Automatic generation of self-scheduling programs,""I. Foster"",""Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""68"",""78"",""Techniques are described for the automatic generation of self-scheduling parallel programs. Both scheduling algorithms and the concurrent components of applications are expressed in a high-level concurrent language. Partitioning and data dependency information are expressed by simple control statements, which may be generated either automatically or manually. A self-scheduling compiler, implemented as a source-to-source transformation, takes application code, control statements, and scheduling routines and generates a new program that can schedule its own execution on a parallel computer. The approach has several advantages compared to previous proposals. It generates programs that are portable over a wide range of parallel computers. There is no need to embed special control structures in application programs. The use of a high-level language to express applications and scheduling algorithms facilitates the development, modification, and reuse of parallel programs.<>"",""1558-2183"","""",""10.1109/71.80190"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80190"","""",""Processor scheduling";Application software;Scheduling algorithm;Program processors;Concurrent computing;Load management;Automatic generation control;Portable computers;Programming profession;"Proposals"","""",""10"",""2"",""17"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Block, multistride vector, and FFT accesses in parallel memory systems,""D. T. Harper"",""Department of Electrical Engineering, University of Texas, Dallas, Richardson, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""43"",""51"",""A discussion is presented of the use of dynamic storage schemes to improve parallel memory performance during three important classes of data accesses: vector accesses in which multiple strides are used to access a single vector, block accesses, and constant-geometry FFT accesses. The schemes investigated are based on linear address transformations, also known as XOR schemes. It has been shown that this class of schemes can be implemented more efficiently in hardware and has more flexibility than schemes based on row rotations or other techniques. Several analytical results are shown. These include: quantitative analysis of buffering effects in pipelined memory systems"; design rules for storage schemes that provide conflict-free access using multiple strides, blocks, and FFT access patterns;" and an analysis of the effects of memory bank cycle time on storage scheme capabilities.<>"",""1558-2183"","""",""10.1109/71.80188"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80188"","""",""Bandwidth";Buffer storage;Vectors;Hardware;Memory architecture;Performance loss;Degradation;Computer architecture;Pattern analysis;"Process design"","""",""51"",""5"",""29"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"CAREL: computer aided reliability evaluator for distributed computing networks,""S. Soh";" S. Rai"",""Department of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA, USA";" Department of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""199"",""213"",""An efficient method to compute the terminal reliability (the probability of communication between a pair of nodes) of a distributed computing system (DCS) is presented. It is assumed that the graph model G(V,E) for DCS is given and that the path and/or cut information for the network G(V,E) is available. Boolean algebraic concepts are used to define four operators: compare, reduce, combine, and generate. The proposed method, called CAREL, uses the four operators to generate exclusive and mutually disjoint events. CAREL has been implemented using bit vector representation on an Encore MULTIMAX 320 system. It is shown that CAREL solves large DCS networks (having a pathset on the order of 780 and a cutset on the order of 7300 or more) with a reasonable memory requirement. A comparison with other algorithms reveals the computational efficiency of the method. The proof of correctness of CAREL is included.<>"",""1558-2183"","""",""10.1109/71.89065"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89065"","""",""Distributed computing";Computer networks;Computer network reliability;Distributed control;Telecommunication network reliability;Boolean functions;Polynomials;Iterative algorithms;Computational efficiency;"Measurement"","""",""86"",""1"",""30"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Clustering on a hypercube multicomputer,""S. Ranka";" S. Sahni"",""School of Computer Science, Syracuse University, Syracuse, NY, USA";" Department of Computer and Information Science, University of Florida, Gainesville, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""129"",""137"",""Squared error clustering algorithms for single-instruction multiple-data (SIMD) hypercubes are presented. The algorithms are shown to be asymptotically faster than previously known algorithms and require less memory per processing element (PE). For a clustering problem with N patterns, M features per pattern, and K clusters, the algorithms complete in O(k+log NM) steps on NM processor hypercubes. This is optimal up to a constant factor. These results are extended to the case in which NMK processors are available. Experimental results from a multiple-instruction, multiple-data (MIMD) medium-grain hypercube are also presented.<>"",""1558-2183"","""",""10.1109/71.89059"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89059"","""",""Hypercubes";Clustering algorithms;Pattern recognition;Computer errors;Pattern analysis;Image segmentation;Silicon carbide;Computer science;Partitioning algorithms;"Iterative algorithms"","""",""23"","""",""13"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic,""S. G. Abraham";" D. E. Hudak"",""Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA";" Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""318"",""328"",""Adaptive data partitioning (ADP) which reduces the execution time of parallel programs by reducing interprocessor communication for iterative parallel loops is discussed. It is shown that ADP can be integrated into a communication-reducing back end for existing parallelizing compilers or as part of a machine-specific partitioner for parallel programs. A multiprocessor model to analyze program execution factors that lead to interprocessor communication and a model for the iterative parallel loop to quantify communication patterns within a program are defined. A vector notation is chosen to quantify communication across a global data set. Communication parameters are computed by examining the indexes of array accesses and are adjusted to reflect the underlying system architecture by compensating for cache line sizes. These values are used to generate rectangular and hexagonal partitions that reduce interprocessor communication.<>"",""1558-2183"","""",""10.1109/71.86107"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86107"","""",""Multiprocessing systems";Program processors;Parallel processing;Bandwidth;Delay;Parallel programming;Costs;Computer architecture;Traffic control;"Optimizing compilers"","""",""24"","""",""22"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Compile-time techniques for data distribution in distributed memory machines,""J. Ramanujam";" P. Sadayappan"",""Department of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA, USA";" Department of Computer and Information Science, Ohio State Uinversity, Columbus, OH, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""472"",""482"",""A solution to the problem of partitioning data for distributed memory machines is discussed. The solution uses a matrix notation to describe array accesses in fully parallel loops, which allows the derivation of sufficient conditions for communication-free partitioning (decomposition) of arrays. A series of examples that illustrate the effectiveness of the technique for linear references, the use of loop transformations in deriving the necessary data decompositions, and a formulation that aids in deriving heuristics for minimizing a communication when communication-free partitions are not feasible are presented.<>"",""1558-2183"","""",""10.1109/71.97903"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97903"","""",""Programming profession";Random access memory;Matrix decomposition;Program processors;Sufficient conditions;Costs;Data analysis;Pattern analysis;Hypercubes;"Information science"","""",""100"",""1"",""25"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Compile-time techniques for improving scalar access performance in parallel memories,""A. Gupta";" M. L. Soffa"",""Department of Computer Science, University of Pittsburgh, Pittsburgh, PA, USA";" Department of Computer Science, University of Pittsburgh, Pittsburgh, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""138"",""148"",""Compile-time techniques for storage allocation of scalar values into memory modules that limit run-time memory-access conflicts are presented. The allocation approach is applicable to those operands in instructions that can be predicted at compile-time, where an instruction is composed of the multiple operations and corresponding operands that execute in parallel. Algorithms to schedule data transfers among memory modules to avoid conflicts that cannot be eliminated by the distribution of values alone are developed. The techniques have been implemented as part of a compiler for a reconfigurable long instruction word architecture. Results of experiments are presented demonstrating that a very high percentage of memory access conflicts can be avoided by scheduling a very low number of data transfers.<>"",""1558-2183"","""",""10.1109/71.89060"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89060"","""",""Bandwidth";Laser mode locking;Scheduling algorithm;Processor scheduling;Memory architecture;Multiprocessor interconnection networks;Degradation;Impedance;Programming profession;"Computer science"","""",""9"","""",""23"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Compiling communication-efficient programs for massively parallel machines,""J. Li";" M. Chen"",""Department of Computer Science, Portland State University, Portland, OR, USA";" Department of Computer Science, Yale University, New Heaven, CT, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""361"",""376"",""A method of generating parallel target code with explicit communication for massively parallel distributed-memory machines is presented. The source programs are shared-memory parallel programs with explicit control structures. The method extracts syntactic reference patterns from a program with shared address space, selects appropriate communication routines, places these routines in appropriate locations in the target program text and sets up correct conditions for invoking these routines. An explicit communication metric is used to guide the selection of data layout strategies.<>"",""1558-2183"","""",""10.1109/71.86111"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86111"","""",""Parallel machines";Hardware;Costs;Communication system control;Aggregates;Pattern matching;Processor scheduling;Concurrent computing;Parallel processing;"Application software"","""",""122"",""4"",""20"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Compiling global name-space parallel loops for distributed execution,""C. Koelbel";" P. Mehrotra"",""CITVCRPC, Rice University, Houston, TX, USA";" NASA Langley Research Center, Hampton, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""440"",""451"",""Compiler support required to allow programmers to express their algorithms using a global name-space is discussed. A general method for the analysis of a high-level source program and its translation into a set of independently executing tasks that communicate using messages is presented. It is shown that if the compiler has enough information, the translation can be carried out at compile time"; otherwise;" run-time code is generated to implement the required data movement. The analysis required in both situations is described, and the performance of the generated code on the Intel iPSC/2 hypercube is presented.<>"",""1558-2183"","""",""10.1109/71.97901"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97901"","""",""Memory architecture";Programming profession;Data structures;NASA;Program processors;Runtime;Algorithm design and analysis;Message passing;Performance analysis;"Costs"","""",""143"","""",""32"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"Consensus with dual failure modes,""F. J. Meyer";" D. K. Pradhan"",""Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA, USA";" Department of Electrical and Computer Engineering, University of Massachusetts, Amherst, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""214"",""222"",""The problem of achieving consensus in a distributed system is discussed. Systems are treated in which either or both of two types of faults may occur: dormant (essentially omission and timing faults) and arbitrary (exhibiting arbitrary behavior, commonly referred to as Byzantine). Previous results showed that are number of dormant faults may be tolerated when there are no arbitrary faults and that, at most, (n-1/3) arbitrary faults may be tolerated when there are no dormant faults (n is the number of processors). A continuum is established between the previous results: an algorithm exists iff n>f/sub max/+2m/sub max/ and c>f/sub max/+m/sub max/ (where c is the system connectivity), when faults are constrained so that there are at most f/sub max/ and at most m/sub max/ of these that are arbitrary. An algorithm is given and compared to known algorithms. A method is given to establish virtual links so that the communications graph appears completely connected.<>"",""1558-2183"","""",""10.1109/71.89066"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89066"","""",""Timing";Fault tolerance;Routing;Computer crashes;Nominations and elections;Distributed algorithms;"Process control"","""",""87"","""",""11"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Consistency in dataflow graphs,""E. A. Lee"",""Department of Electrical Engineering and Computer Science, University of California, Berkeley, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""223"",""235"",""Analytical properties of programming languages with dataflow graph semantics are discussed. It is shown that one of the most serious problems with these languages is that subtle inconsistencies between parts of the dataflow graph can be inadvertently created. These inconsistencies can lead to deadlock, or in the case of nonterminating programs, to unbounded memory requirements. Consistency is defined to mean that the same number of tokens is consumed as produced on any arc, in the long run. A token-flow model is developed for testing for inconsistency. The method is a generalization of consistency checks for synchronous dataflow (SDF) graphs. The token-flow model is compared to similar tests applied to hybrid dynamical systems. It is argued that dataflow semantics make steady-state analysis possible, leading to a simpler method in most cases.<>"",""1558-2183"","""",""10.1109/71.89067"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89067"","""",""Fires";System recovery;Testing;Parallel processing;Signal processing;Iterative algorithms;Signal processing algorithms;Electric breakdown;Computer languages;"Digital signal processing"","""",""76"","""",""27"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Data-parallel programming on MIMD computers,""P. J. Hatcher"; M. J. Quinn; A. J. Lapadula; B. K. Seevers; R. J. Anderson;" R. R. Jones"",""Department of Computer Science, University of New Hampshire, Durham, NH, USA"; Department of Computer Science, Oregon State University, Corvallis, OR, USA; Department of Computer Science, University of New Hampshire, Durham, NH, USA; Department of Computer Science, Oregon State University, Corvallis, OR, USA; Department of Computer Science, Oregon State University, Corvallis, OR, USA;" Department of Computer Science, University of New Hampshire, Durham, NH, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""377"",""383"",""The implementation of two compilers for the data-parallel programming language Dataparallel C is described. One compiler generates code for Intel and nCUBE hypercube multicomputers";" the other generates code for Sequent multiprocessors. A suite of Dataparallel C programs has been compiled and executed, and their execution times and speedups on the Intel iPSC/2, the nCUBE 3200 and the Sequent Symmetry are presented.<>"",""1558-2183"","""",""10.1109/71.86112"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86112"","""",""Program processors";Concurrent computing;Parallel processing;Parallel programming;Programming profession;Computer languages;Hypercubes;Parallel languages;Computer science;"Distributed computing"","""",""47"","""",""32"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals"""
"Design and analysis of even-sized binary shuffle-exchange networks for multiprocessors,""K. Padmanabhan"",""AT and T Bell Laboratories, Inc., Murray Hill, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""385"",""397"",""The architecture and performance of binary shuffle-exchange networks of any size are investigated. It is established that a network with a shuffle-exchange stages whose number equals the least integer >or=log/sub 2/N or a single recirculating stage can provide the connectivity between N inputs and N outputs using a distributed tag-based control algorithm. Control tags depend on both source and destination when N is not a power of two and can be computed in a simple manner. Several structural and dynamic properties of the network are established, contrasting the behavior of the power-of-two and composite sized systems. The performance of the network in a stochastic environment is investigated analytically. It is shown that the shuffle-exchange networks behave in much the same way with respect to traffic and buffer capacity regardless of whether the system size is a power of two or not.<>"",""1558-2183"","""",""10.1109/71.97896"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97896"","""",""LAN interconnection";Concurrent computing;Stochastic processes;Power system interconnection;Distributed control;Performance analysis;Telecommunication traffic;Helium;Multiprocessing systems;"Telephony"","""",""17"","""",""20"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Eliminating memory for fragmentation within partitionable SIMD/SPMD machines,""M. A. Nichols"; H. J. Siegel; H. G. Dietz; R. W. Quong;" W. G. Nation"",""NCR Limited, San Diego, CA, USA"; School of Electrical Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical Engineering, Purdue University, West Lafayette, IN, USA; School of Electrical Engineering, Purdue University, West Lafayette, IN, USA;" IBM, Endicott, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""290"",""303"",""Efficient data layout is an important aspect of the compilation process. A model for the creation of perfect memory maps for large-scale parallel machines capable of user-controlled partitionable single-instruction-multiple data/single-program-multiple data (SIMD/SPMD) operation is developed. The term perfect implies that no memory fragmentation occurs and ensures that the memory map size is kept to a minimum. A major constraint on solving this problem is based on the single program nature of both the SIMD and SPMD modes of parallelism. It is assumed that all processors within the same submachine used identical addresses to access corresponding data items in each of their local memories. Necessary and sufficient conditions are derived for being able to create perfect memory maps, and results are applied to several partitionable interconnection networks.<>"",""1558-2183"","""",""10.1109/71.86105"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86105"","""",""Parallel processing";Parallel machines;Large-scale systems;Multiprocessor interconnection networks;Hypercubes;Sufficient conditions;Broadcasting;Oceans;High performance computing;"Data engineering"","""",""7"",""2"",""39"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;
"Interactive parallel programming using the ParaScope Editor,""K. Kennedy"; K. S. McKinley;" C. . -W. Tseng"",""Department of Computer Science, Rice University, Houston, TX, USA"; Department of Computer Science, Rice University, Houston, TX, USA;" Department of Computer Science, Rice University, Houston, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""329"",""341"",""The ParaScope Editor, an intelligent interactive editor for parallel Fortran programs, which is the centerpiece of the ParaScope project, an integrated collection of tools to help scientific programmers implement correct and efficient parallel programs, is discussed. ParaScope Editor reveals to users potential hazards of a proposed parallelization in a program. It provides a variety of powerful interactive program transformations that have been shown useful in converting programs to parallel form. ParaScope Editor supports general user editing through a hybrid text and structure editing facility that incrementally analyzes the modified program for potential hazards. It is shown that ParaScope Editor supports an exploratory programming style in which users get immediate feedback on their various strategies for parallelization.<>"",""1558-2183"","""",""10.1109/71.86108"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86108"","""",""Parallel programming";Parallel processing;Programming profession;Hazards;Feedback;Helium;Availability;Concurrent computing;Parallel machines;"Computer science"","""",""61"",""4"",""57"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"Job scheduling in a partitionable mesh using a two-dimensional buddy system partitioning scheme,""K. Li";" K. . -H. Cheng"",""Department of Mathematics and Computer Science, State University of New York, New Paltz, New Paltz, NY, USA";" Department of Computer Science, University of Houston, Houston, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""413"",""422"",""The job scheduling problem in a partitionable mesh-connected system in which jobs require square meshes and the system is a square mesh whose size is a power of two is discussed. A heuristic algorithm of time complexity O(n(log n+log p)), in which n is the number of jobs to be scheduled and p is the size of the system is presented. The algorithm adopts the largest-job-first scheduling policy and uses a two-dimensional buddy system as the system partitioning scheme. It is shown that, in the worst case, the algorithm produces a schedule four times longer than an optimal schedule, and, on the average, schedules generated by the algorithm are twice as long as optimal schedules.<>"",""1558-2183"","""",""10.1109/71.97898"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97898"","""",""Optimal scheduling";Scheduling algorithm;Partitioning algorithms;Approximation algorithms;Processor scheduling;Computer science;Heuristic algorithms;Helium;Parallel algorithms;"Mathematics"","""",""25"",""2"",""12"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Lazy task creation: a technique for increasing the granularity of parallel programs,""E. Mohr"; D. A. Kranz;" R. H. Halstead"",""Department of Computer Science, Yale University, New Heaven, CT, USA"; MIT Laboratory for Computer Science, Cambridge, MA, USA;" DEC Cambridge Research Laboratory, Cambridge, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""264"",""280"",""When a parallel algorithm is written naturally, the resulting program often produces tasks of a finer grain than an implementation can exploit efficiently. Two solutions to the granularity problem that combine parallel tasks dynamically at runtime are discussed. The simpler load-based inlining method, in which tasks are combined based on dynamic bad level, is rejected in favor of the safer and more robust lazy task creation method, in which tasks are created only retroactively as processing results become available. The strategies grew out of work on Mul-T, an efficient parallel implementation of Scheme, but could be used with other languages as well. Mul-T implementations of lazy task creation are described for two contrasting machines, and performance statistics that show the method's effectiveness are presented. Lazy task creation is shown to allow efficient execution of naturally expressed algorithms of a substantially finer grain than possible with previous parallel Lisp systems.<>"",""1558-2183"","""",""10.1109/71.86103"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86103"","""",""Programming profession";Costs;Program processors;Parallel algorithms;Concurrent computing;Parallel processing;Computer science;Robustness;Statistics;"Partitioning algorithms"","""",""152"",""3"",""30"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"Limits on interconnection network performance,""A. Agarwal"",""Laboratory for Computer Science, Massachusetts Institute of Technology, Cambridge, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""398"",""412"",""The latency of direct networks is modeled, taking into account both switch and wire delays. A simple closed-form expression for contention in buffered, direct networks is derived and found to agree closely with simulations. The model includes the effects of packet size and communication locality. Network analysis under various constraints and under different workload parameters reveals that performance is highly sensitive to these constraints and workloads. A two-dimensional network is shown to have the lowest latency only when switch delays and network contention are ignored";" three- or four-dimensional networks are favored otherwise. If communication locality exists, two-dimensional networks regain their advantage. Communication locality decreases both the base network latency and the network bandwidth requirements of applications. It is shown that a much larger fraction of the resulting performance improvement arises from the reduction in bandwidth requirements than from the decrease in latency.<>"",""1558-2183"","""",""10.1109/71.97897"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97897"","""",""Multiprocessor interconnection networks";Delay;Switches;Communication switching;Wire;Performance analysis;Bandwidth;Concurrent computing;Multiprocessing systems;"Parallel processing"","""",""362"",""1"",""29"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Optimal parallel initialization algorithms for a class of priority queues,""S. Olariu";" Z. Wen"",""Department of Computer Science, Old Dominion University, Norfolk, VA, USA";" Department of Computer Science, Old Dominion University, Norfolk, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""423"",""429"",""An adaptive parallel algorithm for inducing a priority queue structure on an n-element array is presented. The algorithm is extended to provide optimal parallel construction algorithms for three other heap-like structures useful in implementing double-ended priority queues, namely min-max heaps, deeps, and min-max-pair heaps. It is shown that an n-element array can be made into a heap, a deap, a min-max heap, or a min-max-pair heap in O(log n+(n/p)) time using no more than n/log n processors, in the exclusive-read-exclusive-write parallel random-access machine model.<>"",""1558-2183"","""",""10.1109/71.97899"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97899"","""",""Binary trees";Parallel algorithms;Adaptive arrays;Fault tolerance;Data structures;Software engineering;Sorting;Operating systems;Computer science;"Signal processing"","""",""17"","""",""25"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Orthogonal graphs for the construction of a class of interconnection networks,""I. D. Scherson"",""Department of Information and Computer Science, University of California, Irvine, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""3"",""19"",""A graph theoretical representation for a class of interconnection networks is suggested. The idea is based on a definition of orthogonal binary vectors and leads to a construction rule for a class of orthogonal graphs. An orthogonal graph is first defined as a set of 2/sup m/ nodes, which in turn are linked by 2/sup m-n/ edges for every link model defined in an integer set Q*. The degree and diameter of an orthogonal graph are determined in terms of the parameters n, m, and the number of link modes defined in Q*. Routing in orthogonal graphs is shown to reduce to the node covering problem in bipartite graphs. The proposed theory is applied to describe a number of well-known interconnection networks such as the binary m-cube and spanning-bus meshes. Multidimensional access (MDA) memories are also shown as examples of orthogonal shared memory multiprocessing systems. Finally, orthogonal graphs are applied to the construction of multistage interconnection networks. Connectivity and placement rules are given and shown to yield a number of well-known networks.<>"",""1558-2183"","""",""10.1109/71.80185"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80185"","""",""Multiprocessor interconnection networks";Multidimensional systems;Hypercubes;Routing;Multiprocessing systems;Bipartite graph;Computer architecture;Tree graphs;Image processing;"Concurrent computing"","""",""31"","""",""52"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Parallel implementation of multiple model tracking algorithms,""A. Averbuch"; S. Itzikowitz;" T. Kapon"",""School of Mathematical Sciences, Tel-Aviv University, Tel-Aviv, Israel"; School of Mathematical Sciences, Tel-Aviv University, Tel-Aviv, Israel;" School of Mathematical Sciences, Tel-Aviv University, Tel-Aviv, Israel"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""242"",""252"",""The implementations of the Viterbi algorithm (VA) and the interacting multiple model (IMM) algorithm on a shared-bus and shared-memory multiple-input multiple-data (MIMD) multiprocessor are discussed. The computational complexity as well as the speedup and efficiency are examined in detail. It is shown that the computational complexity of the parallel implementation of these algorithms is about the same in both memory space and processing time categories. Efficiency with P processors is about 1-1/P for small P and is expected to be relatively high for large P, especially when many filters and large state and measurement vectors are considered.<>"",""1558-2183"","""",""10.1109/71.89069"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89069"","""",""Target tracking";Radar tracking;Filters;Equations;Noise measurement;Time measurement;Viterbi algorithm;Computational complexity;White noise;"Bayesian methods"","""",""13"","""",""19"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"Parallel simulated annealing using speculative computation,""E. E. Witte"; R. D. Chamberlain;" M. A. Franklin"",""Computer and Communications Research Center, Washington University, Saint Louis, MO, USA"; Computer and Communications Research Center, Washington University, Saint Louis, MO, USA;" Computer and Communications Research Center, Washington University, Saint Louis, MO, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""483"",""494"",""A parallel simulated annealing algorithm that is problem-independent, maintains the serial decision sequence, and obtains speedup which can exceed log/sub 2/P on P processors is discussed. The algorithm achieves parallelism by using the concurrency technique of speculative computation. Implementation of the parallel algorithm on a hypercube multiprocessor and application to a task assignment problem are described. The simulated annealing solutions are shown to be, on average, 28% better than the solutions produced by a random task assignment algorithm and 2% better than the solutions produced by a heuristic.<>"",""1558-2183"","""",""10.1109/71.97904"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97904"","""",""Computational modeling";Simulated annealing;Concurrent computing;Parallel algorithms;Cost function;Hypercubes;Parallel processing;Temperature distribution;Temperature control;"Data structures"","""",""82"",""1"",""18"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"Partitioning and mapping nested loops on multiprocessor systems,""Jang-Ping Sheu";" Tsu-Huei Tai"",""Department of Electrical Engineering, National Central University, Chungli, Taiwan";" Department of Electrical Engineering, National Central University, Chungli, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""430"",""439"",""A method for executing nested loops with constant loop-carried dependencies in parallel on message-passing multiprocessor systems to reduce communication overhead is presented. In the partitioning phase, the nested loop is divided into blocks that reduce the interblock communication, without regard to the machine topology. The execution ordering of the iterations is defined by a given time function based on L. Lamport's (1974) hyperplane method. The iterations are then partitioned into blocks so that the execution ordering is not disturbed, and the amount of interblock communication is minimized. In the mapping phase, the partitioned blocks are mapped onto a fixed-size multiprocessor system in such a manner that the blocks that have to exchange data frequently are allocated to the same processor or neighboring processors. A heuristic mapping algorithm for hypercube machines is proposed.<>"",""1558-2183"","""",""10.1109/71.97900"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97900"","""",""Multiprocessing systems";Partitioning algorithms;Parallel processing;Topology;Concurrent computing;Hypercubes;Systolic arrays;Magnetic heads;High performance computing;"Heuristic algorithms"","""",""30"","""",""20"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Performance of shared memory in a parallel computer,""K. Donovan"",""Department of Computer Science, New York University, New York, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""253"",""256"",""A method for analyzing the lengths of memory queues when the network is conflict-free is described. An algorithm based on this method is shown to efficiently determine the upper and lower bounds of the queue length. Analysis indicates that the strategy of using hashing to spread data across memory modules is a good one. Results show that if the size of the system is increased while maintaining a constant ratio of numbers of processors to memories, then, asymptotically, the slowdown in performance from conflicts at the memory modules is Theta (log m/log log m). For m and n less than 100000 and lambda between 0.25 and 4.0, the graphical data confirm this growth rate.<>"",""1558-2183"","""",""10.1109/71.89070"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89070"","""",""Concurrent computing";Probability distribution;Computer science;Application software;Computer networks;Hardware;Differential equations;"Queueing analysis"","""",""1"","""",""3"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Performance of synchronous parallel algorithms with regular structures,""S. Madala";" J. B. Sinclair"",""Coherent Systems, Houston, TX, USA";" Department of Electrical and Computer Engineering, Computer and Information Technology Institute, Rice University, Houston, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""105"",""116"",""New methods are presented for bounding and approximating the mean execution time of partitioning algorithm, and these methods are compared to previous approaches. Distribution-driven and program-driven simulations show that two of the methods are usually accurate to within 10% and give good estimates even when certain independence assumptions are violated. Asymptotic approximations and upper bounds are derived for the average execution time of multiphase algorithms when there is no contention for processes in the parallel phase. In addition, the authors bound the average execution time under static and dynamic scheduling policies and determine the optimum number of parallel tasks to be created to minimize the execution time bounds with constant scheduling overhead.<>"",""1558-2183"","""",""10.1109/71.80193"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80193"","""",""Parallel algorithms";Partitioning algorithms;Concurrent computing;Performance analysis;Dynamic scheduling;Delay effects;Upper bound;Processor scheduling;Parallel processing;"Communication channels"","""",""60"","""",""11"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Properties and performance of folded hypercubes,""A. El-Amawy";" S. Latifi"",""Department of Electrical and Computer Engineering, Louisiana State University, Baton Rouge, LA, USA";" Department of Electrical Engineering, University of Nevada, Las Vegas, NV, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""31"",""42"",""A new hypercube-type structure, the folded hypercube (FHC), which is basically a standard hypercube with some extra links established between its nodes, is proposed and analyzed. The hardware overhead is almost 1/n, n being the dimensionality of the hypercube, which is negligible for large n. For this new design, optimal routing algorithms are developed and proven to be remarkably more efficient than those of the conventional n-cube. For one-to-one communication, each node can reach any other node in the network in at most (n/2) hops (each hop corresponds to the traversal of a single link), as opposed to n hops in the standard hypercube. One-to-all communication (broadcasting) can also be performed in only (n/2) steps, yielding a 50% improvement in broadcasting time over that of the standard hypercube. All routing algorithms are simple and easy to implement. Correctness proofs for the algorithms are given. For the proposed architecture, communication parameters such as average distance, message traffic density, and communication time delay are derived. In addition, some fault tolerance capabilities of this architecture are quantified and compared to those of the standard cube. It is shown that this structure offers substantial improvement over existing hypercube-type networks in terms of the above-mentioned network parameters.<>"",""1558-2183"","""",""10.1109/71.80187"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80187"","""",""Hypercubes";Hardware;Computer networks;Fault tolerance;Communication standards;Broadcasting;Routing;Distributed computing;Algorithm design and analysis;"Telecommunication traffic"","""",""326"",""1"",""22"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Removal of redundant dependences in DOACROSS loops with constant dependences,""V. P. Krothapalli";" P. Sadayappan"",""Department of Computer Science, University of Wisconsin Oshkosh, Oshkosh, WI, USA";" Department of Computer and Information Science, Ohio State Uinversity, Columbus, OH, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""281"",""289"",""An efficient algorithm to remove redundant dependences in simple loops with constant dependences is presented. Dependences constrain the parallel execution of programs and are typically enforced by synchronization instructions. The synchronization instructions represent a significant part of the overhead in the parallel execution of a program. Some program dependences are redundant because they are covered by other dependences. It is shown that unlike with single loops, in the case of nested loops, a particular dependence may be redundant at some iterations but not redundant at others, so that the redundancy of a dependence may not be uniform over the entire iteration space. A sufficient condition for the uniformity of redundancy in a doubly nested loop is developed.<>"",""1558-2183"","""",""10.1109/71.86104"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86104"","""",""Sufficient conditions";Message passing;Parallel processing;Computer science;"Integrated circuit testing"","""",""14"",""1"",""21"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Specifying graceful degradation,""M. P. Herlihy";" J. M. Wing"",""Cambridge Research Laboratory, Digital Equipment Corporation, Cambridge, MA, USA";" School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""1"",""93"",""104"",""A description is given of the relaxation lattice method, a new approach to specifying graceful degradation for a large class of programs. A relaxation lattice is a lattice of specifications parameterized by a set of constraints, where the stronger the set of constraints, the more restrictive the specification. While a program is able to satisfy its strongest set of constraints, it satisfies its preferred specification, but if changes to the environment force it to satisfy a weaker set, then it will permit additional weakly consistent computations which are undesired but tolerated. The use of relaxation lattices is illustrated by specifications for programs that tolerate (1) faults, such as site crashes and network partitions, (2) timing anomalies, such as attempting to read a value too soon after it was written, (3) synchronization conflicts, such as choosing the oldest unlocked item from a queue, and (4) security breaches, such as acquiring unauthorized capabilities.<>"",""1558-2183"","""",""10.1109/71.80192"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=80192"","""",""Degradation";Lattices;Displays;Timing;Security;Distributed computing;Computer crashes;Concurrent computing;Fault tolerance;"Formal specifications"","""",""31"","""",""31"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Synthesizing nested loop algorithms using nonlinear transformation method,""J. . -P. Sheu";" C. . -Y. Chang"",""Department of Electrical Engineering, National Central University, Chungli, Taiwan";" Department of Electrical Engineering, National Central University, Chungli, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""304"",""317"",""FOR-loops are the main source of parallelism in programs. A nonlinear transformation algorithm for parallelizing the execution of FOR-loop models is proposed. It is shown that by the mapping of nonlinear transformation, iterations of FOR-loops can be executed in a parallel form. The algorithm is useful in exploiting the parallelism of FOR-loops with one or more partitions on the innermost loop. Algorithms to partition and map the nested FOR-loops onto fixed size systolic arrays are discussed. Based on the time and space mapping schemes, all the iterations of FOR-loops can be correctly executed on the array processors in a parallel form.<>"",""1558-2183"","""",""10.1109/71.86106"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86106"","""",""Partitioning algorithms";Parallel processing;Systolic arrays;Very large scale integration;Algorithm design and analysis;Matrix decomposition;Discrete Fourier transforms;Data analysis;High level languages;"Councils"","""",""7"","""",""16"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"The effect of scheduling discipline on spin overhead in shared memory parallel systems,""J. Zahorjan"; E. D. Lazowska;" D. L. Eager"",""Department of Computer Science and Engineering, University of Washington, Seattle, WA, USA"; Department of Computer Science and Engineering, University of Washington, Seattle, WA, USA;" Department of Computational Science, University of Saskatchewan, Saskatoon, SAS, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""180"",""198"",""Spinning, or busy waiting, is commonly employed in parallel processors when threads of execution must wait for some event, such as synchronization with another thread. Because spinning is purely overhead, it is detrimental to both user response time and system throughput. The effects of two environmental factors, multiprogramming and data-dependent execution times, on spinning are discussed, and it is shown how the choice of scheduling discipline can be used to reduce the amount of spinning in each case.<>"",""1558-2183"","""",""10.1109/71.89064"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89064"","""",""Spinning";Application software;Delay;Processor scheduling;Environmental factors;Hardware;Parallel processing;Marine technology;"Throughput"","""",""44"",""6"",""18"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"The I test: an improved dependence test for automatic parallelization and vectorization,""X. Kong"; D. Klappholz;" K. Psarris"",""Sun Microsystems, Inc., Mountain View, CA, USA"; Department of Electrical Engineering and Computer Science, Stevens Institute of Technology, Hoboken, NJ, USA;" Department of Electrical Engineering and Computer Science, Stevens Institute of Technology, Hoboken, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""3"",""342"",""349"",""The I test is a subscript dependence test which extends both the range of applicability and the accuracy of the GCD and Banerjee tests (U. Banerjee, 1976), standard subscript dependence tests used to determine whether loops may be parallelized/vectorized. It is shown that the I test is useful when, in the event that a positive result must be reported, a definitive positive is of more use than a tentative positive and when insufficient loop iterations are known for the Banerjee test to apply.<>"",""1558-2183"","""",""10.1109/71.86109"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=86109"","""",""Automatic testing";Equations;System testing;Data analysis;Helium;Sun;Computer science;Costs;"Performance evaluation"","""",""65"","""",""14"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Uniform approach for solving some classical problems on a linear array,""D. R. O'Hallaron"",""School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""2"",""236"",""241"",""It is shown that a number of classical problems from linear algebra and graph theory, including instances of the algebraic path problem, matrix multiplication, matrix triangularization, and matrix transpose, can be solved using the same basic recurrence. A simple mapping of the recurrence onto a unidirectional linear array is discussed. Qualitative advantages to programming linear arrays using this approach include uniformity of design, simplicity of programming, and scalability to larger problems. The major disadvantage is that the resulting algorithms are not necessarily optimal.<>"",""1558-2183"","""",""10.1109/71.89068"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=89068"","""",""Linear algebra";Graph theory;Matrices;Parallel algorithms;Linear programming;Scalability;Ear;Process design;Algorithm design and analysis;"Research and development"","""",""16"","""",""18"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Using the dual path property of omega networks to obtain conflict-free message routing,""P. J. Bernhard";" D. J. Rosenkrantz"",""Department of Computer Science, Clemson University, Clemson, SC, USA";" Department of Computer Science, State University of New York, Albany, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2002"",""1991"",""2"",""4"",""503"",""507"",""A strategy for dealing with communication conflicts that occur in omega networks is presented. The strategy operates by implementing the dual path property of omega networks, which allows the source and destination processors to reverse roles for some of the messages that are being transmitted. For certain message patterns, such a reversal produces a modified message pattern for which the network routes are disjoint. For a circuit switching mode in which the network links and switches are bidirectional, the disjoint set of routes for modified message pattern can be used to achieve conflict-free message transmission for the original message pattern. This strategy is investigated, and an efficient algorithm to determine whether it can be successfully applied to a given message pattern is presented.<>"",""1558-2183"","""",""10.1109/71.97906"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=97906"","""",""Routing";Switches;Switching circuits;Multiprocessor interconnection networks;Computer science;Multiprocessing systems;Added delay;Polynomials;"Wire"","""",""4"",""2"",""14"",""IEEE"",""6 Aug 2002"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;