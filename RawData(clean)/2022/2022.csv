"$run$ runData: Re-Distributing Data via Piggybacking for Geo-Distributed Data Analytics Over Edges,""Y. Jin"; Z. Qian; S. Guo; S. Zhang; L. Jiao;" S. Lu"",""Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA;" Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Jun 2021"",""2022"",""33"",""1"",""40"",""55"",""Efficiently analyzing geo-distributed datasets is emerging as a major demand in a cloud-edge system. Since the datasets are often generated in closer proximity to end users, traditional works mainly focus on offloading proper tasks from those hotspot edges to the datacenter to decrease the overall completion time of submitted jobs in a one-shot manner. However, optimizing the completion time of current job alone is insufficient in a long-term scope since some datasets would be used multiple times. Instead, optimizing the data distribution is much more efficient and could directly benefit forthcoming jobs, although it may postpone the execution of current one. Unfortunately, due to the throwaway feature of data fetcher, existing data analytics systems fail to re-distribute corresponding data out of hotspot edges after the execution of data analytics. In order to minimize the overall completion time for a sequence of jobs as well as to guarantee the performance of current one, we propose to re-distribute the data along with task offloading, and formulate corresponding ε-bounded data-driven task scheduling problem over wide area network under the consideration of edge heterogeneity. We design an online schema run Data, which offloads proper tasks and related data via piggybacking to the datacenter based on delicately calculated probabilities. Through rigorous theoretical analysis, run Data is proved concentrated on its optimum with high probability. We implement run Data based on Spark and HDFS. Both testbed results and trace-driven simulations show that run Data re-distributes proper data via piggybacking and achieves up to 37 percent reduction on average response time compared with state-of-the-art schemas."",""1558-2183"","""",""10.1109/TPDS.2021.3086274"",""National Key Research and Development Program of China(grant numbers:2017YFB1001801)"; National Science Foundation of China(grant numbers:61832005,61872175); Ripple Faculty Fellowship; Natural Science Foundation of Jiangsu Province(grant numbers:BK20181252); Collaborative Innovation Center of Novel Software Technology and Industrialization; Nanjing University Innovation and Creative Program(grant numbers:CXCY19-25); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19,R5034-18); General Research Fund(grant numbers:152221/19E,15220320/20E); Collaborative Research Fund(grant numbers:C5026-18G); National Natural Science Foundation of China(grant numbers:61872310); Shenzhen Science and Technology Innovation Commission(grant numbers:R2020A045);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446574"",""Cloud-edge system";data re-distribution;heterogeneity;"online schema"",""Task analysis";Data analysis;Wide area networks;Data models;Servers;Optimization;"Videos"","""",""1"","""",""67"",""IEEE"",""3 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"$TC-Stream$TC-Stream: Large-Scale Graph Triangle Counting on a Single Machine Using GPUs,""J. Huang"; H. Wang; X. Fei; X. Wang;" W. Chen"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Technology and Appications, Qinghai University, Qinghai, China;" Department of Information Technology Center, Qinghai University, Qinghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3067"",""3078"",""In this paper, we build a $TC$TC-$Stream$Stream, a high-performance graph processing system specific for a triangle counting algorithm on graph data with up to tens of billions of edges, which significantly exceeds the device memory capacity of Graphics Processing Units (GPUs). The triangle counting problem is a broad research topic in data mining and social network analysis in the graph processing field. As the scale of the graph data grows, a portion of the graph data must be loaded iteratively. In the existing literature, graphs with billions of edges need to be done distributively, which is cost-intensive. Also, many disk-based triangle counting systems are proposed for CPU architectures, but their tackling performances are inefficient. To solve the above problem, we propose $TC$TC-$Stream$Stream, and it focuses on three issues: 1) For power-law graphs, because the amount of tasks of each vertex or edge is inconsistent, it is bound to cause different demands of computing and memory resources for different task types. We propose a parallel vertex approach and the reordering of vertices for graph data that can be placed in the GPU device memory to ensure the maximum workload balancing"; 2) A binary-search-based set intersection method is designed to achieve the maximum parallelism in GPU;" 3) For the graph data that exceeds the GPU device memory capacity, we develop a novel vertical partition algorithm to guarantee the independent computing on each partition so that the three computation processes, i.e., the computation on GPU, the data transmission between main memory of CPU and SSD, and the communication between the CPU and the GPU can be perfectly overlapped. Moreover, the $TC$TC-$Stream$Stream optimizes edge-iterator models and benefits from multi-thread parallelism. Extensive experiments conducted on large-scale datasets showed that the $TC$TC-$stream$stream running on a single Tesla V100 GPU performs $2.4-6\times$2.4-6× and $1.8-4.4\times$1.8-4.4× faster than the state-of-the-art single-machine in-memory triangle counting system and GPU-based triangle counting system, respectively, and achieves $2.4\times$2.4× faster than the state-of-the-art out-of-core distributed system PDTL running on an 8-node cluster when processing the graph data with 42.5 billion edges, which demonstrates the high performance and cost-effectiveness of the $TC$TC-$Stream$Stream."",""1558-2183"","""",""10.1109/TPDS.2021.3135329"",""National Natural Science Foundation of China(grant numbers:62062059,62162053,61762074)"; National Natural Science Foundation of Qinghai Province(grant numbers:2019-ZJ-7034); Qinghai University(grant numbers:2020-ZZ-03);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650723"",""Triangle counting";vertical partition;out-of-core;GPU;"parallel processing"",""Graphics processing units";Instruction sets;Computer architecture;Parallel processing;Task analysis;Partitioning algorithms;"Data mining"","""",""4"","""",""73"",""IEEE"",""14 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"$TC-Stream$TC-Stream: Large-Scale Graph Triangle Counting on a Single Machine Using GPUs,""J. Huang"; H. Wang; X. Fei; X. Wang;" W. Chen"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Technology and Appications, Qinghai University, Qinghai, China;" Department of Information Technology Center, Qinghai University, Qinghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3067"",""3078"",""In this paper, we build a $TC$TC-$Stream$Stream, a high-performance graph processing system specific for a triangle counting algorithm on graph data with up to tens of billions of edges, which significantly exceeds the device memory capacity of Graphics Processing Units (GPUs). The triangle counting problem is a broad research topic in data mining and social network analysis in the graph processing field. As the scale of the graph data grows, a portion of the graph data must be loaded iteratively. In the existing literature, graphs with billions of edges need to be done distributively, which is cost-intensive. Also, many disk-based triangle counting systems are proposed for CPU architectures, but their tackling performances are inefficient. To solve the above problem, we propose $TC$TC-$Stream$Stream, and it focuses on three issues: 1) For power-law graphs, because the amount of tasks of each vertex or edge is inconsistent, it is bound to cause different demands of computing and memory resources for different task types. We propose a parallel vertex approach and the reordering of vertices for graph data that can be placed in the GPU device memory to ensure the maximum workload balancing"; 2) A binary-search-based set intersection method is designed to achieve the maximum parallelism in GPU;" 3) For the graph data that exceeds the GPU device memory capacity, we develop a novel vertical partition algorithm to guarantee the independent computing on each partition so that the three computation processes, i.e., the computation on GPU, the data transmission between main memory of CPU and SSD, and the communication between the CPU and the GPU can be perfectly overlapped. Moreover, the $TC$TC-$Stream$Stream optimizes edge-iterator models and benefits from multi-thread parallelism. Extensive experiments conducted on large-scale datasets showed that the $TC$TC-$stream$stream running on a single Tesla V100 GPU performs $2.4-6\times$2.4-6× and $1.8-4.4\times$1.8-4.4× faster than the state-of-the-art single-machine in-memory triangle counting system and GPU-based triangle counting system, respectively, and achieves $2.4\times$2.4× faster than the state-of-the-art out-of-core distributed system PDTL running on an 8-node cluster when processing the graph data with 42.5 billion edges, which demonstrates the high performance and cost-effectiveness of the $TC$TC-$Stream$Stream."",""1558-2183"","""",""10.1109/TPDS.2021.3135329"",""National Natural Science Foundation of China(grant numbers:62062059,62162053,61762074)"; National Natural Science Foundation of Qinghai Province(grant numbers:2019-ZJ-7034); Qinghai University(grant numbers:2020-ZZ-03);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650723"",""Triangle counting";vertical partition;out-of-core;GPU;"parallel processing"",""Graphics processing units";Instruction sets;Computer architecture;Parallel processing;Task analysis;Partitioning algorithms;"Data mining"","""",""4"","""",""73"",""IEEE"",""14 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Bayesian Approach To Distributed Anomaly Detection In Edge AI Networks,""M. Odiathevar"; W. K. G. Seah;" M. Frean"",""School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand"; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand;" School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2022"",""2022"",""33"",""12"",""3306"",""3320"",""The challenge of anomaly detection is to obtain an accurate understanding of expected behaviour which is intensified when the data are distributed heterogeneously. Transmitting raw data to a central site incurs high communication overhead and raises privacy issues. The concept of Edge AI allows computation to be performed at the edge site allowing for quick decision making in mission critical scenarios such as self-driving cars. A model is learnt locally and its parameters are transmitted and aggregated. However, existing methods of aggregation do not account for variance and heterogeneous distribution of data. They also do not consider edge constraints such as limited computational, memory and communication capabilities of edge devices. In this work, a fully Bayesian approach is employed by means of a Bayesian Random Vector Functional Link AutoEncoder being incorporated with Expectation Propagation for distributed training. Our anomaly detection system operates without any transmission of raw data, is robust under inhomogeneous network densities and under uneven and biased data distributions. It allows for asynchronous updates to converge in a few iterations and is a relatively simple neural network addressing edge constraints without compromising on performance as compared to existing more complex models."",""1558-2183"","""",""10.1109/TPDS.2022.3151853"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714821"",""Anomaly detection";Bayesian;random vector functional link;expectation propagation;edge AI;"single layer feed-forward neural network"",""Bayes methods";Anomaly detection;Training;Image edge detection;Distributed databases;Data models;"Computational modeling"","""",""5"","""",""61"",""IEEE"",""16 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Bi-Objective Learn-and-Deploy Scheduling Method for Bursty and Stochastic Requests on Heterogeneous Cloud Servers,""X. Cai"; H. Xu; X. Li; K. Wang; L. Chen; R. R. García;" Q. Zhang"",""Key Laboratory of Intelligent Control and Optimization for Industrial Equipment of Ministry of Education (MOE), School of Control Science and Engineering, Dalian University of Technology, Dalian, Liaoning, China"; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Southeast University, Nanjing, Jiangsu, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Southeast University, Nanjing, Jiangsu, China; Grupo de Sistemas de Optimización Aplicada, Universitat Politècnica de València, València, Spain;" Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4547"",""4562"",""In this article, we consider the dynamic allocation of bursty requests stochastically arriving at heterogeneous servers with uncertain setup times. Lower expected response time and less power consumption are desirable objectives of users and service providers respectively. However, sudden increase and decrease of cloud servers caused by bursty requests are rather challenging to get an appropriate trade-off between the two conflicting objectives which are closely related to the launched servers. The heterogeneity of the cloud servers further makes it more difficult to decide how to switch on and off servers and effectively and efficiently allocate bursty requests with balanced objectives. Based on a Markov decision process, a real-time bilevel decision-making model is constructed for unallocated requests which includes: whether to launch a server and which type of server to launch. A learn-and-deploy algorithm framework is proposed which contains two complementary stages. In the first stage, an effective offline bi-objective optimization algorithm is proposed to learn a set of policies, which provides helpful trade-off information for a decision-maker to choose a preferred policy a posteriori. In terms of the system status, a policy decides whether to launch a server according to a state-action table and which server to launch using a server priority sequence. In the second stage, a computationally efficient policy deployment method is proposed to search the corresponding action in the selected policy based on the current system status and apply it to the real-time system. Experimental studies over a large number of random and real instances have been conducted to validate the effectiveness of the proposed bilevel model and algorithm. Compared to the most recent existing method, the performance of the proposed approach can at most achieve an 80% improvement on power consumption and 20% improvement on response time."",""1558-2183"","""",""10.1109/TPDS.2022.3196475"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101200003)"; National Natural Science Foundation of China(grant numbers:62072234,61732006,61832004); Natural Science Foundation of Jiangsu Province of China(grant numbers:BK20181288); China Postdoctoral Science Foundation(grant numbers:2015M571751); Collaborative Innovation Center of Wireless Communications Technology; Spanish Ministry of Science, Innovation, and Universities(grant numbers:RTI2018-094940-B-I00);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857657"",""Cloud computing";bursty and stochastic requests;heterogeneous servers;multiobjective optimization;learn-and-deploy;Markov decision process;"dynamic programming"",""Servers";Cloud computing;Power demand;Optimization;Time factors;Analytical models;"Real-time systems"","""","""","""",""38"",""IEEE"",""16 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"A Bifactor Approximation Algorithm for Cloudlet Placement in Edge Computing,""D. Bhatta";" L. Mashayekhy"",""Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA";" Department of Computer and Information Sciences, University of Delaware, Newark, DE, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Dec 2021"",""2022"",""33"",""8"",""1787"",""1798"",""Emerging applications with low-latency requirements such as real-time analytics, immersive media applications, and intelligent virtual assistants have rendered Edge Computing as a critical computing infrastructure. Existing studies have explored the cloudlet placement problem in a homogeneous scenario with different goals such as latency minimization, load balancing, energy efficiency, and placement cost minimization. However, placing cloudlets in a highly heterogeneous deployment scenario considering the next-generation 5G networks and IoT applications is still an open challenge. The novel requirements of these applications indicate that there is still a gap in ensuring low-latency service guarantees when deploying cloudlets. Furthermore, deploying cloudlets in a cost-effective manner and ensuring full coverage for all users in edge computing are other critical conflicting issues. In this article, we address these issues by designing a bifactor approximation algorithm to solve the heterogeneous cloudlet placement problem to guarantee a bounded latency and placement cost, while fully mapping user applications to appropriate cloudlets. We first formulate the problem as a multi-objective integer programming model and show that it is a computationally NP-hard problem. We then propose a bifactor approximation algorithm, ACP, to tackle its intractability. We investigate the effectiveness of ACP by performing extensive theoretical analysis and experiments on multiple deployment scenarios based on New York City OpenData. We prove that ACP provides a (2,4)-approximation ratio for the latency and the placement cost. The experimental results show that ACP obtains near-optimal results in a polynomial running time making it suitable for both short-term and long-term cloudlet placement in heterogeneous deployment scenarios."",""1558-2183"","""",""10.1109/TPDS.2021.3126256"",""National Science Foundation(grant numbers:CNS-1755913)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609538"",""Edge computing";cloudlets;placement cost;latency;full coverage;"approximation algorithm"",""Cloud computing";Costs;Approximation algorithms;Edge computing;Servers;Low latency communication;"Internet of Things"","""",""13"","""",""44"",""IEEE"",""9 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Block-Based Triangle Counting Algorithm on Heterogeneous Environments,""A. Yaşar"; S. Rajamanickam; J. W. Berry;" Ü. V. Çatalyürek"",""School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA"; Center for Computing Research, Sandia National Laboratories, Albuquerque, USA; Center for Computing Research, Sandia National Laboratories, Albuquerque, USA;" School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2021"",""2022"",""33"",""2"",""444"",""458"",""Triangle counting is a fundamental building block in graph algorithms. In this article, we propose a block-based triangle counting algorithm to reduce data movement during both sequential and parallel execution. Our block-based formulation makes the algorithm naturally suitable for heterogeneous architectures. The problem of partitioning the adjacency matrix of a graph is well-studied. Our task decomposition goes one step further: it partitions the set of triangles in the graph. By streaming these small tasks to compute resources, we can solve problems that do not fit on a device. We demonstrate the effectiveness of our approach by providing an implementation on a compute node with multiple sockets, cores and GPUs. The current state-of-the-art in triangle enumeration processes the Friendster graph in 2.1 seconds, not including data copy time between CPU and GPU. Using that metric, our approach is 20 percent faster. When copy times are included, our algorithm takes 3.2 seconds. This is 5.6 times faster than the fastest published CPU-only time."",""1558-2183"","""",""10.1109/TPDS.2021.3093240"",""National Science Foundation(grant numbers:CCF-1919021)"; Sandia National Laboratories;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468328"",""Triangle counting";task-based;block-based;sub-graph;multi-core;"multi-GPU"",""Task analysis";Partitioning algorithms;Heuristic algorithms;Kernel;Hardware;Graphics processing units;"Ear"","""",""1"","""",""48"",""IEEE"",""29 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Decentralized Federated Learning Framework via Committee Mechanism With Convergence Guarantee,""C. Che"; X. Li; C. Chen; X. He;" Z. Zheng"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China"; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, Guangdong, China;" School of Software Engineering, Sun Yat-sen University, Zhuhai, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Sep 2022"",""2022"",""33"",""12"",""4783"",""4800"",""Federated learning allows multiple participants to collaboratively train an efficient model without exposing data privacy. However, this distributed machine learning training method is prone to attacks from Byzantine clients, which interfere with the training of the global model by modifying the model or uploading the false gradient. In this article, we propose a novel serverless federated learning framework Committee Mechanism based Federated Learning (CMFL), which can ensure the robustness of the algorithm with convergence guarantee. In CMFL, a committee system is set up to screen the uploaded local gradients. The committee system selects the local gradients rated by the elected members for the aggregation procedure through the selection strategy, and replaces the committee member through the election strategy. Based on the different considerations of model performance and defense, two opposite selection strategies are designed for the sake of both accuracy and robustness. Extensive experiments illustrate that CMFL achieves faster convergence and better accuracy than the typical Federated Learning, in the meanwhile obtaining better robustness than the traditional Byzantine-tolerant algorithms, in the manner of a decentralized approach. In addition, we theoretically analyze and prove the convergence of CMFL under different election and selection strategies, which coincides with the experimental results."",""1558-2183"","""",""10.1109/TPDS.2022.3202887"",""National Key R&D Program of China(grant numbers:2020YFB1006001)"; National Natural Science Foundation of China(grant numbers:62176269,62006252); Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010165003); Innovative Research Foundation of Ship General Performance(grant numbers:25622112);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870745"",""Decentralized federated learning";committee mechanism;byzantine robustness;"theoretical convergence analysis"",""Servers";Convergence;Training;Hidden Markov models;Robustness;Collaborative work;"Voting"","""",""16"","""",""58"",""IEEE"",""31 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Fast $f(r,k+1)/k$f(r,k+1)/k-Diagnosis for Interconnection Networks Under MM* Model,""Y. Huang"; L. Lin;" S. -Y. Hsieh"",""College of Computer and Cyber Security, Fujian Normal University, Fuzhou, Fujian, China"; College of Computer and Cyber Security, Key Laboratory of Network Security and Cryptology, Fujian Normal University, Fuzhou, Fujian, China;" Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Nov 2021"",""2022"",""33"",""7"",""1593"",""1604"",""Cyberspace is not a “vacuum space”, and it is normal that there are inevitable viruses and worms in cyberspace. Cyberspace security threats stem from the problem of endogenous security, which is caused by the incompleteness of theoretical system and technology of the information field itself. Thus it is impossible and unnecessary for us to build an “aseptic” cyberspace. On the contrast, we must focus on improving the “self-immunity” of network. Literally, endogenous security is an endogenous effect from its own structural factors rather than external ones. The $t/k$t/k-diagnosis strategy plays a very important role in measuring endogenous network security without prior knowledge, which can significantly enhance the self-diagnosing capability of network. As far as we know, few research involves $t/k$t/k-diagnosis algorithm and $t/k$t/k-diagnosability of interconnection networks under MM* model. In this article, we propose a fast $f(r,k+1)/k$f(r,k+1)/k-diagnosis algorithm of complexity $O(Nr^2)$O(Nr2), say $G$GMIS$k$kDIAGMM*, for a general $r$r-regular network $G$G under MM* model by designing a 0-comparison subgraph $M_0(G)$M0(G), where $N$N is the size of $G$G. We determine that the $t/k$t/k-diagnosability $(t(G)/k)^M$(t(G)/k)M of $G$G under MM* model is $f(r,k+1)$f(r,k+1) by $G$GMIS$k$kDIAGMM* algorithm. Moreover, we establish the $(t(G)/k)^M$(t(G)/k)M of some interconnection networks under MM* model, including BC networks, $(n,l)$(n,l)-star graph networks, and data center network DCells. Finally, we compare $(t(G)/k)^M$(t(G)/k)M with diagnosability, conditional diagnosability, pessimistic diagnosability, extra diagnosability, and good-neighbor diagnosability under MM* model. It can be seen that $(t(G)/k)^M$(t(G)/k)M is greater than other fault diagnosabilities in most cases."",""1558-2183"","""",""10.1109/TPDS.2021.3122440"",""National Natural Science Foundation of China(grant numbers:62102088,62171132,U1905211,61773415)"; Fok Ying Tung Education Foundation(grant numbers:171061); Natural Science Foundation of Fujian Province(grant numbers:2021J05228); Fujian University of Technology(grant numbers:GJ-YB-20-06);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585362"",""Endogenous security";fault diagnosis;reliability; $t/k$    t / k     -diagnosability;interconnection networks;"MM* model"",""Security";Program processors;Cyberspace;Hypercubes;Data centers;Multiprocessing systems;"Computational modeling"","""",""3"","""",""37"",""IEEE"",""26 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Global Cost-Aware Container Scheduling Strategy in Cloud Data Centers,""S. Long"; W. Wen; Z. Li; K. Li; R. Yu;" J. Zhu"",""Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent network, Changsha, Hunan, China"; Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent network, Changsha, Hunan, China; Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent network, Changsha, Hunan, China; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; School of Automation, Guangdong University of Technology, Guangzhou, Guangdong, China;" Key Laboratory of Hunan Province for Internet of Things and Information Security, Hunan International Scientific and Technological Cooperation Base of Intelligent network, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2752"",""2766"",""Large-scale Internet applications running on data centers are typically instantiated as a set of containers. Assigning a container to its affinity machine can reduce communication and transport costs while assigning it to the anti-affinity machine may affect the proper operation of the container. Existing container scheduling methods cannot accommodate these two types of requirements. In order to reduce the operation and maintenance cost of data centers, this article focuses on the container instance allocation problem in heterogeneous server cluster, and proposes a global cost-aware scheduling algorithm (GCCS) to solve it. The purpose is to minimize the total power consumption of the cluster from a global perspective, while trying to meet the affinity/anti-affinity requirements of applications. We study the number of containers per server selected by the application, model it as an integer linear program (ILP), and then propose a heuristic search algorithm to repair the relaxation solution of the ILP into a suboptimal feasible solution. In particular, we use Bayesian optimizer to perform a number of automated development and exploration processes for the selection of the cost coefficient. The experiments are carried out with the best cost coefficient recommended by Bayesian optimizer. Finally, the results demonstrate that GCCS can significantly reduce the total power consumption of the cluster, while maintaining a high affinity satisfaction ratio."",""1558-2183"","""",""10.1109/TPDS.2021.3133868"",""National Natural Science Foundation of China(grant numbers:62172350,62032020,62172349,62076214)"; Hunan Province Department of Education(grant numbers:21B0120); Natural Science Foundation of Hunan Province(grant numbers:2021JJ40544); Hunan Science and Technology Planning(grant numbers:2019RS3019); Hunan Provincial Natural Science Foundation of China for Distinguished Young Scholars(grant numbers:2018JJ1025); Hunan Province Science and Technology(grant numbers:2018TP1036);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645367"",""Container scheduling";Bayesian optimization;container-based clouds;power efficiency;"cost optimization"",""Containers";Costs;Cloud computing;Servers;Power demand;Data centers;"Scheduling"","""",""2"","""",""44"",""IEEE"",""10 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"A GPU-Oriented Application Programming Interface for Digital Audio Workstations,""D. Bianchi"; F. Avanzini; A. Baratè; L. A. Ludovico;" G. Presti"",""Computer Science Department, Università degli Studi di Milano, Milano, Italy"; Computer Science Department, Università degli Studi di Milano, Milano, Italy; Computer Science Department, Università degli Studi di Milano, Milano, Italy; Computer Science Department, Università degli Studi di Milano, Milano, Italy;" Computer Science Department, Università degli Studi di Milano, Milano, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Dec 2021"",""2022"",""33"",""8"",""1924"",""1938"",""A Digital Audio Workstation (DAW) is a hardware and/or software device aiming to ease those operations required for music production, such as arranging, recording, editing, mixing, and, more in general, modifying sounds creatively. A peculiarity of a DAW environment is that most of the work is highly parallelizable, since the basic architecture of a DAW consists in the simultaneous processing of different audio tracks, mainly independent from each other. In order to exploit such a feature, this paper proposes an interface that lets the DAW interact with the Graphics Processing Unit (GPU) in a standardized way. Despite some academic research and experimentation, the professional audio software industry almost never exploited GPUs when implementing entire DAWs, but only when realising very specific tools or third party extensions (plugins). This work also presents and discusses the outcomes of a number of tests conducted in order to choose the optimal architecture. As a result, a GPU-based approach turned to be a valid alternative to the use of CPUs in the computation of audio effects, such as the rendering of audio tracks after mixing and mastering operations, both in real time and offline."",""1558-2183"","""",""10.1109/TPDS.2021.3131659"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629334"",""Graphic processing unit (GPU)";audio;digital audio workstation (DAW);"heterogeneous computing"",""Graphics processing units";Task analysis;Real-time systems;Software;Rendering (computer graphics);Parallel processing;"Central Processing Unit"","""",""1"","""",""50"",""IEEE"",""30 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Highly-Available Move Operation for Replicated Trees,""M. Kleppmann"; D. P. Mulligan; V. B. F. Gomes;" A. R. Beresford"",""University of Cambridge, Cambridge, U.K."; Arm Research, Cambridge, U.K.; Google, Mountain View, CA, USA;" University of Cambridge, Cambridge, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""17 Nov 2021"",""2022"",""33"",""7"",""1711"",""1724"",""Replicated tree data structures are a fundamental building block of distributed filesystems, such as Google Drive and Dropbox, and collaborative applications with a JSON or XML data model. These systems need to support a move operation that allows a subtree to be moved to a new location within the tree. However, such a move operation is difficult to implement correctly if different replicas can concurrently perform arbitrary move operations, and we demonstrate bugs in Google Drive and Dropbox that arise with concurrent moves. In this article we present a CRDT algorithm that handles arbitrary concurrent modifications on trees, while ensuring that the tree structure remains valid (in particular, no cycles are introduced), and guaranteeing that all replicas converge towards the same consistent state. Our algorithm requires no synchronous coordination between replicas, making it highly available in the face of network partitions. We formally prove the correctness of our algorithm using the Isabelle/HOL proof assistant, and evaluate the performance of our formally verified implementation in a geo-replicated setting."",""1558-2183"","""",""10.1109/TPDS.2021.3118603"",""Boeing"; Engineering and Physical Sciences Research Council; REMS: Rigorous Engineering for Mainstream Systems” programme(grant numbers:EP/K008528); Leverhulme Trust Early Career Fellowship; Isaac Newton Trust; Nokia Bell Labs;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563274"",""Conflict-free replicated data types (CRDTs)";formal verification;distributed filesystems;"distributed collaboration"",""Internet";Synchronization;Computer bugs;XML;Software;Drives;"Data models"","""",""4"","""",""57"",""IEEE"",""7 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Low-Power Transprecision Floating-Point Cluster for Efficient Near-Sensor Data Analytics,""F. Montagna"; S. Mach; S. Benatti; A. Garofalo; G. Ottavi; L. Benini; D. Rossi;" G. Tagliavini"",""Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, Italy"; Department of Information Technology and Electrical Engineering (D-ITET), ETH Zürich, 8092, Zürich, Switzerland; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, Italy; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, Italy; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, Italy; Department of Information Technology and Electrical Engineering (D-ITET), ETH Zürich, Zürich, Switzerland; Department of Electrical, Electronic, and Information Engineering (DEI), University of Bologna, Bologna, Italy;" Department of Computer Science and Engineering (DISI), University of Bologna, Bologna, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Oct 2021"",""2022"",""33"",""5"",""1038"",""1053"",""Recent applications in low-power (1-20 mW) near-sensor computing require the adoption of floating-point arithmetic to reconcile high precision results with a wide dynamic range. In this article, we propose a low-power multi-core computing cluster that leverages the fined-grained tunable principles of transprecision computing to provide support to near-sensor applications at a minimum power budget. Our solution – based on the open-source RISC-V architecture – combines parallelization and sub-word vectorization with a dedicated interconnect design capable of sharing floating-point units (FPUs) among the cores. On top of this architecture, we provide a full-fledged software stack support, including a parallel low-level runtime, a compilation toolchain, and a high-level programming model, with the aim to support the development of end-to-end applications. We performed an exhaustive exploration of the design space of the transprecision cluster on a cycle-accurate FPGA emulator, varying the number of cores and FPUs to maximize performance. Orthogonally, we performed a vertical exploration to identify the most efficient solutions in terms of non-functional requirements (operating frequency, power, and area). We conducted an experimental assessment on a set of benchmarks representative of the near-sensor processing domain, complementing the timing results with a post place-&-route analysis of the power consumption. A comparison with the state-of-the-art shows that our solution outperforms the competitors in energy efficiency, reaching a peak of 97 Gflop/s/W on single-precision scalars and 162 Gflop/s/W on half-precision vectors. Finally, a real-life use case demonstrates the effectiveness of our approach in fulfilling accuracy constraints."",""1558-2183"","""",""10.1109/TPDS.2021.3101764"",""European Union's Horizon 2020 research and innovation programme(grant numbers:732631 (OPRECOMP),863337 (WiPLASH),857191 (IOTWINS))";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9506919"",""RISC-V";transprecision;parallel computing;sub-word vectorization;FPU interconnect;"near-sensor computing"",""Hardware";Dynamic range;Pipelines;Open area test sites;Optimization;Open source software;"Energy consumption"","""",""7"","""",""54"",""IEEE"",""4 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"A Native Tensor–Vector Multiplication Algorithm for High Performance Computing,""P. J. Martinez-Ferrer"; A. N. Yzelman;" V. Beltran"",""Barcelona Supercomputing Center (BSC), Barcelona, Spain"; Computing Systems Laboratory, Huawei Technologies Switzerland, Zürich, Switzerland;" Barcelona Supercomputing Center (BSC), Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2022"",""2022"",""33"",""12"",""3363"",""3374"",""Tensor computations are important mathematical operations for applications that rely on multidimensional data. The tensor–vector multiplication (TVM) is the most memory-bound tensor contraction in this class of operations. This article proposes an open-source TVM algorithm which is much simpler and efficient than previous approaches, making it suitable for integration in the most popular BLAS libraries available today. Our algorithm has been written from scratch and features unit-stride memory accesses, cache awareness, mode obliviousness, full vectorization and multi-threading as well as NUMA awareness for non-hierarchically stored dense tensors. Numerical experiments are carried out on tensors up to order 10 and various compilers and hardware architectures equipped with traditional DDR and high bandwidth memory (HBM). For large tensors the average performance of the TVM ranges between 62% and 76% of the theoretical bandwidth for NUMA systems with DDR memory and remains independent of the contraction mode. On NUMA systems with HBM the TVM exhibits some mode dependency but manages to reach performance figures close to peak values. Finally, the higher-order power method is benchmarked with the proposed TVM kernel and delivers on average between 58% and 69% of the theoretical bandwidth for large tensors."",""1558-2183"","""",""10.1109/TPDS.2022.3153113"",""HPC Technology Innovation Lab"; Barcelona Supercomputing Center; Huawei Research Cooperation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720217"",""Parallel algorithms";shared memory;tensor computations;high bandwidth memory;"NUMA"",""Tensors";Kernel;Libraries;Bandwidth;Virtual machine monitors;Layout;"Benchmark testing"","""","""","""",""19"",""IEEE"",""23 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Parallel Algorithm Template for Updating Single-Source Shortest Paths in Large-Scale Dynamic Networks,""A. Khanda"; S. Srinivasan; S. Bhowmick; B. Norris;" S. K. Das"",""Department of Computer Science, Missouri University of Science and Technology, Rolla, MO, USA"; Department of Radiation Oncology, Virginia Commonwealth University, Richmond, VA, USA; Department of Computer Science and Engineering, University of North Texas, Denton, TX, USA; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA;" Department of Computer Science, Missouri University of Science and Technology, Rolla, MO, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""929"",""940"",""The Single Source Shortest Path (SSSP) problem is a classic graph theory problem that arises frequently in various practical scenarios";" hence, many parallel algorithms have been developed to solve it. However, these algorithms operate on static graphs, whereas many real-world problems are best modeled as dynamic networks, where the structure of the network changes with time. This gap between the dynamic graph modeling and the assumed static graph model in the conventional SSSP algorithms motivates this work. We present a novel parallel algorithmic framework for updating the SSSP in large-scale dynamic networks and implement it on the shared-memory and GPU platforms. The basic idea is to identify the portion of the network affected by the changes and update the information in a rooted tree data structure that stores the edges of the network that are most relevant to the analysis. Extensive experimental evaluations on real-world and synthetic networks demonstrate that our proposed parallel updating algorithm is scalable and, in most cases, requires significantly less execution time than the state-of-the-art recomputing-from-scratch algorithms."",""1558-2183"","""",""10.1109/TPDS.2021.3084096"",""National Science Foundation(grant numbers:1725755,1725566,1725585)"; National Science Foundation(grant numbers:1919789);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442368"",""Dynamic networks";single source shortest path (SSSP);shared-memory parallel algorithm;"GPU implementation"",""Heuristic algorithms";Graphics processing units;Parallel algorithms;Synchronization;Multicore processing;Complexity theory;"Wireless sensor networks"","""",""7"","""",""25"",""IEEE"",""26 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Parallel Secure Flow Control Framework for Private Data Sharing in Mobile Edge Cloud,""Q. Huang"; L. Chen;" C. Wang"",""School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China"; School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China;" School of Cyberspace Security, Beijing University of Posts and Telecommunications, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Sep 2022"",""2022"",""33"",""12"",""4638"",""4653"",""Nowadays, the rapid development of edge computing is accelerating the data sharing between cloud computing platforms and mobile users. These data often contain sensitive information, which faces severe leakage risks not only from the semi-trusted cloud servers but also from the malicious senders in the organizations. Fortunately, access control encryption (ACE) has been utilized to secure the data with access control policies, in which a sanitizer (e.g., the edge node) is employed to check all the communications between the sender and receiver, and drop illegal ciphertexts according to the access control policy. However, previous schemes have some limitations in mobile edge cloud, e.g., the sender's attributes are not strictly authenticated in the attribute-based access control policy, or the sanitization time is the bottleneck of fast data sharing. To this end, we introduce PSFlow, a parallel secure flow control framework for private data sharing in mobile edge cloud. First, we propose an attribute-based outsourced ACE (AOACE) scheme, which achieves secure fine-grained data read and write control, and reduces the computational cost of the sender and receiver with outsourced computations in edge nodes. Then, we propose a concrete construction of PSFlow from AOACE, and accelerate the sanitization process with parallel computing. Specifically, PSFlow parallelizes the sanitization operations with a multi-server model in each edge node, and optimizes the sanitization efficiency in each edge server by constructing a shared pool from the attribute universe. The experimental results show that PSFlow is more efficient and practical than previous schemes in mobile edge cloud."",""1558-2183"","""",""10.1109/TPDS.2022.3200959"",""National Natural Science Foundation of China(grant numbers:U1736212,61572080)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9865141"",""Access control encryption";data sharing;sanitization;mobile edge cloud;"parallel computing"",""Receivers";Encryption;Access control;Cloud computing;Servers;Mobile handsets;"Costs"","""",""7"","""",""37"",""IEEE"",""23 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Pattern-Based SpGEMM Library for Multi-Core and Many-Core Architectures,""Z. Xie"; G. Tan; W. Liu;" N. Sun"",""State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Technology, China University of Petroleum, Beijing, China;" State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jul 2021"",""2022"",""33"",""1"",""159"",""175"",""General sparse matrix-matrix multiplication (SpGEMM) is one of the most important mathematical library routines in a number of applications. In recent years, several efficient SpGEMM algorithms have been proposed, however, most of them are based on the compressed sparse row (CSR) format, and the possible performance gain from exploiting other formats has not been well studied. And some specific algorithms are restricted to parameter tuning that has a significant impact on performance. So the particular format, algorithm, and parameter that yield the best performance for SpGEMM remain undetermined. In this article, we conduct a prospective study on format-specific parallel SpGEMM algorithms and analyze their pros and cons. We then propose a pattern-based SpGEMM library, that provides a unified programming interface in the CSR format, analyses the pattern of two input matrices, and automatically determines the best format, algorithm, and parameter for arbitrary matrix pairs. For this purpose, we build an algorithm set that integrates three new designed algorithms with existing popular libraries, and design a hybrid deep learning model called MatNet to quickly identify patterns of input matrices and accurately predict the best solution by using sparse features and density representations. The evaluation shows that this library consistently outperforms the state-of-the-art library. We also demonstrate its adaptability in an AMG solver and a BFS algorithm with 30 percent performance improvement."",""1558-2183"","""",""10.1109/TPDS.2021.3090328"",""National Key Research and Development Program of China(grant numbers:2017YFB0202105,2016YFB0201305,2016YFB0200803,2016YFB0200300)"; National Natural Science Foundation of China(grant numbers:61521092,91430218,31327901,61472395,61432018,61671151);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459513"",""SpGEMM";spare BLAS;sparse format;auto-tuning;"neural network"",""Libraries";Sparse matrices;Prediction algorithms;Neural networks;Predictive models;Memory management;"Tuners"","""",""7"","""",""68"",""IEEE"",""17 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Pessimistic Fault Diagnosability of Large-Scale Connected Networks via Extra Connectivity,""L. Lin"; Y. Huang; L. Xu;" S. -Y. Hsieh"",""College of Computer and Cyber Security, and Key Laboratory of Network Security and Cryptology, Fujian Normal University, Fuzhou, Fujian, China"; College of Mathematics and Informatics, Fujian Normal University, Fuzhou, Fujian, China; College of Computer and Cyber Security, and Key Laboratory of Network Security and Cryptology, Fujian Normal University, Fuzhou, Fujian, China;" Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2021"",""2022"",""33"",""2"",""415"",""428"",""The t/kt/k-diagnosability and hh-extra connectivity are regarded as two important indicators to improve the network reliability. The t/k-diagnosis strategy can significantly improve the self-diagnosing capability of a network at the expense of no more than k fault-free nodes being mistakenly diagnosed as faulty. The h-extra connectivity can tremendously improve the real fault tolerability of a network by insuring that each remaining component has no fewer than h+1 nodes. However, there is few result on the inherent relationship between these two indicators. In this article, we investigate the reason that caused the serious flawed results in (Liu, 2020), and we propose a diagnosis algorithm to establish the t/k-diagnosability for a large-scale connected network G under the PMC model by considering its h-extra connectivity. Let κh(G) be the h-extra connectivity of G. Then, we can deduce that G is κh(G)/h-diagnosable under the PMC model with some basic conditions. All κh(G)faulty nodes can be correctly diagnosed in the large-scale connected network G and at most h fault-free nodes would be misdiagnosed as faulty. The complete fault tolerant method adopts combinatorial properties and linearly many fault analysis to conquer the core of our proofs. We will apply the newly found relationship to directly obtain the κh(G)/h-diagnosability of a series of well known networks, including hypercubes, folded hypercubes, balanced hypercubes, dual-cubes, BC graphs, star graphs, Cayley graphs generated by transposition trees, bubble-sort star graphs, alternating group graphs, split-star networks, k-ary n-cubes and (n,k)-star graphs."",""1558-2183"","""",""10.1109/TPDS.2021.3093243"",""Fok Ying Tung Education Foundation(grant numbers:171061)"; National Natural Science Foundation of China(grant numbers:61702100,U1905211,61702103,61771140);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468369"",""Reliability"; $t/k$    t / k     -diagnosability;fault diagnosis;extra connectivity;"large-scale connected networks"",""Hypercubes";Fault tolerant systems;Fault tolerance;Mathematical model;Fault diagnosis;Multiprocessing systems;"Informatics"","""",""10"","""",""42"",""IEEE"",""29 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Potential Game Theoretic Approach to Computation Offloading Strategy Optimization in End-Edge-Cloud Computing,""Y. Ding"; K. Li; C. Liu;" K. Li"",""College of Information Science and Engineering, Hunan University, Changsha, Hunan, China"; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China;" College of Information Science and Engineering, Hunan University, China"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Oct 2021"",""2022"",""33"",""6"",""1503"",""1519"",""Integrating user ends (UEs), edge servers (ESs), and the cloud into end-edge-cloud computing (EECC) can enhance the utilization of resources and improve quality of experience (QoE). However, the performance of EECC is significantly affected by its architecture. In this article, we classify EECC into two computing architectures types according to the visibility and accessibility of the cloud to UEs, i.e., hierarchical end-edge-cloud computing (Hi-EECC) and horizontal end-edge-cloud computing (Ho-EECC). In Hi-EECC, UEs can offload their tasks only to ESs. When the resources of ESs are exhausted, the ESs request the cloud to provide resources to UEs. In Ho-EECC, UEs can offload their tasks directly to ESs and the cloud. In this article, we construct a potential game for the EECC environment, in which each UE selfishly minimizes its payoff, study the computation offloading strategy optimization problems, and develop two potential game-based algorithms in Hi-EECC and Ho-EECC. Extensive experiments with real-world data are conducted to demonstrate the performance of the proposed algorithms. Moreover, the scalability and applicability of the two computing architectures are comprehensively analyzed. The conclusions of our work can provide useful suggestions for choosing specific computing architectures under different application environments to improve the performance of EECC and QoE."",""1558-2183"","""",""10.1109/TPDS.2021.3112604"",""National Natural Science Foundation of China(grant numbers:61625202)"; National Key Research and Development Program of China(grant numbers:2018YFB1701403); National Natural Science Foundation of China(grant numbers:61876061,62072165,U19A2058,61702170); Postgraduate Scientific Research Innovation Project of Hunan Province(grant numbers:CX20200435); Zhejiang Lab(grant numbers:2020KE0AB01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537304"",""Computation offloading";end-edge-cloud computing (EECC);hierarchical EECC;horizontal EECC;"potential game"",""Task analysis";Computer architecture;Optimization;Delays;Servers;Costs;"Quality of experience"","""",""38"","""",""41"",""IEEE"",""14 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Practical and Efficient Bidirectional Access Control Scheme for Cloud-Edge Data Sharing,""J. Cui"; B. Li; H. Zhong; G. Min; Y. Xu;" L. Liu"",""Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, Anhui, China"; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, Anhui, China; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, Anhui, China; Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; Key Laboratory of Intelligent Computing and Signal Processing of Ministry of Education, School of Computer Science and Technology, Anhui University, Hefei, Anhui, China;" School of Informatics, University of Leicester, Leicester, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""30 Jul 2021"",""2022"",""33"",""2"",""476"",""488"",""The cloud computing paradigm provides numerous tempting advantages, enabling users to store and share their data conveniently. However, users are naturally resistant to directly outsourcing their data to the cloud since the data often contain sensitive information. Although several fine-grained access control schemes for cloud-data sharing have been proposed, most of them focus on the access control of the encrypted data (e.g., restricting the decryption capabilities of the receivers). Distinct from the existing work, this article aims to address this challenging problem by developing a more practical bidirectional fine-grained access control scheme that can restrict the capabilities of both senders and receivers. To this end, we systematically investigate the access control for cloud data sharing. Inspired by the access control encryption (ACE), we propose a novel data sharing framework that combines the cloud side and the edge side. The edge server is located in the middle of all the communications, checking and preventing illegal communications according to the predefined access policy. Next, we develop an efficient access control algorithm by exploiting the attribute-based encryption and proxy re-encryption for the proposed framework. The experimental results show that our scheme exhibits superior performance in the encryption and decryption compared to the prior work."",""1558-2183"","""",""10.1109/TPDS.2021.3094126"",""National Natural Science Foundation of China(grant numbers:U1936220,62011530046,61872001)"; Special Fund for Key Program of Science and Technology of Anhui Province, China(grant numbers:202003A05020043); Open Fund for Discipline Construction; Institute of Physical Science and Information Technology, Anhui University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9470970"",""Cloud computing";data sharing;access control;encryption;"edge computing"",""Cloud computing";Access control;Receivers;Encryption;Servers;Cryptography;"Search problems"","""",""13"","""",""43"",""IEEE"",""1 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Practical Framework for Secure Document Retrieval in Encrypted Cloud File Systems,""J. Fu"; N. Wang; B. Cui;" B. K. Bhargava"",""School of Cyberspace Security and National Engineering Lab for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing, China"; School of Cyber Science and Technology, Beihang University, Beijing, China; School of Cyberspace Security and National Engineering Lab for Mobile Network Technologies, Beijing University of Posts and Telecommunications, Beijing, China;" Department of Computer Science, Purdue University, West Lafayette, IN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1246"",""1261"",""With the development of cloud computing, more and more data owners are motivated to outsource their documents to the cloud and share them with the authorized data users securely and flexibly. To protect data privacy, the documents are generally encrypted before being outsourced to the cloud and hence their searchability decreases. Though many privacy-preserving document search schemes have been proposed, they cannot reach a proper balance among functionality, flexibility, security and efficiency. In this paper, a new encrypted document retrieval system is designed and a proxy server is integrated into the system to alleviate data owner's workload and improve the whole system's security level. In this process, we consider a more practical and stronger threat model in which the cloud server can collude with a small number of data users. To support multiple document search patterns, we construct two AVL trees for the filenames and authors, and a Hierarchical Retrieval Features tree (HRF tree) for the document vectors. A depth-first search algorithm is designed for the HRF tree and the Enhanced Asymmetric Scalar-Product-Preserving Encryption (Enhanced ASPE) algorithm is utilized to encrypt the HRF tree. All the three index trees are linked with each other to efficiently support the search requests with multiple parameters. Theoretical analysis and simulation results illustrate the security and efficiency of the proposed framework."",""1558-2183"","""",""10.1109/TPDS.2021.3107752"",""National Natural Science Foundation of China(grant numbers:62001055,62102017)"; Natural Science Foundation of Beijing Municipality(grant numbers:4204107); Funds of “YinLing”(grant numbers:A02B01C03-201902D0);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524492"",""Cloud computing";privacy-preserving;searchable encryption;"document ranked retrieval"",""Cloud computing";Servers;Indexes;Security;Encryption;Search problems;"Simulation"","""",""1"","""",""43"",""IEEE"",""27 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Resource-Efficient Predictive Resource Provisioning System in Cloud Systems,""H. Shen";" L. Chen"",""Department of Computer Science, University of Virginia, Charlottesville, VA, USA";" VMware World Headquarters, Palo Alto, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3886"",""3900"",""In cloud systems, demand-prediction based resource provisioning schemes help assure the SLOs (service level objectives) of cloud tenants. We notice that if a provisioning scheme does not exclude bursts from historical resource demands in normal demand prediction or always uses a large padding to correct under-prediction, it will lead to resource over-provisioning and low resource utilization. To improve the previous schemes, in this paper, we present a Resource-efficient Predictive Resource Provisioning system in cloud systems (RPRP) that excludes bursts in demand prediction and has algorithms to specifically handle bursts to avoid resource over-provisioning. Rather than setting padding to a possibly high value, RPRP has a load-dependent padding algorithm that adaptively determines padding based on predicted demands. To handle bursts, RPRP has a burst-resilient shared padding algorithm that reserves resource shared by multiple co-located VMs rather than for individual VMs. It also embodies a responsive padding algorithm that adaptively adjusts padding to recover from both under-provisioning and over-provisioning. We implemented RPRP on top of Xen and conducted both trace-driven simulation and real-world testbed experiments. The experimental results show that RPRP achieves higher resource utilization, more accurate demand predictions, and fewer SLO violations than previous schemes."",""1558-2183"","""",""10.1109/TPDS.2022.3172493"",""National Science Foundation(grant numbers:ACI-1719397,CNS-1733596,NSF-1827674,CCF-1822965)"; Microsoft Research Faculty Fellowship(grant numbers:8300751);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9769930"",""Cloud";resource management;"resource provisioning"",""Resource management";Prediction algorithms;Time series analysis;Cloud computing;Heuristic algorithms;System analysis and design;"Monitoring"","""",""5"","""",""57"",""IEEE"",""5 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Survey of GPU Multitasking Methods Supported by Hardware Architecture,""C. Zhao"; W. Gao; F. Nie;" H. Zhou"",""School of Computer, Northwestern Polytechnical University, Xi'an, China"; School of Computer, Northwestern Polytechnical University, Xi'an, China; School of Computer, Northwestern Polytechnical University, Xi'an, China;" Department of Electrical and Computer Engineering, North Carolina State University, Raleigh, NC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Oct 2021"",""2022"",""33"",""6"",""1451"",""1463"",""The ability to support multitasking becomes more and more important in the development of graphic processing unit (GPU). GPU multitasking methods are classified into three types: temporal multitasking, spatial multitasking, and simultaneous multitasking (SMK). This article first introduces the features of some commercial GPU architectures to support multitasking and the common metrics used for evaluating the performance of GPU multitasking methods, and then reviews the GPU multitasking methods supported by hardware architecture (i.e., hardware GPU multitasking methods). The main problems of each type of hardware GPU multitasking methods to be solved are illustrated. Meanwhile, the key idea of each previous hardware GPU multitasking method is introduced. In addition, the characteristics of hardware GPU multitasking methods belonging to the same type are compared. This article also gives some valuable suggestions for the future research. An enhanced GPU simulator is needed to bridge the gap between academia and industry. In addition, it is promising to expand the research space with machine learning technologies, advanced GPU architectural innovations, 3D stacked memory, etc. Because most previous GPU multitasking methods are based on NVIDIA GPUs, this article focuses on NVIDIA GPU architecture, and uses NVIDIA's terminology. To our knowledge, this article is the first survey about hardware GPU multitasking methods. We believe that our survey can help the readers gain insights into the research field of hardware GPU multitasking methods."",""1558-2183"","""",""10.1109/TPDS.2021.3115630"",""National Natural Science Foundation of China(grant numbers:11875221)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9548839"",""GPU multitasking";survey;hardware architecture;temporal multitasking;spatial multitasking;"simultaneous multitasking (SMK)"",""Graphics processing units";Multitasking;Kernel;Hardware;Computer architecture;Registers;"Task analysis"","""",""8"","""",""111"",""IEEE"",""27 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"A Survey of Storage Systems in the RDMA Era,""S. Ma"; T. Ma; K. Chen;" Y. Wu"",""Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China"; Alibaba Group, Hangzhou, China; Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Beijing National Research Center for Information Science and Technology (BNRist), Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4395"",""4409"",""Remote Direct Memory Access (RDMA) based network devices are increasingly being deployed in modern data centers. RDMA brings significant performance improvements over traditional network devices such as Ethernet due to its unique features: protocol offloading and memory semantics. In particular, it can achieve microsecond level latency, which is about 2$\sim$∼3 orders of magnitude improvement. With such improvement in hardware, the software stack, including device drivers and programming libraries, is becoming a new performance bottleneck. Developers need to use new programming libraries to take full advantage of the performance of the underlying hardware. Storage systems are very important in modern data centers. This article surveys the current efforts to use RDMA for optimizing storage systems. We first present five classes of RDMA-based storage systems, including key-value stores, file systems, distributed memory systems, database systems, and systems using smart NICs, to demonstrate different design choices. Then, we examine the core modules of storage systems from different perspectives: communication mode, concurrency control, fault tolerance, caching, and resource management. Finally, we provide some design guidelines for new RDMA-based storage systems, as well as a discussion of opportunities and challenges."",""1558-2183"","""",""10.1109/TPDS.2022.3188656"",""National Key Research & Development Program of China(grant numbers:2020YFC1522702)"; National Natural Science Foundation of China(grant numbers:62141216,61877035); Tsinghua University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9827961"",""Network";storage;RDMA;RPC;key-value store;file system;distributed memory;"smart NIC"",""Semantics";Random access memory;Protocols;Hardware;Programming;Performance evaluation;"File systems"","""","""","""",""122"",""IEEE"",""12 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating Backward Aggregation in GCN Training With Execution Path Preparing on GPUs,""S. Xu"; Z. Shao; C. Yang; X. Liao;" H. Jin"",""National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" National Engineering Research Center for Big Data Technology and System/Services Computing Technology and System Lab/Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Oct 2022"",""2022"",""33"",""12"",""4891"",""4902"",""The emerging Graph Convolutional Network (GCN) has been widely used in many domains, where it is important to improve the efficiencies of applications by accelerating GCN trainings. Due to the sparsity nature and exploding scales of input real-world graphs, state-of-the-art GCN training systems (e.g., GNNAdvisor) employ graph processing techniques to accelerate the message exchanging (i.e., aggregations) among the graph vertices. Nevertheless, these systems treat both the aggregation stages of forward and backward propagation phases as all-active graph processing procedures that indiscriminately conduct computations on all vertices of an input graph. In this article, we first point out that in a GCN training problem with a given training set on an input graph, its aggregation stages of backward propagation phases (called as backward aggregations in this article) can be equivalently converted to partially-active graph processing procedures, which conduct computations on only partial vertices of the input graph. By leveraging such a finding, we propose an execution path preparing method that collects and coalesces the graph data used during different training layers of backward aggregations, and constructs their corresponding sub-graphs (called as execution paths in this article) as inputs to conduct the backward training on GPUs. Further, we propose a structural-aware strategy for the execution paths to compute their optimal group sizes, so as to gain as high as possible performances on GPUs during the backward aggregations. The experiment results by conducting GCN training in typical real-world graphs show that compared with GNNAdvisor, our approach improves the performance of backward aggregations by up to 5.68x on NVIDIA P100 GPU, and up to 6.57x on NVIDIA V100S GPU"",""1558-2183"","""",""10.1109/TPDS.2022.3205642"",""National Natural Science Foundation of China(grant numbers:61972444,61825202,62072195,61832006)"; Zhejiang Lab(grant numbers:2022P10AC02);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887890"",""Backward aggregation";graph convolutional network;graph processing;"graphics processing unit"",""Training";Backpropagation;Graphics processing units;Blogs;Electric breakdown;Computational modeling;"Social networking (online)"","""","""","""",""46"",""IEEE"",""12 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating Bayesian Neural Networks via Algorithmic and Hardware Optimizations,""H. Fan"; M. Ferianc; Z. Que; X. Niu; M. Rodrigues;" W. Luk"",""Department of Computing, Imperial College London, London, U.K."; Department of Electronic and Electrical Engineering, University College London, London, U.K.; Department of Computing, Imperial College London, London, U.K.; Corerain Technologies Ltd., Shenzhen, Guangdong, China; Department of Electronic and Electrical Engineering, University College London, London, U.K.;" Department of Computing, Imperial College London, London, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2022"",""2022"",""33"",""12"",""3387"",""3399"",""Bayesian neural networks (BayesNNs) have demonstrated their advantages in various safety-critical applications, such as autonomous driving or healthcare, due to their ability to capture and represent model uncertainty. However, standard BayesNNs require to be repeatedly run because of Monte Carlo sampling to quantify their uncertainty, which puts a burden on their real-world hardware performance. To address this performance issue, this article systematically exploits the extensive structured sparsity and redundant computation in BayesNNs. Different from the unstructured or structured sparsity in standard convolutional NNs, the structured sparsity of BayesNNs is introduced by Monte Carlo Dropout and its associated sampling required during uncertainty estimation and prediction, which can be exploited through both algorithmic and hardware optimizations. We first classify the observed sparsity patterns into three categories: channel sparsity, layer sparsity and sample sparsity. On the algorithmic side, a framework is proposed to automatically explore these three sparsity categories without sacrificing algorithmic performance. We demonstrated that structured sparsity can be exploited to accelerate CPU designs by up to 49 times, and GPU designs by up to 40 times. On the hardware side, a novel hardware architecture is proposed to accelerate BayesNNs, which achieves a high hardware performance using the runtime adaptable hardware engines and the intelligent skipping support. Upon implementing the proposed hardware design on an FPGA, our experiments demonstrated that the algorithm-optimized BayesNNs can achieve up to 56 times speedup when compared with unoptimized Bayesian nets. Comparing with the optimized GPU implementation, our FPGA design achieved up to 7.6 times speedup and up to 39.3 times higher energy efficiency."",""1558-2183"","""",""10.1109/TPDS.2022.3153682"",""U.K. EPSRC(grant numbers:EP/L016796/1,EP/N031768/1,EP/P010040/1,EP/V028251/1,EP/S030069/1)"; Corerain; Maxeler; Intel(grant numbers:Xilinx);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720069"",""Bayesian neural network (BayesNN)";structured sparsity;field-programmable gate array (FPGA);"deep learning"",""Hardware";Artificial neural networks;Uncertainty;Bayes methods;Standards;Estimation;"Prediction algorithms"","""",""2"","""",""49"",""IEEE"",""23 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating Geostatistical Modeling and Prediction With Mixed-Precision Computations: A High-Productivity Approach With PaRSEC,""S. Abdulah"; Q. Cao; Y. Pei; G. Bosilca; J. Dongarra; M. G. Genton; D. E. Keyes; H. Ltaief;" Y. Sun"",""Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA; Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia; Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia;" Computer, Electrical, and Mathematical Sciences and Engineering Division (CEMSE), King Abdullah University of Science and Technology (KAUST), Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""964"",""976"",""Geostatistical modeling, one of the prime motivating applications for exascale computing, is a technique for predicting desired quantities from geographically distributed data, based on statistical models and optimization of parameters. Spatial data are assumed to possess properties of stationarity or non-stationarity via a kernel fitted to a covariance matrix. A primary workhorse of stationary spatial statistics is Gaussian maximum log-likelihood estimation (MLE), whose central data structure is a dense, symmetric positive definite covariance matrix of the dimension of the number of correlated observations. Two essential operations in MLE are the application of the inverse and evaluation of the determinant of the covariance matrix. These can be rendered through the Cholesky decomposition and triangular solution. In this contribution, we reduce the precision of weakly correlated locations to single- or half- precision based on distance. We thus exploit mathematical structure to migrate MLE to a three-precision approximation that takes advantage of contemporary architectures offering BLAS3-like operations in a single instruction that are extremely fast for reduced precision. We illustrate application-expected accuracy worthy of double-precision from a majority half-precision computation, in a context where uniform single-precision is by itself insufficient. In tackling the complexity and imbalance caused by the mixing of three precisions, we deploy the PaRSEC runtime system. PaRSEC delivers on-demand casting of precisions while orchestrating tasks and data movement in a multi-GPU distributed-memory environment within a tile-based Cholesky factorization. Application-expected accuracy is maintained while achieving up to $1.59X$1.59X by mixing FP64/FP32 operations on 1536 nodes of HAWK or 4096 nodes of Shaheen II, and up to $2.64X$2.64X by mixing FP64/FP32/FP16 operations on 128 nodes of Summit, relative to FP64-only operations. This translates into up to 4.5, 4.7, and 9.1 (mixed) PFlop/s sustained performance, respectively, demonstrating a synergistic combination of exascale architecture, dynamic runtime software, and algorithmic adaptation applied to challenging environmental problems."",""1558-2183"","""",""10.1109/TPDS.2021.3084071"",""U.S. Department of Energy(grant numbers:DE-AC05-00OR22725)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442267"",""Climate/weather prediction";dynamic runtime systems;geospatial statistics;high performance computing;multiple precisions;"user-productivity"",""Computational modeling";Covariance matrices;Runtime;Task analysis;Predictive models;Maximum likelihood estimation;"Data models"","""",""11"","""",""55"",""IEEE"",""26 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Accelerating HDF5 I/O for Exascale Using DAOS,""J. Soumagne"; J. Henderson; M. Chaarawi; N. Fortner; S. Breitenfeld; S. Lu; D. Robinson; E. Pourmal;" J. Lombardi"",""HDF Group, Champaign, IL, USA"; HDF Group, Champaign, IL, USA; Extreme Storage Architecture & Development, Intel Corporation, Santa Clara, CA, USA; HDF Group, Champaign, IL, USA; HDF Group, Champaign, IL, USA; HDF Group, Champaign, IL, USA; HDF Group, Champaign, IL, USA; HDF Group, Champaign, IL, USA;" Extreme Storage Architecture & Development, Intel Corporation, Santa Clara, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""903"",""914"",""The Hierarchical Data Format 5 (HDF5) has long been defined as one of the most prominent data models, binary file formats and I/O libraries for storing and managing scientific data. Introduced in the late 90s when POSIX I/O was the standard, the library has since then been continuously improved to respond and adapt to the ever-growing demands of high-performance computing (HPC) software and hardware. Given the limitations of POSIX I/O and with the emergence of new technologies such as object stores, non-volatile memory, and SSDs, the need for an interface that can efficiently store and access data at scale through new paradigms has become more and more pressing. The Distributed Asynchronous Object Storage (DAOS) file system is an emerging file system that aims at responding to those demands by taking disk-based storage out of the loop. We present in this article the research efforts that have been taking place to prepare the HDF5 library for Exascale using DAOS. By enabling and defining a new storage file format, we focus on the benefits that it delivers to the applications in terms of features and performance."",""1558-2183"","""",""10.1109/TPDS.2021.3097884"",""U.S. Department of Energy"; Argonne National Laboratory(grant numbers:DE-AC02-06CH11357,8F-30005);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490299"",""Parallel I/O";distributed file systems;data storage representations;"object representation"",""Libraries";Semantics;Metadata;Connectors;Nonvolatile memory;Middleware;"Writing"","""",""4"","""",""28"",""IEEE"",""19 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating Large Sparse Neural Network Inference Using GPU Task Graph Parallelism,""D. -L. Lin";" T. -W. Huang"",""Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA";" Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3041"",""3052"",""The ever-increasing size of modern deep neural network (DNN) architectures has put increasing strain on the hardware needed to implement them. Sparsified DNNs can greatly reduce memory costs and increase throughput over standard DNNs, if the loss of accuracy can be adequately controlled. However, sparse DNNs present unique computational challenges. Efficient model or data parallelism algorithms are extremely hard to design and implement. The recent effort MIT/IEEE/Amazon HPEC Graph Challenge has drawn attention to high-performance inference methods for large sparse DNNs. In this article, we introduce SNIG, an efficient inference engine for large sparse DNNs. SNIG develops highly optimized inference kernels and leverages the power of CUDA Graphs to enable efficient decomposition of model and data parallelisms. Our decomposition strategy is flexible and scalable to different partitions of data volumes, model sizes, and GPU numbers. We have evaluated SNIG on the official benchmarks of HPEC Sparse DNN Challenge and demonstrated its promising performance scalable from a single GPU to multiple GPUs. Compared to the champion of the 2019 HPEC Sparse DNN Challenge, SNIG can finish all inference workloads using only a single GPU. At the largest DNN, which has more than 4 billion parameters across 1920 layers each of 65536 neurons, SNIG is up to 2.3× faster than a state-of-the-art baseline under a machine of 4 GPUs. SNIG receives the Champion Award in 2020 HPEC Sparse DNN Challenge."",""1558-2183"","""",""10.1109/TPDS.2021.3138856"",""National Science Foundation(grant numbers:CCF-2126672)"; NumFOCUS;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664223"",""Task graph parallelism"",""Graphics processing units";Kernel;Task analysis;Parallel processing;Programming;Neurons;"Data models"","""",""1"","""",""29"",""IEEE"",""28 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating Restarted GMRES With Mixed Precision Arithmetic,""N. Lindquist"; P. Luszczek;" J. Dongarra"",""Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA"; Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA;" Innovative Computing Laboratory, University of Tennessee, Knoxville, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""1027"",""1037"",""The generalized minimum residual method (GMRES) is a commonly used iterative Krylov solver for sparse, non-symmetric systems of linear equations. Like other iterative solvers, data movement dominates its run time. To improve this performance, we propose running GMRES in reduced precision with key operations remaining in full precision. Additionally, we provide theoretical results linking the convergence of finite precision GMRES with classical Gram-Schmidt with reorthogonalization (CGSR) and its infinite precision counterpart which helps justify the convergence of this method to double-precision accuracy. We tested the mixed-precision approach with a variety of matrices and preconditioners on a GPU-accelerated node. Excluding the incomplete LU factorization without fill in (ILU(0)) preconditioner, we achieved average speedups ranging from 8 to 61 percent relative to comparable double-precision implementations, with the simpler preconditioners achieving the higher speedups."",""1558-2183"","""",""10.1109/TPDS.2021.3090757"",""University of Tennessee(grant numbers:MSE E01-1315-038)"; UT-Battelle(grant numbers:4000123266); National Science Foundation(grant numbers:2004541);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462418"",""Linear systems";"multiple precision arithmetic"",""Iterative methods";Lifting equipment;Convergence;Stability analysis;Linear systems;Kernel;"Error correction"","""",""9"","""",""41"",""IEEE"",""22 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Accelerating Tensor Swapping in GPUs With Self-Tuning Compression,""P. Chen"; S. He; X. Zhang; S. Chen; P. Hong; Y. Yin;" X. -H. Sun"",""College of Computer Science and Technology, Zhejiang University, Hangzhou, China"; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; School of Engineering and Computer Science, Washington State University Vancouver, Vancouver, WA, USA; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Institute of Open Source Chip, Beijing, China;" Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4484"",""4498"",""Data swapping between CPUs and GPUs is widely used to address the GPU memory shortage issue when training deep neural networks (DNNs) requiring a larger amount of memory than that a GPU may have. Data swapping may become a bottleneck when its latency is longer than the latency of DNN computations. Tensor compression in GPUs can reduce the data swapping time. However, existing works on compressing tensors in the virtual memory of GPUs have three major issues: lack of portability because its implementation requires additional (de)compression units in memory controllers, sub-optimal compression performance for varying tensor compression ratios and sizes, and poor adaptation to dense tensors because they only focus on sparse tensors. We propose a self-tuning tensor compression framework, named CSwap+, for improving the virtual memory management of GPUs. It uses GPUs for (de)compression directly and thus has high portability and is minimally dependent on GPU architecture features. Furthermore, it only applies compression on tensors that are deemed to be cost-effective considering their compression ratio, size, and the characteristics of compression algorithms at runtime. Finally, to adapt to DNN models with dense tensors, it also supports cost-effective lossy compression for dense tensors with nearly no model training accuracy degradation. We conduct the experiments through six representative memory-intensive DNN models. Compared to vDNN, CSwap+ reduces tensor swapping latency by up to 50.9% and 46.1% with NVIDIA V100 GPU, for DNN models with sparse and dense tensors, respectively."",""1558-2183"","""",""10.1109/TPDS.2022.3193867"",""National Key Research and Development Program of China(grant numbers:2021ZD0110700)"; National Natural Science Foundation of China(grant numbers:62172361); Zhejiang Lab Research Project(grant numbers:2020KC0AC01); National Science Foundation(grant numbers:CNS 1906541);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9841008"",""DNN";GPU;tensor;swapping;"compression"",""Tensors";Training;Graphics processing units;Memory management;Adaptation models;Prefetching;"Computational modeling"","""",""1"","""",""59"",""IEEE"",""26 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"AccTFM: An Effective Intra-Layer Model Parallelization Strategy for Training Large-Scale Transformer-Based Models,""Z. Zeng"; C. Liu; Z. Tang; K. Li;" K. Li"",""College of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Hunan, China"; College of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Hunan, China; College of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Hunan, China; College of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Hunan, China;" College of Information Science and Engineering, National Supercomputing Center in Changsha, Hunan University, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4326"",""4338"",""Transformer-based deep neural networks have recently swept the field of natural language processing due to their outstanding performance, and are gradually spreading to more applications such as image/video processing. However, compared with general DNNs, training a sizeable transformer-based model is further time-consuming and memory-hungry. The existing distributed training strategies for general DNNs are not appropriate or can not efficiently handle transformer-based networks. In view of this, we propose an intra-layer model parallelization optimization strategy, AccTFM, which introduces a novel fine-grained pipeline execution and hybrid communication compression strategy to overcome the synchronization bottleneck. Specifically, on one hand, it first decouples the inter-layer computation and communication dependencies, and then searches for the optimal partitioning strategy to maximize the overlap of computation and communication. On the other hand, the hybrid communication compression module consists of token-level top-$k$k sparsification and piecewise quantization methods aiming at minimizing communication traffic. Experimental results show that AccTFM accelerates transformer-based DNNs training by up to 2.08x compared to state-of-the-art distributed training techniques."",""1558-2183"","""",""10.1109/TPDS.2022.3187815"",""National Key Research and Development Program of China(grant numbers:2021YFB0300300)"; National Natural Science Foundation of China(grant numbers:62072165); Fundamental Research Funds for the Central Universities;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815837"",""Communication hiding";deep learning;intra-layer model parallelization;quantization;"Top- k  sparsification"",""Training";Transformers;Pipelines;Computational modeling;Tensors;Load modeling;"Synchronization"","""","""","""",""37"",""IEEE"",""5 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Adaptive and Efficient Resource Allocation in Cloud Datacenters Using Actor-Critic Deep Reinforcement Learning,""Z. Chen"; J. Hu; G. Min; C. Luo;" T. El-Ghazawi"",""Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K"; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K;" Department of Electrical and Computer Engineering, The George Washington University, Washington, DC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Dec 2021"",""2022"",""33"",""8"",""1911"",""1923"",""The ever-expanding scale of cloud datacenters necessitates automated resource provisioning to best meet the requirements of low latency and high energy-efficiency. However, due to the dynamic system states and various user demands, efficient resource allocation in cloud faces huge challenges. Most of the existing solutions for cloud resource allocation cannot effectively handle the dynamic cloud environments because they depend on the prior knowledge of a cloud system, which may lead to excessive energy consumption and degraded Quality-of-Service (QoS). To address this problem, we propose an adaptive and efficient cloud resource allocation scheme based on Actor-Critic Deep Reinforcement Learning (DRL). First, the actor parameterizes the policy (allocating resources) and chooses actions (scheduling jobs) based on the scores assessed by the critic (evaluating actions). Next, the resource allocation policy is updated by using gradient ascent while the variance of policy gradient is reduced with an advantage function, which improves the training efficiency of the proposed method. We conduct extensive simulation experiments using real-world data from Google cloud datacenters. The results show that our method can obtain the superior QoS in terms of latency and job dismissing rate with enhanced energy-efficiency, compared to two advanced DRL-based and five classic cloud resource allocation methods."",""1558-2183"","""",""10.1109/TPDS.2021.3132422"",""EU Horizon 2020 INITIATE(grant numbers:101008297)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635652"",""Cloud computing";datacenters;resource allocation;energy-efficiency;"deep reinforcement learning"",""Resource management";Cloud computing;Quality of service;Training;Dynamic scheduling;Reinforcement learning;"Energy consumption"","""",""18"","""",""39"",""IEEE"",""3 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Adaptive DRL-Based Virtual Machine Consolidation in Energy-Efficient Cloud Data Center,""J. Zeng"; D. Ding; K. Kang; H. Xie;" Q. Yin"",""School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China"; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China; School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China;" School of Computer and Information Technology, Beijing Jiaotong University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2991"",""3002"",""The dramatic increasing of data and demands for computing capabilities may result in excessive use of resources in cloud data centers, which not only causes the raising of energy consumption, but also leads to the violation of Service Level Agreement (SLA). Dynamic consolidation of virtual machines (VMs) is proven to be an efficient way to tackle this issue. In this paper, we present an Adaptive Deep Reinforcement Learning (DRL)-based Virtual Machine Consolidation (ADVMC) framework for energy-efficient cloud data centers. ADVMC has two phases. In the first phase, Influence Coefficient is introduced to measure the impact of a VM on producing host overload, and a dynamic Influence Coefficient-based VM selection algorithm (ICVMS) is proposed to preferentially choose those VMs with the greatest impact for migration in order to remove the excessive workloads of the overloaded host quickly and accurately. In the second phase, a Prediction Aware DRL-based VM placement method (PADRL) is further proposed to automatically find suitable hosts for VMs to be migrated, in which a state prediction network is designed based on LSTM to provide DRL-based model more reasonable environment states so as to accelerate the convergence of DRL. Simulation experiments on the real-world workload provided by Google Cluster Trace have shown that our ADVMC approach can largely cut down system energy consumption and reduce SLA violation of users as compared to many other VM consolidation policies."",""1558-2183"","""",""10.1109/TPDS.2022.3147851"",""R&D Program of Beijing Municipal Education Commission(grant numbers:KJZD20191000402)"; Beijing Natural Science Foundation(grant numbers:L211015); Fundamental Research Funds for the Central Universities(grant numbers:2019JBM025,2019JBZ104);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698981"",""Cloud computing";vm consolidation;energy efficient;influence coefficient;"deep reinforcement learning"",""Cloud computing";Energy consumption;Data centers;Heuristic algorithms;Resource management;Predictive models;"Costs"","""",""8"","""",""42"",""IEEE"",""1 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Adaptive Federated Deep Reinforcement Learning for Proactive Content Caching in Edge Computing,""D. Qiao"; S. Guo; D. Liu; S. Long; P. Zhou;" Z. Li"",""Key Laboratory of Dependable Service Computing in Cyber-Physical-Society (Ministry of Education), College of computer science, Chongqing University, Chongqing, China"; Key Laboratory of Dependable Service Computing in Cyber-Physical-Society (Ministry of Education), College of computer science, Chongqing University, Chongqing, China; Key Laboratory of Biorheological Science and Technology, Ministry of Education, College of Bioengineering, Chongqing University, Chongqing, China; National & Local Joint Engineering Research Center of Network Security Detection and Protection Technology, Guangdong Provincial Key Laboratory of Data Security and Privacy Protection College of Information Science and Technology, Jinan University, Guangzhou, China; Key Laboratory of Dependable Service Computing in Cyber-Physical-Society (Ministry of Education), College of computer science, Chongqing University, Chongqing, China;" National & Local Joint Engineering Research Center of Network Security Detection and Protection Technology, Guangdong Provincial Key Laboratory of Data Security and Privacy Protection College of Information Science and Technology, Jinan University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Sep 2022"",""2022"",""33"",""12"",""4767"",""4782"",""With the aggravation of data explosion and backhaul loads on 5 G edge network, it is difficult for traditional centralized cloud to meet the low latency requirements for content access. The federated learning (FL)-based proactive content caching (FPC) can alleviate the matter by placing content in local cache to achieve fast and repetitive data access while protecting the users’ privacy. However, due to the non-independent and identically distributed (Non-IID) data across the clients and limited edge resources, it is unrealistic for FL to aggregate all participated devices in parallel for model update and adopt the fixed iteration frequency in local training process. To address this issue, we propose a distributed resources-efficient FPC policy to improve the content caching efficiency and reduce the resources consumption. Through theoretical analysis, we first formulate the FPC problem into a stacked autoencoders (SAE) model loss minimization problem while satisfying resources constraint. We then propose an adaptive FPC (AFPC) algorithm combined deep reinforcement learning (DRL) consisting of two mechanisms of client selection and local iterations number decision. Next, we show that when training data are Non-IID, aggregating the model parameters of all participated devices may be not an optimal strategy to improve the FL-based content caching efficiency, and it is more meaningful to adopt adaptive local iteration frequency when resources are limited. Finally, experimental results in three real datasets demonstrate that AFPC can effectively improve cache efficiency up to 38.4$\%$% and 6.84$\%$%, and save resources up to 47.4$\%$% and 35.6$\%$%, respectively, compared with traditional multi-armed bandit (MAB)-based and FL-based algorithms."",""1558-2183"","""",""10.1109/TPDS.2022.3201983"",""Natural Science Key Foundation of Chongqing(grant numbers:cstc2020jcyj-zdxmX0026)"; National Key Research and Development Program of China(grant numbers:2021YFB3101200); National Natural Science Foundation of China(grant numbers:62032020,62172350); Hunan Science and Technology Planning Project(grant numbers:2019RS3019); Hunan Province Department of Education(grant numbers:21B0120); Natural Science Foundation of Hunan(grant numbers:2021JJ40544);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9868114"",""Content caching";deep reinforcement learning;edge computing;federated learning;"resource constraint"",""Servers";Data models;Adaptation models;Internet of Things;Reinforcement learning;Feature extraction;"Delays"","""",""8"","""",""50"",""IEEE"",""26 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Adaptive Resource Efficient Microservice Deployment in Cloud-Edge Continuum,""K. Fu"; W. Zhang; Q. Chen; D. Zeng;" M. Guo"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Computer Science, China University of Geosciences, Wuhan, Hubei, China;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Dec 2021"",""2022"",""33"",""8"",""1825"",""1840"",""User-facing services are now evolving towards the microservice architecture where a service is built by connecting multiple microservice stages. Since the entire service is heavy, the microservice architecture shows the opportunity to only offload some microservice stages to the edge devices that are close to the end users. However, emerging techniques often result in the violation of Quality-of-Service (QoS) of microservice-based services in cloud-edge continuum, as they do not consider the communication overhead or the resource contention between microservices and external co-located tasks. We propose Nautilus, a runtime system that effectively deploys microservice-based user-facing services in cloud-edge continuum. Nautilus ensures the QoS of microservice-based user-facing services while minimizing the required computational resources, which is comprised of a communication-aware microservice mapper, a contention-aware resource manager and an IO-sensitive and load-aware microservice migration scheduler. The mapper divides the microservice graph into multiple partitions based on the communication overhead and maps the partitions to appropriate nodes. On each node, the resource manager determines the optimal resource allocation for its microservices based on reinforcement learning that may capture the complex contention behaviors. Once the microservices are suffered from external IO pressure, the IO-sensitive microservice scheduler migrates the critical one to idle nodes. Furthermore, when the load of microservices changes dynamically, the load-aware microservice scheduler migrates microservices from busy nodes to idle ones to ensure the QoS goal of the entire service. Our experimental results show that Nautilus can guarantee the required QoS target under external shared resources contention while the state-of-the-art suffers from QoS violations. Meanwhile, Nautilus reduces the computational resource usage by 23.9% and the network bandwidth usage by 53.4%, while achieving the required 99%-ile latency."",""1558-2183"","""",""10.1109/TPDS.2021.3128037"",""National Key Research and Development Program of China(grant numbers:2018YFB1004800)"; National Natural Science Foundation of China(grant numbers:62022057,61832006,61632017,61872240); Open Research Projects of Zhejiang Lab(grant numbers:2021KE0AB02);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9615028"",""Cloud-edge continuum";QoS;"microservice resources management"",""Quality of service";Cloud computing;Task analysis;Resource management;Computer architecture;Runtime;"Bandwidth"","""",""30"","""",""57"",""IEEE"",""15 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Adaptive Vertical Federated Learning on Unbalanced Features,""J. Zhang"; S. Guo; Z. Qu; D. Zeng; H. Wang; Q. Liu;" A. Y. Zomaya"",""Department of Computing, The Hong Kong Polytechnic University, Hong Kong"; Department of Computing, The Hong Kong Polytechnic University, Hong Kong; Key Laboratory of Water Resources Big Data Technology of Ministry of Water Resources, School of Computer and Information, Hohai University, Nanjing, China; School of Computer Science and Technology, China University of Geosciences, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science, Hong Kong Baptist University, Hong Kong;" High Performance Computing & Networking, School of Information Technologies, Sydney University, Camperdown, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4006"",""4018"",""Most of the existing FL systems focus on a data-parallel architecture where training data are partitioned by samples among several parties. In some real-life applications, however, partitioning by features is also of practical relevance and the number of features is usually unbalanced among parties. The corresponding learning framework is referred to as Vertical Federated Learning (VFL). Though some pioneering work focused on VFL, the convergence properties of VFL on unbalanced features, especially when parties conduct different numbers of local updates concerning heterogeneous computational capabilities are still unknown. In this article, we propose a new learning framework to improve the training efficiency of VFL on unbalanced features. Given the number of features and the computational capability owned by each party, our thorough theoretical analysis exhibits that the number of local updates conducted by each party has a great effect on the convergence rate and the computational complexity, both of which jointly determine the overall training efficiency in an interrelated and sophisticated way. Based on our theoretical findings, we formulate an optimization problem and derive the optimal solution by selecting an adaptive number of local training rounds for each party. Extensive experiments on various datasets and models demonstrate that our approach significantly improves the training efficiency of VFL."",""1558-2183"","""",""10.1109/TPDS.2022.3178443"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101400003)"; Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,152203/20E,152244/21E); National Natural Science Foundation of China(grant numbers:61872310,62102131); Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ20200109142008673); Natural Science Foundation of Jiangsu Province(grant numbers:BK20210361);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9783035"",""Vertical federated learning";unbalanced feature distribution;"convergence analysis"",""Training";Convergence;Collaborative work;Computational modeling;Data models;Training data;"Servers"","""",""2"","""",""52"",""IEEE"",""27 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Addictive Incentive Mechanism in Crowdsensing From the Perspective of Behavioral Economics,""J. Liu"; S. Huang; D. Li; S. Wen;" H. Liu"",""School of Computer Science and Engineering, Central South University, Changsha, Hunan, China"; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia;" Department of Computer Science, Missouri State University, Springfield, MO, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Oct 2021"",""2022"",""33"",""5"",""1109"",""1127"",""In mobile crowdsensing, many mobile devices are collectively used to complete complex sensing tasks. Most tasks require users to consume resources to ensure continuous performance over multiple periods of time. Therefore, it is important to incentivize enough users to continuously participate in the tasks. However, there are two issues with current incentive mechanisms. First, most studies are designed for maximizing the revenue of a single round of tasks rather than long-term incentives. Second, although some studies use historical data to design mechanisms for long-term operation, the law of diminishing marginal utility is not considered";" thus, the actual performance is lower than expected. In this study, the concepts of capital deposit and intertemporal choice from behavioral economics are introduced to explain the principle of addiction, which is a representative long-term incentive. Consequently, an Addiction Incentive Mechanism (AIM) is proposed. It influences the utility and demand functions of users by accelerating the accumulation of capital deposits and promoting users to become addicted to cooperative behavior. It also mitigates the effect of diminishing marginal utility through intertemporal choice theory to maintain user engagement. Simulations demonstrate that AIM improves participation and repetition rates compared with the state-of-the-art mechanisms."",""1558-2183"","""",""10.1109/TPDS.2021.3104247"",""National Natural Science Foundation of China(grant numbers:61873352)"; Natural Science Foundation of Hunan Province(grant numbers:2020JJ5770);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511828"",""Behavioural economics";crowdsensing;incentive mechanism;"intertemporal choice"",""Task analysis";Sensors;Economics;Crowdsensing;Computer science;Monitoring;"Maintenance engineering"","""",""6"","""",""59"",""IEEE"",""11 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Addressing the Read-Performance Impact of Reconfigurations in Replicated Key-Value Stores,""A. Papaioannou";" K. Magoutis"",""Institute of Computer Science (ICS), Foundation for Research and Technology – Hellas (FORTH), Heraklion, Greece";" Institute of Computer Science (ICS), Foundation for Research and Technology – Hellas (FORTH), Heraklion, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Feb 2022"",""2022"",""33"",""9"",""2106"",""2119"",""Raw data are often orders of magnitude larger than main memory for many applications. As the performance of storage devices is still significantly slower than main memory, systems still rely on memory caching to improve performance. Data replication schemes are prevalent in data stores for high availability and reliability. In such schemes, while data updates are propagated to all replicas (either synchronously or in the background), reads are usually served by only a subset of replica group members (e.g., as in primary-backup and quorum systems). As a result, non-serving replicas cannot keep their memory cache state updated";" thus, during a reconfiguration or a fail-over action, the system suffers from a high read-performance impact for a significant amount of time due to cold-cache misses. In our study we observed up to 70% hit after a reconfiguration due to cold cache misses, taking almost 18 minutes in some cases to fully restore to the pre-reconfiguration level of performance. In this article we propose a mechanism to maintain up-to-date read caches across replicas by sending read hints to the non-serving replicas to keep their caches warm. Thus the system is able to seamlessly achieve the same performance level even in the face of a replica group reorganization. This is especially important under the read-intensive workloads that are common today. Our evaluation shows that our mechanism has significant benefits during reconfigurations, with low performance impact under periods of resource strain. Given its advisory nature, the maintenance of read hints can be reduced or held off if needed during such periods."",""1558-2183"","""",""10.1109/TPDS.2021.3135137"",""Hellenic Foundation for Research and Innovation(grant numbers:HFRI-FM17-1998)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650699"",""Data replication";caching;"performance"",""Switches";Reliability;Task analysis;Production systems;Performance evaluation;Memory management;"Market research"","""","""","""",""42"",""CCBY"",""14 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Advancing Adoption of Reproducibility in HPC: A Preface to the Special Section,""S. L. Harrell"; S. Michael;" C. Maltzahn"",""Texas Advanced Computing Center, University of Texas at Austin, Austin, TX, USA"; Research Software, Indiana University, Bloomington, IN, USA;" Center for Research in Open Source Software, University of California Santa Cruz, Santa Cruz, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Jan 2022"",""2022"",""33"",""9"",""2011"",""2013"",""In this special section we bring you a practice and experience effort in reproducibility for large-scale computational science at SC20. This section includes nine critiques, each by a student team that reproduced results from a paper published at SC19, during the following year’s Student Cluster Competition. The paper is also included in this section and has been expanded upon, now including an analysis of the outcomes of the students’ reproducibility experiments. Lastly, this special section encapsulates a variety of advances in reproducibility in the SC conference series technical program."",""1558-2183"","""",""10.1109/TPDS.2021.3128796"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622175"",""Open science";computational science;reproducibility;"practice and experience"",""Reproducibility of results";Standards;Scientific computing;Special issues and sections;High performance computing;Hardware;"Graphics processing units"","""","""","""",""1"",""IEEE"",""19 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Accurate and Efficient Large-Scale Regression Method Through Best Friend Clustering,""K. Li"; L. Yuan; Y. Zhang;" G. Chen"",""State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China"; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China;" Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, Chinese Academy of Sciences (CAS), Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3129"",""3140"",""As the data size in Machine Learning fields grows exponentially, it is inevitable to accelerate the computation by utilizing the ever-growing large number of available cores provided by high-performance computing hardware. However, existing parallel methods for clustering or regression often suffer from problems of low accuracy, slow convergence, and complex hyperparameter-tuning. Furthermore, the parallel efficiency is usually difficult to improve while striking a balance between preserving model properties and partitioning computing workloads on distributed systems. In this article, we propose a novel and simple data structure capturing the most important information among data samples. It has several advantageous properties supporting a hierarchical clustering strategy that contains well-defined metrics for determining optimal hierarchy, balanced partition for maintaining the clustering property, and efficient parallelization for accelerating computation phases. Then we combine the clustering with regression techniques as a parallel library and utilize a hybrid structure of data and model parallelism to make predictions. Experiments illustrate that our library obtains remarkable performance on convergence, accuracy, and scalability."",""1558-2183"","""",""10.1109/TPDS.2021.3134336"",""National Natural Science Foundation of China(grant numbers:61972376,62072431,62032023)"; Science Foundation of Beijing(grant numbers:L182053);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647869"",""Distributed machine learning";scalable algorithm;large-scale clustering;"parallel regression"",""Clustering algorithms";Training;Mathematical models;Computational modeling;Libraries;Kernel;"Support vector machines"","""","""","""",""54"",""IEEE"",""13 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Automated Tool for Analysis and Tuning of GPU-Accelerated Code in HPC Applications,""K. Zhou"; X. Meng; R. Sai; D. Grubisic;" J. Mellor-Crummey"",""Computer Science Department, Rice University, Houston, TX, USA"; Computer Science Department, Rice University, Houston, TX, USA; Computer Science Department, Rice University, Houston, TX, USA; Computer Science Department, Rice University, Houston, TX, USA;" Computer Science Department, Rice University, Houston, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""854"",""865"",""The US Department of Energy’s fastest supercomputers and forthcoming exascale systems employ Graphics Processing Units (GPUs) to increase the computational performance of compute nodes. However, the complexity of GPU architectures makes tailoring sophisticated applications to achieve high performance on GPU-accelerated systems a major challenge. At best, prior performance tools for GPU code only provide coarse-grained tuning advice at the kernel level. In this article, we describe GPA, a performance advisor that suggests potential code optimizations at a hierarchy of levels, including individual lines, loops, and functions. To gather the fine-grained measurements needed to produce such insights, GPA uses instruction sampling and binary instrumentation to monitor execution of GPU code. At the time of this writing, GPU instruction sampling is only available on NVIDIA GPUs. To understand performance losses, GPA uses data flow analysis to approximately attribute measured instruction stalls back to their causes. GPA then analyzes patterns of stalls using information about a program’s structure and the GPU architecture to identify optimization strategies that address inefficiencies observed. GPA then employs detailed performance models to estimate the potential speedup that each optimization might provide. Experiments with benchmarks and applications show that GPA provides useful advice for tuning GPU code. We applied GPA to analyze and tune a collection of codes on NVIDIA V100 and A100 GPUs. GPA suggested optimizations that it estimates will accelerate performance across the set of codes by a geometric mean of 1.21×. Applying these optimizations suggested by GPA accelerated these codes by a geometric mean of 1.19×."",""1558-2183"","""",""10.1109/TPDS.2021.3094169"",""Exascale Computing(grant numbers:17-SC-20-SC)"; U.S. Department of Energy; National Nuclear Security Administration; Lawrence Livermore National Laboratory(grant numbers:B639429); ExxonMobil Graduate Fellowship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9470950"",""High performance computing";performance analysis;parallel programming;"parallel architectures"",""Graphics processing units";Optimization;Tools;Measurement;Instruments;Tuning;"Registers"","""",""1"","""",""45"",""IEEE"",""1 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An Efficient Index-Based Approach to Distributed Set Reachability on Small-World Graphs,""Y. Zeng"; K. Li; X. Zhou; W. Luo;" Y. Gao"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China;" Key Lab of Big Data Intelligent Computing of Zhejiang Province, Zhejiang University, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2022"",""2022"",""33"",""10"",""2358"",""2371"",""Set reachability query in directed graphs has a plethora of graph-based applications such as dependency analysis and graph centrality calculation. Given two sets $S$S and $T$T of source and target vertices, set reachability query needs to acquire all pairs $(s,t)$(s,t) where $s{\in }S$s∈S, $t{\in }T$t∈T and $s$s can reach $t$t. The state-of-the-art approach distributed set reachability (DSR) investigates the set reachability query in a distributed environment and adopts a static graph-based index to enhance the query efficiency. Nevertheless, DSR needs to store the graph-based index in all partitions, which causes a huge space overhead. Furthermore, it cannot efficiently solve the negative query $(s,t)$(s,t) where $s$s cannot reach $t$t, since DSR needs to traverse the whole reachable paths and becomes unable to efficiently reduce the computations. To alleviate these issues, we propose a novel multi-level 2-hop (ML2hop) index for the set reachability query in a distributed environment. Based on ML2hop, we further present a bi-directional query algorithm, called MLQA, to achieve efficient support for both positive and negative queries in Pregel-like systems. Generally, MLQA is equipped with the following three significant properties: (1) Low computation costs. It reduces redundant local computations in each partition by controlling the rounds of path traversals. (2) Low communication costs. It restricts the message exchange among different partitions within one single round with guaranteed accuracy of query results. (3) High parallelism. It adopts a bi-directional query technique for message propagation, achieving the better query efficiency than the forward-traversal query strategy utilized in DSR. Experimental results over several real-world graphs demonstrate that MLQA significantly outperforms the state-of-the-art algorithm by up to two orders of magnitude speedup."",""1558-2183"","""",""10.1109/TPDS.2021.3139111"",""National Key Research and Development Program of China(grant numbers:2020YFB2104000)"; National Natural Science Foundation of China(grant numbers:62172146,62172157); Open Research Projects of Zhejiang Lab(grant numbers:2021KD0AB02); Hunan Leading plan for scientific and technological innovation of high-tech industries(grant numbers:2020GK2037);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665221"",""2-hop labeling";distributed processing;indexing;"set reachabiity"",""Indexes";Partitioning algorithms;Parallel processing;Directed graphs;Costs;Computational modeling;"Bidirectional control"","""",""3"","""",""24"",""IEEE"",""29 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"An In-Depth Study of Microservice Call Graph and Runtime Performance,""S. Luo"; H. Xu; C. Lu; K. Ye; G. Xu; L. Zhang; J. He;" C. Xu"",""Shenzhen Institute of Advanced Technology, CAS, University of CAS, Beijing, China"; University of Macau, Taipa, Macau, China; Shenzhen Institute of Advanced Technology, CAS, University of CAS, Beijing, China; Shenzhen Institute of Advanced Technology, CAS, University of CAS, Beijing, China; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China;" University of Macau, Taipa, Macau, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3901"",""3914"",""Loosely-coupled and light-weight microservices running in containers are replacing monolithic applications gradually. Understanding the characteristics of microservices is critical to make good use of microservice architectures. However, there is no comprehensive study about microservice and its related systems in production environments so far. In this paper, we present a solid analysis of large-scale deployments of microservices at Alibaba clusters. Our study focuses on the characterization of microservice dependency as well as its runtime performance. We conduct an in-depth anatomy of microservice call graphs to quantify the difference between them and traditional DAGs of data-parallel jobs. In particular, we observe that microservice call graphs are heavy-tail distributed and their topology is similar to a tree and moreover, many microservices are hot-spots. We also discover that the structure of call graphs for long-term developed applications is much simpler so as to provide better performance. Our investigation on microservice runtime performance indicates most microservices are much more sensitive to CPU interference than memory interference. Moreover, we design resource management policies to efficiently tune memory resources."",""1558-2183"","""",""10.1109/TPDS.2022.3174631"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010164003)"; National Natural Science Foundation of China(grant numbers:62072451); Start-up Research Grant of University of Macau(grant numbers:SRG2021-00004-FST); Alibaba Innovative Research Program; Youth Innovation Promotion Association of the Chinese Academy of Sciences(grant numbers:2019349);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774016"",""Trace analysis";microservice;"performance characterization"",""Microservice architectures";Runtime;Containers;Interference;Topology;Production;"Memory management"","""",""16"","""",""53"",""IEEE"",""12 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Anomaly Detection and Anticipation in High Performance Computing Systems,""A. Borghesi"; M. Molan; M. Milano;" A. Bartolini"",""DISI and DEI Department, University of Bologna, Bologna, Italy"; DISI and DEI Department, University of Bologna, Bologna, Italy; DISI and DEI Department, University of Bologna, Bologna, Italy;" DISI and DEI Department, University of Bologna, Bologna, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""739"",""750"",""In their quest toward Exascale, High Performance Computing (HPC) systems are rapidly becoming larger and more complex, together with the issues concerning their maintenance. Luckily, many current HPC systems are endowed with data monitoring infrastructures that characterize the system state, and whose data can be used to train Deep Learning (DL) anomaly detection models, a very popular research area. However, the lack of labels describing the state of the system is a wide-spread issue, as annotating data is a costly task, generally falling on human system administrators and thus does not scale toward exascale. In this article we investigate the possibility to extract labels from a service monitoring tool (Nagios) currently used by HPC system administrators to flag the nodes which undergo maintenance operations. This allows to automatically annotate data collected by a fine-grained monitoring infrastructure"; this labelled data is then used to train and validate a DL model for anomaly detection. We conduct the experimental evaluation on a tier-0 production supercomputer hosted at CINECA, Bologna, Italy. The results reveal that the DL model can accurately detect the real failures, and, moreover, it can predict the insurgency of anomalies, by systematically anticipating the actual labels (i.e., the moment when system administrators realize when an anomalous event happened);" the average advance time computed on historical traces is around 45 minutes. The proposed technology can be easily scaled toward exascale systems to easy their maintenance."",""1558-2183"","""",""10.1109/TPDS.2021.3082802"",""EU H2020-ICT-11-2018-2019 IoTwins(grant numbers:857191)"; H2020-JTI-EuroHPC-2019-1 Regale(grant numbers:956560); Emilia-Romagna POR-FESR 2014-2020;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439169"",""High performance computing";anomaly detection;"deep learning"",""Supercomputers";Data models;Monitoring;Anomaly detection;Tools;Bridges;"Computational modeling"","""",""15"","""",""40"",""IEEE"",""21 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Artemis: A Latency-Oriented Naming and Routing System,""X. Li"; Y. Chen; M. Zhou; T. Guo; C. Wang; Y. Xiao; J. Wan;" X. Wang"",""School of Computer Science, Fudan University, Shanghai, China"; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; Department of Communications and Networking, Aalto University, Espoo, Finland; Huawei Technologies Co. Ltd., Shenzhen, China;" School of Computer Science, Fudan University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Sep 2022"",""2022"",""33"",""12"",""4874"",""4890"",""Today, Internet service deployment is typically implemented with server replication at multiple locations. Domain name system (DNS), which translates human-readable domain names into network-routable IP addresses, is typically used for distributing users to different server replicas. However, DNS relies on several network-based queries and the queries delay the connection setup process between the client and the server replica. In this article, we propose Artemis, a practical low-latency naming and routing system that supports optimal server (replica) selection based on user-defined policies and provides lower query latencies than DNS. Artemis uses a DNS-like domain name-IP mapping for replica selection and achieves low query latency by combining the name resolution process with the transport layer handshake process. In Artemis, all server replicas at different locations share the same anycast IP address, called Service Address. Clients use the Service Address to establish a transport layer connection with the server. The client's initial handshake packet is routed over an overlay network to reach the optimal server. Then the server migrates the transport layer connection to its original unicast IP address after finishing the handshake process. After that, service discovery is completed, and the client communicates with the server directly via IP addresses. To validate the effectiveness of Artemis, we evaluate its performance via both real trace-driven simulation and real-world deployment. The result shows that Artemis can handle a large number of connections and reduce the connection setup latency compared with state-of-the-art solutions. More specifically, our deployment across 11 Google data centers shows that Artemis reduces the connection setup latency by 39.4% compared with DNS."",""1558-2183"","""",""10.1109/TPDS.2022.3207189"",""National Natural Science Foundation of China(grant numbers:61971145)"; HUAWEI(grant numbers:YBN2019125184); Academy of Finland(grant numbers:317432);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894091"",""Service discovery";name resolution;overlay routing;"anycast"",""Servers";Routing;IP networks;Internet;Delays;Data centers;"Overlay networks"","""",""2"","""",""56"",""IEEE"",""16 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"ASTRAEA: A Fair Deep Learning Scheduler for Multi-Tenant GPU Clusters,""Z. Ye"; P. Sun; W. Gao; T. Zhang; X. Wang; S. Yan;" Y. Luo"",""School of Computer Science, Peking University, Beijing, China"; SenseTime Research, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science, Peking University, Beijing, China; SenseTime Research, Beijing, China;" School of Computer Science, Peking University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2781"",""2793"",""Modern GPU clusters are designed to support distributed Deep Learning jobs from multiple tenants concurrently. Each tenant may have varied and dynamic resource demands. Unfortunately, existing GPU schedulers fail to thoroughly consider the fairness among the tenants and jobs, which can result in unbalanced resource allocation and unfair user experience. In this article, we present an efficient solution to provide strong fairness while maintaining high scheduling effectiveness in multi-tenant GPU clusters. First, we introduce a novel Long-Term GPU-time Fairness metric, which can comprehensively evaluate the fairness at both the tenant and job levels, based on both the temporal and spatial impacts of resource allocation. Second, we design a new and practical GPU scheduler, Astraea, to enforce the desired fairness among tenants and jobs. Large-scale evaluations show that Astraea can improve tenant fairness by up to 9.42× compared to state-of-the-art schedulers, without sacrificing the average job completion time."",""1558-2183"","""",""10.1109/TPDS.2021.3136245"",""National Natural Science Foundation of China(grant numbers:62032001,61672053,U1611461,62032008)"; RIE2020 Industry Alignment Fund - Industry Collaboration Projects;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9655467"",""Distributed systems";deep learning;"GPU cluster scheduling"",""Graphics processing units";Resource management;Training;Measurement;Deep learning;Venus;"Dynamic scheduling"","""",""2"","""",""55"",""IEEE"",""17 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Astrea: Auto-Serverless Analytics Towards Cost-Efficiency and QoS-Awareness,""J. Jarachanthan"; L. Chen; F. Xu;" B. Li"",""School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA"; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Computer Science and Technology, East China Normal University, Shanghai, China;" Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3833"",""3849"",""With the ability to simplify the code deployment with one-click upload and lightweight execution, serverless computing has emerged as a promising paradigm with increasing popularity. However, there remain open challenges when adapting data-intensive analytics applications to the serverless context, in which users of serverless analytics encounter the difficulty in coordinating computation across different stages and provisioning resources in a large configuration space. This paper presents our design and implementation of Astrea, which configures and orchestrates serverless analytics jobs in an autonomous manner, while taking into account flexibly-specified user requirements. Astrea relies on the modeling of performance and cost which characterizes the intricate interplay among multi-dimensional factors (e.g., function memory size, degree of parallelism at each stage). We formulate an optimization problem based on user-specific requirements towards performance enhancement or cost reduction, and develop a set of algorithms based on graph theory to obtain the optimal job execution. We deploy Astrea in the AWS Lambda platform and conduct real-world experiments over representative benchmarks, including Big Data analytics and machine learning workloads, at different scales. Extensive results demonstrate that Astrea can achieve the optimal execution decision for serverless data analytics, in comparison with various provisioning and deployment baselines. For example, when compared with three provisioning baselines, Astrea manages to reduce the job completion time by 21% to 69% under a given budget constraint, while saving cost by 20% to 84% without violating performance requirements."",""1558-2183"","""",""10.1109/TPDS.2022.3172069"",""National Science Foundation(grant numbers:OIA-2019511)"; Louisiana Board of Regents(grant numbers:LEQSF(2019-22)-RD-A-21); National Natural Science Foundation of China(grant numbers:61972158); Science and Technology Commission of Shanghai Municipality(grant numbers:20511102802,18DZ2270800); RGC RIF(grant numbers:R6021-20); RGC GRF(grant numbers:16209120,16200221);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9767624"",""Cloud computing";serverless computing;resource provisioning;modeling;"optimization"",""Costs";Serverless computing;Data analysis;Quality of service;Optimization;Machine learning;"Parallel processing"","""","""","""",""54"",""IEEE"",""3 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"AUCTION: Automated and Quality-Aware Client Selection Framework for Efficient Federated Learning,""Y. Deng"; F. Lyu; J. Ren; H. Wu; Y. Zhou; Y. Zhang;" X. Shen"",""Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, P.R. China"; School of Computer Science and Engineering, Central South University, Changsha, P.R. China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, P.R. China; Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, P.R. China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, P.R. China;" Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Dec 2021"",""2022"",""33"",""8"",""1996"",""2009"",""The emergency of federated learning (FL) enables distributed data owners to collaboratively build a global model without sharing their raw data, which creates a new business chance for building data market. However, in practical FL scenarios, the hardware conditions and data resources of the participant clients can vary significantly, leading to different positive/negative effects on the FL performance, where the client selection problem becomes crucial. To this end, we propose AUCTION, an Automated and qUality-aware Client selecTION framework for efficient FL, which can evaluate the learning quality of clients and select them automatically with quality-awareness for a given FL task within a limited budget. To design AUCTION, multiple factors such as data size, data quality, and learning budget that can affect the learning performance should be properly balanced. It is nontrivial since their impacts on the FL model are intricate and unquantifiable. Therefore, AUCTION is designed to encode the client selection policy into a neural network and employ reinforcement learning to automatically learn client selection policies based on the observed client status and feedback rewards quantified by the federated learning performance. In particular, the policy network is built upon an encoder-decoder deep neural network with an attention mechanism, which can adapt to dynamic changes of the number of candidate clients and make sequential client selection actions to reduce the learning space significantly. Extensive experiments are carried out based on real-world datasets and well-known learning models to demonstrate the efficiency, robustness, and scalability of AUCTION."",""1558-2183"","""",""10.1109/TPDS.2021.3134647"",""National Natural Science Foundation of China(grant numbers:62002389,62122095,62072472,U19A2067)"; Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010137005); National Key Research and Development Program of China(grant numbers:2019YFA0706403); Natural Science Foundation of Hainan Province(grant numbers:2020JJ2050,2021JJ20079); Higher Education Discipline Innovation Project(grant numbers:B18059); Young Elite Scientists Sponsorship Program by Tianjin(grant numbers:YESS20200238); Young Talents Plan of Hunan Province of China(grant numbers:2019RS2001,2021RC3004);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647925"",""Federated learning";distributed system;client selection;data quality;"reinforcement learning"",""Data models";Training;Distributed databases;Task analysis;Data integrity;Collaborative work;"Data privacy"","""",""42"","""",""50"",""IEEE"",""13 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Auto-GNAS: A Parallel Graph Neural Architecture Search Framework,""J. Chen"; J. Gao; Y. Chen; B. M. Oloulade; T. Lyu;" Z. Li"",""School of Computer Science and Engineering, Central South University, Changsha, Hunan, China"; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; State Grid Hunan Electric Power Company Limited, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China;" Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, Hangzhou, Zhejiang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3117"",""3128"",""Graph neural networks (GNNs) have received much attention as GNNs have recently been successfully applied on non-euclidean data. However, artificially designed graph neural networks often fail to get satisfactory model performance for a given graph data. Graph neural architecture search effectively constructs the GNNs that achieve the expected model performance with the rise of automatic machine learning. The challenge is efficiently and automatically getting the optimal GNN architecture in a vast search space. Existing search methods serially evaluate the GNN architectures, severely limiting system efficiency. To solve these problems, we develop an Automatic Graph Neural Architecture Search framework (Auto-GNAS) with parallel estimation to implement an automatic graph neural search process that requires almost no manual intervention. In Auto-GNAS, we design the search algorithm with multiple genetic searchers. Each searcher can simultaneously use evaluation feedback information, information entropy, and search results from other searchers based on sharing mechanism to improve the search efficiency. As far as we know, this is the first work using parallel computing to improve the system efficiency of graph neural architecture search. According to the experiment on the real datasets, Auto-GNAS obtain competitive model performance and better search efficiency than other search algorithms. Since the parallel estimation ability of Auto-GNAS is independent of search algorithms, we expand different search algorithms based on Auto-GNAS for scalability experiments. The results show that Auto-GNAS with varying search algorithms can achieve nearly linear acceleration with the increase of computing resources."",""1558-2183"","""",""10.1109/TPDS.2022.3151895"",""National Natural Science Foundation of China(grant numbers:61873288,61836016)"; State Grid Hunan Electirc Power Company(grant numbers:5216A6210075); Hunan Key Laboratory for Internet of Things in Electricity; CAAI-Huawei MindSpore Open Fund; Hunan Provincial Science and Technology Program(grant numbers:2021JJ30055);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714826"",""Neural architecture search";parallel search;"graph neural network"",""Computer architecture";Estimation;Graph neural networks;Genetics;Search problems;Prediction algorithms;"Parallel processing"","""",""5"","""",""29"",""IEEE"",""16 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Automated Scheduling Algorithm Selection and Chunk Parameter Calculation in OpenMP,""A. Mohammed"; J. H. M. Korndörfer; A. Eleliemy;" F. M. Ciorba"",""HPE's HPC/AI EMEA Research Lab (ERL), Basel, Switzerland"; Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland; Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland;" Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4383"",""4394"",""Increasing node and cores-per-node counts in supercomputers render scheduling and load balancing critical for exploiting parallelism. OpenMP applications can achieve high performance via careful selection of scheduling kind and chunk parameters on a per-loop, per-application, and per-system basis from a portfolio of advanced scheduling algorithms (Korndörfer et al., 2022). This selection approach is time-consuming, challenging, and may need to change during execution. We propose Auto4OMP, a novel approach for automated load balancing of OpenMP applications. With Auto4OMP, we introduce three scheduling algorithm selection methods and an expert-defined chunk parameter for OpenMP's schedule clause's kind and chunk, respectively. Auto4OMP extends the OpenMP schedule(auto) and chunk parameter implementation in LLVM's OpenMP runtime library to automatically select a scheduling algorithm and calculate a chunk parameter during execution. Loop characteristics are inferred in Auto4OMP from the loop execution over the application's time-steps. The experiments performed in this work show that Auto4OMP improves applications performance by up to $11\%$11% compared to LLVM's schedule(auto) implementation and outperforms manual selection. Auto4OMP improves MPI+OpenMP applications performance by explicitly minimizing thread- and implicitly reducing process-load imbalance."",""1558-2183"","""",""10.1109/TPDS.2022.3189270"",""Swiss National Science Foundation(grant numbers:169123)"; Swiss Platform for Advanced Scientific Computing; DAPHNE; European Union's Horizon 2020 research and innovation programme(grant numbers:957407);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825675"",""Automatic selection";algorithm selection problem;dynamic load balancing;self-scheduling;runtime library;OpenMP;multithreaded programming;"shared-memory systems"",""Scheduling algorithms";Heuristic algorithms;Runtime library;Dynamic scheduling;Load management;Standards;"Parallel processing"","""",""1"","""",""45"",""CCBY"",""11 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Automatic Generation of High-Performance Convolution Kernels on ARM CPUs for Deep Learning,""J. Meng"; C. Zhuang; P. Chen; M. Wahib; B. Schmidt; X. Wang; H. Lan; D. Wu; M. Deng; Y. Wei;" S. Feng"",""Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China"; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan; National Institute of Advanced Industrial Science and Technology (AIST), Tokyo, Japan; Institute of Computer Science, Johannes Gutenberg University Mainz, Mainz, Germany; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Tencent AI Lab, Shenzhen, Guangdong, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China; Tencent AI Lab, Shenzhen, Guangdong, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, Guangdong, China;" National Supercomputer Center in Shenzhen, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2885"",""2899"",""We present FastConv, a template-based code auto-generation open-source library that can automatically generate high-performance deep learning convolution kernels of arbitrary matrices/tensors shapes. FastConv is based on the Winograd algorithm, which is reportedly the highest performing algorithm for the time-consuming layers of convolutional neural networks. ARM CPUs cover a wide range of designs and specifications, from embedded devices to HPC-grade CPUs. The leads to the dilemma of how to consistently optimize Winograd-based convolution solvers for convolution layers of different shapes. FastConv addresses this problem by using templates to auto-generate multiple shapes of tuned kernels variants suitable for skinny tall matrices. As a performance portable library, FastConv transparently searches for the best combination of kernel shapes, cache tiles, scheduling of loop orders, packing strategies, access patterns, and online/offline computations. Auto-tuning is used to search the parameter configuration space for the best performance for a given target architecture and problem size. Results show 1.02x to 1.40x, 1.14x to 2.17x, and 1.22x and 2.48x speedup is achieved over NNPACK, ARM NN, and FeatherCNN on Kunpeng 920. Furthermore, performance portability experiments with various convolution shapes show that FastConv achieves 1.2x to 1.7x speedup and 2x to 22x speedup over NNPACK and ARM NN inference engine using Winograd on Kunpeng 920. CPU performance portability evaluation on VGG–16 show an average speedup over NNPACK of 1.42x, 1.21x, 1.26x, 1.37x, 2.26x, and 11.02x on Kunpeng 920, Snapdragon 835, 855, 888, Apple M1, and AWS Graviton2, respectively."",""1558-2183"","""",""10.1109/TPDS.2022.3146257"",""National Key Research and Development Program of China(grant numbers:2018YFB0204403)"; Strategic Priority CAS(grant numbers:XDB38050100); National Natural Science Foundation of China(grant numbers:U1813203); Shenzhen Basic Research Fund(grant numbers:RCYX2020071411473419,KQTD20200820113106007,JSGG20190220164202211); CAS Key Lab(grant numbers:2011DP173015); JST, PRESTO(grant numbers:JPMJPR20MA); JSPS KAKENHI(grant numbers:JP21K17750); AIST Emerging Research, Japan(grant numbers:AAZ2029701B); Artificial Intelligence Initiative at Oak Ridge National Laboratory; UT-Battelle(grant numbers:DE-AC05-00OR22725); U.S. Department of Energy;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695263"",""AI";convolution;"deep learning"",""Convolution";Program processors;Libraries;Tensors;Shape;Codes;"Artificial intelligence"","""",""1"","""",""60"",""IEEE"",""27 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Bandwidth-Aware Scheduling Repair Techniques in Erasure-Coded Clusters: Design and Analysis,""H. Zhou"; D. Feng;" Y. Hu"",""Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education of China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, Engineering Research Center of Data Storage Systems and Technology, Ministry of Education of China, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2022"",""2022"",""33"",""12"",""3333"",""3348"",""Erasure codes offer a storage-efficient redundancy mechanism for maintaining data availability guarantees in storage clusters, yet also incur high network traffic consumption and recovery time in failure repair. Extensive research has been carried out to reduce the recovery time. However, previous works either target specific erasure code constructions which are not commonly used in today’s distributed storage clusters or neglect the heterogeneous bandwidth property in real network environments. Since erasure-coded clusters are typically composed of multi-node with heterogeneous bandwidth and accessed in parallel, the whole recovery time is mainly restricted by the low-bandwidth links. In this article, we propose SMFRepair, a single-node multi-level forwarding repair technique that is designed to improve the performance in heterogeneous networks based on Reed-Solomon codes for general fault tolerance. SMFRepair carefully selects the helper nodes and uses idle nodes to bypass low-bandwidth links. Idle nodes have sufficient and unused network bandwidth. It also pipelines the repair links that are optimized by idle nodes. Furthermore, a multi-node scheduling repair technique, called MSRepair, is proposed. MSRepair carefully schedules the multi-node repair link to saturate the most unoccupied bandwidth and transfers data from as large-bandwidth links as possible, with the primary objective of minimizing the recovery time. Large-scale simulation and Amazon EC2 real experiments show that compared to state-of-the-art repair techniques, SMFRepair can accelerate the single-node recovery by up to 47.69%, and MSRepair can reduce the multi-node recovery time by 33.78%$\sim$∼67.53%."",""1558-2183"","""",""10.1109/TPDS.2022.3153061"",""National Key R&D Program of China(grant numbers:2018YFB1003305)"; National Natural Science Foundation of China(grant numbers:61872414); Key Laboratory of Information Storage System Ministry of Education of China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720121"",""Erasure coding";recovery time;heterogeneous network;"repair link"",""Maintenance engineering";Bandwidth;Codes;Encoding;Task analysis;Heterogeneous networks;"Data centers"","""",""5"","""",""62"",""IEEE"",""23 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"BARM: A Batch-Aware Resource Manager for Boosting Multiple Neural Networks Inference on GPUs With Memory Oversubscription,""Z. -W. Qiu"; K. -S. Liu;" Y. -S. Chen"",""Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan"; Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan;" Department of Electrical Engineering, National Taiwan University of Science and Technology, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Aug 2022"",""2022"",""33"",""12"",""4612"",""4624"",""Modern intelligent devices usually execute multiple neural networks to improve service quality. However, system performance degrades significantly when the working set exceeds the physical memory capability, a phenomenon called memory oversubscription. To support the execution of multiple independent neural networks with limited physical memory, this article explores resource management in GPUs with unified virtual memory and demand paging. We first analyze the relationship between the simultaneous execution of multiple neural networks from streaming multiprocessors (SM) assignment and page fault overhead from memory thrashing. To boost performance by reducing the page fault penalty, we propose a batch-aware resource management approach, BARM, including (1) batch-aware SM resource allocation to increase the batch size and (2) thrashing-preventing memory allocation to eliminate run-time thrashing. The performance of the proposed method was evaluated using a series of workloads, and response latency is reduced significantly over the state-of-the-art page fault prefetcher and batch-aware TLP management. The proposed framework was also implemented on the real platform and evaluated by a case study, and impressive results were obtained."",""1558-2183"","""",""10.1109/TPDS.2022.3199806"",""Ministry of Science and Technology, Taiwan(grant numbers:MOST 107-2221-E-011-028-MY3,MOST 110-2628-E-011-002)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9861695"",""Memory oversubscription";memory management;"memory thrashing"",""Graphics processing units";Neural networks;Memory management;Resource management;Kernel;Registers;"Random access memory"","""",""1"","""",""51"",""IEEE"",""18 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Batch Crowdsourcing for Complex Tasks Based on Distributed Team Formation in E-Markets,""J. Jiang"; K. Di; B. An; Y. Jiang; Z. Bu;" J. Cao"",""Jiangsu Provincial Key Laboratory of E-Business, School of Information Engineering, Nanjing University of Finance and Economics, Nanjing, China"; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Southeast University, Nanjing, China; Jiangsu Provincial Key Laboratory of E-Business, School of Information Engineering, Nanjing University of Finance and Economics, Nanjing, China;" Jiangsu Provincial Key Laboratory of E-Business, School of Information Engineering, Nanjing University of Finance and Economics, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3600"",""3615"",""Team formation has been extensively studied for complex task crowdsourcing in E-markets, in which a set of workers are hired to form a team to complete a complex task collaboratively. However, existing studies have two typical drawbacks: 1) each team is created for only one task, which may be costly and cannot accommodate crowdsourcing markets with a large number of tasks"; and 2) most existing studies form teams in a centralized manner by the requesters, which may place a heavy burden on requesters. In fact, we observe that many complex tasks at real-world crowdsourcing platforms have similar skill requirements and workers are often connected through social networks. Therefore, this paper explores distributed team formation-based batch crowdsourcing for complex tasks to address the drawbacks in existing studies, in which similar tasks can be addressed in a batch to reduce computational costs and workers can self-organize through their social networks to form teams. To solve such an NP-hard problem, this paper presents two approaches: one is to form a fixed team for all tasks in the batch; the other is to form a basic team that can be dynamically adjusted for each task in the batch. In comparison, the former approach has lower computational complexity but the latter approach performs better in reducing the total payments by requesters. With the experiments on a real-world dataset comparing with previous benchmark approaches, it is shown that the presented approaches have better performance in saving the costs of forming teams, payments by requesters, and communication among team members;" moreover, the presented approaches have higher success rate of tasks and much better scalability."",""1558-2183"","""",""10.1109/TPDS.2022.3161019"",""National Key Research and Development Program of China(grant numbers:2019YFB1405000)"; National Natural Science Foundation of China(grant numbers:62076060,61932007,92046026,71871109); Natural Science Foundation of Jiangsu Province(grant numbers:BK20201394);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739859"",""Batch crowdsourcing";complex tasks;team formation;social network;"distributed manner"",""Task analysis";Crowdsourcing;Social networking (online);Costs;Training;NP-hard problem;"Reliability"","""",""3"","""",""41"",""IEEE"",""22 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Benchmarking 50-Photon Gaussian Boson Sampling on the Sunway TaihuLight,""Y. Li"; L. Gan; M. Chen; Y. Chen; H. Lu; C. Lu; J. Pan; H. Fu;" G. Yang"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Microscale Nanoscience Laboratory and Department of Modern Physics, Hefei National Laboratory for Physical Sciences, University of Science and Technology of China, Hefei, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; National Supercomputing Center, Wuxi, China; Microscale Nanoscience Laboratory and Department of Modern Physics, Hefei National Laboratory for Physical Sciences, University of Science and Technology of China, Hefei, China; Microscale Nanoscience Laboratory and Department of Modern Physics, Hefei National Laboratory for Physical Sciences, University of Science and Technology of China, Hefei, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1357"",""1372"",""Boson sampling is expected to be an important milestone that will demonstrate quantum computational advantage (or quantum supremacy). This work establishes the benchmarking of Gaussian boson sampling (GBS) with threshold detection based on the Sunway TaihuLight supercomputer. To achieve the best performance and provide a competitive scenario for future quantum computing studies, the selected simulation algorithm is fully optimized based on a set of innovative approaches, including a parallel framework with almost perfect load balance and an instruction-level optimizing scheme based on a shortest-path-based instruction scheduling. In addition, data precision is carefully processed by an integer-instruction-based and multiple-precision fixed-point implementation, including 128- and 256-bit precison mode, which can be appropriately selected based on an adaptive precision optimizing scheme. Based on these methods, a highly efficient parallel quantum sampling algorithm is designed. The largest run enables us to obtain one Torontonian function of a $100\times 100$100×100 submatrix from 50-photon GBS within 20 hours in 128-bit precision and 2 days in 256-bit precision. To our knowledge, this was the largest quantum computing simulation based on Boson Sampling by using modern supercomputers."",""1558-2183"","""",""10.1109/TPDS.2021.3111185"",""National Key Research and Development Program of China(grant numbers:2020YFB0204700)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534667"",""Boson sampling simulation";quantum computation;parallel computing;"sunway TaihuLight supercomputer"",""Quantum computing";Photonics;Benchmark testing;Supercomputers;Detectors;Computational modeling;"Standards"","""",""3"","""",""72"",""IEEE"",""9 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"BLB-gcForest: A High-Performance Distributed Deep Forest With Adaptive Sub-Forest Splitting,""Z. Chen"; T. Wang; H. Cai; S. K. Mondal;" J. P. Sahoo"",""Shanghai Key Laboratory of Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, China"; Shanghai Key Laboratory of Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, China; Shanghai Key Laboratory of Trustworthy Computing, Software Engineering Institute, East China Normal University, Shanghai, China; Faculty of Information Technology, Macau University of Science and Technology, Macao, China;" Department of Computer Science & Information Technology, Institute of Technical Education and Research, Siksha ‘O’ Anusandhan University, Bhubaneswar, Odisha, India"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3141"",""3152"",""As an emulous alternative to deep neural networks, Deep Forest emerges with features like low complexity, fewer hyper-parameters, and good robustness, which are predominantly desired in distributed computing applications and ecosystems. Recently, an efficient distributed Deep Forest system, named ForestLayer, was proposed, designing a fine-grained sub-Forest-based task-parallel algorithm to improve the parallel computing efficiency of Deep Forest. However, the sub-Forest splitting of ForestLayer is static and one-off without adaptability to the computing environment, nevertheless, the size of splitting granularity has a significant impact on the system performance. To further improve the computing efficiency and scalability of the distributed Deep Forest, in this paper, we propose a novel distributed Deep Forest algorithm, named BLB-gcForest (Bag of Little Bootstraps-gcForest), which augments the gcForest (multi-Grained Cascade Forest) approach for constructing Deep Forest. BLB-gcForest carries out parallel computation for each tree in sub-Forests at a finer parallel granularity and integrates with the Bag of Little Bootstraps (BLB) mechanism to reduce massive transmitted feature instances for Cascade Forest Layers, utterly improving both computation efficiency and communication efficiency. Moreover, to solve the problem of the forest splitting granularity, we further design an adaptive sub-Forest splitting algorithm to ensure the maximum resource utilization for parallel computation of each sub-Forest. Experimental results on four well-known large-scale datasets, namely YEAST, LETTER, MNIST, CIFAR10, show that the training efficiency of BLB-gcForest achieves up to 20.3x and 1.64x speedups compared with the state-of-the-art gcForest and ForestLayer, respectively while guaranteeing higher accuracy and better robustness"",""1558-2183"","""",""10.1109/TPDS.2021.3133544"",""National Key Research and Development Program of China(grant numbers:2018AAA0100503,2020AAA0107400)"; Fundamental Research Funds for the Central Universities(grant numbers:40500-20103-222131);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645202"",""Deep forest";distributed computing;big data bootstrap;"distributed AI"",""Forestry";Computational modeling;Training;Parallel processing;Distributed computing;Data models;"Adaptation models"","""",""3"","""",""30"",""IEEE"",""10 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Blockchain Assisted Decentralized Federated Learning (BLADE-FL): Performance Analysis and Resource Allocation,""J. Li"; Y. Shao; K. Wei; M. Ding; C. Ma; L. Shi; Z. Han;" H. V. Poor"",""School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China"; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Data61, CSIRO, Sydney, NSW, Australia; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; School of Electronic and Optical Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Department of Electrical and Computer Engineering, University of Houston, Houston, TX, USA;" Department of Electrical and Computer Engineering, Princeton University, Princeton, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Mar 2022"",""2022"",""33"",""10"",""2401"",""2415"",""Federated learning (FL), as a distributed machine learning paradigm, promotes personal privacy by local data processing at each client. However, relying on a centralized server for model aggregation, standard FL is vulnerable to server malfunctions, untrustworthy servers, and external attacks. To address these issues, we propose a decentralized FL framework by integrating blockchain into FL, namely, blockchain assisted decentralized federated learning (BLADE-FL). In a round of the proposed BLADE-FL, each client broadcasts its trained model to other clients, aggregates its own model with received ones, and then competes to generate a block before its local training on the next round. We evaluate the learning performance of BLADE-FL, and develop an upper bound on the global loss function. Then we verify that this bound is convex with respect to the number of overall aggregation rounds $K$K, and optimize the computing resource allocation for minimizing the upper bound. We also note that there is a critical problem of training deficiency, caused by lazy clients who plagiarize others’ trained models and add artificial noises to disguise their cheating behaviors. Focusing on this problem, we explore the impact of lazy clients on the learning performance of BLADE-FL, and characterize the relationship among the optimal $K$K, the learning parameters, and the proportion of lazy clients. Based on the MNIST and Fashion-MNIST datasets, we see that the experimental results are consistent with the analytical ones. To be specific, the gap between the developed upper bound and experimental results is lower than $5\%$5%, and the optimized $K$K based on the upper bound can effectively minimize the loss function."",""1558-2183"","""",""10.1109/TPDS.2021.3138848"",""National Natural Science Foundation of China(grant numbers:61872184,62002170)"; Natural Science Foundation of Jiangsu Province(grant numbers:BK20210331); National Science Foundation(grant numbers:CNS-2128368,CNS-2107216,ECCS-2039716); Toyota Motor Corporation; Amazon Catalyst;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664296"",""Federated learning";blockchain;lazy client;"computing resource allocation"",""Blockchains";Training;Servers;Computational modeling;Upper bound;Resource management;"Privacy"","""",""49"","""",""53"",""IEEE"",""28 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Boosting Graph Embedding on a Single GPU,""A. A. Aljundi"; T. A. Akyildiz;" K. Kaya"",""Faculty of Engineering and Natural Sciences, Sabancı University, Istanbul, Turkey"; Faculty of Engineering and Natural Sciences, Sabancı University, Istanbul, Turkey;" Faculty of Engineering and Natural Sciences, Sabancı University, Istanbul, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3092"",""3105"",""Graphs are ubiquitous, and they can model unique characteristics and complex relations of real-life systems. Although using machine learning (ML) on graphs is promising, their raw representation is not suitable for ML algorithms. Graph embedding represents each node of a graph as a $d$d-dimensional vector which is more suitable for ML tasks. However, the embedding process is expensive, and CPU-based tools do not scale to real-world graphs. In this work, we present GOSH, a GPU-based tool for embedding large-scale graphs with minimum hardware constraints. GOSH employs a novel graph coarsening algorithm to enhance the impact of updates and minimize the work for embedding. It also incorporates a decomposition schema that enables any arbitrarily large graph to be embedded with a single GPU. As a result, GOSH sets a new state-of-the-art in link prediction both in accuracy and speed, and delivers high-quality embeddings for node classification at a fraction of the time compared to the state-of-the-art. For instance, it can embed a graph with over 65 million vertices and 1.8 billion edges in less than 30 minutes on a single GPU."",""1558-2183"","""",""10.1109/TPDS.2021.3129617"",""Scientific and Technological Research Council of Turkey"; EuroHPC Joint Undertaking(grant numbers:220N254,956213);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623416"",""Parallel graph embedding";graph coarsening;machine learning;GPU;link prediction;"node classification"",""Graphics processing units";Training;Tools;Task analysis;Kernel;Classification algorithms;"Distributed computing"","""","""","""",""34"",""CCBY"",""22 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Bridging the Gap between Deep Learning and Frustrated Quantum Spin System for Extreme-Scale Simulations on New Generation of Sunway Supercomputer,""M. Li"; J. Chen; Q. Xiao; F. Wang; Q. Jiang; X. Zhao; R. Lin; H. An; X. Liang;" L. He"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; CAS Key Lab of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China;" CAS Key Lab of Quantum Information, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 May 2022"",""2022"",""33"",""11"",""2846"",""2859"",""Efficient numerical methods are promising tools for delivering unique insights into the fascinating properties of physics, such as the highly frustrated quantum many-body systems. However, the computational complexity of obtaining the wave functions for accurately describing the quantum states increases exponentially with respect to particle number. Here we present a novel convolutional neural network (CNN) for simulating the two-dimensional highly frustrated spin-$1/2$   1 / 2    $J_1-J_2$    J 1  -  J 2     Heisenberg model, meanwhile the simulation is performed at an extreme scale system with low cost and high scalability. By ingenious employment of transfer learning and CNN’s translational invariance, we successfully investigate the quantum system with the lattice size up to $24\times 24$   24 × 24   , within 30 million cores of the new generation of sunway supercomputer. The final achievement demonstrates the effectiveness of CNN-based representation of quantum-state and brings the state-of-the-art record up to a brand-new level from both aspects of remarkable accuracy and unprecedented scales."",""1558-2183"","""",""10.1109/TPDS.2022.3145163"",""National Key Research and Development Program of China(grant numbers:2016YFB1000403)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693260"",""Quantum system";deep learning;new generation sunway supercomputer;"spin-1/2 J1 – J2 Heisenberg model"",""Quantum system";Lattices;Supercomputers;Stationary state;Wave functions;Monte Carlo methods;"Convolutional neural networks"","""",""7"","""",""47"",""IEEE"",""25 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Building High-Throughput Neural Architecture Search Workflows via a Decoupled Fitness Prediction Engine,""A. Keller Rorabaugh"; S. Caíno-Lores; T. Johnston;" M. Taufer"",""University of Tennessee at Knoxville, Knoxville, TN, USA"; University of Tennessee at Knoxville, Knoxville, TN, USA; Striveworks, Austin, TX, USA;" University of Tennessee at Knoxville, Knoxville, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2913"",""2926"",""Neural networks (NN) are used in high-performance computing and high-throughput analysis to extract knowledge from datasets. Neural architecture search (NAS) automates NN design by generating, training, and analyzing thousands of NNs. However, NAS requires massive computational power for NN training. To address challenges of efficiency and scalability, we propose PENGUIN, a decoupled fitness prediction engine that informs the search without interfering in it. PENGUIN uses parametric modeling to predict fitness of NNs. Existing NAS methods and parametric modeling functions can be plugged into PENGUIN to build flexible NAS workflows. Through this decoupling and flexible parametric modeling, PENGUIN reduces training costs: it predicts the fitness of NNs, enabling NAS to terminate training NNs early. Early termination increases the number of NNs that fixed compute resources can evaluate, thus giving NAS additional opportunity to find better NNs. We assess the effectiveness of our engine on 6,000 NNs across three diverse benchmark datasets and three state of the art NAS implementations using the Summit supercomputer. Augmenting these NAS implementations with PENGUIN can increase throughput by a factor of 1.6 to 7.1. Furthermore, walltime tests indicate that PENGUIN can reduce training time by a factor of 2.5 to 5.3."",""1558-2183"","""",""10.1109/TPDS.2022.3140681"",""National Science Foundation(grant numbers:1741057,1740990,1741040,1841758)"; Joint Directed Research Development; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9674227"",""Machine learning";artificial intelligence;performance prediction;"neural networks"",""Training";Artificial neural networks;Predictive models;Parametric statistics;Engines;Search problems;"Data models"","""",""4"","""",""55"",""CCBY"",""7 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Busy-Time Scheduling on Heterogeneous Machines: Algorithms and Analysis,""M. Liu";" X. Tang"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore";" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""3942"",""3958"",""We study a generalized busy-time scheduling model on heterogeneous machines. The input to the model includes a set of jobs and a set of machine types. Each job has a size and a time interval during which it should be processed. Each job is to be placed on a machine for execution. Different types of machines have distinct capacities and cost rates. The total size of the jobs running on a machine must always be kept within the machine's capacity, giving rise to placement restrictions for jobs of various sizes among the machine types. Each machine used is charged according to the time duration in which it is busy, i.e., it is processing jobs. The objective is to schedule the jobs into machines to minimize the total cost of all the machines used. We develop an $O(1)$O(1)-approximation algorithm in the offline setting and an $O(\mu)$O(μ)-competitive algorithm in the online setting (where $\mu$μ is the max/min job length ratio), both of which are asymptotically optimal. This article significantly improves the analysis of the algorithms over our preliminary work."",""1558-2183"","""",""10.1109/TPDS.2022.3176665"",""Ministry of Education - Singapore(grant numbers:MOE-T2EP20121-0005)"; Academic Research Fund(grant numbers:RG112/19 (S));" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779914"",""Busy time";scheduling;analysis of algorithms;approximation ratio;"competitive ratio"",""Costs";Approximation algorithms;Scheduling;Cloud computing;Servers;Schedules;"Optimal scheduling"","""",""1"","""",""25"",""IEEE"",""23 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Busy-Time Scheduling on Heterogeneous Machines: Algorithms and Analysis,""M. Liu";" X. Tang"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore";" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""3942"",""3958"",""We study a generalized busy-time scheduling model on heterogeneous machines. The input to the model includes a set of jobs and a set of machine types. Each job has a size and a time interval during which it should be processed. Each job is to be placed on a machine for execution. Different types of machines have distinct capacities and cost rates. The total size of the jobs running on a machine must always be kept within the machine's capacity, giving rise to placement restrictions for jobs of various sizes among the machine types. Each machine used is charged according to the time duration in which it is busy, i.e., it is processing jobs. The objective is to schedule the jobs into machines to minimize the total cost of all the machines used. We develop an $O(1)$O(1)-approximation algorithm in the offline setting and an $O(\mu)$O(μ)-competitive algorithm in the online setting (where $\mu$μ is the max/min job length ratio), both of which are asymptotically optimal. This article significantly improves the analysis of the algorithms over our preliminary work."",""1558-2183"","""",""10.1109/TPDS.2022.3176665"",""Ministry of Education - Singapore(grant numbers:MOE-T2EP20121-0005)"; Academic Research Fund(grant numbers:RG112/19 (S));" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779914"",""Busy time";scheduling;analysis of algorithms;approximation ratio;"competitive ratio"",""Costs";Approximation algorithms;Scheduling;Cloud computing;Servers;Schedules;"Optimal scheduling"","""",""1"","""",""25"",""IEEE"",""23 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CAMIG: Concurrency-Aware Live Migration Management of Multiple Virtual Machines in SDN-Enabled Clouds,""T. He"; A. N. Toosi;" R. Buyya"",""CLOUDS Lab, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia"; Department of Software Systems and Cybersecurity, Faculty of Information Technology, Monash University, Clayton, VIC, Australia;" CLOUDS Lab, School of Computing and Information Systems, University of Melbourne, Parkville, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2022"",""2022"",""33"",""10"",""2318"",""2331"",""By integrating Software-Defined Networking and cloud computing, virtualized networking and computing resources can be dynamically reallocated through live migration of Virtual Machines (VMs). Dynamic resource management such as load balancing and energy-saving policies can request multiple migrations when the algorithms are triggered periodically. There exist notable research efforts in dynamic resource management that alleviate single migration overheads, such as single migration time and co-location interference while selecting the potential VMs and migration destinations. However, by neglecting the resource dependency among potential migration requests, the existing solutions of dynamic resource management can result in the Quality of Service (QoS) degradation and Service Level Agreement (SLA) violations during the migration schedule. Therefore, it is essential to integrate both single and multiple migration overheads into VM reallocation planning. In this paper, we propose a concurrency-aware multiple migration selector that operates based on the maximal cliques and independent sets of the resource dependency graph of multiple migration requests. Our proposed method can be integrated with existing dynamic resource management policies. The experimental results demonstrate that our solution efficiently minimizes migration interference and shortens the convergence time of reallocation by maximizing the multiple migration performance while achieving the objective of dynamic resource management."",""1558-2183"","""",""10.1109/TPDS.2021.3139014"",""Australian Research Council(grant numbers:DP160102414)"; China Scholarship Council;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664322"",""Live migration";dynamic resource management;migration scheduling;software-defined networking;"cloud computing"",""Resource management";Dynamic scheduling;Heuristic algorithms;Costs;Cloud computing;Load modeling;"Interference"","""",""9"","""",""39"",""IEEE"",""28 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Capelin: Data-Driven Compute Capacity Procurement for Cloud Datacenters Using Portfolios of Scenarios,""G. Andreadis"; F. Mastenbroek; V. van Beek;" A. Iosup"",""Electrical Engineering, Mathematics and Computer Science Faculty, Delft University of Technology, Delft, CD, The Netherlands"; Electrical Engineering, Mathematics and Computer Science Faculty, Delft University of Technology, Delft, CD, The Netherlands; Electrical Engineering, Mathematics and Computer Science Faculty, Delft University of Technology, CD, The Netherlands;" Electrical Engineering, Mathematics & Computer Science faculty, Delft University of Technology, Delft, CD, The Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""25 Jun 2021"",""2022"",""33"",""1"",""26"",""39"",""Cloud datacenters provide a backbone to our digital society. Inaccurate capacity procurement for cloud datacenters can lead to significant performance degradation, denser targets for failure, and unsustainable energy consumption. Although this activity is core to improving cloud infrastructure, relatively few comprehensive approaches and support tools exist for mid-tier operators, leaving many planners with merely rule-of-thumb judgement. We derive requirements from a unique survey of experts in charge of diverse datacenters in several countries. We propose Capelin, a data-driven, scenario-based capacity planning system for mid-tier cloud datacenters. Capelin introduces the notion of portfolios of scenarios, which it leverages in its probing for alternative capacity-plans. At the core of the system, a trace-based, discrete-event simulator enables the exploration of different possible topologies, with support for scaling the volume, variety, and velocity of resources, and for horizontal (scale-out) and vertical (scale-up) scaling. Capelin compares alternative topologies and for each gives detailed quantitative operational information, which could facilitate human decisions of capacity planning. We implement and open-source Capelin, and show through comprehensive trace-based experiments it can aid practitioners. The results give evidence that reasonable choices can be worse by a factor of 1.5-2.0 than the best, in terms of performance degradation or energy consumption."",""1558-2183"","""",""10.1109/TPDS.2021.3084816"",""NWO Vidi MagnaData"; TOP OffSense;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444213"",""Cloud";procurement;capacity planning;datacenter;practitioner survey;"simulation"",""Capacity planning";Tools;Cloud computing;Containers;Topology;Procurement;"Planning"","""","""","""",""67"",""IEEE"",""28 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Characterizing Performance of Graph Neighborhood Communication Patterns,""S. Ghosh"; N. R. Tallent;" M. Halappanavar"",""Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA"; Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA;" Advanced Computing, Mathematics, and Data Division, Pacific Northwest National Laboratory, Richland, WA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""915"",""928"",""Distributed-memory graph algorithms are fundamental enablers in scientific computing and analytics workflows. A majority of graph algorithms rely on the graph neighborhood communication pattern, i.e., repeated asynchronous communication between a vertex and its neighbors in the graph. The pattern is adversarial for communication software and hardware due to high message injection rates and input-dependent, many-to-one traffic with variable destinations and volumes. We present benchmarks and performance analysis of graph neighborhood communication on modern large-scale network interconnects from four supercomputers: ALCF Theta, NERSC Cori, OLCF Summit and R-CCS Fugaku. Our benchmarks characterize communication from the perspectives of latency and throughput. Benchmark parameters make it possible to mimic the behaviors of complex applications on real world or synthetic graphs by varying work distribution, remote edges, message volume, and per-vertex work. We find that minor changes in the input graph can substantially increase latencies";" and contention can develop in memory caches and network stacks before contention in the network itself. Further, latencies and contention vary significantly for different graph neighborhoods, motivating the need for exploring asynchronous algorithms in greater detail. When adding work, load imbalance on real-world graphs can be pronounced: latencies for the 99th percentile were 8–128× than the corresponding average latencies. Our results help analysts and developers understand the performance implications of this important pattern, especially for the impending exascale platforms."",""1558-2183"","""",""10.1109/TPDS.2021.3101425"",""U.S. Department of Energy(grant numbers:DE-AC02-05CH11231,DE-AC02-06CH11357)"; RIKEN CCS; U.S. Department of Energy(grant numbers:17-SC-20-SC); Advanced Scientific Computing Research; Data-Model Convergence;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9503355"",""Graphs and networks";neighborhood communication;MPI;network communication;network contention;distributed memories;"benchmarking"",""Benchmark testing";Pattern matching;Heating systems;Clustering algorithms;Software;Topology;"Surges"","""",""1"","""",""52"",""IEEE"",""2 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cloud Object Storage Synchronization: Design, Analysis, and Implementation,""F. Chen"; Z. Li; C. Jiang; T. Xiang;" Y. Yang"",""College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China"; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science, Chongqing University, Chongqing, China;" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4295"",""4310"",""Cloud storage synchronization among different computing terminals has attracted large-scale uses among enterprise and individual users. It enables users to maintain the same copy of data in real time, which eases users the tedious yet error-prone data management burden. However, existing cloud storage synchronization systems are in a closed form. Users are fixed to a certain cloud service provider, which makes it hard to transfer from one provider to another when balancing factors such as performance, cost, security, etc. To bridge this gap, this article proposes a new synchronization system based on standard cloud object storage. Specifically, we first formulate the cloud object storage synchronization problem by defining some useful concepts. We then use the idea of state encoding and a push-pull paradigm to propose a cloud object storage synchronization system. The proposed system supports real-time, multiple-terminal, and cloud-independent storage synchronization. We also prototyped the proposed system. The experimental results show that the proposed system is promising for practical usages."",""1558-2183"","""",""10.1109/TPDS.2022.3185067"",""National Natural Science Foundation of China(grant numbers:61872243,62072062,U20A20176,61902255)"; Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515011489); Science and Technology Plan Projects of Shenzhen(grant numbers:JCYJ20180305124126741,JCYJ20190808163417094); Natural Science Foundation of Chongqing(grant numbers:cstc2022ycjh-bgzxm0031);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9802905"",""Cloud object storage";synchronization;state encoding;push-pull paradigm;"system prototype"",""Cloud computing";Synchronization;Real-time systems;History;Computers;Browsers;"Costs"","""","""","""",""37"",""IEEE"",""21 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CNNPC: End-Edge-Cloud Collaborative CNN Inference With Joint Model Partition and Compression,""S. Yang"; Z. Zhang; C. Zhao; X. Song; S. Guo;" H. Li"",""National Engineering Laboratory for Big Data Analytics (NEL-BDA), Ministry of Education Key Laboratory for Intelligent Networks and Network Security (MOE KLINNS Lab), Xi'an Jiaotong University, Xi'an, China"; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China; Department of Computing, Imperial College London, London, U.K.; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China; National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China;" National Engineering Laboratory for Big Data Analytics (NEL-BDA), Xi'an Jiaotong University, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4039"",""4056"",""Edge Intelligence (EI) aims at addressing concerns like response latency risen by the conflict between predominating Cloud-based deployments of computationally intensive AI applications and the expensive uploading of explosive end data. Convolutional Neural Networks (CNNs) leading the latest flourish of AI inevitably suffer from the aforementioned conflict. There emerge increasing EI-driven attempts on fast CNN inference with high accuracy in the End-Edge-Cloud (EEC) collaborative computing paradigm, where, however, neither model compression approaches for on-device inference nor collaborative inference methods across devices can effectively achieve the trade-off between latency and accuracy of End-to-End (E2E) inference. In this article, we present CNNPC that jointly partitions and compresses CNNs for fast inference with high accuracy in collaborative EEC systems. We implemented CNNPC (source code available at https://github.com/IoTDATALab/CNNPC) and evaluated its performance within extensive real-world EEC scenarios. Experimental results demonstrate that, compared with state-of-the-art single-end and collaborative approaches, without obvious accuracy loss, collaborative inference based on CNNPC is up to $1.6\times$1.6× and $5.6\times$5.6× faster, and requires as low as $4.30\%$4.30% and $6.48\%$6.48% communications, respectively. Besides, when determines the optimal strategy, CNNPC requires as low as $0.1\%$0.1% actual compression operations that the traversal method (the only viable method providing the theoretically optimal strategy) requires."",""1558-2183"","""",""10.1109/TPDS.2022.3177782"",""National Key Research and Development Program of China(grant numbers:2020YFA0713900)"; National Natural Science Foundation of China(grant numbers:61772410,61802298,62172329,U1811461,U21A6005,11690011); China Postdoctoral Science Foundation(grant numbers:2020T130513,2019M663726);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782528"",""Edge computing";edge intelligence;collaborative CNN inference;"CNN partition and compression"",""Collaboration";Convolutional neural networks;Computational modeling;Solid modeling;Data models;Artificial intelligence;"Cloud computing"","""",""4"","""",""60"",""IEEE"",""26 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Co-Concurrency Mechanism for Multi-GPUs in Distributed Heterogeneous Environments,""X. Zhang"; Z. Tang; X. Zhang;" K. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China; Alibaba Group, Hangzhou, Zhejiang, China;" College of Computer Science and Electronic Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Oct 2022"",""2022"",""33"",""12"",""4935"",""4947"",""The high concurrency and high throughput characteristics of graphics processing units (GPUs) have made researchers continue to use it to optimize distributed parallel computing architectures. With the upgrading of processor architecture, GPUs allow multiple kernels to execute concurrently through stream queues. However, due to the different hardware characteristics and kernel properties in distributed architectures, existing research lacks careful consideration of optimization schemes for concurrent streams and kernel block sizes. Unreasonable stream concurrency and kernel block size configuration will lead to prolonged execution time and waste of computing resources during application execution. Therefore, we propose a multi-GPU multi-stream co-concurrency mechanism (MGSC) in a distributed heterogeneous environment, dynamically adjusting the number of concurrent streams and exploring the optimal block size in task scheduling. According to the memory resources and startup overhead occupied in concurrent stream scheduling, a resource-aware concurrent stream adaptive adjustment mechanism is proposed, which can dynamically adjust the number of streams. To explore the optimal block size, we abstract it as a multi-armed bandit problem (MAB) and propose a block size adjustment algorithm based on the upper confidence bound (UCB). We implement MGSC in Spark 3.1.1 and NVIDIA CUDA 11.2. We conduct comparative experiments with multiple typical benchmarks to evaluate the performance of MGSC. The experimental results show that the algorithm can make full use of the computing power of the GPU and significantly reduce the execution time of tasks."",""1558-2183"","""",""10.1109/TPDS.2022.3208082"",""National Key Research and Development Program of China(grant numbers:2018YFB1701400)"; National Natural Science Foundation of China(grant numbers:62225205,92055213,61873090); Guangdong Province research and development plan(grant numbers:2020B0101100001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9896169"",""Concurrent kernel execution";heterogeneous computing;kernel;"stream scheduling"",""Kernel";Graphics processing units;Task analysis;Processor scheduling;Concurrent computing;Computer architecture;"Hardware"","""","""","""",""36"",""IEEE"",""20 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Coarse Grained FPGA Overlay for Rapid Just-In-Time Accelerator Compilation,""A. K. Jain"; D. L. Maskell;" S. A. Fahmy"",""Xilinx Inc., San Jose, CA, USA"; School of Computer Science and Engineering, Nanyang Technological University, Singapore;" King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1478"",""1490"",""Coarse-grained FPGA overlays built around the runtime programmable DSP blocks in modern FPGAs can achieve high throughput and improved scalability compared to traditional overlays built without detailed consideration of FPGA architecture. These overlays can be mapped to using higher level compilers, achieving fast compilation, software-like programmability and run-time management, and high-level design abstraction. OpenCL allows programs running on a host computer to launch accelerator kernels which can be compiled at run-time for a specific architecture, thus enabling portability. However, prohibitive hardware compilation times in traditional design flows mean that the tools cannot effectively use just-in-time (JIT) compilation or runtime performance scaling on FPGAs. We present a methodology for runtime compilation of dataflow graphs expressed as OpenCL kernels onto coarse-grained overlays. The methodology benefits from the high level of abstraction afforded by using the OpenCL programming model, while the mapping to the overlay significantly reduces compilation and load times. Key characteristics of this work include highly performant DSP-optimized functional units that scale to large overlays on modern devices and the ability to perform automatic resource-aware kernel replication up to the size of the overlay. We demonstrate place and route times orders of magnitude better than traditional HLS flows, even when running on an embedded processor in the Xilinx Zynq."",""1558-2183"","""",""10.1109/TPDS.2021.3116859"",""Ministry of Education(grant numbers:MOE2017-T2-1-002)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555373"",""Field programmable gate arrays";parallel processing;"hardware accelerators"",""Field programmable gate arrays";Kernel;Computer architecture;Hardware;Runtime;Performance evaluation;"Throughput"","""",""2"","""",""60"",""CCBYNCND"",""30 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CoFilter: High-Performance Switch-Accelerated Stateful Packet Filter for Bare-Metal Servers,""J. Cao"; Y. Liu; Y. Zhou; L. He; C. Sun; Y. Wang;" M. Xu"",""Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China"; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Alibaba Infrastructure Service, Alibaba Group, Hangzhou, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Alibaba Infrastructure Service, Alibaba Group, Hangzhou, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China;" Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Feb 2022"",""2022"",""33"",""9"",""2249"",""2262"",""As one of the most critical cloud services, Bare-Metal Servers (BMS) introduce stringent performance requirements on data center networks (DCN). Stateful packet filter is an integral DCN component of ensuring connection security for BMS. However, the off-the-shelf stateful packet filters either are costly for cloud DCNs or introduce significant performance bottlenecks. In this article, we present CoFilter, which leverages low-cost programmable switches to accelerate the stateful packet filter for BMS. CoFilter uses (1) stateful process partition to enable complex stateful packet filtering logic on programmability-limited switching ASICs, (2) state compression to track tens of millions of connections with constrained hardware memory, and (3) per-tenant packet rate limit and tenant-aware flow migration to achieve efficient performance isolation among different tenants. Overall, CoFilter implements a high-performance stateful packet filter via the co-design of programmable switching ASIC and CPU. We evaluate CoFilter under various data center traffic traces with real-world flow distributions. The evaluation results show that CoFilter remarkably outperforms NetFilter, i.e., forwarding packets at line rate (13x throughput of NetFilter), keeping packet delay within 1us, and freeing a significant quantity of CPU cores, with rather small memory usage, i.e., accommodating over $10^7$107 connections with only 16MB SRAM."",""1558-2183"","""",""10.1109/TPDS.2021.3136575"",""National Natural Science Foundation of China(grant numbers:61772307)"; National Key Research and Development Program of China(grant numbers:2018YFB1800405); Tsinghua University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9656674"",""Bare-metal server stateful packet filter";"programmable switch"",""Servers";Random access memory;Hardware;Switches;Security;Delays;"Data centers"","""",""2"","""",""38"",""IEEE"",""20 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"COIN: A Container Workload Prediction Model Focusing on Common and Individual Changes in Workloads,""Z. Ding"; B. Feng;" C. Jiang"",""Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China"; Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China;" Key Laboratory of Embedded System and Service Computing, Ministry of Education, Tongji University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Sep 2022"",""2022"",""33"",""12"",""4738"",""4751"",""Recently, containers have become the primary deployment form for cloud applications. Predicting container workload accurately is critical to ensure the quality of service (QoS) and cost-efficiency of the applications and meet service level agreements (SLAs) with users. However, facing multiple challenges, including model unavailability due to insufficient data, model maladaptation due to dynamic workload changes, and model non-generalization due to changeable workload patterns in container workload prediction, existing methods have not yet provided a united and effective solution. To this end, we propose a novel integrated forecasting model named COIN that combines COmmon and INdividual changes in container workloads to ensure the availability, adaptivity, and generality of the prediction model based on transfer learning and online learning. Besides, we present a container similarity calculation algorithm for real cloud scenarios, which combines the static and dynamic information of containers and comprehensively depicts the similarity between containers. Through experiments based on two public datasets, the COIN model achieves a higher accuracy than existing state-of-the-art solutions, demonstrating the effectiveness and robustness of our proposed model, which provides a new solution to container workload prediction."",""1558-2183"","""",""10.1109/TPDS.2022.3202833"",""National Key Research and Development Program of China(grant numbers:2019YFB1704102)"; China National Scientific Seafloor Observatory;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9870561"",""Cloud Computing";container;workload prediction;container similarity;online learning;transfer learning;"integrated model"",""Containers";Predictive models;Data models;Adaptation models;Forecasting;Cloud computing;"Load modeling"","""",""3"","""",""50"",""IEEE"",""30 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Combinatorial BLAS 2.0: Scaling Combinatorial Algorithms on Distributed-Memory Systems,""A. Azad"; O. Selvitopi; M. T. Hussain; J. R. Gilbert;" A. Buluç"",""Indiana University, Bloomington, IN, USA"; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Indiana University, Bloomington, IN, USA; University of California, Santa Barbara, Santa Barbara, CA, USA;" Lawrence Berkeley National Laboratory, Berkeley, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""989"",""1001"",""Combinatorial algorithms such as those that arise in graph analysis, modeling of discrete systems, bioinformatics, and chemistry, are often hard to parallelize. The Combinatorial BLAS library implements key computational primitives for rapid development of combinatorial algorithms in distributed-memory systems. During the decade since its first introduction, the Combinatorial BLAS library has evolved and expanded significantly. This article details many of the key technical features of Combinatorial BLAS version 2.0, such as communication avoidance, hierarchical parallelism via in-node multithreading, accelerator support via GPU kernels, generalized semiring support, implementations of key data structures and functions, and scalable distributed I/O operations for human-readable files. Our article also presents several rules of thumb for choosing the right data structures and functions in Combinatorial BLAS 2.0, under various common application scenarios."",""1558-2183"","""",""10.1109/TPDS.2021.3094091"",""Advanced Scientific Computing Research(grant numbers:DE-AC02-05CH11231)"; National Science Foundation(grant numbers:1823034); Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration; National Science Foundation(grant numbers:CCF-1637564); U.S. Department of Energy(grant numbers:DE-AC05-00OR22725);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9470983"",""Sparse matrices";parallel computing;combinatorics;graph theory;"communication-avoidance algorithms"",""Sparse matrices";Libraries;Data structures;Indexes;Three-dimensional displays;Data analysis;"Computational modeling"","""",""8"","""",""46"",""IEEE"",""1 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"ComboTree: A Persistent Indexing Structure With Universal Operational Efficiency and Scalability,""Z. Wang"; T. Yao; J. Wan; H. Jiang; Q. Cui; L. Tang; Y. Zhang;" Q. Zhang"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China"; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; University of Texas at Arlington, Arlington, TX, USA; PingCAP, Beijing, China; PingCAP, Beijing, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China;" Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Mar 2022"",""2022"",""33"",""10"",""2277"",""2290"",""To leverage the larger-than-DRAM capacity and close-to-DRAM performance of persistent memory (PM) to build future memory systems, more scalable and efficient indexing structures are of paramount importance. However, as we evaluate existing PM indexing structures, we find that (1) both ordered and unordered indexing structures cannot support efficiently all KV operation types, i.e., Put, Get, Delete and Scan, and (2) the majority of indexes scale poorly as the PM capacity and dataset increases, i.e., the decreasing throughput of ordered indexes with the growth of dataset and the blocking of foreground requests by costly hash resizing of unordered indexes. To provide better operational efficiency on both point accesses and range queries for persistent memory in the ever-increasing volume of data, this article proposes ComboTree, a three-tiered indexing structure with a sorted key space. In ComboTree, we break the global B+Tree into multiple low height B+Trees (Tier C) and arrange them with a sorted array (Tier B). Further, we accelerate the lookup of the sorted array by cumulative distribution function (Tier A). Last but not least, a background resizing policy is proposed to avoid performance degradation when the capacity of the ComboTree grows. We implement and evaluate ComboTree on Intel’s Optane DCPMM. Test results show that ComboTree delivers $2.1\times -3.6\times$2.1×-3.6× put throughput and $1.5\times -2.1\times$1.5×-2.1× get throughput of the state-of-art sorted indexes. Furthermore, ComboTree is $1.27\times$1.27× faster than the efficient B+Tree variant in various scan granularities, and it is open-sourced1."",""1558-2183"","""",""10.1109/TPDS.2021.3137247"",""National Natural Science Foundation of China(grant numbers:62072196)"; National Natural Science Foundation of China(grant numbers:61821003); Science Technology and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20190809095001781); National Science Foundation(grant numbers:CNS-2008835);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658202"",""Indexing structure";persistent index;persistent memory;"key-value store"",""Indexing";Scalability;Throughput;Random access memory;Memory management;Distribution functions;"Currencies"","""",""1"","""",""61"",""IEEE"",""21 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Communication-Efficient $k$k-Means for Edge-Based Machine Learning,""H. Lu"; T. He; S. Wang; C. Liu; M. Mahdavi; V. Narayanan; K. S. Chan;" S. Pasteris"",""Pennsylvania State University, University Park, PA, USA"; Pennsylvania State University, University Park, PA, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; Pennsylvania State University, University Park, PA, USA; Pennsylvania State University, University Park, PA, USA; Army Research Laboratory, Adelphi, MD, USA;" University College London, London, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""4 Apr 2022"",""2022"",""33"",""10"",""2509"",""2523"",""We consider the problem of computing the $k$k-means centers for a large high-dimensional dataset in the context of edge-based machine learning, where data sources offload machine learning computation to nearby edge servers. $k$k-Means computation is fundamental to many data analytics, and the capability of computing provably accurate $k$k-means centers by leveraging the computation power of the edge servers, at a low communication and computation cost to the data sources, will greatly improve the performance of these analytics. We propose to let the data sources send small summaries, generated by joint dimensionality reduction (DR), cardinality reduction (CR), and quantization (QT), to support approximate $k$k-means computation at reduced complexity and communication cost. By analyzing the complexity, the communication cost, and the approximation error of $k$k-means algorithms based on carefully designed composition of DR/CR/QT methods, we show that: (i) it is possible to compute near-optimal $k$k-means centers at a near-linear complexity and a constant or logarithmic communication cost, (ii) the order of applying DR and CR significantly affects the complexity and the communication cost, and (iii) combining DR/CR methods with a properly configured quantizer can further reduce the communication cost without compromising the other performance metrics. Our theoretical analysis has been validated through experiments based on real datasets."",""1558-2183"","""",""10.1109/TPDS.2022.3144595"",""Army Research Laboratory"; Ministry of Defence(grant numbers:W911NF-16-3-0001); National Science Foundation(grant numbers:1822923);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9690512"",""k-Means";dimensionality reduction;coreset;random projection;quantization;"edge-based machine learning"",""Costs";Soft sensors;Approximation algorithms;Servers;Quantization (signal);Dimensionality reduction;"Heuristic algorithms"","""",""1"","""",""42"",""IEEE"",""25 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Communication-Efficient Federated Learning With Compensated Overlap-FedAvg,""Y. Zhou"; Q. Ye;" J. Lv"",""College of Computer Science, Sichuan University, Chengdu, China"; College of Computer Science, Sichuan University, Chengdu, China;" College of Computer Science, Sichuan University, Chengdu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2021"",""2022"",""33"",""1"",""192"",""205"",""While petabytes of data are generated each day by a number of independent computing devices, only a few of them can be finally collected and used for deep learning (DL) due to the apprehension of data security and privacy leakage, thus seriously retarding the extension of DL. In such a circumstance, federated learning (FL) was proposed to perform model training by multiple clients' combined data without the dataset sharing within the cluster. Nevertheless, federated learning with periodic model averaging (FedAvg) introduced massive communication overhead as the synchronized data in each iteration is about the same size as the model, and thereby leading to a low communication efficiency. Consequently, variant proposals focusing on the communication rounds reduction and data compression were proposed to decrease the communication overhead of FL. In this article, we propose Overlap-FedAvg, an innovative framework that loosed the chain-like constraint of federated learning and paralleled the model training phase with the model communication phase (i.e., uploading local models and downloading the global model), so that the latter phase could be totally covered by the former phase. Compared to vanilla FedAvg, Overlap-FedAvg was further developed with a hierarchical computing strategy, a data compensation mechanism, and a nesterov accelerated gradients (NAG) algorithm. In Particular, Overlap-FedAvg is orthogonal to many other compression methods so that they could be applied together to maximize the utilization of the cluster. Besides, the theoretical analysis is provided to prove the convergence of the proposed framework. Extensive experiments conducting on both image classification and natural language processing tasks with multiple models and datasets also demonstrate that the proposed framework substantially reduced the communication overhead and boosted the federated learning process."",""1558-2183"","""",""10.1109/TPDS.2021.3090331"",""National Key Research and Development Program of China(grant numbers:2017YFB1002201)"; National Natural Science Fund for Distinguished Young Scholar(grant numbers:61625204); National Natural Science Foundation of China(grant numbers:61836006);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459540"",""Distributed computing";federated learning;overlap;"efficient communication"",""Collaborative work";Data models;Training;Servers;Computational modeling;Convergence;"Deep learning"","""",""57"","""",""52"",""IEEE"",""17 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Communicational and Computational Efficient Federated Domain Adaptation,""H. Kang"; Z. Li;" Q. Zhang"",""Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China"; ByteDance Inc., Hangzhou, Zhejiang, China;" Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3678"",""3689"",""The emerging paradigm of Federated Learning enables mobile users to collaboratively train a model without disclosing their privacy-sensitive data. Nevertheless, data collected from different mobile users may not be independent and identically distributed. Thus directly applying the trained model to a new mobile user usually leads to performance degradation due to the so-called domain shift. Unsupervised Domain Adaptation is an effective technique to mitigate domain shift and transfer knowledge from labeled source domains to the unlabeled target domain. In this article, we design a Federated Domain Adaptation framework that extends Domain Adaptation with the constraints of Federated Learning to train a model for the target domain and preserve the data privacy of all the source and target domains. As mobile devices usually have limited computation and communication capabilities, we design a set of optimization methods that significantly enhance our framework’s computation and communication efficiency, making it more friendly to resource-constrained edge devices. Evaluation results on three datasets show that our framework has comparable performance with the standard centralized training approach, and the optimization methods can reduce the computation and communication overheads by up to two orders of magnitude."",""1558-2183"","""",""10.1109/TPDS.2022.3167457"",""RGC(grant numbers:CERG 16204418,16203719,16204820,R8015)"; Natural Science Foundation of Guangdong Province(grant numbers:2017A030312008);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757821"",""Federated learning";domain adaptation;"communicational efficient"",""Feature extraction";Training;Adaptation models;Computational modeling;Data models;Optimization methods;"Transfer learning"","""","""","""",""38"",""IEEE"",""14 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Compiler-Assisted Compaction/Restoration of SIMD Instructions,""J. M. Cebrian"; T. Balem; A. Barredo; M. Casas; M. Moretó; A. Ros;" A. Jimborean"",""Computer Engineering Department, University of Murcia, Murcia, Spain"; ENS Rennes, Rennes, France; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Barcelona Supercomputing Center, Barcelona, Spain; Computer Engineering Department, University of Murcia, Murcia, Spain;" Computer Engineering Department, University of Murcia, Murcia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""779"",""791"",""Vector processors (e.g., SIMD or GPUs) are ubiquitous in high performance systems. All the supercomputers in the world exploit data-level parallelism (DLP), for example by using single instructions to operate over several data elements. Improving vector processing is therefore key for exascale computing. However, despite its potential, vector code generation and execution have significant challenges. Among these challenges, control flow divergence is one of the main performance limiting factors. Most modern vector instruction sets, including SIMD, rely on predication to support divergence control. Nevertheless, the performance and energy consumption in predicated codes is usually insensitive to the number of active elements in a predicated mask. Since the trend is that vector register size increases, the energy efficiency of exascale computing systems will become sub-optimal. This article proposes a novel approach to improve execution efficiency in predicated vector codes, the Compiler-Assisted Compaction/Restoration (CACR) technique. Baseline CR delays predicated SIMD instructions with inactive elements, compacting active elements from instances of the same instruction of consecutive loop iterations. Compacted elements form an equivalent dense vector instruction. After executing the dense instructions, their results are restored to the original instructions. However, CR has a significant performance and energy penalty when it fails to find active elements, either due to lack of resources when unrolling or because of inter-loop dependencies. In CACR, the compiler analyzes the code looking for key information required to configure CR. Then, it passes this information to the processor via new instructions inserted in the code. This prevents CR from waiting for active elements on scenarios when it would fail to form dense instructions. Simulated results (gem5) show that CACR improves performance by up to 29 percent and reduces dynamic energy by up to 24.2 percent on average, for a a set of applications with predicated execution. The baseline CR only achieves 18.6 percent performance and 14 percent energy improvements for the same configuration and applications."",""1558-2183"","""",""10.1109/TPDS.2021.3091015"",""Spanish Government(grant numbers:SEV2015-0493,BES-2017-080635)"; Ministerio de Ciencia e Innovación(grant numbers:PID2019-107255GB-C21/AEI/10.13039/501100011033,RTI2018-098156-B-C53); ECHO and RoMoL ERC(grant numbers:819134,321253); European HiPEAC Network(grant numbers:EU-FP7-610402,EU-H2020-779877); Spanish Ministry of Economy, Industry and Competitiveness(grant numbers:RYC-2016-21104,RYC-2017-23269,RYC-2018-025200-I);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462482"",""SIMD";predication;LLVM;"density-time performance"",""Registers";Parallel processing;Hardware;Computer architecture;Out of order;Delays;"Energy consumption"","""","""","""",""48"",""IEEE"",""22 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Completely Independent Spanning Trees on BCCC Data Center Networks With an Application to Fault-Tolerant Routing,""X. -Y. Li"; W. Lin; X. Liu; C. -K. Lin; K. -J. Pai;" J. -M. Chang"",""College of Computer and Data Science, Fuzhou University, Fuzhou, China"; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; College of Computer and Data Science, Fuzhou University, Fuzhou, China; Department of Industrial Engineering and Management, Ming Chi University of Technology, New Taipei, Taiwan;" Institute of Information and Decision Sciences, National Taipei University of Business, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Dec 2021"",""2022"",""33"",""8"",""1939"",""1952"",""A set of $k$k spanning trees in a graph $G$G are called completely independent spanning trees (CISTs for short) if the paths joining every pair of vertices $x$x and $y$y in any two trees have neither vertex nor edge in common, except for $x$x and $y$y. The existence of multiple CISTs in the underlying graph of a network has applications in fault-tolerant broadcasting and secure message distribution. In this paper, we investigate the construction of CISTs in a server-centric data center network called BCube connected crossbars (BCCC), which can provide good network performance using inexpensive commodity off-the-shelf switches and commodity servers with only two network interface card (NIC) ports. The significant advantages of BCCC are its good expandability, lower communication latency, and higher robustness in component failure. Based on the structure of compound graphs of BCCC, we provide efficient algorithms to construct $\lceil \frac{n}{4}\rceil$⌈n4⌉ CISTs in the logical graph of BCCC, denoted by $L$L-$BCCC(n,k)$BCCC(n,k), for $n\geqslant 5$n⩾5. As a by-product, we obtain a fault-tolerant routing that takes the constructed CISTs as its routing table. We then evaluate the performance of the fault-tolerant routing through simulation results."",""1558-2183"","""",""10.1109/TPDS.2021.3133595"",""National Natural Science Foundation of China(grant numbers:61872257,62002062,62072109)"; Ministry of Science and Technology, Taiwan(grant numbers:MOST-110-2221-E-141-004);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645299"",""Completely independent spanning trees (CISTs)";data center networks (DCNs);BCube connected crossbars (BCCC);server-centric DCNs;"compound graphs"",""Servers";Routing;Fault tolerant systems;Fault tolerance;Data centers;Compounds;"Ad hoc networks"","""",""23"","""",""45"",""IEEE"",""10 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Construction of Dual-CISTs on an Infinite Class of Networks,""X. -W. Qin"; R. -X. Hao;" J. Wu"",""Department of Mathematics, Beijing Jiaotong University, Beijing, P.R. China"; Department of Mathematics, Beijing Jiaotong University, Beijing, P.R. China;" Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Dec 2021"",""2022"",""33"",""8"",""1902"",""1910"",""The main method to achieve fault-tolerant network systems is by exploiting and effectively utilizing the edge-disjoint and/or inner-vertex-disjoint paths between pairs of source and destination vertices. Completely independent spanning trees (CISTs for short) are powerful tools for reliable broadcasting/unicasting and secure message distribution. Particularly, it has been shown that two CISTs have an application on configuring a protection routing in IP networks, such as mobile ad hoc networks and relatively large (static) network topologies with scalability in [IEEE/ACM Trans. Netw., 27 (2019) 1112-1123]. Many results focus on CISTs in specific networks in the literature, however, few results are given on an infinite class of networks having common properties. In this article, we prove the existence of dual-CISTs in an infinite number of networks satisfying some Hamilton sufficient conditions. A unique algorithm to construct a CIST-partition is proposed, which can be applied to not only many kinds of networks, but our algorithm can also be implemented very easily in parallel or distributed systems satisfying the conditions. In addition, we make a comparative analysis between the proposed conditions and several known results on an infinite number of networks, the advantage of our result is significant. In particular, the bound in our conditions is sharp. The results will provide a powerful framework for the design of fault-tolerant network topologies and routing protocols for future networks."",""1558-2183"","""",""10.1109/TPDS.2021.3132412"",""National Natural Science Foundation of China(grant numbers:11971054,11731002)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635685"",""Completely independent spanning trees";a protection routing;CIST-partition;Hamilton bipartition sufficient condition;"constructive algorithm"",""Fault tolerant systems";Fault tolerance;Hypercubes;Tools;Routing;Network topology;"Broadcasting"","""",""1"","""",""40"",""IEEE"",""3 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Content Collaborative Caching Strategy in the Edge Maintenance of Communication Network: A Joint Download Delay and Energy Consumption Method,""L. Rui"; D. Song; S. Chen; Y. Yang; Y. Yang;" Z. Gao"",""State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China"; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China; State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China;" State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4148"",""4163"",""With the development of Big Data technology and Internet, the surge of data in the network will cause network congestion and untimely task processing. Additionally, caching content in the core network may cause redundant access of content and backhaul bottlenecks. Due to the increasing requirements of users for task processing efficiency, the centralized maintenance system based on traditional cloud computing cannot meet the current computing requirements. In view of these problems, we propose a content collaborative caching mechanism based on joint decision of download delay and energy consumption. By integrating network coding and content caching technology, the work content maintained in the communication network is deployed near the edge of the network in the form of coding to reduce the redundant transmission of content and acquisition time of content. This article establishes a user QoE satisfaction model, which consists of two indexes that measure time delay and energy consumption. This article proposes a $\varepsilon$ɛ-hybrid Q-learning algorithm to optimize the placement of cache files, and made the cache action selection based on the combination of improved heuristic greedy algorithm and simulated annealing algorithm. The experimental results show that the proposed cache strategy can reduce the delay of users downloading content and the energy consumption of content cache, so as to improve the quality of field maintenance work in communication network."",""1558-2183"","""",""10.1109/TPDS.2022.3179271"",""National Key R&D Program of China(grant numbers:2020YFB1807802,2020YFB1807800)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785848"",""Multi-access edge computing";communication network;content caching;"quality of experience"",""Delays";Maintenance engineering;Energy consumption;Servers;Device-to-device communication;Communication networks;"Heuristic algorithms"","""",""1"","""",""32"",""IEEE"",""31 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Context-Aware Online Client Selection for Hierarchical Federated Learning,""Z. Qu"; R. Duan; L. Chen; J. Xu; Z. Lu;" Y. Liu"",""Department of Electrical Engineering, University of South Florida, Tampa, FL, USA"; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA; Institute of Cyber Science and Technology, School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA; Department of Electrical Engineering, University of South Florida, Tampa, FL, USA;" Department of Computer Science and Engineering, University of South Florida, Tampa, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Aug 2022"",""2022"",""33"",""12"",""4353"",""4367"",""Federated Learning (FL) has been considered as an appealing framework to tackle data privacy issues of mobile devices compared to conventional Machine Learning (ML). Using Edge Servers (ESs) as intermediaries to perform model aggregation in proximity can reduce the transmission overhead, and it enables great potential in low-latency FL, where the hierarchical architecture of FL (HFL) has been attracted more attention. Designing a proper client selection policy can significantly improve training performance, and it has been widely investigated in conventional FL studies. However, to the best of our knowledge, systematic client selection policies have not yet been fully studied for HFL. In addition, client selection for HFL faces more challenges than conventional FL (e.g., the time-varying connection of client-ES pairs and the limited budget of the Network Operator (NO)). In this article, we investigate a client selection problem for HFL, where the NO learns the number of successful participating clients to improve training performance (i.e., select as many clients in each round) as well as under the limited budget on each ES. An online policy, called Context-aware Online Client Selection (COCS), is developed based on Contextual Combinatorial Multi-Armed Bandit (CC-MAB). COCS observes the side-information (context) of local computing and transmission of client-ES pairs and makes client selection decisions to maximize NO's utility given a limited budget. Theoretically, COCS achieves a sublinear regret compared to an Oracle policy on both strongly convex and non-convex HFL. Simulation results also support the efficiency of the proposed COCS policy on real-world datasets."",""1558-2183"","""",""10.1109/TPDS.2022.3186960"",""National Science Foundation(grant numbers:CNS-2044516,ECCS-2033681,ECCS-2029858,CNS-2044991)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9809926"",""Hierarchical federated learning";client selection;"contextual combinatorial multi-armed bandit"",""Training";Servers;Computational modeling;Data models;Computer architecture;Convergence;"Faces"","""",""11"","""",""48"",""IEEE"",""28 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cooperative Edge Caching Based on Temporal Convolutional Networks,""X. Zhang"; Z. Qi; G. Min; W. Miao; Q. Fan;" Z. Ma"",""Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K."; School of Electronic Science and Engineering, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; Department of Computer Science, College of Engineering, Mathematics, and Physical Sciences, University of Exeter, Exeter, U.K.; School of Big Data and Software Engineering, Chongqing University, Chongqing, China;" School of Electronic Science and Engineering, Nanjing University, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Feb 2022"",""2022"",""33"",""9"",""2093"",""2105"",""With the rapid growth of networked multimedia services in the Internet, wireless network traffic has increased dramatically. However, the current mainstream content caching schemes do not take into account the cooperation of different edge servers, resulting in deteriorated system performance. In this paper, we propose a learning-based edge caching scheme to enable mutual cooperation among different edge servers with limited caching resources, thus effectively reducing the content delivery latency. Specifically, we formulate the cooperative content caching problem as an optimization problem, which is proven to be NP-hard. To solve this problem, we design a new learning-based cooperative caching strategy (LECS) that encompasses three key components. Firstly, a temporal convolutional network driven content popularity prediction model is developed to estimate the content popularity with high accuracy. Secondly, with the predicted content popularity, the concept of content caching value (CCV) is introduced to weigh the value of a content cached on a given edge server. Thirdly, an novel dynamic programming algorithm is developed to maximize the overall CCV. Extensive simulation results have demonstrated the superiority of our approach. Compared with the state-of-the-art caching schemes, LECS can improve the cache hit rate by 8.3%-10.1%, and reduce the average content delivery delay by 9.1%-15.1%."",""1558-2183"","""",""10.1109/TPDS.2021.3135257"",""National Key Research and Development Program of China(grant numbers:2018YFB2100804)"; EU Horizon 2020 Research and Innovation Programme; Marie Sklodowska-Curie Actions(grant numbers:898588); EU Horizon 2020 INITIATE(grant numbers:101008297); National Natural Science Foundation of China(grant numbers:61902178,92067206,62102053,61972222); Natural Science Foundation of Jiangsu Province(grant numbers:BK20190295); Leading Technology of Jiangsu Basic Research Plan(grant numbers:BK20192003); Chongqing Key Laboratory of Digital Cinema Art Theory and Technology(grant numbers:2021KF01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650595"",""Cooperative edge caching";temporal convolutional networks;content caching value;"content popularity"",""Conferences";Portable document format;Indexes;Typesetting;Printing;Loading;"Web sites"","""",""20"","""",""49"",""IEEE"",""14 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Cooperative Scheduling Schemes for Explainable DNN Acceleration in Satellite Image Analysis and Retraining,""W. -J. Kim";" C. -H. Youn"",""School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea";" School of Electrical Engineering, Korea Advanced Institute of Science and Technology, Daejeon, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Nov 2021"",""2022"",""33"",""7"",""1605"",""1618"",""The deep learning-based satellite image analysis and retraining systems are getting emerging technologies to enhance the capability of the sophisticated analysis of terrestrial objects. In principle, to apply the explainable DNN model for the process of satellite image analysis and retraining, we consider a new acceleration scheduling mechanism. Especially, the conventional DNN acceleration schemes cause serious performance degradation due to computational complexity and costs in satellite image analysis and retraining. In this article, to overcome the performance degradation, we propose cooperative scheduling schemes for explainable DNN acceleration in analysis and retraining process. For the purpose of it, we define the latency and energy cost modeling to derive the optimized processing time and cost required for explainable DNN acceleration. Especially, we show a minimum processing cost considered in the proposed scheduling via layer-level management of the explainable DNN on FPGA-GPU acceleration system. In addition, we evaluate the performance using an adaptive unlabeled data selection scheme with confidence threshold and a semi-supervised learning driven data parallelism scheme in accelerating retraining process. The experimental results demonstrate that the proposed schemes reduce the energy cost of the conventional DNN acceleration systems by up to about 40% while guaranteeing the latency constraints."",""1558-2183"","""",""10.1109/TPDS.2021.3122454"",""Defense Challengeable Future Technology Program"; Samsung Electronics Co., Ltd(grant numbers:IO201210-07976-01); Institute for Information and Communications Technology Promotion(grant numbers:2017-0-00294);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585555"",""Cooperative satellite image analysis and retraining";DNN acceleration;"distributed deep learning"",""Satellites";Image analysis;Field programmable gate arrays;Costs;Graphics processing units;Task analysis;"Labeling"","""",""3"","""",""32"",""CCBY"",""26 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Coordinated Batching and DVFS for DNN Inference on GPU Accelerators,""S. M. Nabavinejad"; S. Reda;" M. Ebrahimi"",""School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran"; School of Engineering, Brown University, Providence, RI, USA;" KTH Royal Institute of Technology, Stockholm, Sweden"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Apr 2022"",""2022"",""33"",""10"",""2496"",""2508"",""Employing hardware accelerators to improve the performance and energy-efficiency of DNN applications is on the rise. One challenge of using hardware accelerators, including the GPU-based ones, is that their performance is limited by internal and external factors, such as power caps. A common approach to meet the power cap constraint is using the Dynamic Voltage Frequency Scaling (DVFS) technique. However, the functionally of this technique is limited and platform-dependent. To tackle this challenge, we propose a new control knob, which is the size of input batches fed to the GPU accelerator in DNN inference applications. We first evaluate the impact of batch size on power consumption and performance of DNN inference. Then, we introduce the design and implementation of a fast and lightweight runtime system, called BatchDVFS. Dynamic batching is implemented in BatchDVFS to adaptively change the batch size, and hence, trade-off throughput with power consumption. It employs an approach based on binary search to find the proper batch size within a short period of time. Combining dynamic batching with the DVFS technique, BatchDVFS can control the power consumption in wider ranges, and hence, yield higher throughput in the presence of power caps. To find near-optimal solution for long-running jobs that can afford a relatively significant profiling overhead, compared with BatchDVFS overhead, we also design an approach, called BOBD, that employs Bayesian Optimization to wisely explore the vast state space resulted by combination of the batch size and DVFS solutions. Conducting several experiments using a modern GPU and several DNN models and input datasets, we show that our BatchDVFS can significantly surpass the techniques solely based on DVFS or batching, regarding throughput (up to 11.2x and 2.2x, respectively), while successfully meeting the power cap."",""1558-2183"","""",""10.1109/TPDS.2022.3144614"",""National Science Foundation(grant numbers:1814920)"; DoD ARO(grant numbers:W911NF-19-1-0484);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689937"",""Deep neural networks";GPU accelerator;power consumption;throughput;batch size;"dynamic voltage frequency scaling"",""Throughput";Graphics processing units;Power demand;Runtime;Bayes methods;Resource management;"Optimization"","""",""14"","""",""58"",""IEEE"",""21 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Coordinating Fast Concurrency Adapting With Autoscaling for SLO-Oriented Web Applications,""J. Liu"; S. Zhang; Q. Wang;" J. Wei"",""Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, LA, USA"; School of Cyber and Computer Sciences, Augusta University, Augusta, GA, USA; Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, LA, USA;" Department of Software and Information Systems, University of North Carolina at Charlotte, Charlotte, NC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2022"",""2022"",""33"",""12"",""3349"",""3362"",""Cloud providers tend to support dynamic computing resources reallocation (e.g., Autoscaling) to handle the bursty workload for web applications (e.g., e-commerce) in the cloud environment. Nevertheless, we demonstrate that directly scaling a bottleneck server without quickly adjusting its soft resources (e.g., server threads and database connections) can cause significant response time fluctuations of the target web application. Since soft resources determine the request processing concurrency of each server in the system, simply scaling out/in the bottleneck service can unintentionally change the concurrency level of related services, inducing either under- or over-utilization of the critical hardware resource. In this paper, we propose the Scatter-Concurrency-Throughput (SCT) model, which can rapidly identify the near-optimal soft resource allocation of each server in the system using the measurement of each server’s real-time throughput and concurrency. Furthermore, we implement a Concurrency-aware autoScaling (ConScale) framework that integrates the SCT model to quickly reallocate the soft resources of the key servers in the system to best utilize the new hardware resource capacity after the system scaling. Based on extensive experimental comparisons with two widely used hardware-only scaling mechanisms for web applications: EC2-AutoScaling (VM-based autoscaler) and Kubernetes HPA (container-based autoscaler), we show that ConScale can successfully mitigate the response time fluctuations over the system scaling phase in both VM-based and container-based environments."",""1558-2183"","""",""10.1109/TPDS.2022.3151512"",""National Science Foundation(grant numbers:CNS-2000681)"; Office of Naval Research(grant numbers:N00014-21-1-2171/N00014-19-1-2371); Army Research Office(grant numbers:W911NF-17-1-0437); Fujitsu;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714008"",""Scalability";auto-scaling;soft resource;"cloud-based applications"",""Concurrent computing";Servers;Hardware;Throughput;Time factors;Resource management;"Topology"","""",""2"","""",""53"",""IEEE"",""14 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"CoPA: Cold Page Awakening to Overcome Retention Failures in STT-MRAM Based I/O Buffers,""M. Hadizadeh"; E. Cheshmikhani; M. Rahmanpour; O. Mutlu;" H. Asadi"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Electrical and Computer Engineering, ETH Zurich, Zurich, Switzerland;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2022"",""2022"",""33"",""10"",""2304"",""2317"",""Performance and reliability are two prominent factors in the design of data storage systems. To achieve higher performance, recently storage system designers use $Dynamic$Dynamic $RAM$RAM (DRAM)-based buffers. The volatility of DRAM brings up the possibility of data loss and data inconsistency. Thus, a part of the main storage is conventionally used as the journal area to be able of recovering unflushed data pages in the case of power failure. Moreover, periodically flushing buffered data pages to the main storage is a common mechanism to preserve a high level of reliability. This scheme, however, leads to a considerable increase in storage write traffic, which adversely affects the performance. To address this shortcoming, recent studies offer a small $Non-Volatile$Non-Volatile $Memory$Memory (NVM) as the $Persistent$Persistent $Journal$Journal $Area$Area (PJA) along with DRAM as an efficient approach to overcome DRAM vulnerability against power failure while effectively reducing storage write traffic. This approach, named $NVM-Backed$NVM-Backed $Buffer$Buffer (NVB-Buffer), features from advantages of NVMs and addresses DRAM shortcomings. In this article, we employ the most promising technologies for PJA among the emerging technologies, which is $Spin-Transfer$Spin-Transfer $Torque$Torque $Magnetic$Magnetic $Random$Random $Access$Access $Memory$Memory (STT-MRAM) to meet the requirements of efficient PJA by providing high endurance, non-volatility, and DRAM-like latency. Despite these advantages, STT-MRAM faces major reliability challenges, i.e., Retention Failure, Read Disturbance, and Write Failure, which have not been addressed in previously suggested NVB-Buffers. In this article, we first demonstrate that the retention failure is the dominant source of errors in NVB-Buffers as it suffers from long and unpredictable page idle intervals (i.e., the time interval between two consecutive accesses to a PJA page). Then, we propose a novel NVB-Buffer management scheme, named, $\underline{Co}ld$Co̲ld $\underline{P}age$P̲age $\underline{A}wakening$A̲wakening (CoPA), which predictably reduces the idle time of PJA pages. To this aim, CoPA employs $Distant$Distant $Refreshing$Refreshing to periodically overwrite the vulnerable PJA page contents by opportunistically using their replica in DRAM-based buffer. We compare CoPA with the state-of-the-art schemes over several well-known storage workloads based on physical journaling. Our evaluations show that CoPA significantly reduces the maximum page idle time, which leads to three orders of magnitude lower failure rate with negligible performance degradation (1.1%) and memory overhead (1.2%)."",""1558-2183"","""",""10.1109/TPDS.2021.3137315"",""Sharif University of Technology"; Eidgenssische Technische Hochschule Zrich;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9662259"",""Data storage systems";persistent journal area;STT-MRAM;"retention failure"",""Nonvolatile memory";Random access memory;Reliability;Magnetic tunneling;Ferroelectric films;Degradation;"Resistive RAM"","""",""1"","""",""75"",""IEEE"",""23 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"COSCO: Container Orchestration Using Co-Simulation and Gradient Based Optimization for Fog Computing Environments,""S. Tuli"; S. R. Poojara; S. N. Srirama; G. Casale;" N. R. Jennings"",""Department of Computing, Imperial College London, London, U.K."; Institute of Computer Science, University of Tartu, Tartu, Estonia; School of Computer and Information Sciences, University of Hyderabad, Telangana, India; Department of Computing, Imperial College London, London, U.K.;" Department of Computing, Imperial College London, London, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jul 2021"",""2022"",""33"",""1"",""101"",""116"",""Intelligent task placement and management of tasks in large-scale fog platforms is challenging due to the highly volatile nature of modern workload applications and sensitive user requirements of low energy consumption and response time. Container orchestration platforms have emerged to alleviate this problem with prior art either using heuristics to quickly reach scheduling decisions or AI driven methods like reinforcement learning and evolutionary approaches to adapt to dynamic scenarios. The former often fail to quickly adapt in highly dynamic environments, whereas the latter have run-times that are slow enough to negatively impact response time. Therefore, there is a need for scheduling policies that are both reactive to work efficiently in volatile environments and have low scheduling overheads. To achieve this, we propose a Gradient Based Optimization Strategy using Back-propagation of gradients with respect to Input (GOBI). Further, we leverage the accuracy of predictive digital-twin models and simulation capabilities by developing a Coupled Simulation and Container Orchestration Framework (COSCO). Using this, we create a hybrid simulation driven decision approach, GOBI*, to optimize Quality of Service (QoS) parameters. Co-simulation and the back-propagation approaches allow these methods to adapt quickly in volatile environments. Experiments conducted using real-world data on fog applications using the GOBI and GOBI* methods, show a significant improvement in terms of energy consumption, response time, Service Level Objective and scheduling time by up to 15, 40, 4, and 82 percent respectively when compared to the state-of-the-art algorithms."",""1558-2183"","""",""10.1109/TPDS.2021.3087349"",""Imperial College London"; European Social Fund; EU's Horizon 2020 Program(grant numbers:825040);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448450"",""Fog computing";coupled simulation;container orchestration;back-propagation to input;"QoS optimization"",""Optimization";Quality of service;Containers;Adaptation models;Genetic algorithms;Time factors;"Task analysis"","""",""45"","""",""63"",""IEEE"",""8 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cost-Effective Web Application Replication and Deployment in Multi-Cloud Environment,""T. Shi"; H. Ma; G. Chen;" S. Hartmann"",""School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand"; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand;" Department of Informatics, Clausthal University of Technology, Clausthal-Zellerfeld, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Dec 2021"",""2022"",""33"",""8"",""1982"",""1995"",""Multi-cloud is becoming a popular cloud ecosystem because it allows enterprise users to share the workload across multiple cloud service providers to achieve high-quality services with lower operation cost and higher application resilience. In multi-cloud, cloud services are widely distributed at different locations with differentiated prices. Therefore, Web application providers face the challenge to select proper cloud services for application replication and deployment with the aim of minimizing the deployment cost. Meanwhile, the deployed application replicas must satisfy the constraint on request response time to maintain the quality of user experience. To meet the two major requirements, this article studies a new problem of Web application replication and deployment in multi-cloud (WARDMC) that jointly considers both the cost minimization and constraints on average response time, including particularly request processing time and network latency. To address the problem, we develop a new approach named MCApp. MCApp combines iterative mixed integer linear programming with domain-tailored large neighborhood search to optimize both application replicas deployment and user requests dispatching. Extensive experiments using the real-world datasets demonstrate that MCApp significantly outperforms several recently proposed approaches."",""1558-2183"","""",""10.1109/TPDS.2021.3133884"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645306"",""Multi-cloud";Web application deployment;service replication;cost optimization;mixed integer linear programming;"large neighborhood search"",""Cloud computing";Costs;Time factors;Optimization;Upper bound;Pricing;"User experience"","""",""9"","""",""59"",""IEEE"",""10 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cost-Efficient Server Configuration and Placement for Mobile Edge Computing,""Z. He"; K. Li;" K. Li"",""School of Software of Yunnan University, Kunming, Yunnan, China"; School of Computer Science and Electronic Engineering, Hunan University, Hunan, China;" School of Computer Science and Electronic Engineering, Hunan University, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Feb 2022"",""2022"",""33"",""9"",""2198"",""2212"",""Computing resource configuration and site selection of edge servers (ESs) are two critical steps to build up a mobile edge computing (MEC) platform. In this paper, the joint optimization problem of configuration and placement for ES in the MEC environment is investigated. First, we treat each ES as an M/G/m queueing model, and establish mathematical models to characterize the MEC environment, such that the performance and operational expenditures (OPEX) of the system can be calculated analytically. Then, we design a two-stage method and develop a series of algorithms based on bisection algorithm and genetic algorithm (GA) to obtain the optimal configuration scheme and sub-optimal placement scheme (including the deployment quantity) of ESs, with the goal of minimizing OPEX while maintaining system performance at a predetermined level. Finally, we conduct experiments based on a real base station dataset provided by Shanghai Telecom to show the effectiveness of the proposed algorithms. To the best of our knowledge, this work is the first research of the joint optimization problem of configuration and placement for ES in the MEC environment, where the main objective is to increase the cost efficiency."",""1558-2183"","""",""10.1109/TPDS.2021.3135955"",""National Natural Science Foundation of China(grant numbers:61876061)"; Applied Basic Research Foundation of Yunnan Province(grant numbers:202001BB050034); Open Foundation of Key Laboratory in Software Engineering of Yunnan Province(grant numbers:2020SE405);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653813"",""Cost efficiency";configuration scheme;edge server;mobile edge computing;"placement scheme"",""Servers";Costs;Resource management;Optimization;Heuristic algorithms;Quality of service;"Dynamic scheduling"","""",""13"","""",""42"",""IEEE"",""16 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Cost-Efficient Workflow Scheduling Algorithm for Applications With Deadline Constraint on Heterogeneous Clouds,""X. Tang"; W. Cao; H. Tang; T. Deng; J. Mei; Y. Liu; C. Shi; M. Xia;" Z. Zeng"",""School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China"; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; Applied Economics, Beijing Normal University-Hong Kong Baptist University United International College (UIC), Xiangzhou, Zhuhai, China; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; College of Information Science and Engineering, Hunan Normal University, Changsha, Hunan, China; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China; School of Computer and Communications Engineering, Changsha University of Science & Technology, Changsha, Hunan, China;" I2R, A*Star, Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Feb 2022"",""2022"",""33"",""9"",""2079"",""2092"",""In recent years, more and more large-scale data processing and computing workflow applications run on heterogeneous clouds. Such cloud applications with precedence-constrained tasks are usually deadline-constrained and their scheduling is an essential problem faced by cloud providers. Moreover, minimizing the workflow execution cost based on cloud billing periods is also a complex and challenging problem for clouds. In realizing this, we first model the workflow applications as I/O Data-aware Directed Acyclic Graph (DDAG), according to clouds with global storage systems. Then, we mathematically state this deadline-constrained workflow scheduling problem with the goal of minimum execution financial cost. We also prove that the time complexity of this problem is NP-hard by deducing from a multidimensional multiple-choice knapsack problem. Third, we propose a heuristic cost-efficient task scheduling strategy called CETSS, which includes workflow DDAG model building, task subdeadline initialization, greedy workflow scheduling algorithm, and task adjusting method. The greedy workflow scheduling algorithm mainly consists of dynamical task renting billing period sharing method and unscheduled task subdeadline relax technique. We perform rigorous simulations on some synthetic randomly generated applications and real-world applications, such as Epigenomics, CyberShake, and LIGO. The experimental results clearly demonstrate that our proposed heuristic CETSS outperforms the existing algorithms and can effective save the total workflow execution cost. In particular, CETSS is very suitable for large workflow applications."",""1558-2183"","""",""10.1109/TPDS.2021.3134247"",""National Natural Science Foundation of China(grant numbers:61972146)"; Natural Science Foundation of Hunan Province(grant numbers:2020JJ4376);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647942"",""Workflow application";cost;heterogeneous clouds;schedule length;"task scheduling"",""Cloud computing";Task analysis;Costs;Computational modeling;Scheduling;Job shop scheduling;"Heuristic algorithms"","""",""20"","""",""48"",""IEEE"",""13 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"CSEdge: Enabling Collaborative Edge Storage for Multi-Access Edge Computing Based on Blockchain,""L. Yuan"; Q. He; F. Chen; J. Zhang; L. Qi; X. Xu; Y. Xiang;" Y. Yang"",""Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia"; Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Information Technology, Deakin University, Geelong, VIC, Australia; Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; School of Computer Science, Qufu Normal University, Jining, Shandong, China; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China; Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia;" Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Dec 2021"",""2022"",""33"",""8"",""1873"",""1887"",""Multi-access Edge Computing (MEC), as an extension of cloud computing, provides storage resources at the network edge to enable low-latency data retrieval for users. Due to limited physical sizes and constrained storage resources, individual edge servers cannot store a large amount of data when operating independently. They often need to offload data to other edge servers to serve users collaboratively. Operated by different edge infrastructure providers, edge servers usually work in a distrusted environment. Incentive and trust are the two main challenges in facilitating collaborative edge storage. This article proposes CSEdge, a novel decentralized system that tackles these challenges to enable collaborative edge storage based on blockchain. On CSEdge, edge servers can submit data offloading requests for others to contend for. Winners are selected based on their reputations. They will store the offloaded data and receive rewards for successfully finishing data offloading tasks. Via a distributed consensus, their performance will be recorded on blockchain for future reputation evaluation. A prototype of CSEdge is built on Hyperledger Sawtooth and experimentally evaluated against a baseline system and two start-of-the-art systems in a simulated MEC environment. The results demonstrate that CSEdge can effectively and efficiently facilitate collaborative edge storage among edge servers."",""1558-2183"","""",""10.1109/TPDS.2021.3131680"",""Australian Research Council(grant numbers:DP180100212,DP200102491)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629346"",""Edge computing";cooperative edge computing;blockchain;distributed consensus;"data offloading"",""Servers";Task analysis;Collaboration;Blockchains;Time factors;Cloud computing;"Reliability"","""",""47"","""",""56"",""IEEE"",""30 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"cuNH: Efficient GPU Implementations of Post-Quantum KEM NewHope,""Y. Gao"; J. Xu;" H. Wang"",""NUS-Singtel Cyber Security Research and Development Laboratory, National University of Singapore, Singapore"; NUS-Singtel Cyber Security Research and Development Laboratory, National University of Singapore, Singapore;" NUS-Singtel Cyber Security Research and Development Laboratory, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Aug 2021"",""2022"",""33"",""3"",""551"",""568"",""Post-quantum cryptography was proposed in the past years due to the foreseeable emergence of quantum computers that are able to break the conventional public key cryptosystems at acceptable costs. However, post-quantum schemes are usually less efficient than conventional ones, which makes them less practical in scenarios with limited resources or high concurrency. Server-side applications always feature multiple users, therefore requiring efficient execution of batch tasks. GPU is intrinsically well-suited to batch tasks owing to its SIMD/SIMT execution fashion, so it naturally helps to achieve high performance. However, a naive GPU-based implementation cannot make the best use of hardware resources of the GPU regardless of task loads. In this article, we propose SIMD parallelization paradigms for fine-grained GPU implementations and then apply them to a post-quantum key encapsulation algorithm called NewHope, where we carefully design every module, especially NTT and inverse NTT, to fit into the SIMD parallelization paradigms. In addition, we employ multi-streaming to improve performance in user's perspective. Finally, our evaluations are made on two testbeds with GPU accelerators NVIDIA GeForce MX150 and GeForce GTX 1650, respectively. The experimental results show that the fine-grained implementations save up to 98 percent latency at low task loads, and their throughputs increase by up to 86 percent at high task loads, when compared with the naive ones in kernel's perspective, and the multi-streaming implementations greatly reduce the latency overhead percentage at high task loads by up to 86 percent, when compared with the fine-grained implementation in user's perspective. Moreover, our fine-grained implementation and multi-streaming implementation are respectively 51.5 and 45.5 percent faster than Gupta et al.'s implementations when compared with it under reasonable assumptions. Furthermore, as lattice-based post-quantum schemes have similar operations, our proposal also easily applies to other lattice-based post-quantum schemes."",""1558-2183"","""",""10.1109/TPDS.2021.3097277"",""National Research Foundation Singapore"; Singapore Telecommunications Limited; National Natural Science Foundation of China(grant numbers:61632020,U1936209);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9485052"",""NewHope";number-theoretic transform (NTT);post-quantum cryptography (PQC);ring-LWE;"CUDA-enabled GPU"",""Graphics processing units";Task analysis;Parallel processing;Cryptography;Kernel;Instruction sets;"Hardware"","""",""9"","""",""26"",""IEEE"",""14 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Customer Adaptive Resource Provisioning for Long-Term Cloud Profit Maximization under Constrained Budget,""P. Cong"; Z. Zhang; J. Zhou; X. Liu; Y. Liu;" T. Wei"",""School of Computer Science and Technology, Shanghai Key Laboratory of Trustworthy Computing, East China Normal University, Shanghai, China"; Software Engineering Institute, East China Normal University, Shanghai, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; National Research Centre of Parallel Computer Engineering and Technology, Wuxi, China; School of Data Science and Engineering, East China Normal University, Shanghai, China;" School of Computer Science and Technology, Shanghai Key Laboratory of Trustworthy Computing, East China Normal University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1373"",""1392"",""As an efficient commercial information technology, cloud computing has attracted more and more users and enterprises to use it. Faced with such a large number and variety of customers, it is necessary for cloud providers (CPs) with limited budget to provide satisfactory customized pricing services, profitable customer and system investments, and flexible system resource provisioning strategies to improve both customer experience and long-term profit. Existing profit optimization research rarely considers customer diversity and dynamics, which may have a negative impact on long-term profit growth due to poor management of customer relations. In this article, we implement customer relationship management by considering both customer diversity and dynamics, and propose a customer adaptive resource provisioning scheme to maximize long-term profit under constrained budget. We consider four customer types (i.e., loyal, old, new, and lost) that can transition to each other during the customer's lifetime of interaction with the CP. The CP builds multiple cloud service sub-platforms, each of which contains multiple multiserver systems and serves the same type of customers. For the cloud service platform, we first analyze single multiserver system using an analytical method to obtain its optimal profit, invested funding, and system configuration. In particular, for systems serving new and lost customers, we develop a novel customer lifetime value (CLV)-based customer investment scheme that selects valuable customers for investment under limited marketing budget. Based on the above analysis, we then present a customer retention rate (CRR)-driven three-stage heuristic scheme that prioritizes investment in multiserver systems with endangered customers under limited infrastructure budget for reducing customer churn and promoting long-term profit growth. We conduct extensive simulation experiments to validate the effectiveness of our method. Simulation results show that compared with the benchmark algorithms, our method can improve the long-term profit and CRR by up to 3.4x and 7.8x, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3112562"",""National Key Research and Development Program of China(grant numbers:2018YFB2101300)"; National Natural Science Foundation of China(grant numbers:62172224,61802185); China Postdoctoral Science Foundation(grant numbers:BX2021128,2021T140327,2020M680068);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9537613"",""Cloud pricing";profit maximization;budget allocation;multiservers;customer lifetime value;"customer retention rate"",""Pricing";Cloud computing;Biological system modeling;Quality of service;Investment;Optimization;"Costs"","""",""8"","""",""41"",""IEEE"",""14 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"DAG Scheduling and Analysis on Multi-Core Systems by Modelling Parallelism and Dependency,""S. Zhao"; X. Dai;" I. Bate"",""Department of Computer Science, University of York, York, U.K."; Department of Computer Science, University of York, York, U.K.;" Department of Computer Science, University of York, York, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4019"",""4038"",""With ever more complex functionalities being implemented in emerging real-time applications, multi-core systems are demanded for high performance, with directed acyclic graphs (DAG) being used to model functional dependencies. For a single DAG task, our previous work presented a concurrent provider and consumer (CPC) model that captures the node-level dependency and parallelism, which are the two key factors of a DAG. Based on the CPC, scheduling and analysis methods were constructed to reduce makespan and tighten the analytical bound of the task. However, the CPC-based methods cannot support multi-DAGs as the interference between DAGs (i.e., inter-task interference) is not taken into account. To address this limitation, this article proposes a novel multi-DAG scheduling approach which specifies the number of cores a DAG can utilise so that it does not incur the inter-task interference. This is achieved by modelling and understanding the workload distribution of the DAG and the system. By avoiding the inter-task interference, the constructed schedule provides full compatibility for the CPC-based methods to be applied on each DAG and reduces the pessimism of the existing analysis. Experimental results show that the proposed multi-DAG method achieves an improvement up to 80% in schedulability against the original work that it extends, and outperforms the existing multi-DAG methods by up to 60% for tightening the interference."",""1558-2183"","""",""10.1109/TPDS.2022.3177046"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779935"",""Multi-core systems";directed acyclic graphs;"schedulability tests"",""Task analysis";Interference;Schedules;Parallel processing;Time factors;Computational modeling;"Analytical models"","""",""6"","""",""39"",""IEEE"",""23 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Data, User and Power Allocations for Caching in Multi-Access Edge Computing,""X. Xia"; F. Chen; Q. He; G. Cui; J. C. Grundy; M. Abdelrazek; X. Xu;" H. Jin"",""School of Information Technology, Deakin University, Melbourne, VIC, Australia"; School of Information Technology, Deakin University, Melbourne, VIC, Australia; Department of Computer Science and Software Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; Department of Computer Science and Software Engineering, Swinburne University of Technology, Melbourne, VIC, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Information Technology, Deakin University, Melbourne, VIC, Australia; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, Jiangsu, China;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1144"",""1155"",""In the multi-access edge computing (MEC) environment, app vendors’ data can be cached on edge servers to ensure low-latency data retrieval. Massive users can simultaneously access edge servers with high data rates through flexible allocations of transmit power. The ability to manage networking resources offers unique opportunities to app vendors but also raises unprecedented challenges. To ensure fast data retrieval for users in the MEC environment, edge data caching must take into account the allocations of data, users, and transmit power jointly. We make the first attempt to study the Data, User, and Power Allocation (DUPA$^3$3) problem, aiming to serve the most users and maximize their overall data rate. First, we formulate the DUPA$^3$3 problem and prove its $\mathcal {NP}$NP-completeness. Then, we model the DUPA$^3$3 problem as a potential DUPA$^3$3 game admitting at least one Nash equilibrium and propose a two-phase game-theoretic decentralized algorithm named DUPA$^3$3Game to achieve the Nash equilibrium as the solution to the DUPA$^3$3 problem. To evaluate DUPA$^3$3Game, we analyze its theoretical performance and conduct extensive experiments to demonstrate its effectiveness and efficiency."",""1558-2183"","""",""10.1109/TPDS.2021.3104241"",""Australian Research Council(grant numbers:FL190100035)"; Discovery Projects(grant numbers:DP180100212,DP200102491);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511831"",""Edge computing";data allocation;user allocation;power allocation;optimization;"multi-access"",""Servers";Resource management;Games;Interference;NOMA;Intercell interference;"Distributed databases"","""",""31"","""",""33"",""IEEE"",""11 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Deadline and Reliability Aware Multiserver Configuration Optimization for Maximizing Profit,""T. Wang"; J. Zhou; L. Li; G. Zhang; K. Li;" X. S. Hu"",""School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China"; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Department of Computer Science and Technology, East China Normal University, Shanghai, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; Department of Computer Science, State University of New York, New York, NY, USA;" Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3772"",""3786"",""Maximizing profit is a key goal for cloud service providers in the modern cloud business market. Service revenue and business cost are two major factors in determining profit and highly depend on multiserver configuration. Understanding the relationship between multiserver configuration and profit is important to service providers. Although existing articles have explored this issue, few of them consider deadline miss rate and soft error reliability of cloud services in multiserver configuration for profit maximization. Since deadline misses violate cloud services’ real-time requirements and soft error prevents successful processing of cloud services, it is necessary to consider the impact of deadline miss rate and soft error reliability on service providers’ profits when configuring the multiserver. This article introduces a deadline miss rate and soft error reliability aware multiserver configuration scheme for maximizing cloud service providers’ profit. Specifically, we derive the deadline miss rate considering the heterogeneity of cloud service requests, and propose an analytical method to compute the soft error reliability of multiserver systems. Based on the new deadline miss rate and soft error reliability models, we formulate the multiserver configuration optimization problem and introduce an augmented Lagrange multiplier-based iterative method to find the optimal multiserver configuration. Extensive experiments evaluate the efficacy of the proposed multiserver configuration approach. Compared with the two state-of-the-art methods, the profit gained by our scheme can be up to 11.92% higher."",""1558-2183"","""",""10.1109/TPDS.2022.3170305"",""National Natural Science Foundation of China(grant numbers:62172224,61802185)"; China Postdoctoral Science Foundation(grant numbers:BX2021128,2021T140327,2020M680068); Postdoctoral Science Foundation of Jiangsu Province(grant numbers:2021K066A); Open Research Fund of the State Key Laboratory of Computer Architecture; Institute of Computing Technology, Chinese Academy of Sciences(grant numbers:CARCHA202105); Future Network Scientific Research Fund(grant numbers:FNSRFP-2021-YB-6); Open Research Fund of National Trusted Embedded Software Engineering Technology Research Center;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763428"",""Cloud service";deadline miss rate;multiserver configuration;profit maximization;"soft error reliability"",""Cloud computing";Servers;Reliability;Business;Computer architecture;Real-time systems;"Processor scheduling"","""",""2"","""",""47"",""IEEE"",""26 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Decentralised Data Quality Control in Ground Truth Production for Autonomic Decisions,""P. Gkikopoulos"; V. Schiavoni;" J. Spillner"",""InIT, Zurich University of Applied Sciences, Winterthur, Switzerland"; Department of Computer Science, University of Neuchǎtel, Neuchatel, Switzerland;" InIT, Zurich University of Applied Sciences, Winterthur, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Mar 2022"",""2022"",""33"",""10"",""2416"",""2427"",""Autonomic decision-making based on rules and metrics is inevitably on the rise in distributed software systems. Often, the metrics are acquired from system observations such as static checks and runtime traces. To avoid bias propagation and hence reduce wrong decisions in increasingly autonomous systems due to poor observation data quality, multiple independent observers can exchange their findings and produce a majority-accepted, complete and outlier-cleaned ground truth in the form of consensus-supported metrics. In this work, we motivate the growing importance of metrics for informed and autonomic decisions in clouds and other distributed systems, present reasons for diverging observations, and describe a federated approach to produce ground truth with data-centric consensus voting for more reliable decision making processes. We validate the system design with experiments in the area of cloud software artefact observations and highlight benefits for reproducible distributed system behaviour."",""1558-2183"","""",""10.1109/TPDS.2022.3142967"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681269"",""Distributed systems";redundant design;"algorithms for data and knowledge management"",""Measurement";Software;Decision making;System analysis and design;Redundancy;Peer-to-peer computing;"Data integrity"","""",""1"","""",""49"",""IEEE"",""13 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Decentralized Application Placement in Fog Computing,""Z. Á. Mann"",""University of Amsterdam, Amsterdam, WX, The Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2022"",""2022"",""33"",""12"",""3262"",""3273"",""In recent years, cloud computing concepts have been extended towards the network edge, leading to paradigms like fog and edge computing. As a result, applications can be placed on a variety of resources, including fog nodes and cloud data centers. Application placement has significant impact on important metrics like latency. Finding an optimal application placement is computationally challenging, particularly because of the potentially huge number of infrastructure nodes and application components. To overcome the limited scalability of application placement algorithms, optimization can be decentralized, i.e., performed separately for different parts of the infrastructure. The infrastructure can be split into fog colonies, where a fog colony consists of the computational resources in a given geographical region. Application placement can then be performed for the individual fog colonies, thus mitigating the scalability problem. However, independent optimization of application placement in different fog colonies may lead to missed synergies and thus to sub-optimal overall results. Hence, some kind of coordination between fog colonies may be beneficial. In this article, we analyze the effects of decentralization and coordination on the optimization results. In particular, we compare empirically four different approaches: (i) centralized decision-making, where decisions are made in one go for the entire infrastructure, (ii) independent fog colonies, where optimization is carried out in each fog colony independently from each other, (iii) fog colonies with communication, where excess application components in one fog colony can be sent to a neighboring fog colony, and (iv) fog colonies with overlaps, where shared resources may be dynamically distributed between neighboring fog colonies. Our experiments show that, for large problem instances, decentralization combined with coordination leads to the best results."",""1558-2183"","""",""10.1109/TPDS.2022.3148985"",""EU’s Horizon 2020 Research and Innovation programme(grant numbers:871525 (FogProtect))";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706284"",""Fog computing";edge computing;application placement;distributed algorithms;"fog colonies"",""Peer-to-peer computing";Cloud computing;Optimization;Scalability;Edge computing;Task analysis;"Bandwidth"","""",""12"","""",""38"",""IEEE"",""7 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Decentralized Edge Intelligence: A Dynamic Resource Allocation Framework for Hierarchical Federated Learning,""W. Y. B. Lim"; J. S. Ng; Z. Xiong; J. Jin; Y. Zhang; D. Niyato; C. Leung;" C. Miao"",""Alibaba Group and Alibaba-NTU Joint Research Institute (JRI), Nanyang Technological University (NTU), Singapore, Singapore"; Alibaba Group and Alibaba-NTU Joint Research Institute (JRI), Nanyang Technological University (NTU), Singapore, Singapore; Information Systems Technology and Design (ISTD) Pillar, Singapore University of Technology and Design, Singapore, Singapore; TuSimple, Beijing 100016, China; College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, China; School of Computer Science and Engineering, Nanyang Technological University (NTU), Singapore, Singapore; Joint NTU-UBC Research Centre of Excellence in Active Living for the Elderly (LILY), Singapore, Singapore;" SCSE, Nanyang Technological University (NTU), Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Aug 2021"",""2022"",""33"",""3"",""536"",""550"",""To enable the large scale and efficient deployment of Artificial Intelligence (AI), the confluence of AI and Edge Computing has given rise to Edge Intelligence, which leverages on the computation and communication capabilities of end devices and edge servers to process data closer to where it is produced. One of the enabling technologies of Edge Intelligence is the privacy preserving machine learning paradigm known as Federated Learning (FL), which enables data owners to conduct model training without having to transmit their raw data to third-party servers. However, the FL network is envisioned to involve thousands of heterogeneous distributed devices. As a result, communication inefficiency remains a key bottleneck. To reduce node failures and device dropouts, the Hierarchical Federated Learning (HFL) framework has been proposed whereby cluster heads are designated to support the data owners through intermediate model aggregation. This decentralized learning approach reduces the reliance on a central controller, e.g., the model owner. However, the issues of resource allocation and incentive design are not well-studied in the HFL framework. In this article, we consider a two-level resource allocation and incentive mechanism design problem. In the lower level, the cluster heads offer rewards in exchange for the data owners' participation, and the data owners are free to choose which cluster to join. Specifically, we apply the evolutionary game theory to model the dynamics of the cluster selection process. In the upper level, each cluster head can choose to serve a model owner, whereas the model owners have to compete amongst each other for the services of the cluster heads. As such, we propose a deep learning based auction mechanism to derive the valuation of each cluster head's services. The performance evaluation shows the uniqueness and stability of our proposed evolutionary game, as well as the revenue maximizing properties of the deep learning based auction."",""1558-2183"","""",""10.1109/TPDS.2021.3096076"",""Alibaba Group"; Alibaba-NTU Singapore Joint Research Institute; National Research Foundation Singapore(grant numbers:AISG2-RP-2020-019,AISG-GC-2019-003); WASP/NTU)(grant numbers:M4082187 (4080)); Ministry of Education - Singapore(grant numbers:Tier 1 (RG16/20)); National Natural Science Foundation of China(grant numbers:62071343); Singapore University of Technology and Design(grant numbers:SRG-ISTD-2021-165);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9479786"",""Federated learning";edge intelligence;resource allocation;evolutionary game;"auction"",""Training";Computational modeling;Resource management;Magnetic heads;Data models;Games;"Servers"","""",""107"","""",""40"",""IEEE"",""9 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Deep Neural Network Training With Distributed K-FAC,""J. G. Pauloski"; L. Huang; W. Xu; K. Chard; I. T. Foster;" Z. Zhang"",""Department of Computer Science, University of Chicago, Chicago, IL, USA"; Texas Advanced Computing Center, Austin, TX, USA; Texas Advanced Computing Center, Austin, TX, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA;" Texas Advanced Computing Center, Austin, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3616"",""3627"",""Scaling deep neural network training to more processors and larger batch sizes is key to reducing end-to-end training time";" yet, maintaining comparable convergence and hardware utilization at larger scales is challenging. Increases in training scales have enabled natural gradient optimization methods as a reasonable alternative to stochastic gradient descent and variants thereof. Kronecker-factored Approximate Curvature (K-FAC), a natural gradient method, preconditions gradients with an efficient approximation of the Fisher Information Matrix to improve per-iteration progress when optimizing an objective function. Here we propose a scalable K-FAC algorithm and investigate K-FAC’s applicability in large-scale deep neural network training. Specifically, we explore layer-wise distribution strategies, inverse-free second-order gradient evaluation, and dynamic K-FAC update decoupling, with the goal of preserving convergence while minimizing training time. We evaluate the convergence and scaling properties of our K-FAC gradient preconditioner, for image classification, object detection, and language modeling applications. In all applications, our implementation converges to baseline performance targets in 9–25% less time than the standard first-order optimizers on GPU clusters across a variety of scales."",""1558-2183"","""",""10.1109/TPDS.2022.3161187"",""U.S. Department of Energy"; Office of Science, Advanced Scientific Computing Research(grant numbers:DE-AC02-06CH11357); Exascale Computing Project(grant numbers:17-SC-20-SC); National Science Foundation(grant numbers:OAC-1931354,OAC-1818253,OAC-2106661,OAC-2107511);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739867"",""Optimization methods";neural networks;scalability;"high-performance computing"",""Training";Parallel processing;Program processors;Convergence;Computational modeling;Data models;"Deep learning"","""","""","""",""55"",""IEEE"",""22 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Deep Reinforcement Learning Enhanced Greedy Optimization for Online Scheduling of Batched Tasks in Cloud HPC Systems,""Y. Yang";" H. Shen"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China";" School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3003"",""3014"",""In a large cloud data center HPC system, a critical problem is how to allocate the submitted tasks to heterogeneous servers that will achieve the goal of maximizing the system’s gain defined as the value of completed tasks minus system operation costs. We consider this problem in the online setting that tasks arrive in batches and propose a novel deep reinforcement learning (DRL) enhanced greedy optimization algorithm of two-stage scheduling interacting task sequencing and task allocation. For task sequencing, we deploy a DRL module to predict the best allocation sequence for each arriving batch of tasks based on the knowledge (allocation strategies) learnt from previous batches. For task allocation, we propose a greedy strategy that allocates tasks to servers one by one online following the allocation sequence to maximize the total gain increase. We show that our greedy strategy has a performance guarantee of competitive ratio $\frac{1}{1+\kappa }$11+κ to the optimal offline solution, which improves the existing result for the same problem, where $\kappa$κ is upper bounded by the maximum cost-to-gain ratio of each task. While our DRL module enhances the greedy algorithm by providing the likely-optimal allocation sequence for each batch of arriving tasks, our greedy strategy bounds DRL’s prediction error within a proven worst-case performance guarantee for any allocation sequence. It enables a better solution quality than that obtainable from both DRL and greedy optimization alone. Extensive experiment evaluation results in both simulation and real application environments demonstrate the effectiveness and efficiency of our proposed algorithm. Compared with the state-of-the-art baselines, our algorithm increases the system gain by about 10% to 30%. Our algorithm provides an interesting example of combining machine learning (ML) and greedy optimization techniques to improve ML-based solutions with a worst-case performance guarantee for solving hard optimization problems."",""1558-2183"","""",""10.1109/TPDS.2021.3138459"",""Key-Area Research and Development Plan of Guangdong Province(grant numbers:#2020B010164003)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664254"",""Task scheduling";deep reinforcement learning;greedy optimization;"approximation algorithm"",""Task analysis";Servers;Costs;Resource management;Processor scheduling;Optimization;"Approximation algorithms"","""",""1"","""",""30"",""IEEE"",""28 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Deep Reinforcement Learning for Load-Balancing Aware Network Control in IoT Edge Systems,""Q. Liu"; T. Xia; L. Cheng; M. van Eijk; T. Ozcelebi;" Y. Mao"",""Information Technology Group, Wageningen University and Research, Wageningen, The Netherlands"; Interconnected Resource-aware Intelligent Systems Group, Eindhoven University of Technology, Eindhoven, The Netherlands; School of Control and Computer Engineering, North China Electric Power University, Beijing, China; Prodrive Technologies, Eindhoven, The Netherlands; Interconnected Resource-aware Intelligent Systems Group, Eindhoven University of Technology, Eindhoven, The Netherlands;" Fordham University in New York City, Bronx, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1491"",""1502"",""Load balancing is directly associated with the overall performance of a parallel and distributed computing system. Although the relevant problems in communication and computation have been well studied in data center environments, few works have considered the issues in an Internet of Things (IoT) edge scenario. In fact, processing data in a load balancing way for the latter case is more challenging. The main reason is that, unlike a data center, both the data sources and the network infrastructure in an IoT edge system can be dynamic. Moreover, with different performance requirements from IoT networks and edge servers, it will be hard to characterize the performance model and to perform runtime optimization for the whole system. To tackle this problem, in this work, we propose a load-balancing aware networking approach for efficient data processing in IoT edge systems. Specifically, we introduce an IoT network dynamic clustering solution using the emerging deep reinforcement learning (DRL), which can both fulfill the communication balancing requirements from IoT networks and the computation balancing requirements from edge servers. Moreover, we implement our system with a long short term memory (LSTM) based Dueling Double Deep Q-Learning Network (D3QN) model, and our experiments with real-world datasets collected from an autopilot vehicle demonstrate that our proposed method can achieve significant performance improvement compared to benchmark solutions."",""1558-2183"","""",""10.1109/TPDS.2021.3116863"",""Fundamental Research Funds for the Central Universities(grant numbers:2021MS017)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555233"",""Load balancing";Internet of Things;network;edge computing;distributed systems;deep reinforcement learning;"LSTM"",""Servers";Internet of Things;Image edge detection;Computational modeling;Sensors;Data models;"Long short term memory"","""",""22"","""",""44"",""IEEE"",""30 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Dependent Function Embedding for Distributed Serverless Edge Computing,""S. Deng"; H. Zhao; Z. Xiang; C. Zhang; R. Jiang; Y. Li; J. Yin; S. Dustdar;" A. Y. Zomaya"",""College of Computer Science and Technology, Zhejiang University, Hangzhou, China"; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Zhejiang University City College, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Institute of Intelligence Applications, Yunnan University of Finance and Economics, Kunming, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Distributed Systems Group, Technische Universität Wien, Vienna, Austria;" School of Computer Science, University of Sydney, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2022"",""2022"",""33"",""10"",""2346"",""2357"",""Edge computing is booming as a promising paradigm to extend service provisioning from the centralized cloud to the network edge. Benefit from the development of serverless computing, an edge server can be configured as a carrier of limited serverless functions, in the way of deploying Docker runtime and Kubernetes engine. Meanwhile, an application generally takes the form of directed acyclic graphs (DAGs), where vertices represent dependent functions and edges represent data traffic. The status quo of minimizing the completion time (a.k.a. makespan) of the application motivates the study on optimal function placement. However, current approaches lose sight of proactively splitting and mapping the traffic to the logical data paths between the heterogeneous edge servers, which could affect the makespan significantly. To remedy that, we propose an algorithm, termed as Dependent Function Embedding (DPE), to get the optimal edge server for each function to execute and the moment it starts executing. DPE finds the best segmentation of each data traffic by exquisitely solving several infinity norm minimization problems. DPE is theoretically verified to achieve the global optimality. Extensive experiments on Alibaba cluster trace show that DPE significantly outperforms two baseline algorithms in makespan by 43.19% and 40.71%, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3137380"",""Key Research Project of Zhejiang Province(grant numbers:2022C01145)"; National Natural Science Foundation of China(grant numbers:U20A20173,62125206); Zhejiang University De-qing Institute of Advanced technology and Industrilization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665233"",""Edge computing";dependent function embedding;directed acyclic graph;function placement;"task scheduling"",""Servers";Routing;Edge computing;Virtual links;Power measurement;Internet of Things;"Surveillance"","""",""15"","""",""41"",""IEEE"",""29 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"DePo: Dynamically Offload Expensive Event Processing to the Edge of Cyber-Physical Systems,""M. Ma"; J. Zhang;" P. Wang"",""National Engineering Research Center for Software Engineering, Peking University, Beijing, China"; Autonomous Driving Laboratory, Tencent, Beijing, China;" National Engineering Research Center for Software Engineering, School of Software and Microelectronics, Peking University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Feb 2022"",""2022"",""33"",""9"",""2120"",""2132"",""Event processing is one of the cornerstones to manage massive data streams in Cyber-Physical Systems (CPS). Due to CPS applications' increasing complexity, detecting highly complicated events (aka. “expensive” events) leads to significant performance degradation, particularly harmful to mission-critical systems. To tackle this challenge, we define a new task - dynamic event processing offloading to CPS-edges. This paper proves the problem NP-hard and proposes a solution - DePo. DePo splits the expensive events into sub-models and offloads them to CPS edges. We design a long and short-term event memory mechanism in DePo that enables the edges and server to process expensive events collaboratively within their capabilities. Besides, we propose a concept called Edge Utility to measure the optimality of offloading schemes. A heuristic algorithm is presented in this study to guide how to dispatch events to edges, thereby helping DePo generate a sub-optimal solution in polynomial computational complexity. Our extensive experiments show that the performance gap between DePo and the optimal benchmark is less than 5%. DePo effectively reduces more than 40% redundant states and provides over 100% higher throughput than state-of-the-art approaches. Experimental results verified the high stability and scalability of DePo, especially when dealing with a large number of expensive events."",""1558-2183"","""",""10.1109/TPDS.2021.3135441"",""National Key Research and Development Program of China(grant numbers:2020YFB1805400)"; National Natural Science Foundation of China(grant numbers:62072006,92167104); Science and Technology on Communication Networks Laboratory(grant numbers:6142104200103);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9650526"",""Complex event processing";edge computing;cyber-physical systems;"task offloading"",""Task analysis";Image edge detection;Cyber-physical systems;Servers;Heuristic algorithms;Data models;"Transforms"","""",""2"","""",""42"",""IEEE"",""14 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Design and Implementation of 2D Convolution on x86/x64 Processors,""V. Kelefouras";" G. Keramidas"",""School of Engineering, Computing and Mathematics, University of Plymouth, Plymouth, U.K.";" School of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3800"",""3815"",""In this paper, a new method for accelerating the 2D direct Convolution operation on x86/x64 processors is presented. It includes efficient vectorization by using SIMD intrinsics, bit-twiddling optimizations, the optimization of the division operation, multi-threading using OpenMP, register blocking and the shortest possible bit-width value of the intermediate results. The proposed method, which is provided as open-source, is general and can be applied to other processor families too, e.g., Arm. The proposed method has been evaluated on two different multi-core Intel CPUs, by using twenty different image sizes, 8-bit integer computations and the most commonly used kernel sizes (3x3, 5x5, 7x7, 9x9). It achieves from $2.8\times$2.8× to $40\times$40× speedup over the Intel IPP library (OpenCV GaussianBlur and Filter2D routines), from $105 \times$105× to $400 \times$400× speedup over the gemm-based convolution method (by using Intel MKL int8 matrix multiplication routine), and from $8.5\times$8.5× to $618\times$618× speedup over the vslsConvExec Intel MKL direct convolution routine. The proposed method is superior as it achieves far fewer arithmetical and load/store instructions."",""1558-2183"","""",""10.1109/TPDS.2022.3171471"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9765671"",""Convolution";gaussian blur;code optimization;vectorization;AVX;OpenMP;OpenCV;Intel MKL;Intel IPP;high performance computing (HPC);"image processing"",""Convolution";Kernel;Libraries;Optimization;Codes;Registers;"Quantization (signal)"","""",""6"","""",""34"",""CCBY"",""29 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Design and Performance Characterization of RADICAL-Pilot on Leadership-Class Platforms,""A. Merzky"; M. Turilli; M. Titov; A. Al-Saadi;" S. Jha"",""Rutgers, The State University of New Jersey, Piscataway, NJ, USA"; Rutgers, The State University of New Jersey, Piscataway, NJ, USA; Rutgers, The State University of New Jersey, Piscataway, NJ, USA; Rutgers, The State University of New Jersey, Piscataway, NJ, USA;" Rutgers, The State University of New Jersey, Piscataway, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""818"",""829"",""Many extreme scale scientific applications have workloads comprised of a large number of individual high-performance tasks. The Pilot abstraction decouples workload specification, resource management, and task execution via job placeholders and late-binding. As such, suitable implementations of the Pilot abstraction can support the collective execution of large number of tasks on supercomputers. We introduce RADICAL-Pilot (RP) as a portable, modular and extensible pilot-enabled runtime system. We describe RP's design, architecture and implementation. We characterize its performance and show its ability to scalably execute workloads comprised of tens of thousands heterogeneous tasks on DOE and NSF leadership-class HPC platforms. Specifically, we investigate RP's weak/strong scaling with CPU/GPU, single/multi core, (non)MPI tasks and Python functions when using most of ORNL Summit and TACC Frontera. RADICAL-Pilot can be used stand-alone, as well as the runtime for third-party workflow systems."",""1558-2183"","""",""10.1109/TPDS.2021.3105994"",""National Science Foundation(grant numbers:NSF-1931512,NSF-1835449)"; ECP CANDLE and ExaWorks; Exascale Computing Project 17-SC-20-SC(grant numbers:DESC0012704); U.S. Department of Energy(grant numbers:DE-AC05-00OR22725); XSEDE resources(grant numbers:TG-MCB090174);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9519521"",""Middleware";high performance computing;RADICAL-Pilot;"python"",""Task analysis";Runtime;Resource management;Parallel processing;Tools;Throughput;"Programming"","""",""13"","""",""48"",""IEEE"",""19 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Design and Simulation of Content-Aware Hybrid DRAM-PCM Memory System,""Y. Fu"; Y. Lu; Z. Chen; Y. Wu;" N. Xiao"",""School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, Guangdong, China"; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, Guangdong, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, Guangdong, China; College of Command and Control Engineering, Army Engineering University, Nanjing, Jiangsu, China;" School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2021"",""2022"",""33"",""7"",""1666"",""1677"",""Phase Change Memory (PCM) can directly connect persistent memory to main memory bus, while it achieves high read throughput and low standby power, the critical concerns are its poor write performance and limited durability. A naturally in-spired design is the hybrid memory architecture that fuses DRAM and PCM, so as to exploit the positive aspects of both types of memory. Unfortunately, existing solutions are seriously challenged by the limited main memory size, which is the primary bottleneck of in-memory computing. In this paper, we introduce a novel Content Aware Hybrid DRAM-PCM memory system framework—CAHRAM, which exploits deduplication to improve line sharing with high memory efficiency. It reduces write traffic to hybrid memory by removing unnecessary duplicate line writes, thereby further enhancing the write endurance of PCM. And it also substantially extends available free memory space by coalescing redundant lines in hybrid memory. We also design a reference-based page migration technique to minimize the access overheads caused by the performance gap between DRAM and PCM. Compared with the state-of-the-art in a hybrid memory simulator, our experiment results show that CAHRAM can achieve the highest I/O performance and the longest PCM lifetime with the competitive efficiencies in space and energy."",""1558-2183"","""",""10.1109/TPDS.2021.3123539"",""National Natural Science Foundation of China(grant numbers:61832020,61872392)"; Natural Science Foundation of Jiangsu Province(grant numbers:BK20191327); Zhejiang Lab(grant numbers:2021KC0AB04); Guangdong Major Project of Basic and Applied Basic Research(grant numbers:2019B030302002); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211); Natural Science Foundation of Guangdong Province(grant numbers:2018B030312002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591354"",""Hybrid memory management";data deduplication;content awareness;"page migration"",""Random access memory";Phase change materials;Memory management;Nonvolatile memory;Memory architecture;Power demand;"Metadata"","""",""3"","""",""45"",""IEEE"",""27 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Detecting Performance Variance for Parallel Applications Without Source Code,""J. Zhai"; L. Zheng; F. Zhang; X. Tang; H. Wang; T. Yu; Y. Jin; S. L. Song;" W. Chen"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering, School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Computer Science, University of Sydney, Sydney, NSW, Australia;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4239"",""4255"",""For parallel applications, performance variance is a critical issue that can degrade performance and make applications’ behavior difficult to explain. Therefore, users and application developers should be able to detect and diagnose performance variance. Previous detection methods either introduce too much overhead and slow down applications, or rely on nontrivial source code analysis, which is impractical for production-run parallel systems. In this article, we propose Vapro, a framework for detecting and diagnosing performance variance in production-run parallel systems. Our method is based on an observation that most parallel programs contain code snippets that are executed repeatedly with a fixed workload and can be utilized to detect performance variance. We present State Transition Graph (STG) to track program execution and then do light-weight workload analysis on STG to locate performance variance. Vapro is able to successfully identify these snippets at runtime even without program source code. To diagnose the discovered variation, Vapro uses a progressive diagnosis method based on a hybrid model combining variance breakdown and statistical analysis. According to evaluating results, Vapro's performance overhead is only 1.38% on average. Vapro can identify performance variance in real applications caused by hardware issues, such as memory and IO. The standard deviation of the execution time is decreased by up to 73.5% when the identified variance is fixed. Vapro achieves 30.0% larger detection coverage than the state-of-the-art variance detection approach based on source code analysis."",""1558-2183"","""",""10.1109/TPDS.2022.3181799"",""National Key R&D Program of China(grant numbers:2021YFB0300300)"; National Natural Science Foundation of China(grant numbers:U20A20226,62072459,62172419); Beijing Natural Science Foundation(grant numbers:4202031,L192027,L192027); SOAR fellowship; University of Sydney faculty startup funding; Australian Research Council(grant numbers:DP210101984); China Postdoctoral Science Foundation(grant numbers:2020TQ0169); ShuiMu Tsinghua Scholar Fellowship(grant numbers:2019SM131); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program(grant numbers:20191080594);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9794597"",""Performance variance";anomaly detection;system noise;"parallel computing"",""Codes";Runtime;Hardware;Performance evaluation;Libraries;Benchmark testing;"Electric breakdown"","""",""1"","""",""71"",""IEEE"",""13 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"DeTraS: Delaying Stores for Friendly-Fire Mitigation in Hardware Transactional Memory,""R. Titos-Gil"; R. Fernández-Pascual; A. Ros;" M. E. Acacio"",""Deptartment de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain"; Deptartment de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Deptartment de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain;" Deptartment de Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Jun 2021"",""2022"",""33"",""1"",""1"",""13"",""Commercial Hardware Transactional Memory (HTM) systems are best-effort designs that leverage the coherence substrate to detect conflicts eagerly. Resolving conflicts in favor of the requesting core is the simplest option for ensuring deadlock freedom, yet it is prone to livelocks. In this work, we propose and evaluate DeTraS (Delayed Transactional Stores), an HTM-aware store buffer design aimed at mitigating such livelocks. DeTraS takes advantage of the fact that modern commercial processors implement a large store buffer, and uses it to prevent transactional stores predicted to conflict from performing early in the transaction. By leveraging existing processor structures, we propose a simple design that improves the ability of requester-wins HTM systems to achieve forward progress in spite of high contention while side-stepping the performance penalty of falling back to mutual exclusion. With just over 50 extra bytes, DeTraS captures the advantages of lazy conflict management without the complexity brought into the coherence fabric by commit arbitration schemes nor the relaxation of the single-writer invariant of prior works. Through detailed simulations of a 16-core tiled CMP using gem5, we demonstrate that DeTraS brings reductions in average execution time of 25 percent when compared to an Intel RTM-like design."",""1558-2183"","""",""10.1109/TPDS.2021.3085210"",""Spanish MCIU"; AEI; European Commission(grant numbers:RTI2018-098156-B-C53); Spanish MCIU(grant numbers:ERC2018-092826); European Research Council(grant numbers:819134);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444639"",""Multicore architectures";transactional memory;store buffer;"coherence protocol"",""Buffer storage";Program processors;Coherence;Hardware;Out of order;Complexity theory;"System recovery"","""",""1"","""",""31"",""IEEE"",""31 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"DH-SVRF: A Reconfigurable Unicast/Multicast Forwarding for High-Performance Packet Forwarding Engines,""Z. Jin";" W. -K. Jia"",""College of Photonic and Electronic Engineering, Fujian Normal University, Fuzhou, Fujian, China";" College of Photonic and Electronic Engineering, Fujian Normal University, Fuzhou, Fujian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1262"",""1275"",""High-performance multicast-enabled packet forwarding engines (PFEs), as an essential component of high-end switches, use a polynomial-time membership query algorithm to determine which port(s) the data packet should be forwarded. The currently widely used query algorithm is Bloom Filter (BF), which has been proven to have many fatal flaws. Another error-free membership query algorithm includes Scalar-pair Vectors Routing Forwarding (SVRF), Fractional-N Scalar-pair Vectors Routing Forwarding (Frac-N SVRF), and the Per-Port Prime Filter Array (P3FA) also have some shortcomings in space and time efficiencies. In this paper, we proposed a hybrid strategy: Divaricate Heterogeneous SVRF (DH-SVRF) scheme, which based on the P3FA and Frac-N SVRF, which randomly divides all member ships into N groups, and each group has the same structure and is independent of each other to obtain higher time efficiency and space utilization. Finally, we also discussed the selection of the optimal egress-diversity threshold. Through mathematical modeling and simulation, we validate that the proposed DH-SVRF scheme is superior to the SVRF/Frac-N SVRF and traditional BF in terms of scalability, space utilization, and time efficiency in specific conditions such as appropriate egress-diversity thresholds."",""1558-2183"","""",""10.1109/TPDS.2021.3108899"",""National Natural Science Foundation of China(grant numbers:61871131,U1805262)"; Key Laboratory of OptoElectronic Science and Technology for Medicine of Ministry of Education; Fujian Provincial Key Laboratory of Photonics Technology; Fujian Normal University, China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9527084"",""Multicast";packet forwarding engine (PFE);membership querying;bloom filter (BF);scalar-pair vectors routing and forwarding (SVRF);"divaricate heterogeneous"",""Engines";Routing;Unicast;Scalability;Multicast algorithms;Switches;"Filtering algorithms"","""",""5"","""",""46"",""IEEE"",""1 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"DHash: Dynamic Hash Tables With Non-Blocking Regular Operations,""J. Wang"; D. Liu; X. Fu; F. Xiao;" C. Tian"",""School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China"; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2022"",""2022"",""33"",""12"",""3274"",""3290"",""Once started, existing hash tables cannot change their pre-defined hash functions, even if the incoming data cannot be evenly distributed to the hash table buckets. In this paper, we present DHash, a type of hash table for shared memory systems, that can change its hash function and rebuild the hash table on the fly, without noticeably degrading its service. The major technical novelty of DHash stems from an efficient distributing mechanism that can atomically distribute every node when rebuilding, without locking the corresponding hash table buckets. This not only enables non-blocking lookup, insert, and delete operations, but more importantly, makes DHash independent of the implementation of hash table buckets, such that DHash allows programmers to select the set algorithms that meet their requirements best from a variety of existing lock-free and wait-free set algorithms. Evaluations show that DHash can efficiently change its hash function on the fly. Moreover, when rebuilding, DHash consistently outperforms the state-of-the-art hash tables in terms of throughput and response time of concurrent operations, at different concurrency levels, and with different operation mixes and average load factors."",""1558-2183"","""",""10.1109/TPDS.2022.3151499"",""National Science Fund for Distinguished Young Scholars(grant numbers:62125203)"; National Natural Science Foundation of China(grant numbers:61932013,62072228);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714033"",""Concurrent programming";parallelism and concurrency;"dynamic hash tables"",""Hash functions";Heuristic algorithms;Robustness;Kernel;Synchronization;Linux;"Time factors"","""","""","""",""42"",""IEEE"",""14 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"DIESEL+: Accelerating Distributed Deep Learning Tasks on Image Datasets,""L. Wang"; Q. Luo;" S. Yan"",""Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China"; Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China;" SenseTime Research, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1173"",""1184"",""We observe that data access and processing takes a significant amount of time in large-scale deep learning training tasks (DLTs) on image datasets. Three factors contribute to this problem: (1) the massive and recurrent accesses to large numbers of small files"; (2) the repeated, expensive decoding computation on each image, and (3) the frequent communication between computation nodes and storage nodes. Existing work has addressed some aspects of these problems; however, no end-to-end solutions have been proposed. In this article, we propose DIESEL+, an all-in-one system which accelerates the entire I/O pipeline of deep learning training tasks. DIESEL+ contains several components: (1) local metadata snapshot; (2) per-task distributed caching; (3) chunk-wise shuffling;" (4) GPU-assisted image decoding and (5) online region-of-interest (ROI) decoding. The metadata snapshot removes the bottleneck on metadata access in frequent reading of large numbers of files. The per-task distributed cache across the worker nodes of a DLT task to reduce the I/O pressure on the underlying storage. The chunk-based shuffle method converts small file reads into large chunk reads, so that the performance is improved without sacrificing the training accuracy. The GPU-assisted image decoding and the online ROI method minimize the image decoding workloads and reduce the cost of data movement between nodes. These techniques are seamlessly integrated into the system. In our experiments, DIESEL+ outperforms existing systems by a factor of two to three times on the overall training time."",""1558-2183"","""",""10.1109/TPDS.2021.3104252"",""SenseTime Group(grant numbers:18191980)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511819"",""Storage system";dataset management;deep learning;distributed cache;dataset shuffling;image decoding;"GPU"",""Decoding";Training;Task analysis;Transform coding;Deep learning;Metadata;"Image coding"","""",""2"","""",""44"",""IEEE"",""11 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Differentially Private Byzantine-Robust Federated Learning,""X. Ma"; X. Sun; Y. Wu; Z. Liu; X. Chen;" C. Dong"",""School of Cyber Science and Engineering, Qufu Normal University, Qufu, China"; School of Cyber Science and Engineering, Qufu Normal University, Qufu, China; College of Cyber Science and College of Computer Science, Nankai University, Tianjin, China; College of Cyber Science and College of Computer Science, Nankai University, Tianjin, China; State Key Laboratory of Integrated Service Networks (ISN), Xidian University, Xi’an, China;" School of Computing, Newcastle University, Newcastle Upon Tyne, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3690"",""3701"",""Federated learning is a collaborative machine learning framework where a global model is trained by different organizations under the privacy restrictions. Promising as it is, privacy and robustness issues emerge when an adversary attempts to infer the private information from the exchanged parameters or compromise the global model. Various protocols have been proposed to counter the security risks, however, it becomes challenging when one wants to make federated learning protocols robust against Byzantine adversaries while preserving the privacy of the individual participant. In this article, we propose a differentially private Byzantine-robust federated learning scheme (DPBFL) with high computation and communication efficiency. The proposed scheme is effective in preventing adversarial attacks launched by the Byzantine participants and achieves differential privacy through a novel aggregation protocol in the shuffle model. The theoretical analysis indicates that the proposed scheme converges to the approximate optimal solution with the learning error dependent on the differential privacy budget and the number of Byzantine participants. Experimental results on MNIST, FashionMNIST and CIFAR10 demonstrate that the proposed scheme is effective and efficient."",""1558-2183"","""",""10.1109/TPDS.2022.3167434"",""National Natural Science Foundation of China(grant numbers:62072132,61960206014,62032012)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9757841"",""Federated learning";differential privacy;"byzantine-robust"",""Collaborative work";Privacy;Servers;Differential privacy;Computational modeling;Training;"Data models"","""",""11"","""",""44"",""IEEE"",""14 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Algorithms for Multi-Resource Allocation,""F. Fossati"; S. Rovedakis;" S. Secci"",""CentraleSupélec, Gif-sur-Yvette, France"; Cnam, Cedric, Paris, France;" Cnam, Cedric, Paris, France"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Apr 2022"",""2022"",""33"",""10"",""2524"",""2539"",""Novel network infrastructures require the distribution of computing and network resource control to meet stringent requirements in terms of latency, reliability and bitrate. 5G systems bring a key novelty in systems design that it the ‘network slice’as a new resource provisioning entity. A network slice is meant to serve end-to-end services as a composition of different network and system resources as radio, link, storage and computing resources. Conventionally, each resource is managed by a distinct decision-maker, platform, provider, orchestrator or controller. Naturally, centralized slice orchestration approaches are proposed in the literature, where a multi-domain orchestrator allocates the resources, for instance using a multi-resource allocation rule. Nonetheless, while simplifying the algorithmic approach, centralization can come at the expense of scalability and performance. In this article, we propose new ways to distribute the slice multi-resource resource allocation problem, using cascade and parallel resource allocations that are functionally compatible with novel software platforms. We also show how to adapt the proposed algorithms to make them able to guarantee service level agreements on the minimum resource needed, and to take into account deadline priority policy scheduling. We provide an exhaustive analysis of the advantages and disadvantages of the different approaches, including a numerical analysis for a realistic setting."",""1558-2183"","""",""10.1109/TPDS.2022.3144376"",""Agence Nationale de la Recherche(grant numbers:ANR-18-CE25-0012)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695170"",""Multi-resource allocation";network slicing;"network resource control"",""Resource management";Network slicing;Service level agreements;Radio access networks;Games;Distributed algorithms;"Delays"","""",""2"","""",""33"",""IEEE"",""27 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Evolution Strategies for Black-Box Stochastic Optimization,""X. He"; Z. Zheng; C. Chen; Y. Zhou; C. Luo;" Q. Lin"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Software, Beihang University, Beijing, China;" Microsoft Research, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Jul 2022"",""2022"",""33"",""12"",""3718"",""3731"",""This work concerns the evolutionary approaches to distributed stochastic black-box optimization, in which each worker can individually solve an approximation of the problem with nature-inspired algorithms. We propose a distributed evolution strategy (DES) algorithm grounded on a proper modification to evolution strategies, a family of classic evolutionary algorithms, as well as a careful combination with existing distributed frameworks. On smooth and nonconvex landscapes, DES has a convergence rate competitive to existing zeroth-order methods, and can exploit the sparsity, if applicable, to match the rate of first-order methods. The DES method uses a Gaussian probability model to guide the search and avoids the numerical issue resulted from finite-difference techniques in existing zeroth-order methods. The DES method is also fully adaptive to the problem landscape, as its convergence is guaranteed with any parameter setting. We further propose two alternative sampling schemes which significantly improve the sampling efficiency while leading to similar performance. Simulation studies on several machine learning problems suggest that the proposed methods show much promise in reducing the convergence time and improving the robustness to parameter settings."",""1558-2183"","""",""10.1109/TPDS.2022.3168873"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B010165003)"; National Natural Science Foundation of China(grant numbers:62006252,62176269); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515011840);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762038"",""Evolution strategies";distributed optimization;black-box optimization;stochastic optimization;"zeroth-order methods"",""Smoothing methods";Stochastic processes;Convergence;Optimization methods;Machine learning;Linear programming;"Distributed databases"","""",""1"","""",""55"",""IEEE"",""22 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Distributed Graph Realizations,""J. Augustine"; K. Choudhary; A. Cohen; D. Peleg; S. Sivasubramaniam;" S. Sourav"",""Indian Institute of Technology Madras, Chennai, Tamil Nadu, India"; Tel Aviv University, Tel Aviv-Yafo, Israel; Weizmann Institute of Science, Rehovot, Israel; Weizmann Institute of Science, Rehovot, Israel; Indian Institute of Technology Madras, Chennai, Tamil Nadu, India;" Advanced Digital Sciences Center, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1321"",""1337"",""We study graph realization problems for the first time from a distributed perspective. Graph realization problems are encountered in distributed construction of overlay networks that must satisfy certain degree or connectivity properties. We study them in the node capacitated clique (NCC) model of distributed computing, recently introduced for representing peer-to-peer overlay networks. We focus on two central variants, degree-sequence realization and minimum threshold-connectivity realization. In the degree sequence problem, each node $v$v is associated with a degree $d(v)$d(v), and the resulting degree sequence is realizable if it is possible to construct an overlay network in which the degree of each node $v$v is $d(v)$d(v). The minimum threshold-connectivity problem requires us to construct an overlay network that satisfies connectivity constraints specified between every pair of nodes. Overlay network realizations can be either explicit or implicit. Explicit realizations require both endpoints of any edge in the realized graph to be aware of the edge. In implicit realizations, on the other hand, at least one endpoint of each edge of the realized graph needs to be aware of the edge. The main realization algorithms we present are the following. (Note that all our algorithms are randomized Las Vegas algorithms unless specified otherwise. The stated running times hold with high probability.) 1) An $\tilde{O}(\min \lbrace \sqrt{m},\Delta \rbrace)$O˜(min{m,Δ}) time algorithm for implicit realization of a degree sequence. Here, $\Delta = \max _v d(v)$Δ=maxvd(v) is the maximum degree and $m = (1/2) \sum _v d(v)$m=(1/2)∑vd(v) is the number of edges in the final realization. 2) $\tilde{O}(\Delta)$O˜(Δ) time algorithm for an explicit realization of a degree sequence. We first compute an implicit realization and then transform it into an explicit one in $\tilde{O}(\Delta)$O˜(Δ) additional rounds. 3) An $\tilde{O}(\Delta)$O˜(Δ) time algorithm for the threshold connectivity problem that obtains an explicit solution and an improved $\tilde{O}(1)$O˜(1) algorithm for implicit realization when all nodes know each other’s IDs. These algorithms yield 2-approximations w.r.t. the number of edges. We complement our upper bounds with lower bounds to show that the above algorithms are tight up to factors of $\log n$logn. Additionally, we provide algorithms for realizing trees (including a procedure for obtaining a tree with a minimal diameter), an $\tilde{O}(1)$O˜(1) round algorithm for approximate degree sequence realization and finally an $O(\log ^2 n)$O(log2n) algorithm for degree sequence realization in the non-preassigned case namely, where the input degree sequence may be permuted among the nodes."",""1558-2183"","""",""10.1109/TPDS.2021.3104239"",""DST/SERB Extra Mural(grant numbers:EMR/2016/00301)"; DST/SERB MATRICS(grant numbers:MTR/2018/001198); Indian Institute of Technology Madras;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511805"",""Peer-to-peer overlay networks";node capacitated clique model;graph realization;degree realization;"connectivity realization"",""Peer-to-peer computing";Overlay networks;Graphics;Approximation algorithms;Computational modeling;Adaptation models;"Upper bound"","""",""2"","""",""43"",""IEEE"",""11 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"DLS: A Fast and Flexible Neural Network Training System With Fine-grained Heterogeneous Device Orchestration,""P. Park"; J. Lee; H. Jeong;" J. Kim"",""Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea"; Facebook, Menlo Park, CA, USA; Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea;" Department of Electrical and Computer Engineering, Seoul National University, Seoul, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3194"",""3206"",""Neural network accelerators (e.g., TPUs) have become mainstream devices in computing systems. Unfortunately, the existing accelerator-based systems for neural networks fail to fully leverage the acceleration opportunities due to the limited flexibility. Specifically, the majority of the accelerators focus on only the compute-intensive operations of neural networks (e.g., convolution and fully-connected layers). However, we identify that sub-optimal handling of auxiliary operations such as embedding and compression can incur non-trivial loss in terms of accuracy, training speed, and adaptability to new domains. The problem persists considering that recent advancements in neural networks often come from auxiliary operations. To effectively handle rapidly evolving auxiliary operations and maximize acceleration opportunities, we propose DLS, a holistic neural network acceleration system using heterogeneous computing devices. The key idea is to distribute compute-intensive operations on highly specialized ASICs for maximum performance, and auxiliary operations on flexible devices (e.g., FPGA, GPU) for better adaptability. We emphasize that a naïve integration of different devices fails to deliver high performance due to high communication overheads. To address this communication inefficiency, we propose an efficient FPGA-based device orchestration utilizing direct device-to-device communication and fine-grained operation scheduling. In this way, our system alleviates the communication overhead between heterogeneous devices by removing expensive kernel stack traversal and leveraging computation units and communication links in parallel. The evaluation using popular neural networks with emerging auxiliary operations shows that our system achieves both flexibility and high performance for various cases from single-accelerator training to distributed training (2.6–8.9× speedup)."",""1558-2183"","""",""10.1109/TPDS.2022.3144453"",""Samsung(grant numbers:SRFC-IT1901-12)"; Seoul National University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9689955"",""Neural network";accelerators;GPU;FPGA;flexibility;heterogeneous devices;"inter-device communication"",""Artificial neural networks";Training;Performance evaluation;Field programmable gate arrays;Device-to-device communication;Task analysis;"Predictive models"","""","""","""",""37"",""IEEE"",""21 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"DONE: Distributed Approximate Newton-type Method for Federated Edge Learning,""C. T. Dinh"; N. H. Tran; T. D. Nguyen; W. Bao; A. R. Balef; B. B. Zhou;" A. Y. Zomaya"",""School of Computer Science, The University of Sydney, Sydney, NSW, Australia"; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Computing, The Australian National University, Canberra, ACT, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; Sharif University of Technology, Tehran, Iran; School of Computer Science, The University of Sydney, Sydney, NSW, Australia;" School of Computer Science, The University of Sydney, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2648"",""2660"",""There is growing interest in applying distributed machine learning to edge computing, forming federated edge learning. Federated edge learning faces non-i.i.d. and heterogeneous data, and the communication between edge workers, possibly through distant locations and with unstable wireless networks, is more costly than their local computational overhead. In this work, we propose ${{\sf DONE}}$DONE, a distributed approximate Newton-type algorithm with fast convergence rate for communication-efficient federated edge learning. First, with strongly convex and smooth loss functions, ${{\sf DONE}}$DONE approximates the Newton direction in a distributed manner using the classical Richardson iteration on each edge worker. Second, we prove that ${{\sf DONE}}$DONE has linear-quadratic convergence and analyze its communication complexities. Finally, the experimental results with non-i.i.d. and heterogeneous data show that ${{\sf DONE}}$DONE attains a comparable performance to Newton's method. Notably, ${{\sf DONE}}$DONE requires fewer communication iterations compared to distributed gradient descent and outperforms DANE, FEDL, and GIANT, state-of-the-art approaches, in the case of non-quadratic loss functions."",""1558-2183"","""",""10.1109/TPDS.2022.3146253"",""Vietnam National Foundation for Science and Technology Development(grant numbers:102.02-2019.321)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695269"",""Distributed machine learning";federated learning;"optimization decomposition"",""Newton method";Distributed databases;Machine learning;Convergence;Costs;Complexity theory;"Approximation algorithms"","""",""5"","""",""36"",""IEEE"",""27 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"DRAS: Deep Reinforcement Learning for Cluster Scheduling in High Performance Computing,""Y. Fan"; B. Li; D. Favorite; N. Singh; T. Childers; P. Rich; W. Allcock; M. E. Papka;" Z. Lan"",""Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA"; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA;" Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Oct 2022"",""2022"",""33"",""12"",""4903"",""4917"",""Cluster schedulers are crucial in high-performance computing (HPC). They determine when and which user jobs should be allocated to available system resources. Existing cluster scheduling heuristics are developed by human experts based on their experience with specific HPC systems and workloads. However, the increasing complexity of computing systems and the highly dynamic nature of application workloads have placed tremendous burden on manually designed and tuned scheduling heuristics. More aggressive optimization and automation are needed for cluster scheduling in HPC. In this work, we present an automated HPC scheduling agent named DRAS (Deep Reinforcement Agent for Scheduling) by leveraging deep reinforcement learning. DRAS is built on a hierarchical neural network incorporating special HPC scheduling features such as resource reservation and backfilling. An efficient training strategy is presented to enable DRAS to rapidly learn the target environment. Once being provided a specific scheduling objective given by the system manager, DRAS automatically learns to improve its policy through interaction with the scheduling environment and dynamically adjusts its policy as workload changes. We implement DRAS into a HPC scheduling platform called CQGym. CQGym provides a common platform allowing users to flexibly evaluate DRAS and other scheduling methods such as heuristic and optimization methods. The experiments using CQGym with different production workloads demonstrate that DRAS outperforms the existing heuristic and optimization approaches by up to 50%."",""1558-2183"","""",""10.1109/TPDS.2022.3205325"",""National Science Foundation(grant numbers:CNS-1717763,CCF-2109316,CCF-2119294)"; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357,DE-AC02-05CH11231); National Energy Research Scientific Computing Center;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894371"",""High-performance computing";cluster scheduling;deep reinforcement learning;job starvation;backfilling;resource reservation;"OpenAI Gym"",""Processor scheduling";Dynamic scheduling;Runtime;Neural networks;Training;Q-learning;"Production"","""",""3"","""",""51"",""IEEE"",""16 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"DrTM+B: Replication-Driven Live Reconfiguration for Fast and General Distributed Transaction Processing,""S. Shen"; X. Wei; R. Chen; H. Chen;" B. Zang"",""Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China"; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China;" Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Apr 2022"",""2022"",""33"",""10"",""2628"",""2643"",""Recent in-memory database systems leverage advanced hardware features like RDMA to provide transaction processing at millions of transactions per second. Distributed transaction processing systems can scale to even higher rates, especially for partitionable workloads. Unfortunately, it is challenging to sustain such high rates during live reconfiguration of partitions. In this article, we observe that state-of-the-art approaches would cause notable performance disruption under fast transaction processing. To this end, this article presents DrTM+B, a live reconfiguration approach that seamlessly repartitions data with little performance disruption to running transactions. DrTM+B uses a pre-copy-based mechanism to avoid excessive data transfer by leveraging common properties in recent transactional systems. DrTM+B's reconfiguration plans reduce data movement by preferring existing data replicas, while copying data from multiple replicas asynchronously and in parallel. It further reuses the log forwarding mechanism in primary-backup replication to seamlessly track and forward dirty database tuples and avoids iterative copying costs. To commit a reconfiguration plan in a transactional-safe way, DrTM+B designs a cooperative commit protocol for synchronization of data and state among replicas. To boost the performance during data migration, DrTM+B combines the pre-copy and post-copy schemes to propose a hybrid copy scheme. The live reconfiguration approach can also coexist with fault-tolerance mechanisms of primary-backup replication to provide high availability. Evaluation on a working system based on DrTM+R with 3-way replication using typical OLTP workloads like TPC-C and SmallBank shows that DrTM+B incurs only very small performance degradation during live reconfiguration and provides high availability. Both the reconfiguration time and the downtime are also minimal."",""1558-2183"","""",""10.1109/TPDS.2022.3148251"",""National Key Research & Development Program(grant numbers:2020AAA0108500)"; National Natural Science Foundation of China(grant numbers:61732010,61925206,62172272);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9705084"",""Distributed transactions";load balance;live reconfiguration;data replication;"RDMA"",""Throughput";Fault tolerant systems;Fault tolerance;Protocols;Low latency communication;Optimization;"Distributed databases"","""",""1"","""",""55"",""IEEE"",""4 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"DS-ADMM++: A Novel Distributed Quantized ADMM to Speed up Differentially Private Matrix Factorization,""F. Zhang"; E. Xue; R. Guo; G. Qu; G. Zhao;" A. Y. Zomaya"",""School of Computer Science, China University of Geoscience, Wuhan, Hubei, China"; School of Computer Science, China University of Geoscience, Wuhan, Hubei, China; School of Computer Science, China University of Geoscience, Wuhan, Hubei, China; Department of Engineering and Computer Science, Oakland University, Rochester, MI, USA; School of Computer Science, South China Normal University, Guangzhou, Guangdong, China;" School of Computer Science, University of Sydney, Camperdown, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Nov 2021"",""2022"",""33"",""6"",""1289"",""1302"",""Matrix factorization is a powerful method to implement collaborative filtering recommender systems. This article addresses two major challenges, privacy and efficiency, which matrix factorization is facing. We based our work on DS-ADMM, a distributed matrix factorization algorithm with decent efficiency, to achieve the following two pieces of work: (1) Integrated local differential privacy paradigm into DS-ADMM to provide the privacy-preserving property";" (2) Introduced a stochastic quantized function to reduce transmission overheads in ADMM to further improve efficiency. We named our work DS-ADMM++, in which one ’+’ refers to differential privacy, and the other ’+’ refers to quantized techniques. DS-ADMM++ is the first to perform efficient and private matrix factorization under the scenarios of differential privacy and DS-ADMM. We conducted experiments with benchmark data sets to demonstrate that our approach provides differential privacy and excellent scalability with a decent loss of accuracy."",""1558-2183"","""",""10.1109/TPDS.2021.3110104"",""National Key Research and Development Program of China(grant numbers:2018YFB1404402)"; Overseas Scientific and Technological Cooperation(grant numbers:2020197001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9529010"",""Matrix factorization";parallel and distributed computing;differential privacy;ADMM;"quantization"",""Differential privacy";Quantization (signal);Matrix decomposition;Recommender systems;Charge coupled devices;Machine learning;"Costs"","""",""7"","""",""45"",""IEEE"",""3 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Dynamic Controller/Switch Mapping: A Service Oriented Assignment Approach,""C. Pham"; D. T. Nguyen; N. H. Tran; K. K. Nguyen;" M. Cheriet"",""Synchromedia - Ecole de Technologie Superieure, Universite du Quebec, Quebec, Canada"; Synchromedia - Ecole de Technologie Superieure, Universite du Quebec, Quebec, Canada; School of Computer Science, University of Sydney, NSW, Australia; Synchromedia - Ecole de Technologie Superieure, Universite du Quebec, Quebec, Canada;" Synchromedia - Ecole de Technologie Superieure, Universite du Quebec, Quebec, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Apr 2022"",""2022"",""33"",""10"",""2482"",""2495"",""With the capability of decoupling the control plane and the data plane of networks, Software-Defined Network (SDN) enables flexible and efficient implementations in networks. In addition, Network Function Virtualization (NFV) with Virtual Network Function (VNF) service chain capabilities provides high-performance networks with greater scalability, elasticity, and adaptability. Such an elastic deployment of service chains results in different Service Level Agreements (SLA) and resource requirements on the control plane. In this work, we illustrate the impact of service chains on the control plane and formulate the dynamic controller/switch mapping (DCSM) problem in NFV networks in order to reduce the operational cost. We address the combinatorial optimization problem, DCSM, by designing a novel mechanism to relax DCSM into a tractable problem based on the Penalty Successive Upper Bound Minimization (PSUM) method. In doing so, we conduct several simulation scenarios to evaluate the performance. The experimental results show that our proposed algorithms can achieve a near-optimal result and reduce the operational cost up to 31.7% and 28.3% compared to K-Mean and the matching game-based approaches, respectively."",""1558-2183"","""",""10.1109/TPDS.2022.3144116"",""Natural Sciences and Engineering Research Council of Canada(grant numbers:506319-17)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9690940"",""Software-defined network";network function virtualization;"service chain"",""Control systems";Routing;Costs;Resilience;Reliability;Optimization;"Process control"","""","""","""",""43"",""IEEE"",""25 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Dynamic GPU Energy Optimization for Machine Learning Training Workloads,""F. Wang"; W. Zhang; S. Lai; M. Hao;" Z. Wang"",""School of Cyberspace Science, Harbin Institute of Technology, Harbin, China"; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China;" School of Computing, University of Leeds, Leeds, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2943"",""2954"",""GPUs are widely used to accelerate the training of machine learning workloads. As modern machine learning models become increasingly larger, they require a longer time to train, leading to higher GPU energy consumption. This paper presents GPOEO, an online GPU energy optimization framework for machine learning training workloads. GPOEO dynamically determines the optimal energy configuration by employing novel techniques for online measurement, multi-objective prediction modeling, and search optimization. To characterize the target workload behavior, GPOEO utilizes GPU performance counters. To reduce the performance counter profiling overhead, it uses an analytical model to detect the training iteration change and only collects performance counter data when an iteration shift is detected. GPOEO employs multi-objective models based on gradient boosting and a local search algorithm to find a trade-off between execution time and energy consumption. We evaluate the GPOEO by applying it to 71 machine learning workloads from two AI benchmark suites running on an NVIDIA RTX3080Ti GPU. Compared with the NVIDIA default scheduling strategy, GPOEO delivers a mean energy saving of 16.2% with a modest average execution time increase of 5.1%."",""1558-2183"","""",""10.1109/TPDS.2021.3137867"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101360001)"; National Key Research and Development Program of China(grant numbers:2020YFB1406902); Shenzhen Science and Technology Research and Development Foundation(grant numbers:JCYJ20190806143418198); National Natural Science Foundation of China(grant numbers:61872110,61872294); Fundamental Research Funds for the Central Universities(grant numbers:HIT.OCEF.2021007); Peng Cheng Laboratory Project(grant numbers:PCL2021A02); CCF-Huawei(grant numbers:CCF-HuaweiHP2021002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661449"",""Dynamic energy optimization";online application iteration detection;multi-objective machine learning;"GPU"",""Clocks";Graphics processing units;Optimization;Training;Micromechanical devices;Machine learning;"Predictive models"","""",""6"","""",""25"",""IEEE"",""23 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"E3NE: An End-to-End Framework for Accelerating Spiking Neural Networks With Emerging Neural Encoding on FPGAs,""D. Gerlinghoff"; Z. Wang; X. Gu; R. S. M. Goh;" T. Luo"",""Institute of High Performance Computing, Agency for Science Technology and Research (A*STAR), Singapore, Singapore"; Institute of High Performance Computing, Agency for Science Technology and Research (A*STAR), Singapore, Singapore; Future Network of Intelligence Institute, Chinese University of Hong Kong, Shenzhen, China; Institute of High Performance Computing, Agency for Science Technology and Research (A*STAR), Singapore, Singapore;" Institute of High Performance Computing, Agency for Science Technology and Research (A*STAR), Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3207"",""3219"",""Compiler frameworks are crucial for the widespread use of FPGA-based deep learning accelerators. They allow researchers and developers, who are not familiar with hardware engineering, to harness the performance attained by domain-specific logic. There exists a variety of frameworks for conventional artificial neural networks. However, not much research effort has been put into the creation of frameworks optimized for spiking neural networks (SNNs). This new generation of neural networks becomes increasingly interesting for the deployment of AI on edge devices, which have tight power and resource constraints. Our end-to-end framework E3NE automates the generation of efficient SNN inference logic for FPGAs. Based on a PyTorch model and user parameters, it applies various optimizations and assesses trade-offs inherent to spike-based accelerators. Multiple levels of parallelism and the use of an emerging neural encoding scheme result in an efficiency superior to previous SNN hardware implementations. For a similar model, E3NE uses less than 50% of hardware resources and 20% less power, while reducing the latency by an order of magnitude. Furthermore, scalability and generality allowed the deployment of the large-scale SNN models AlexNet and VGG."",""1558-2183"","""",""10.1109/TPDS.2021.3128945"",""Singapore Governments Research, Innovation and Enterprise 2020 Plan(grant numbers:A1687b0033)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9619972"",""Spiking neural network";neuromorphic computing;neural encoding;compiler framework;"FPGA"",""Hardware";Neurons;Field programmable gate arrays;Encoding;Biological neural networks;Computational modeling;"Optimization"","""",""8"","""",""53"",""IEEE"",""18 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"EdgeDR: An Online Mechanism Design for Demand Response in Edge Clouds,""S. Chen"; L. Jiao; F. Liu;" L. Wang"",""National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab in the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab in the School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China;" Department of Computer Science, Vrije Universiteit Amsterdam, Amsterdam, HV, The Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2021"",""2022"",""33"",""2"",""343"",""358"",""The computing frontier is moving from centralized mega datacenters towards distributed cloudlets at the network edge. We argue that cloudlets are well-suited for handling power demand response to help the grid maintain stability due to more flexible workload management attributed to their distributed nature. However, they also require computing demand response to avoid overload and maintain reliability. To this end, we propose a novel online market mechanism, EdgeDR, to achieve cost efficiency in edge demand response programs. At a high level, we observe that the cloudlet operator can dynamically switch on/off entire cloudlets to compensate for the energy reduction required by the power grid or provide enough computing resources to the edge service. We formulate a long-term social cost minimization problem and decompose it into a series of one-round procurement auctions. In each auction instance, we propose to let the cloudlet tenants bid with cost functions of their two-dimension service quality degradation tolerance, and let the cloudlet operator choose the service quality, manage the workload, and schedule the cloudlet activation status. In addition, we present a dynamic payment mechanism for the operator to balance the tradeoff between short-term profit and long-term benefit in more practical scenarios. Via rigorous analysis, we exhibit that our bidding policy is individually rational and truthful"; our workload management algorithm has near-optimal performance in each auction;" and our overall online algorithm achieves a provable competitive ratio. We further confirm the performance of our mechanism through extensive trace-driven simulations."",""1558-2183"","""",""10.1109/TPDS.2021.3087360"",""National Natural Science Foundation of China(grant numbers:61722206,61761136014,61520106005)"; DFG(grant numbers:392046569); National Key Research and Development Program of China(grant numbers:2017YFB1001703); Fundamental Research Funds for the Central Universities(grant numbers:2017KFKJXX009,3004210116); National Program for Support of Top-notch Young Professionals; Ripple Faculty Fellowship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448461"",""Edge demand response";energy saving;cloudlet control;"online mechanism"",""Cloud computing";Throughput;Degradation;Load management;Switches;Propagation delay;"Delays"","""",""20"","""",""55"",""IEEE"",""8 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"EdgeTB: A Hybrid Testbed for Distributed Machine Learning at the Edge With High Fidelity,""L. Yang"; F. Wen; J. Cao;" Z. Wang"",""School of Software Engineering, South China University of Technology, Guangzhou, China"; School of Software Engineering, South China University of Technology, Guangzhou, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong, China;" School of Software Engineering, South China University of Technology, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Apr 2022"",""2022"",""33"",""10"",""2540"",""2553"",""Distributed Machine Learning (DML) at the edge has become an essential topic for providing low-latency intelligence near the data sources. However, both the development and testing of DMLs lack sufficient support. Reusable libraries that abstract the general functionalities of DMLs are needed for rapid development. Moreover, existing physical testbeds are usually small and lack network flexibility, while virtual testbeds like simulators and emulators lack fidelity. This paper proposes a novel hybrid testbed EdgeTB, which provides numerous emulated nodes to generate large-scale and network-flexible test environments while incorporating physical nodes to guarantee fidelity. EdgeTB manages physical nodes and emulated nodes uniformly and supports arbitrary network topologies between nodes through dynamic configurations. Importantly, we propose Role-oriented development to support the rapid development of DMLs. Through case studies and experiments, we demonstrate that EdgeTB provides convenience for efficiently developing and testing DMLs in various structures with high fidelity and scalability."",""1558-2183"","""",""10.1109/TPDS.2022.3144994"",""National Natural Science Foundation of China(grant numbers:61972161)"; Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515011496); Key Research and Development Program of Guangdong(grant numbers:2019B010154004); Hong Kong RGC General Research Fund(grant numbers:PolyU 152133/18,PolyU 15217919); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693130"",""Testbed";emulator;edge computing;"distributed machine learning"",""Edge computing";Costs;Machine learning;Computational modeling;Peer-to-peer computing;Libraries;"Cloud computing"","""",""3"","""",""35"",""IEEE"",""25 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient and Accurate Flow Record Collection With HashFlow,""Z. Zhao"; X. Shi; Z. Wang; Q. Li; H. Zhang;" X. Yin"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Peng Cheng Laboratory, Shenzhen, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Oct 2021"",""2022"",""33"",""5"",""1069"",""1083"",""Traditional tools like NetFlow face great challenges as both the speed and the complexity of the network traffic increase. To keep the pace up, we propose HashFlow for more efficient and accurate collection of flow records. HashFlow keeps large flows in its main flow table and uses an ancillary table to summarize the other flows when the main table is full. With our flow collision resolution and flow record promotion schemes, a flow in the ancillary table is promoted back to the main flow table with a guaranteed probability when it becomes large enough. These operations can be performed highly efficiently, so HashFlow can keep up with ultra-high traffic speed. We implement HashFlow in a Tofino switch, and using traces from different operational networks, we compare its performance against some state-of-the-art flow measurement algorithms. Our experiments show that, for various types of traffic analysis applications, HashFlow consistently demonstrates clearly better performance than its competitors. For example, the performance of HashFlow in flow size estimation, flow size distribution estimation and heavy hitter detection is up to 21, 60 and 35 percent better than those of the best competitors respectively, and these merits of HashFlow come with almost no degradation of throughput."",""1558-2183"","""",""10.1109/TPDS.2021.3099442"",""National Key Research and Development Program of China(grant numbers:2018YFB1800401)"; National Natural Science Foundation of China(grant numbers:61972189,62002009);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495221"",""Network measurement";flow record measurement;"sketch"",""Switches";Data structures;Tools;Random access memory;Hash functions;Throughput;"Telecommunication traffic"","""",""4"","""",""42"",""IEEE"",""26 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient and Automated Deployment Architecture for OpenStack in TianHe SuperComputing Environment,""B. Jiang"; Z. Tang; X. Xiao; J. Yao; R. Cao;" K. Li"",""College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China"; College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China; College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China; School of Computer Science and Engineering, Central South University, Changsha, China; College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China;" College of Information Science and Engineering, and the National Supercomputing Center in Changsha, Hunan University, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Dec 2021"",""2022"",""33"",""8"",""1811"",""1824"",""Recently, with the large-scale outbreak of the global financial crisis and public safety incidents (such as COVID-19), high-performance computing has been widely applied to risk prediction, vaccine development, and other fields. In scenarios where high-performance computing infrastructure responds to the instantaneous explosion of computing demands, a crucial issue is to provide large-scale flexible allocation and adjustment of computing capability by rapidly constructing computing clusters. Existing large-scale computing cluster deployment solutions usually utilize source code deployment or other deployment tools. The great challenge of existing deployment methods is to reduce excessive image distribution time and refrain from configuration defects. In this article, we design an intelligent distributed registry deployment (IDRD) architecture based on the OpenStack cloud platform, which adaptively places distributed image repositories using the containerized deployment of multiple registries. We propose a server load priority algorithm to solve multiple registries placement problems in IDRD. Furthermore, we devise a clustering algorithm based on demand density that can optimize the global performance of IDRD and improve large-scale cluster load balancing capabilities, which has been implemented in the TianHe Supercomputing environment. Extensive experimental results demonstrate that IDRD can effectively reduce $30\%$30%-$50\%$50% of the distribution time of component images and significantly improve the efficiency of large-scale cluster deployment."",""1558-2183"","""",""10.1109/TPDS.2021.3127128"",""National Key Research and Development Program of China(grant numbers:2018YFB1701400)"; National Natural Science Foundation of China(grant numbers:92055213,61873090,L1924056,62002114); Guangdong Province research and development plan project(grant numbers:2020B0101100001); China Knowledge Centre for Engineering Sciences and Technology(grant numbers:CKCEST-2021-2-7);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9610176"",""Large-scale computing";supercomputing system;distributed containerized deployment;registries placement;"OpenStack"",""Cloud computing";Containers;Computer architecture;Tools;Servers;Clustering algorithms;"Resource management"","""","""","""",""37"",""IEEE"",""10 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient and Secure Deep Learning Inference in Trusted Processor Enabled Edge Clouds,""Y. Li"; D. Zeng; L. Gu; Q. Chen; S. Guo; A. Zomaya;" M. Guo"",""School of Computer Science, Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China"; School of Computer Science, Hubei Key Laboratory of Intelligent Geo-Information Processing, China University of Geosciences, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; School of Computer Science, The University of Sydney, Camperdown, NSW, Australia;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4311"",""4325"",""Edge intelligence has emerged as a prevalent enabling technology to support various intelligent applications. Along with the prosperity, it also raises great concern on the security and privacy since the edge servers are usually shared and untrusted. The security-sensitive code (i.e., the pre-trained model) and data may be easily stolen by malicious tenants, and even untrusted infrastructure providers. To this end, Software Guard Extensions (SGX) is proposed to provide an isolated Trust Execution Environment (TEE) for security and privacy guarantee. However, we find that running tasks in SGX suffer certain performance degradation due to the limited Enclave Page Cache (EPC) size. This further leads to frequent page swapping operations and the high enclave call overhead, which are also influenced by the task (i.e., DNN layer) dispatching and scheduling. To this end, in this paper, we design Lasagna, as an SGX based secure DNN inference acceleration framework, which explores the layered-structure of DNN models to well balance the usage of the scarce EPC resources and the computation resources. Lasagna mainly consists of a global task balancer and a local task scheduler, responding for task dispatching across distributed edge servers and task scheduling in local server, respectively. We evaluate Lasagna over different well-known DNN models, and the results show that Lasagna effectively speeds up the inference performance by $1.11\times -1.51\times$1.11×-1.51×."",""1558-2183"","""",""10.1109/TPDS.2022.3187772"",""National Natural Science Foundation of China(grant numbers:62172375,61972171,61872240,61872310)"; Open Research Projects of Zhejiang Lab(grant numbers:2021KE0AB02); Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101400003); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,152203/20E,152244/21E); Shenzhen Science and Technology Innovation Commission(grant numbers:JCYJ20200109142008673);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9817054"",""DNN Inference";edge intelligence;SGX;"task scheduling"",""Task analysis";Security;Servers;Memory management;Dispatching;Deep learning;"Computational modeling"","""",""2"","""",""45"",""IEEE"",""6 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Data Redistribution Algorithms From Irregular to Block Cyclic Data Distribution,""S. Li"; H. Jiang; D. Dong; C. Huang; J. Liu; X. Liao;" X. Chen"",""College of Computer Science and Technology, National University of Defense Technology, Changsha, China"; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China; Science and Technology on Parallel and Distributed Processing Laboratory, Laboratory of Software Engineering for Complex Systems, College of Computer Science and Technology, National University of Defense Technology, Changsha, China; College of Computer Science and Technology, National University of Defense Technology, Changsha, China;" College of Computer Science and Technology, National University of Defense Technology, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3667"",""3677"",""In this paper, we propose some efficient data redistribution algorithms for redistributing matrices from 1D or 2D irregular format to block cyclic data distribution (BCDD) format, which can be much faster than the BLACS routine PXGEMR2D. These algorithms can be used to combine direct methods with iterative methods. The proposed algorithms divide the communication into two phases: one for processes in the same column and the other for processes in the same row, and the whole data redistribution task is divided into several independent sub-communications. The communication time can be reduced a lot compared with BLACS. Performance results show that our algorithms can be $2\times$2×–$5\times$5× faster than the BLACS routine PXGEMR2D when using 4096 processes and the experiments are performed on Tianhe-2A supercomputer."",""1558-2183"","""",""10.1109/TPDS.2022.3166484"",""National Key R&D Program of China(grant numbers:2021YFB0300101)"; National Natural Science Foundation of China(grant numbers:61902411,62032023,12002382,11275269,42104078); 173 Program of China(grant numbers:2020-JCJQ-ZD-029); Open Research Fund; State Key Laboratory of High Performance Computing of China(grant numbers:202101-01); Excellent Youth Foundation of Hunan Province(grant numbers:2021JJ10050);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9756281"",""BLACS";data redistribution;irregular data;scalapack;BCDD;"exascale computing"",""Arrays";Program processors;Processor scheduling;Iterative methods;Heuristic algorithms;Eigenvalues and eigenfunctions;"Task analysis"","""",""2"","""",""35"",""IEEE"",""12 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Efficient Distributed Approaches to Core Maintenance on Large Dynamic Graphs,""T. Weng"; X. Zhou; K. Li; P. Peng;" K. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Hunan, China"; College of Computer Science and Electronic Engineering, Hunan University, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Hunan, China; College of Computer Science and Electronic Engineering, Hunan University, Hunan, China;" College of Computer Science and Electronic Engineering, Hunan University, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Jul 2021"",""2022"",""33"",""1"",""129"",""143"",""As a fundamental problem in graph analysis, core decomposition aims to compute the core numbers of vertices in a given graph. It is a powerful tool for mining important graph structures. For dynamic graphs with real-time updates of vertices/edges, core maintenance has been utilized to update the core numbers of vertices. The previous approaches to core maintenance face challenges in terms of storage and efficiency. In this article, we investigate distributed approaches to core maintenance on a pregel-like system, which is a famous graph computing system. We first design a core decomposition algorithm to obtain core numbers of vertices in a given graph. Based on it, a distributed batch-stream combined algorithm (DBCA) is devised to efficiently maintain the core numbers when vertex/edge updates happen. In particular, we introduce a new task assignment strategy to DBCA based on diversity of the edge-cores of updated edges. To ensure that DBCA can accurately process core maintenance, we develop a message interaction protocol to resolve the problem of crosstalk among different tasks. Comprehensive experiments have been conducted on real/synthetic graphs, more specifically, in two typical distributed environments built on Supercomputing Center and Alibaba Cloud. The experiment results demonstrate that our proposed algorithms are efficient and scalable."",""1558-2183"","""",""10.1109/TPDS.2021.3090759"",""National Key Research and Development Program of China(grant numbers:2018YFB0204302)"; National Natural Science Foundation of China(grant numbers:61772182); Key Area Research Program of Hunan(grant numbers:2019GK2091); Zhejiang Laboratory(grant numbers:2021KD0AB02);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462402"",""Core decomposition";core maintenance;distributed system;"dynamic graphs"",""Task analysis";Maintenance engineering;Heuristic algorithms;Crosstalk;Synchronization;Protocols;"Distributed algorithms"","""",""19"","""",""39"",""IEEE"",""22 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Flow-Based Scheduling for Geo-Distributed Simulation Tasks in Collaborative Edge and Cloud Environments,""Z. Miao"; P. Yong; Z. Jiancheng;" Y. Quanjun"",""College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, China"; College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, China; College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, China;" College of Systems Engineering, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2022"",""2022"",""33"",""12"",""3442"",""3459"",""Edge computing is a good complement to cloud computing for deploying large-scale geo-distributed simulation applications, which are very sensitive to the communication delay among different simulation components (also called tasks in this paper) and users. We mainly focus on the efficient scheduling of simulation components in collaborative edge and cloud environments. As components should be deployed jointly with the consideration of capacity constraints of hosts, it is actually an NP-complete multi-dimensional bin packing problem. Meanwhile, dynamic changes of component and host states require the low deployment latency of scheduling algorithms. Unfortunately, most of the existing schedulers for modern clusters are queue-based, in which tasks are scheduled sequentially, thus lacking the ability to process tightly coupled tasks jointly. Other batching-based placement algorithms are usually time-consuming. This paper describes Pond, a novel flow-based scheduler with the awareness of interactions among tasks and users as well as heterogeneous multi-dimensional resources. First, characteristics of distributed simulation tasks are analysed and the scheduling problem is formulated as a min-cost max-flow (MCMF) problem over the flow network by mapping the communication overhead among tasks and users to the costs of arcs in the network. Considering the inherent defects of existing flow-based schedulers in dealing with multi-dimensional resources, a new method based on dominant resource is proposed and some problem specific heuristics are also designed. Extensive simulation experiments based on Alibaba production trace and some random synthetic parameters are conducted. Results show that Pond can reduce the average communication cost for each task significantly in a quite low deployment latency compared with some baselines."",""1558-2183"","""",""10.1109/TPDS.2022.3155713"",""National Natural Science Foundation of China(grant numbers:62103425,62103428)"; Natural Science Foundation of Hunan Province(grant numbers:2021JJ40697);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9726854"",""Task scheduling";distributed simulation;minimum cost maximum flow;"cloud and edge environment"",""Task analysis";Computational modeling;Data models;Costs;Cloud computing;Optimization;"Clustering algorithms"","""",""4"","""",""66"",""IEEE"",""3 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Function Queryable and Privacy Preserving Data Aggregation Scheme in Smart Grid,""Y. Zhan"; L. Zhou; B. Wang; P. Duan;" B. Zhang"",""State Key Laboratory of Integrated Service Networks and the Cryptographic Research Center, Xidian University, Xi’an, China"; State Key Laboratory of Integrated Service Networks and the Cryptographic Research Center, Xidian University, Xi’an, China; State Key Laboratory of Integrated Service Networks and the Cryptographic Research Center, Xidian University, Xi’an, China; Secure Collaborative Intelligence Laboratory, Ant Group, Hangzhou, China;" Secure Collaborative Intelligence Laboratory, Ant Group, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2022"",""2022"",""33"",""12"",""3430"",""3441"",""The collection of users’ near-real-time electricity consumption data brings advantages to the operation of smart grids, while raising some security and privacy issues. Multiple privacy preserving data aggregation schemes have been proposed to address these problems. However, most schemes only focus on the aggregation of electricity consumption data without considering the data availability. In addition, although a data aggregation scheme that supports function queries on encrypted data has also been developed, its efficiency is insufficient. In this paper, we first propose an EC-ElGamal encryption algorithm with a double trapdoor decryption mechanism. Through employing the proposed algorithm and the elliptic curve Schnorr signature scheme, an efficient data aggregation scheme supporting privacy protection and function query is proposed for smart grids. This solution allows the control center and users to initiate various function queries on encrypted data. In order to lighten the calculation burden of the control center, we propose another cryptosystem named ElGamal-OU to improve the decryption efficiency, which also supports two independent decryption methods. Finally, the security analysis and performance comparison with related work show that our schemes have advantages in terms of computational, communication and storage overhead."",""1558-2183"","""",""10.1109/TPDS.2022.3153930"",""National Natural Science Foundation of China(grant numbers:U19B2021,61972457)"; Key Research and Development Program of Shaanxi(grant numbers:2020ZDLGY08-04); Innovation Scientists and Technicians Troop Construction Projects of Henan Province;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9720954"",""Fog computing";function queryable;homomorphic encryption;"privacy-preserving"",""Data aggregation";Smart grids;Smart meters;Data privacy;Costs;Real-time systems;"Privacy"","""",""4"","""",""35"",""IEEE"",""24 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient Renaming in Sequence CRDTs,""M. Nicolas"; G. Oster;" O. Perrin"",""CNRS, Inria, LORIA, Université de Lorraine, Nancy, France"; CNRS, Inria, LORIA, Université de Lorraine, Nancy, France;" CNRS, Inria, LORIA, Université de Lorraine, Nancy, France"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3870"",""3885"",""To achieve high availability, large-scale distributed systems have to replicate data and to minimise coordination between nodes. For these purposes, literature and industry increasingly adopt Conflict-free Replicated Data Types (CRDTs) to design such systems. Conflict-free Replicated Data Types (CRDTs) are new specifications of existing data types, e.g., Set or Sequence. While CRDTs have the same behaviour as previous specifications in sequential executions, they actually shine in distributed settings as they natively support concurrent updates. To this end, CRDTs embed in their specification conflict resolution mechanisms. These mechanisms usually rely on identifiers attached to elements of the data structure to resolve conflicts in a deterministic and coordination-free manner. Identifiers have to comply with several constraints, such as being unique or belonging to a dense total order. These constraints may hinder the identifier size from being bounded. Identifiers hence tend to grow as the system progresses, which increases the overhead of CRDTs over time and leads to performance issues. To address this issue, we propose a novel Sequence CRDT which embeds a renaming mechanism. It enables nodes to reassign shorter identifiers to elements in an uncoordinated manner. Experimental results demonstrate that this mechanism decreases the overhead of the replicated data structure and eventually minimises it."",""1558-2183"","""",""10.1109/TPDS.2022.3172570"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770022"",""CRDTs";replication;real-time collaborative editing;eventual consistency;memory-wise optimisation;"performance"",""Peer-to-peer computing";Data structures;Metadata;Collaboration;Semantics;Real-time systems;"Distributed databases"","""","""","""",""41"",""IEEE"",""5 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Efficient, Dynamic Multi-Task Execution on FPGA-Based Computing Systems,""U. I. Minhas"; R. Woods; D. S. Nikolopoulos;" G. Karakonstantis"",""School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K."; School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K.; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA;" School of Electronics, Electrical Engineering and Computer Science, Queen’s University Belfast, Belfast, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""17 Aug 2021"",""2022"",""33"",""3"",""710"",""722"",""With growing Field Programmable Gate Array (FPGA) device sizes and their integration in environments enabling sharing of computing resources such as cloud and edge computing, there is a requirement to share the FPGA area between multiple tasks. The resource sharing typically involves partitioning the FPGA space into fix-sized slots. This results in suboptimal resource utilisation and relatively poor performance, particularly as the number of tasks increase. Using OpenCL’s exploration capabilities, we employ clever clustering and custom, task-specific partitioning and mapping to create a novel, area sharing methodology where task resource requirements are more effectively managed. Using models with varying resource/throughput profiles, we select the most appropriate distribution based on the runtime, workload needs to enhance temporal compute density. The approach is enabled in the system stack by a corresponding task-based virtualisation model. Using 11 high performance tasks from graph analysis, linear algebra and media streaming, we demonstrate an average $2.8\times$2.8× higher system throughput at $2.3\times$2.3× better energy efficiency over existing approaches."",""1558-2183"","""",""10.1109/TPDS.2021.3101153"",""European Commission(grant numbers:Grant 6876281 (VINEYARD))";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502563"",""FPGA systems";resource sharing;scheduling and runtime;partitioning and mapping;task-based virtualisation;"clustering"",""Task analysis";Field programmable gate arrays;Runtime;Throughput;Resource management;Dynamic scheduling;"Space exploration"","""",""5"","""",""36"",""CCBY"",""30 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"EiC Editorial – Advancing Reproducibility in Parallel and Distributed Systems Research,""M. Parashar"",NA,""IEEE Transactions on Parallel and Distributed Systems"",""28 Jan 2022"",""2022"",""33"",""9"",""2010"",""2010"","", the TPDS Reproducibility Initiative [2][3] has being exploring postpublication peer review of code associated with published articles for a few years. Authors who have published in TPDS can make their published article more reproducible and earn a reproducibility badge by submitting their associated code for post-publication peer review. To date, this pilot has largely focused on two badges: 1. Code Available: The code, including any associated data and documentation, provided by the authors is reasonable and complete and can potentially be used to support reproducibility of the published results. 2. Code Reviewed: The code, including any associated data and documentation, provided by the authors is reasonable and complete, runs to produce the outputs described, and can support reproducibility of the published results. While TPDS’ goal has always been to include badges for reproducing research results using the code and/or data provided, the nature of research in parallel and distributed systems covered by TPDS makes it challenging to evaluate code and data challenging for reproducibility. This is because such an evaluation may require access to specific hardware, system architectures and scales, OS configurations, and so on, which may not be feasible or practical. vel system/middleware services, is typically infeasible. Consequently, TPDS has piloted an alternate approach where members of the community can submit short, supplemental ‘critique’ papers that present their experiences in reproducing published results using the artifacts, and/or evaluations or experiences with published artifacts. These supplemental paper submissions are reviewed and, if accepted, are linked to the original publication and are citable, serving to help validate the reproducibility of the original publication. This approach was first implemented in a special section guest edited by Stephen Lien Harrell and Beth Plale and consisting of a primary paper and 6 critique papers that reproduce the results of the primary paper. This special section continues this effort, building on the SC20 Student Cluster Competition, which was part of the SC20 conference. It consists of 9 critique papers that reproduce the results of the primary paper."",""1558-2183"","""",""10.1109/TPDS.2021.3137871"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9696236"","""","""","""",""2"","""",""4"",""IEEE"",""28 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Eiffel: Efficient and Fair Scheduling in Adaptive Federated Learning,""A. Sultana"; M. M. Haque; L. Chen; F. Xu;" X. Yuan"",""School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA"; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; Department of Computer Science and Technology, East China Normal University, Shanghai, China;" School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4282"",""4294"",""Emerging machine learning (ML) technologies, in combination with the increasing computational power of mobile devices, lead to the extensive adoption of ML-based applications. Different from conventional model training that needs to collect all the user data in centralized cloud servers, federated learning (FL) has recently drawn increasing research attention as it enables privacy-preserving model training. With FL, decentralized edge devices in participation, train their model copies locally over their siloed datasets, and periodically synchronize the model parameters. However, model training is computationally extensive which easily drains the battery of mobile devices. In addition, due to the uneven distribution of siloed datasets, the shared model may become biased. To address the efficiency and fairness concerns in a resource-constrained federated learning setting, in this paper, we propose Eiffel to judiciously select mobile devices to participate in the global model aggregation, and adaptively adjust the frequency of local and global model updates. Eiffel aims to make scheduling and coordination for the federated learning towards both resource efficiency and model fairness. We have conducted theoretical analysis of Eiffel from the perspectives of fairness and convergence. Extensive experiments with a wide variety of real-world datasets and models, both on a networked prototype system and in a larger-scale simulated environment, have demonstrated that while maintaining similar accuracy performance, Eiffel outperforms existing baselines with respect to reducing communication overhead by up to 6× for higher efficiency and improving the fairness metric by up to 57% compared to the state-of-the-art algorithms."",""1558-2183"","""",""10.1109/TPDS.2022.3187365"",""National Science Foundation(grant numbers:2019511,1763620,1948374)"; Louisiana Board of Regents(grant numbers:LEQSF(2019-22)-RD-A-21); National Natural Science Foundation of China(grant numbers:61972158); Science and Technology Commission of Shanghai Municipality(grant numbers:20511102802,18DZ2270800);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9810502"",""Efficiency";fairness;federated learning;resource constraints;"scheduling"",""Adaptation models";Collaborative work;Data models;Computational modeling;Performance evaluation;Training;"Convergence"","""",""6"","""",""37"",""IEEE"",""29 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Elastic and Reliable Bandwidth Reservation Based on Distributed Traffic Monitoring and Control,""X. Zhang";" T. Wang"",""School of Information Science and Engineering, Shandong Normal University, Jinan, China";" Department of Computer Science, The University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4563"",""4580"",""Bandwidth reservation can effectively improve the service quality for data transfers because of dedicated network resources. However, it is difficult to achieve a desired tradeoff between resource utilization and reliable bandwidth guarantees for data transfers with time-varying traffic. In this article, we study a novel bandwidth reservation solution based on distributed traffic monitoring and control for applications that require reliable bandwidth guarantees. In the proposed solution, designated bandwidth is allocated for an application in advance according to its maximum traffic peak, and idle reserved bandwidth resources are dynamically shared according to regular traffic. Dynamic resource sharing evidently improves resource utilization and effectively eliminates the potential congestion caused by sudden traffic bursts. To ensure that the congestion that occurs occasionally can dissipate rapidly, our solution monitors and manages traffic by a distributed monitoring and control strategy. Hence, we study a delay-constrained and proxy-assisted traffic monitoring structure construction problem and propose an algorithm to solve it. The proposed algorithm can also be used to build a delay-constrained traffic control structure. In addition to the above algorithm, we propose a dynamic traffic control algorithm that can achieve a desirable tradeoff between resource utilization and congestion avoidance capability."",""1558-2183"","""",""10.1109/TPDS.2022.3196840"",""National Natural Science Foundation of China(grant numbers:92067108)"; Shandong Provincial Natural Science Foundation of China(grant numbers:ZR2020MF057); Basic Research Enhancement Plan of Qilu University of Technology(grant numbers:2021JC03001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9851561"",""Bandwidth reservation";resource utilization;traffic monitoring;traffic control;"industrial internet"",""Bandwidth";Monitoring;Reliability;Resource management;Delays;Heuristic algorithms;"Dynamic scheduling"","""",""34"","""",""46"",""IEEE"",""5 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Elastic Deep Learning in Multi-Tenant GPU Clusters,""Y. Wu"; K. Ma; X. Yan; Z. Liu; Z. Cai; Y. Huang; J. Cheng; H. Yuan;" F. Yu"",""Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong"; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, Chinese University of Hong Kong, Hong Kong; Huawei Technologies Co. Ltd, Shenzhen, China;" Huawei Technologies Co. Ltd, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Jul 2021"",""2022"",""33"",""1"",""144"",""158"",""We study how to support elasticity, that is, the ability to dynamically adjust the parallelism (i.e., the number of GPUs), for deep neural network (DNN) training in a GPU cluster. Elasticity can benefit multi-tenant GPU cluster management in many ways, for example, achieving various scheduling objectives (e.g., job throughput, job completion time, GPU efficiency) according to cluster load variations, utilizing transient idle resources, and supporting performance profiling, job migration, and straggler mitigation. We propose EDL, which enables elastic deep learning with a simple API and can be easily integrated with existing deep learning frameworks such as TensorFlow and PyTorch. EDL also incorporates techniques that are necessary to reduce the overhead of parallelism adjustments, such as stop-free scaling and dynamic data pipeline. We demonstrate with experiments that EDL can indeed bring significant benefits to the above-listed applications in GPU cluster management."",""1558-2183"","""",""10.1109/TPDS.2021.3064966"",""GRF(grant numbers:14208318)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373916"",""Deep learning system";elastic deep learning;"GPU cluster management"",""Graphics processing units";Training;Parallel processing;Elasticity;Throughput;Deep learning;"Transient analysis"","""",""12"","""",""61"",""IEEE"",""9 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Elastic Parameter Server: Accelerating ML Training With Scalable Resource Scheduling,""S. Wang"; A. Pi;" X. Zhou"",""Department of Computer Science, University of Colorado, Boulder, CO, USA"; Department of Computer Science, University of Colorado, Boulder, CO, USA;" Department of Computer Science, University of Colorado, Boulder, CO, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1128"",""1143"",""Parameter server (PS) based on worker-server communication is designed for distributed machine learning (ML) training in clusters. In feedback-driven exploration of ML model training, users exploit early feedback from each job to decide whether to kill the job or keep it running so as to find the optimal model configuration. However, PS does not support adjusting the number of workers and servers of a job at runtime. It becomes the bottleneck of scalable distributed ML training because the cluster resources cannot be dynamically allocated or deallocated to jobs, resulting in significant early feedback latency and resource under-utilization. This article rethinks the principle of PS architecture. We present Elastic Parameter Server (EPS), a lightweight and user-transparent PS that accelerates feedback-driven exploration for distributed ML training. EPS allows to remove a subset of workers and servers from running jobs and allocate the released resources to an incoming job at runtime so as to reduce its early feedback latency. It can also use the released resources from a killed job to add workers and servers to running jobs to improve resource utilization and the training speed. We develop a heuristic scheduler that leverages EPS and offers scalable resource scheduling for multiple ML jobs. We implement EPS in Tencent Angel and the scheduler in Apache Yarn, and conduct evaluations with various ML models. Experimental results show that EPS achieves up to 1.5x improvement on the ML training speed compared to PS."",""1558-2183"","""",""10.1109/TPDS.2021.3104242"",""National Science Foundation(grant numbers:SHF-1816850)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511790"",""Parameter server";feedback-driven exploration;ML model training;resource scheduling;"elasticity"",""Servers";Training;Runtime;Clocks;Yarn;Resource management;"Training data"","""",""9"","""",""56"",""IEEE"",""11 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Elastic Resource Allocation Against Imbalanced Transaction Assignments in Sharding-Based Permissioned Blockchains,""H. Huang"; Z. Yue; X. Peng; L. He; W. Chen; H. -N. Dai; Z. Zheng;" S. Guo"",""School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China"; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; Department of Computing and Decision Sciences, Lingnan University, Hong Kong; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China;" Department of Computing, Hong Kong Polytechnic University, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Mar 2022"",""2022"",""33"",""10"",""2372"",""2385"",""This article studies the PBFT-based sharded permissioned blockchain, which executes in either a local datacenter or a rented cloud platform. In such permissioned blockchain, the transaction (TX) assignment strategy could be malicious such that the network shards may possibly receive imbalanced transactions or even bursty-TX injection attacks. An imbalanced transaction assignment brings serious threats to the stability of the sharded blockchain. A stable sharded blockchain can ensure that each shard processes the arrived transactions timely. Since the system stability is closely related to the blockchain throughput, how to maintain a stable sharded blockchain becomes a challenge. To depict the transaction processing in each network shard, we adopt the Lyapunov Optimization framework. Exploiting drift-plus-penalty (DPP) technique, we then propose an adaptive resource-allocation algorithm, which can yield the near-optimal solution for each network shard while the shard queues can also be stably maintained. We also rigorously analyze the theoretical boundaries of both the system objective and the queue length of shards. The numerical results show that the proposed algorithm can achieve a better balance between resource consumption and queue stability than other baselines. We particularly evaluate two representative cases of bursty-TX injection attacks, i.e., the continued attacks against all network shards and the drastic attacks against a single network shard. The evaluation results show that the DPP-based algorithm can well alleviate the imbalanced TX assignment, and simultaneously maintain high throughput while consuming fewer resources than other baselines."",""1558-2183"","""",""10.1109/TPDS.2022.3141737"",""National Key Research and Development Program of China(grant numbers:2020YFB1006005)"; National Natural Science Foundation of China(grant numbers:61902445); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019A1515011798); Guangzhou Basic and Applied Basic Research Foundation(grant numbers:202102020613); Guangdong Provincial Pearl River Talents Program(grant numbers:2019QN01X130); CCF-Huawei Populus euphratica forest fund(grant numbers:CCF-HuaweiBC2021004); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,152203/20E,152244/21E); National Natural Science Foundation of China(grant numbers:61872310); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:R2020A045);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9678070"",""System stability";sharded blockchain;queueing theory;"imbalanced transaction assignment"",""Blockchains";Protocols;Stability analysis;Throughput;Numerical stability;Bitcoin;"Scalability"","""",""18"","""",""31"",""IEEE"",""11 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Elastic Significant Bit Quantization and Acceleration for Deep Neural Networks,""C. Gong"; Y. Lu; K. Xie; Z. Jin; T. Li;" Y. Wang"",""College of Computer Science, Nankai University, Tianjin, China"; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China;" Department of Electrical and Computer Engineering, Khoury College of Computer Science (Affiliated), Northeastern University, Boston, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3178"",""3193"",""Quantization has been proven to be a vital method for improving the inference efficiency of deep neural networks (DNNs). However, it is still challenging to strike a good balance between accuracy and efficiency while quantizing DNN weights or activation values from high-precision formats to their quantized counterparts. We propose a new method called elastic significant bit quantization (ESB) that controls the number of significant bits of quantized values to obtain better inference accuracy with fewer resources. We design a unified mathematical formula to constrain the quantized values of the ESB with a flexible number of significant bits. We also introduce a distribution difference aligner (DDA) to quantitatively align the distributions between the full-precision weight or activation values and quantized values. Consequently, ESB is suitable for various bell-shaped distributions of weights and activation of DNNs, thus maintaining a high inference accuracy. Benefitting from fewer significant bits of quantized values, ESB can reduce the multiplication complexity. We implement ESB as an accelerator and quantitatively evaluate its efficiency on FPGAs. Extensive experimental results illustrate that ESB quantization consistently outperforms state-of-the-art methods and achieves average accuracy improvements of 4.78%, 1.92%, and 3.56% over AlexNet, ResNet18, and MobileNetV2, respectively. Furthermore, ESB as an accelerator can achieve 10.95 GOPS peak performance of 1k LUTs without DSPs on the Xilinx ZCU102 FPGA platform. Compared with CPU, GPU, and state-of-the-art accelerators on FPGAs, the ESB accelerator can improve the energy efficiency by up to 65×, 11×, and 26×, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3129615"",""National Key Research and Development Program of China(grant numbers:2018YFB2100300)"; National Natural Science Foundation of China(grant numbers:62002175); Natural Science Foundation of Tianjin City(grant numbers:19JCQNJC00600); State Key Laboratory of Computer Architecture(grant numbers:CARCHB202016,CARCH201905); Innovation Fund of Chinese Universities Industry-University-Research(grant numbers:2020HYA01003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623507"",""DNN quantization";significant bits;fitting distribution;cheap projection;distribution aligner;"FPGA accelerator"",""Quantization (signal)";Distributed databases;Field programmable gate arrays;Degradation;Computer science;Open area test sites;"Hardware"","""",""3"","""",""47"",""IEEE"",""22 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Enabling In-Network Floating-Point Arithmetic for Efficient Computation Offloading,""P. Cui"; H. Pan; Z. Li; P. Zhang; T. Miao; J. Zhou; H. Guan;" G. Xie"",""Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China"; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China; Southern University of Science and Technology, Shenzhen, China; Chinese Academy of Sciences, Institute of Computing Technology, Beijing, China;" Computer Network Information Center, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Oct 2022"",""2022"",""33"",""12"",""4918"",""4934"",""Programmable switches are recently used for accelerating data-intensive distributed applications. Some computational tasks, traditionally performed on servers in data centers, are offloaded into the network on programmable switches. These tasks may require the support of on-the-fly floating-point operations. Unfortunately, programmable switches are restricted to simple integer arithmetic operations. Existing systems circumvent this restriction by converting floats to integers or relying on local CPUs of switches, incurring extra processing delayed and accuracy loss. To address this gap, we propose NetFC, a table-lookup method to achieve on-the-fly in-network floating-point arithmetic operations nearly without accuracy loss. Specifically, NetFC utilizes logarithm projection and transformation to convert the original huge table enumerating all operands and results into several much smaller tables that can fit into the data plane of programmable switches. To cope with the table inflation problem on 32-bit floats, we also propose an approximation method that further breaks the large tables into smaller ones. In addition, NetFC leverages two optimizations to improve accuracy and reduce on-chip memory consumption. We use both synthetic and real-life datasets to evaluate NetFC. The experimental results show that the average accuracy of NetFC is above 99.9% with only 448KB memory consumption for 16-bit floats and 99.1% with 496KB memory consumption for 32-bit floats. Furthermore, we integrate NetFC into two distributed applications and two in-network telemetry systems to show its effectiveness in further improving the performance."",""1558-2183"","""",""10.1109/TPDS.2022.3208425"",""National Key R&D Program of China(grant numbers:2020YFB1805600)"; National Natural Science Foundation of China(grant numbers:U20A20180,62002344); Beijing Natural Science Foundation(grant numbers:JQ20024);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9896997"",""In-network computation";computation offloading;"floating-point operation"",""Open area test sites";Arithmetic;Memory management;Task analysis;Training;Standards;"Servers"","""",""1"","""",""54"",""IEEE"",""21 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Enabling Large Scale Simulations for Particle Accelerators,""K. Iliakis"; H. Timko; S. Xydis; P. Tsapatsaris;" D. Soudris"",""Microprocessors and Digital Systems Laboratory, Electrical and Computer Engineering Department, National Technical University of Athens, Athens, Greece"; Accelerator Systems Department, European Organization for Nuclear Research (CERN), Geneva, Switzerland; Digital Technology Department, Harokopio University of Athens, Athens, Greece; Microprocessors and Digital Systems Laboratory, Electrical and Computer Engineering Department, National Technical University of Athens, Athens, Greece;" Microprocessors and Digital Systems Laboratory, Electrical and Computer Engineering Department, National Technical University of Athens, Athens, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4425"",""4439"",""International high-energy particle physics research centers, like CERN and Fermilab, require excessive studies and simulations to plan for the upcoming upgrades of the world's largest particle accelerators, and the design of future machines given the technological challenges and tight budgetary constraints. The Beam Longitudinal Dynamics (BLonD) simulator suite incorporates the most detailed and complex physics phenomena in the field of longitudinal beam dynamics, required for providing extremely accurate predictions. Modern challenges in beam dynamics dictate for longer, larger and numerous simulation studies to draw meaningful conclusions that will drive the baseline choices for the daily operation of current machines and the design choices of future projects. These studies are extremely time consuming, and would be impractical to perform without a High-Performance Computing oriented simulator framework. In this article, at first, we design and evaluate a highly-optimized distributed version of BLonD. We combine approximate computing techniques, and leverage a dynamic load-balancing scheme to relax synchronization and improve scalability. In addition, we employ GPUs to accelerate the distributed implementation. We evaluate the highly optimized distributed beam longitudinal dynamics simulator in a supercomputing system and demonstrate speedups of more than two orders of magnitude when run on 32 GPU platforms, w.r.t. the previous state-of-art. By driving a wide range of new studies, the proposed high performance beam longitudinal dynamics simulator forms an invaluable tool for accelerator physicists."",""1558-2183"","""",""10.1109/TPDS.2022.3192707"",""European Organization for Nuclear Research";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9834128"",""Distributed systems";GPU acceleration;approximate computing;network traffic optimisation;"accelerator physics"",""Radio frequency";Particle beams;Synchrotrons;Physics;Computational modeling;Voltage;"Large Hadron Collider"","""",""3"","""",""41"",""CCBY"",""20 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Enabling Scalable and Extensible Memory-Mapped Datastores in Userspace,""I. B. Peng"; M. B. Gokhale; K. Youssef; K. Iwabuchi;" R. Pearce"",""Lawrence Livermore National Laboratory, Livermore, CA, USA"; Lawrence Livermore National Laboratory, Livermore, CA, USA; Virginia Tech, Blacksburg, VA, USA; Lawrence Livermore National Laboratory, Livermore, CA, USA;" Lawrence Livermore National Laboratory, Livermore, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""866"",""877"",""Exascale workloads are expected to incorporate data-intensive processing in close coordination with traditional physics simulations. These emerging scientific, data-analytics and machine learning applications need to access a wide variety of datastores in flat files and structured databases. Programmer productivity is greatly enhanced by mapping datastores into the application process's virtual memory space to provide a unified “in-memory” interface. Currently, memory mapping is provided by system software primarily designed for generality and reliability. However, scalability at high concurrency is a formidable challenge on exascale systems. Also, there is a need for extensibility to support new datastores potentially requiring HPC data transfer services. In this article, we present UMap, a scalable and extensible userspace service for memory-mapping datastores. Through decoupled queue management, concurrency aware adaptation, and dynamic load balancing, UMap enables application performance to scale even at high concurrency. We evaluate UMap in data-intensive applications, including sorting, graph traversal, database operations, and metagenomic analytics. Our results show that UMap as a userspace service outperforms an optimized kernel-based service across a wide range of intra-node concurrency by 1.22-1.9 ${\times}$×. We performed two case studies to demonstrate UMap's extensibility. First, a new datastore residing in remote memory is incorporated into UMap as an application-specific plugin. Second, we present a persistent memory allocator Metall built atop UMap for unified storage/memory."",""1558-2183"","""",""10.1109/TPDS.2021.3086302"",""Lawrence Livermore National Laboratory"; U.S. Department of Energy(grant numbers:DE-AC52-07NA27344 (LLNL-JRNL-819817)); Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9446641"",""Data-intensive";userspace;memory mapping;mmap;"memory-mapped I/O"",""Concurrent computing";Kernel;Databases;Optimization;Task analysis;Scalability;"Prefetching"","""",""2"","""",""29"",""IEEE"",""3 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Endurance-Aware Mapping of Spiking Neural Networks to Neuromorphic Hardware,""T. Titirsha"; S. Song; A. Das; J. Krichmar; N. Dutt; N. Kandasamy;" F. Catthoor"",""Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA"; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA; Department of Computer Science, University of California, Irvine, CA, USA; Department of Computer Science, University of California, Irvine, CA, USA; Department of Electrical and Computer Engineering, Drexel University, Philadelphia, PA, USA;" Imec, Leuven, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Aug 2021"",""2022"",""33"",""2"",""288"",""301"",""Neuromorphic computing systems are embracing memristors to implement high density and low power synaptic storage as crossbar arrays in hardware. These systems are energy efficient in executing Spiking Neural Networks (SNNs). We observe that long bitlines and wordlines in a memristive crossbar are a major source of parasitic voltage drops, which create current asymmetry. Through circuit simulations, we show the significant endurance variation that results from this asymmetry. Therefore, if the critical memristors (ones with lower endurance) are overutilized, they may lead to a reduction of the crossbar's lifetime. We propose eSpine, a novel technique to improve lifetime by incorporating the endurance variation within each crossbar in mapping machine learning workloads, ensuring that synapses with higher activation are always implemented on memristors with higher endurance, and vice versa. eSpine works in two steps. First, it uses the Kernighan-Lin Graph Partitioning algorithm to partition a workload into clusters of neurons and synapses, where each cluster can fit in a crossbar. Second, it uses an instance of Particle Swarm Optimization (PSO) to map clusters to tiles, where the placement of synapses of a cluster to memristors of a crossbar is performed by analyzing their activation within the workload. We evaluate eSpine for a state-of-the-art neuromorphic hardware model with phase-change memory (PCM)-based memristors. Using 10 SNN workloads, we demonstrate a significant improvement in the effective lifetime."",""1558-2183"","""",""10.1109/TPDS.2021.3065591"",""National Science Foundation(grant numbers:CCF-1942697)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380671"",""Neuromorphic computing";spiking neural networks (SNNs);non-volatile memory (NVM);memristor;"endurance"",""Memristors";Neurons;Hardware;Synapses;Phase change materials;Neuromorphics;"Random access memory"","""",""21"","""",""71"",""IEEE"",""17 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Energy-Aware Non-Preemptive Task Scheduling With Deadline Constraint in DVFS-Enabled Heterogeneous Clusters,""Q. Wang"; X. Mei; H. Liu; Y. -W. Leung; Z. Li;" X. Chu"",""Harbin Institute of Technology, Shenzhen, China"; Jefferson Laboratory, Newport News, VA, USA; Department of Computing, Hang Seng University of Hong Kong, Siu Lek Yuen, Hong Kong; Department of Computer Science, Hong Kong Baptist University, Kowloon Tong, Hong Kong; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China;" Data Science and Analytics Thrust, The Hong Kong University of Science and Technology (Guangzhou), Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4083"",""4099"",""Energy conservation of large data centers for high performance computing workloads, such as deep learning with Big Data, is of critical significance, where cutting down a few percent of electricity translates into million-dollar savings. This work studies energy conservation on emerging CPU-GPU hybrid clusters through dynamic voltage and frequency scaling (DVFS). We aim at minimizing the total energy consumption of processing a batch of offline tasks or a sequence of real-time tasks under deadline constraints. We derive a fast and accurate analytical model to compute the appropriate voltage/frequency setting for each task, and assign multiple tasks to the cluster with heuristic scheduling algorithms. In particular, our model stresses the nonlinear relationship between task execution time and processor speed for GPU-accelerated applications, for more accurately capturing real-world GPU energy consumption. In performance evaluation driven by real-world power measurement traces, our scheduling algorithm shows comparable energy savings to the theoretical upper bound. With a GPU scaling interval where analytically at most 36% of energy can be saved, we record 33-35% of energy savings. Our results are applicable to energy management on modern heterogeneous clusters."",""1558-2183"","""",""10.1109/TPDS.2022.3181096"",""Hong Kong RGC GRF(grant numbers:HKBU 12200418)"; Hong Kong Research Matching Scheme(grant numbers:RMGS2019_1_23); Hang Seng University of Hong Kong;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9790352"",""Graphics processing units";dynamic voltage and frequency scaling;"task scheduling"",""Graphics processing units";Task analysis;Energy consumption;Voltage;Scheduling algorithms;Voltage control;"Kernel"","""",""5"","""",""57"",""IEEE"",""8 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Energy-Efficient Cache-Aware Scheduling on Heterogeneous Multicore Systems,""S. Z. Sheikh";" M. A. Pasha"",""Department of Electrical Engineering, School of Science and Engineering (SSE), Lahore University of Management Sciences (LUMS), Lahore, Pakistan";" Department of Electrical Engineering, School of Science and Engineering (SSE), Lahore University of Management Sciences (LUMS), Lahore, Pakistan"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jul 2021"",""2022"",""33"",""1"",""206"",""217"",""The adoption of heterogeneous multicore architectures into deadline-constrained embedded systems has various benefits in terms of schedulability and energy-efficiency. Existing energy-efficient algorithms, in this domain, allocate tasks to their energy-favorable core-types while using dynamic voltage and frequency scaling to reduce energy consumption. However, the practicality of such algorithms is limited due to the underlying assumptions made to simplify the analysis. This article paves the way for more practical approaches to minimize the energy consumption on heterogeneous multicores. Specifically, we investigate the nonlinear impacts that core-frequency and cache-partitioning have on task-executions in a heterogeneous multicore environment. In doing so, we propose an algorithm that exploits this relationship to effectively allocate tasks to specific cores and core-types, and determine the number of cache-partitions for each core. Extensive simulations using real-world benchmarks show the proficiency of our approach by achieving an average and maximum energy savings of 14.9 and 20.4 percent, respectively for core-level energy consumption, and 20.2 and 60.4 percent, respectively for system-level energy consumption."",""1558-2183"","""",""10.1109/TPDS.2021.3090587"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460819"",""Scheduling";multicores;heterogeneous;"energy-efficiency"",""Multicore processing";Task analysis;Optimal scheduling;Energy consumption;Radio spectrum management;Resource management;"Scheduling algorithms"","""",""7"","""",""26"",""IEEE"",""18 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Energy-Efficient Offloading for DNN-Based Smart IoT Systems in Cloud-Edge Environments,""X. Chen"; J. Zhang; B. Lin; Z. Chen; K. Wolter;" G. Min"",""College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China"; College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China; College of Physics and Energy, Fujian Provincial Key Laboratory of Quantum Manipulation and New Energy Materials, Fujian Normal University, Fuzhou, China; Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K.; Institut für Informatik, Freie Universität Berlin, Berlin, Germany;" Department of Computer Science, College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2021"",""2022"",""33"",""3"",""683"",""697"",""Deep Neural Networks (DNNs) have become an essential and important supporting technology for smart Internet-of-Things (IoT) systems. Due to the high computational costs of large-scale DNNs, it might be infeasible to directly deploy them in energy-constrained IoT devices. Through offloading computation-intensive tasks to the cloud or edges, the computation offloading technology offers a feasible solution to execute DNNs. However, energy-efficient offloading for DNN based smart IoT systems with deadline constraints in the cloud-edge environments is still an open challenge. To address this challenge, we first design a new system energy consumption model, which takes into account the runtime, switching, and computing energy consumption of all participating servers (from both the cloud and edge) and IoT devices. Next, a novel energy-efficient offloading strategy based on a Self-adaptive Particle Swarm Optimization algorithm using the Genetic Algorithm operators (SPSO-GA) is proposed. This new strategy can efficiently make offloading decisions for DNN layers with layer partition operations, which can lessen the encoding dimension and improve the execution time of SPSO-GA. Simulation results demonstrate that the proposed strategy can significantly reduce energy consumption compared to other classic methods."",""1558-2183"","""",""10.1109/TPDS.2021.3100298"",""National Natural Science Foundation of China(grant numbers:62072108)"; Natural Science Foundation of Fujian Province(grant numbers:2020J06014); Natural Science Foundation of Fujian Province(grant numbers:2019J01286); Young and Middle-aged Teacher Education Foundation of Fujian Province(grant numbers:JT180098);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9497712"",""Cloud-edge computing";IoT systems;energy-efficient offloading;deep neural networks;"particle swarm optimization"",""Energy consumption";Internet of Things;Cloud computing;Servers;Data communication;Quality of service;"Task analysis"","""",""82"","""",""38"",""IEEE"",""27 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"EnosLib: A Library for Experiment-Driven Research in Distributed Computing,""R. -A. Cherrueau"; M. Delavergne; A. van Kempen; A. Lebre; D. Pertin; J. R. Balderrama; A. Simonet;" M. Simonin"",""Inria, LS2N, IMT Atlantique, Nantes, France"; Inria, LS2N, IMT Atlantique, Nantes, France; Inria, LS2N, IMT Atlantique, Nantes, France; Inria, LS2N, IMT Atlantique, Nantes, France; Inria, LS2N, IMT Atlantique, Nantes, France; Inria, LS2N, IMT Atlantique, Nantes, France; iExec Blockchain Tech, Lyon, Auvergne-Rhône-Alpes, France;" Inria Rennes - Bretagne Atlantique, Rennes, France"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Oct 2021"",""2022"",""33"",""6"",""1464"",""1477"",""Despite the importance of experiment-driven research in the distributed computing community, there has been little progress in helping researchers conduct their experiments. In most cases, they have to achieve tedious and time-consuming development and instrumentation activities to deal with the specifics of testbeds and the system under study. In order to relieve researchers of the burden of those efforts, we have developed EnosLib: a Python library that takes into account best experimentation practices and leverages modern toolkits on automatic deployment and configuration systems. EnosLib helps researchers not only in the process of developing their experimental artifacts, but also in running them over different infrastructures. To demonstrate the relevance of our library, we discuss three experimental engines built on top of EnosLib, and used to conduct empirical studies on complex software stacks between 2016 and 2019 (database systems, communication buses and OpenStack). By introducing EnosLib, our goal is to gather academic and industrial actors of our community around a library that aggregates everyday experiment-driven research operations. A library that has been already adopted by open-source projects and members of the scientific community thanks to its ease of use and extension."",""1558-2183"","""",""10.1109/TPDS.2021.3111159"",""Inria and Orange Labs"; Institut national de recherche en informatique et en automatique; CNRS; RENATER;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9534688"",""Experiment-driven research";performance evaluation;"distributed computing experimentation library"",""Libraries";Software;Task analysis;Tools;Protocols;Codes;"Benchmark testing"","""",""4"","""",""44"",""IEEE"",""9 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Error-Compensated Sparsification for Communication-Efficient Decentralized Training in Edge Environment,""H. Wang"; S. Guo; Z. Qu; R. Li;" Z. Liu"",""School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong; School of Computer and Information, Hohai University, Nanjing, Jiangsu, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China;" Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""25 Jun 2021"",""2022"",""33"",""1"",""14"",""25"",""Communication has been considered as a major bottleneck in large-scale decentralized training systems since participating nodes iteratively exchange large amounts of intermediate data with their neighbors. Although compression techniques like sparsification can significantly reduce the communication overhead in each iteration, errors caused by compression will be accumulated, resulting in a severely degraded convergence rate. Recently, the error compensation method for sparsification has been proposed in centralized training to tolerate the accumulated compression errors. However, the analog technique and the corresponding theory about its convergence in decentralized training are still unknown. To fill in the gap, we design a method named ECSD-SGD that significantly accelerates decentralized training via error-compensated sparsification. The novelty lies in that we identify the component of the exchanging information in each iteration (i.e., the sparsified model update) and make targeted error compensation over the component. Our thorough theoretical analysis shows that ECSD-SGD supports arbitrary sparsification ratio and achieves the same convergence rate as the non-sparsified decentralized training methods. We also conduct extensive experiments on multiple deep learning models to validate our theoretical findings. Results show that ECSD-SGD outperforms all the start-of-the-art sparsified methods in terms of both the convergence speed and the final generalization accuracy."",""1558-2183"","""",""10.1109/TPDS.2021.3084104"",""National Key Research and Development Program of China(grant numbers:2016QY01W0202)"; Hong Kong RGC Research Impact Fund(grant numbers:R5060-19,R5034-18); General Research Fund(grant numbers:152221/19E,15220320/20E); Collaborative Research Fund(grant numbers:C5026-18G); National Natural Science Foundation of China(grant numbers:61872310,U1836204,U1936108); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:R2020A045); Shenzhen Basic Research Funding Scheme(grant numbers:JCYJ20170818103849343); Fundamental Research Funds for the Central Universities(grant numbers:B210202079);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442310"",""Distributed machine learning";decentralized training;communication compression;"error compensation"",""Training";Deep learning;Design methodology;Error compensation;"Convergence"","""",""11"","""",""47"",""IEEE"",""26 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Ethernet for High-Throughput Computing at CERN,""R. Krawczyk"; T. Colombo; N. Neufeld; F. Pisani;" S. Valat"",""LHCb, EP-LBC, CERN, Meyrin, Switzerland"; LHCb, EP-LBC, CERN, Meyrin, Switzerland; LHCb, EP-LBC, CERN, Meyrin, Switzerland; LHCb, EP-LBC, CERN, Meyrin, Switzerland;" Atos, Bezons, France"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3640"",""3650"",""When high throughput and utilization of fabric at close-to-the-link capacity are most needed in a cluster, Ethernet is a potential candidate, rivaling traditional HPC interconnects. The distributed real-time data acquisition at particle physics experiments presents an interesting use case. This article evaluates possible Ethernet-based solutions for aggregating data from hundreds of data sources at a throughput of dozens of Tb/s. This leads us to many-to-one data exchanges where we strive for a cost-optimized setup sustaining more than 80 % of the theoretical link-load. We investigate possible Ethernet-based traffic patterns to handle data acquisition on large multi-source apparatuses. Different numbers of producers and receivers and different link speeds are allowed in a large-scale network. Performance tests were conducted using customized benchmarks and evaluation test benches. The article presents tested scenarios and problems encountered in practice. We describe how our findings influenced the design of a large production system at CERN. We also present relevant general conclusions for a broader range of applications of Ethernet in HPC."",""1558-2183"","""",""10.1109/TPDS.2022.3163472"",""CERN";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9745357"",""Computer system implementation";distributed architectures;ethernet;high energy physics;high-speed;network protocols;network topology;parallel architectures;"real-time distributed"",""Ethernet";Filtering;Throughput;Real-time systems;Servers;Protocols;"Large Hadron Collider"","""","""","""",""48"",""IEEE"",""30 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Evaluating Data Redistribution in PaRSEC,""Q. Cao"; G. Bosilca; N. Losada; W. Wu; D. Zhong;" J. Dongarra"",""Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA"; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA; Los Alamos National Laboratory, New Mexico, USA; Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA;" Department of Electrical Engineering and Computer Science, University of Tennessee, Knoxiville, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Dec 2021"",""2022"",""33"",""8"",""1856"",""1872"",""Data redistribution aims to reshuffle data to optimize some objective for an algorithm. The objective can be multi-dimensional, such as improving computational load balance or decreasing communication volume or cost, with the ultimate goal of increasing the efficiency and therefore reducing the time-to-solution for the algorithm. The classic redistribution problem focuses on optimally scheduling communications when reshuffling data between two regular, usually block-cyclic, data distributions. Besides distribution, data size is also a performance-critical parameter because it affects the reshuffling algorithm in terms of cache, communication efficiency, and potential parallelism. In addition, task-based runtime systems have gained popularity recently as a potential candidate to address the programming complexity on the way to exascale. In this scenario, it becomes paramount to develop a flexible redistribution algorithm for task-based runtime systems, which could support all types of regular and irregular data distributions and take data size into account. In this article, we detail a flexible redistribution algorithm and implement an efficient approach in a task-based runtime system, PaRSEC. Performance results show great capability compared to the theoretical bound and ScaLAPACK, and applications highlight an increased efficiency with little overhead in terms of data distribution, data size, and data format."",""1558-2183"","""",""10.1109/TPDS.2021.3131657"",""Exascale Computing(grant numbers:17-SC-20-SC)"; U.S. Department of Energy; National Nuclear Security Administration;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629320"",""Data redistribution";data distribution;data size;data format;task-based programming model;dynamic runtime system;"high-performance computing"",""Task analysis";Runtime;Distributed databases;Programming;Parallel processing;Costs;"Program processors"","""",""2"","""",""68"",""IEEE"",""30 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Evaluating Spatial Accelerator Architectures with Tiled Matrix-Matrix Multiplication,""G. E. Moon"; H. Kwon; G. Jeong; P. Chatarasi; S. Rajamanickam;" T. Krishna"",""Department of Software, Korea Aerospace University, Goyang, Gyeonggi, Republic of Korea"; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA; Center for Computing Research, Sandia National Laboratories, Albuquerque, NM, USA;" School of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""1002"",""1014"",""There is a growing interest in custom spatial accelerators for machine learning applications. These accelerators employ a spatial array of processing elements (PEs) interacting via custom buffer hierarchies and networks-on-chip. The efficiency of these accelerators comes from employing optimized dataflow (i.e., spatial/temporal partitioning of data across the PEs and fine-grained scheduling) strategies to optimize data reuse. The focus of this work is to evaluate these accelerator architectures using a tiled general matrix-matrix multiplication (GEMM) kernel. To do so, we develop a framework that finds optimized mappings (dataflow and tile sizes) for a tiled GEMM for a given spatial accelerator and workload combination, leveraging an analytical cost model for runtime and energy. Our evaluations over five spatial accelerators demonstrate that the tiled GEMM mappings systematically generated by our framework achieve high performance on various GEMM workloads and accelerators."",""1558-2183"","""",""10.1109/TPDS.2021.3104240"",""U.S. Department of Energy"; National Technology & Engineering Solutions of Sandia, LLC; National Nuclear Security Administration(grant numbers:DE-NA0003525); Korea Aerospace University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511786"",""Spatial accelerator";DNN accelerator;dataflow;"GEMM mapping"",""Kernel";Analytical models;Runtime;Hardware;Shape;Parallel processing;"Sparse matrices"","""",""7"","""",""41"",""IEEE"",""11 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"EXA2PRO: A Framework for High Development Productivity on Heterogeneous Computing Systems,""L. Papadopoulos"; D. Soudris; C. Kessler; A. Ernstsson; J. Ahlqvist; N. Vasilas; A. I. Papadopoulos; P. Seferlis; C. Prouveur; M. Haefele; S. Thibault; A. Salamanis; T. Ioakimidis;" D. Kehagias"",""Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece"; Department of Electrical and Computer Engineering, National Technical University of Athens, Athens, Greece; Department of Computer and Information Science, Linköping University, Linköping, Sweden; Department of Computer and Information Science, Linköping University, Linköping, Sweden; Department of Computer and Information Science, Linköping University, Linköping, Sweden; Centre for Research and Technology Hellas, Chemical Process and Energy Resources Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Chemical Process and Energy Resources Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Chemical Process and Energy Resources Institute, Thessaloniki, Greece; Maison de la Simulation, CEA, CNRS, Paris, France; Université de Pau et des Pays de l'Adour, Pau, France; Bordeaux University, Bordeaux, France; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece; Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece;" Centre for Research and Technology Hellas, Information Technologies Institute, Thessaloniki, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""792"",""804"",""Programming upcoming exascale computing systems is expected to be a major challenge. New programming models are required to improve programmability, by hiding the complexity of these systems from application developers. The EXA2PRO programming framework aims at improving developers’ productivity for applications that target heterogeneous computing systems. It is based on advanced programming models and abstractions that encapsulate low-level platform-specific optimizations and it is supported by a runtime that handles application deployment on heterogeneous nodes. It supports a wide variety of platforms and accelerators (CPU, GPU, FPGA-based Data-Flow Engines), allowing developers to efficiently exploit heterogeneous computing systems, thus enabling more HPC applications to reach exascale computing. The EXA2PRO framework was evaluated using four HPC applications from different domains. By applying the EXA2PRO framework, the applications were automatically deployed and evaluated on a variety of computing architectures, enabling developers to obtain performance results on accelerators, test scalability on MPI clusters and productively investigate the degree by which each application can efficiently use different types of hardware resources."",""1558-2183"","""",""10.1109/TPDS.2021.3104257"",""European Union's Horizon 2020 research and innovation programme(grant numbers:801015)"; National Infrastructures for Research and Technology; PRACE(grant numbers:EXA2PRO);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511802"",""Programming models";skeleton programming;task-based runtime systems;programming productivity;heterogeneous systems;"exascale computing"",""Programming";Skeleton;Computational modeling;Runtime;Exascale computing;Task analysis;"Productivity"","""",""4"","""",""33"",""CCBY"",""11 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Execution Time Estimation of Multithreaded Programs With Critical Sections,""D. Kagaris"; S. Dutta;" S. Eyerman"",""School of Electrical, Computer, and Biomedical Engineering, Southern Illinois University, Carbondale, IL, USA"; School of Theoretical and Applied Science, Ramapo College of New Jersey, Mahwah, NJ, USA;" Intel Corporation, Kontich, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Apr 2022"",""2022"",""33"",""10"",""2470"",""2481"",""The ideal benefit of parallelizing/multithreading a program is diminished in practice by several factors such as hardware scaling, memory bandwidth, power constraints, and synchronization due to critical sections. Several models have been proposed in the past to estimate the resulting performance and extend the traditional Amdahl’s law. In this work, we focus on the effect of synchronization, and develop a model for the execution time estimation of multithreaded programs under the presence of critical sections. The proposed model is applicable to multiple different critical sections and generalizes and improves previously proposed models. Experimental results on simulated, synthetic and benchmark examples show that the proposed model provides accurate approximations."",""1558-2183"","""",""10.1109/TPDS.2022.3143455"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684674"",""Parallel programming";performance estimation;critical sections;mutual exclusion;synchronization;concurrency;"modelling"",""Codes";Message systems;Instruction sets;Estimation;Delays;Computational modeling;"Synchronization"","""",""2"","""",""23"",""IEEE"",""18 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Exploiting Concurrency in Sharded Parallel State Machine Replication,""A. Burgos"; E. Alchieri; F. Dotti;" F. Pedone"",""Universidade de Brasília, Brasília, Brazil"; Universidade de Brasília, Brasília, Brazil; PUCRS - Escola Politécnica, Porto Alegre, RS, Brazil;" Università della Svitzzera italiana, Lugano, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Feb 2022"",""2022"",""33"",""9"",""2133"",""2147"",""State machine replication (SMR) is a well-known approach to implementing fault-tolerant services, providing high availability and strong consistency. In classic SMR, commands are executed sequentially, in the same order by all replicas. To improve performance, two classes of protocols have been proposed to parallelize the execution of commands. Early scheduling protocols reduce scheduling overhead but introduce costly synchronization of worker threads";" late scheduling protocols, instead, reduce the cost of thread synchronization but suffer from scheduling overhead. Depending on the characteristics of the workload, one class can outperform the other. We introduce a hybrid scheduling technique that builds on the existing protocols. An experimental evaluation has revealed that the hybrid approach not only inherits the advantages of each technique but also scales better than either one of them, improving the system performance by up to  $3\times$3× in a workload with conflicting commands."",""1558-2183"","""",""10.1109/TPDS.2021.3135761"",""Coordenação de Aperfeiçoamento de Pessoal de Nível Superior"; Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9652105"",""Parallel state machine replication";scheduling;"dependability"",""Scheduling";Message systems;Data structures;Servers;Protocols;Synchronization;"Schedules"","""",""1"","""",""46"",""IEEE"",""15 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Exploring Data Analytics Without Decompression on Embedded GPU Systems,""Z. Pan"; F. Zhang; Y. Zhou; J. Zhai; X. Shen; O. Mutlu;" X. Du"",""Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, ETH Zurich, Zurich, Switzerland;" Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Nov 2021"",""2022"",""33"",""7"",""1553"",""1568"",""With the development of computer architecture, even for embedded systems, GPU devices can be integrated, providing outstanding performance and energy efficiency to meet the requirements of different industries, applications, and deployment environments. Data analytics is an important application scenario for embedded systems. Unfortunately, due to the limitation of the capacity of the embedded device, the scale of problems handled by the embedded system is limited. In this paper, we propose a novel data analytics method, called G-TADOC, for efficient text analytics directly on compression on embedded GPU systems. A large amount of data can be compressed and stored in embedded systems, and can be processed directly in the compressed state, which greatly enhances the processing capabilities of the systems. Particularly, G-TADOC has three innovations. First, a novel fine-grained thread-level workload scheduling strategy for GPU threads has been developed, which partitions heavily-dependent loads adaptively in a fine-grained manner. Second, a GPU thread-safe memory pool has been developed to handle inconsistency with low synchronization overheads. Third, a sequence-support strategy is provided to maintain high GPU parallelism while ensuring sequence information for lossless compression. Moreover, G-TADOC involves special optimizations for embedded GPUs, such as utilizing the CPU-GPU shared unified memory. Experiments show that G-TADOC provides 13.2× average speedup compared to the state-of-the-art TADOC. G-TADOC also improves performance-per-cost by 2.6× and energy efficiency by 32.5× over TADOC."",""1558-2183"","""",""10.1109/TPDS.2021.3119402"",""National Key Research and Development Program of China(grant numbers:2018YFB1004401)"; National Natural Science Foundation of China(grant numbers:61732014,62172419,U20A20226,61802412); Natural Science Foundation of Beijing Municipality(grant numbers:4202031); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program(grant numbers:20191080594); State Key Laboratory of Computer Architecture(grant numbers:CARCHA202007); GHfund A(grant numbers:20210701); CCF-Tencent Open Research Fund;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9568774"",""TADOC";embedded GPU systems;compression;"data analytics"",""Graphics processing units";Embedded systems;Data analysis;Parallel processing;Instruction sets;Optimization;"Random access memory"","""",""15"","""",""78"",""IEEE"",""12 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Exploring Query Processing on CPU-GPU Integrated Edge Device,""J. Liu"; F. Zhang; H. Li; D. Wang; W. Wan; X. Fang; J. Zhai;" X. Du"",""Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4057"",""4070"",""Huge amounts of data have been generated on edge devices every day, which requires efficient data analytics and management. However, due to the limited computing capacity of these edge devices, query processing at the edge faces tremendous pressure. Fortunately, in recent years, hardware vendors have integrated heterogeneous coprocessors, such as GPUs, into the edge device, which can provide much more computing power. Furthermore, the CPU-GPU integrated edge device has shown significant benefits in a variety of situations. Therefore, the exploration of query processing on such CPU-GPU integrated edge devices becomes an urgent need. In this article, we develop a fine-grained query processing engine, called FineQuery, which can perform efficient query processing on CPU-GPU integrated edge devices. Particularly, FineQuery can take advantage of both architectural features of edge devices and query characteristics by performing fine-grained workload scheduling between the CPU and the GPU. Experiments show that on TPC-H workloads, FineQuery reduces 42.81% latency and improves 2.39× bandwidth utilization on average compared to the implementation of using only GPU or CPU. Furthermore, query processing at the edge can bring significant performance-per-cost benefits and energy efficiency. On average, FineQuery at the edge brings 21× performance-per-cost ratio and 4× energy efficiency compared with processing the data on a discrete GPU platform."",""1558-2183"","""",""10.1109/TPDS.2022.3177811"",""National Natural Science Foundation of China(grant numbers:61732014,62172419,U20A20226,62072459)"; Beijing Natural Science Foundation(grant numbers:4202031); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program(grant numbers:20191080594); CCF-Tencent Open Research Fund;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782546"",""CPU";GPU;integrated architecture;edge device;"query processing"",""Graphics processing units";Query processing;Performance evaluation;Computer architecture;Databases;Structured Query Language;"Engines"","""",""5"","""",""62"",""IEEE"",""26 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Exploring the Galaxyfly Family to Build Flexible-Scale Interconnection Networks,""F. Lei"; D. Dong;" X. Liao"",""College of Computer, National University of Defense Technology, Changsha, China"; College of Computer, National University of Defense Technology, Changsha, China;" College of Computer, National University of Defense Technology, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Oct 2021"",""2022"",""33"",""5"",""1054"",""1068"",""Interconnection networks play an essential role in the architecture of high-performance computing (HPC) systems. In this article, we explore the Galaxyfly family to build flexible-scale interconnection networks. Galaxyfly is guaranteed to retain a small constant diameter while achieving a flexible tradeoff between network scale and bisection bandwidth. Galaxyfly not only supports small-scale interconnection networks with smaller diameter but also lowers the demands for high-radix routers and is able to utilize routers with moderate radix to build exascale interconnection networks. We analyze the constructible configuration of Galaxyfly and evaluate the properties of Galaxyfly. We conduct extensive simulations and analysis to evaluate the performance, cost, and power consumption of Galaxyfly on physical layout against state-of-the-art topologies. The results show that our design achieves better performance than most existing topologies under typical HPC workloads, and is cost-effective to deploy for exascale HPC systems."",""1558-2183"","""",""10.1109/TPDS.2021.3100783"",""National Key Research and Development Program of China(grant numbers:2018YFB0204300)"; Excellent Youth Foundation of Hunan Province;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501493"",""High-performance computing";interconnection;topology;flexible-radix;"low-diameter"",""Topology";Network topology;Multiprocessor interconnection;Scalability;Generators;Routing;"Power demand"","""",""1"","""",""48"",""IEEE"",""29 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Fairness-Aware VNF Sharing and Rate Coordination for High Efficient Service Scheduling,""B. Yi"; X. Wang; M. Huang; S. K. Das;" K. Li"",""College of Computer Science and Engineering, Northeastern University, Shenyang, China"; College of Computer Science and Engineering, Northeastern University, Shenyang, China; College of Information Science and Engineering, Northeastern University, Shenyang, China; Department of Computer Science, Missouri University of Science and Technology, Rolla, MO, USA;" Department of Computer Science, State University of New York, New York, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Sep 2022"",""2022"",""33"",""12"",""4597"",""4611"",""Network service provisioning becomes flexible and programmable with the help of Network Function Virfitualization (NFV), since NFV abstracts various service functions into software components called Virtual Network Function (VNF) and VNFs can be flexibly and quickly composed to form new services. It is commonly known that sharing the same VNF among different services can improve the resource utilization. However, we should be aware that such sharing also leads to serious resource preemption. In addition, VNF sharing aggravates the generation of the performance bottleneck, which then causes the rate mismatch problem between the upstream and downstream VNFs belonging to the same service chain. In this article, we propose a dynamic and flexible algorithm to jointly address the VNF sharing resource allocation and the rate coordination between the upstream and downstream VNFs. Specifically, 1) the VNFs are shared among different service chains with a fairness factor considered for the purpose of reducing the resource preemption probability and improving the resource utilization";" 2) the backpressure indicator of each VNF is defined to judge its pressure condition, based on which we can dynamically adjust the processing rates between it and its downstream or upstream VNFs by maximizing the idle resource utilization. The experimental results indicate that the proposed algorithm outperforms the other methods in terms of the average delay, the flow completion time, the throughput and the backlog, etc. Meanwhile, the proposed algorithm achieves more stable performance than the other methods."",""1558-2183"","""",""10.1109/TPDS.2022.3199392"",""National Key R&D Program of China(grant numbers:2019YFB1802800)"; National Natural Science Foundation of China(grant numbers:62002055,62032013,61872073); China Postdoctoral Science Foundation(grant numbers:2020M680972); Postdoctoral Research Fund of Northeastern University of China(grant numbers:20200103); Fundamental Research Funds for the Central Universities(grant numbers:N2016012);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858330"",""Network function virtualization";performance bottleneck;rate coordination;service chain;"traffic scheduling"",""Resource management";Noise measurement;Heuristic algorithms;Network function virtualization;Software;Computer science;"Mathematical models"","""",""8"","""",""42"",""IEEE"",""17 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"FARNN: FPGA-GPU Hybrid Acceleration Platform for Recurrent Neural Networks,""H. Cho"; J. Lee;" J. Lee"",""Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea"; Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea;" Department of Computer Science and Engineering, Seoul National University, Seoul, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Nov 2021"",""2022"",""33"",""7"",""1725"",""1738"",""GPU-based platforms provide high computation throughput for large mini-batch deep neural network computations. However, a large batch size may not be ideal for some situations, such as aiming at low latency, training on edge/mobile devices, partial retraining for personalization, and having irregular input sequence lengths. GPU performance suffers from low utilization especially for small-batch recurrent neural network (RNN) applications where sequential computations are required. In this article, we propose a hybrid architecture, called FARNN, which combines a GPU and an FPGA to accelerate RNN computation for small batch sizes. After separating RNN computations into GPU-efficient and GPU-inefficient tasks, we design special FPGA computation units that accelerate the GPU-inefficient RNN tasks. FARNN off-loads the GPU-inefficient tasks to the FPGA. We evaluate FARNN with synthetic RNN layers of various configurations on the Xilinx UltraScale+ FPGA and the NVIDIA P100 GPU in addition to evaluating it with real RNN applications. The evaluation result indicates that FARNN outperforms the P100 GPU platform for RNN training by up to 4.2$\times {}$× with small batch sizes, long input sequences, and many RNN cells per layer."",""1558-2183"","""",""10.1109/TPDS.2021.3124125"",""National Research Foundation of Korea(grant numbers:NRF-2016M3C4A7952587,NRF-2019M3E4A1080386,NRF-2019R1F1A1062335)"; Institute for Information and Communications Technology Promotion(grant numbers:2018-0-00581); CUDA Programming Environment for FPGA Clusters; Ministry of Science and ICT, South Korea;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9600618"",""FPGA";GPU;hybrid platform;"RNN"",""Task analysis";Training;Graphics processing units;Field programmable gate arrays;Logic gates;Recurrent neural networks;"Throughput"","""",""10"","""",""66"",""IEEE"",""3 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FarSpot: Optimizing Monetary Cost for HPC Applications in the Cloud Spot Market,""A. C. Zhou"; J. Lao; Z. Ke; Y. Wang;" R. Mao"",""College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China"; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China;" Shenzhen Institute of Computing Sciences, Shenzhen University, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2955"",""2967"",""Recently, we have witnessed many HPC applications developed and hosted in the cloud, which can benefit from the elastic and diversified resources on the cloud, while on the other hand confronting high costs for executing the long-running HPC applications. Although public clouds such as Amazon EC2 offer spot instances with dynamic and usually low prices compared to on-demand ones, the spot prices can vary significantly and sometimes can even be more expensive than on-demand prices of the same type. Previous work on reducing the monetary cost for HPC applications using spot instances focused on designing fault tolerance techniques or selecting appropriate instance types/bid prices to make good usage of the low spot prices. However, with the recent update of spot pricing model on Amazon EC2, these work may become either inefficient or invalid. In this article, we present FarSpot which is an optimization framework for HPC applications in the latest cloud spot market with the goal of minimizing application cost while ensuring performance constraints. FarSpot provides accurate long-term price prediction for a wide range of spot instance types using ensemble-based learning method. It further incorporates a cost-aware deadline assignment algorithm to distribute application deadline to each task according to spot price changes. With the assigned subdeadline of each task, FarSpot dynamically migrates tasks among spot instances to reduce execution cost. Evaluation results using real HPC benchmark show that 1) the prediction error of FarSpot is very low (below 3%), 2) FarSpot reduced the monetary cost by 32% on average compared to state-of-the-art algorithms, and 3) FarSpot satisfies the user-specified deadline constraints at all time."",""1558-2183"","""",""10.1109/TPDS.2021.3134644"",""National Natural Science Foundation of China(grant numbers:62172282,61802260,62072311,61972259,62122056,U2001212)"; Guangdong Basic and Applied Basic Research Foundation(grant numbers:2020B1515120028,2019B151502055); Guangdong NSF(grant numbers:2019A1515012053); Shenzhen Science and Technology Foundation(grant numbers:JCYJ20210324094402008,JCYJ20210324093212034); Tencent;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9648022"",""Cloud computing";spot market;price prediction;"ensemble models"",""Costs";Predictive models;Pricing;Task analysis;Cloud computing;Prediction algorithms;"Fault tolerant systems"","""",""4"","""",""42"",""IEEE"",""13 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Fast and Accurate Statistical Simulation of Shared-Memory Applications on Multicore Systems,""F. Jiang"; R. K. V. Maeda; J. Feng; S. Chen; L. Chen; X. Li;" J. Xu"",""Microelectronics Thrust, Hong Kong University of Science and Technology, Hong Kong"; Microelectronics Thrust, Hong Kong University of Science and Technology, Hong Kong; Microelectronics Thrust, Hong Kong University of Science and Technology, Hong Kong; Microelectronics Thrust, Hong Kong University of Science and Technology, Hong Kong; Microelectronics Thrust, Hong Kong University of Science and Technology, Hong Kong; Microelectronics Thrust, Hong Kong University of Science and Technology, Hong Kong;" Microelectronics Thrust, Hong Kong University of Science and Technology, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Mar 2022"",""2022"",""33"",""10"",""2455"",""2469"",""Detailed cycle-accurate simulation of multicore systems is naturally slow. Statistical simulation is one alternative that permits trading off simulation speed for accuracy. However, there is a lack of effective memory locality models for multicore applications. Hence, existing statistical simulators neglect data-sharing between threads. Additionally, the standard method to speed up statistical simulations is to blindly reduce the trace length to be synthesized. While this gives good control over the speedup, it leaves the simulation error unbounded. In this work, we introduce a novel statistical simulation methodology for exploration of shared-memory multicore systems. It includes a new sharing-locality model (Shalom) that captures and reproduces data-sharing in multithread applications. Furthermore, we propose a method to bound the simulation error for a particular metric while maximizing speedup. The technique works by monitoring the convergence of the statistical synthesis. It is referred to as convergence-deterministic simulation (Condens). The combination of Shalom and Condens is around 130x faster than cycle-accurate simulations with reasonable accuracy loss. Our approach is also 5x faster than state-of-the-art sampling simulation under the same accuracy level. Compared to previous statistical simulators ignoring sharing, our approach is 2x more accurate for performance metrics and 8x more accurate for cache miss estimations."",""1558-2183"","""",""10.1109/TPDS.2022.3143535"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9684737"",""Statistical simulation";reuse distance;performance modeling;simulation speedup;locality model;"multicore system"",""Instruction sets";Computational modeling;Multicore processing;Synchronization;Computer architecture;Message systems;"Benchmark testing"","""","""","""",""64"",""IEEE"",""18 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Fast and Portable Concurrent FIFO Queues With Deterministic Memory Reclamation,""O. Giersch";" J. Nolte"",""Brandenburg University of Technology (BTU) Cottbus-Senftenberg, Cottbus, Germany";" Brandenburg University of Technology (BTU) Cottbus-Senftenberg, Cottbus, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2021"",""2022"",""33"",""3"",""604"",""616"",""In this article we present an algorithm for a high performance, unbounded, portable, multi-producer/multi-consumer, lock-free FIFO (first-in first-out) queue. Aside from its competitive performance on current hardware, it is further characterized by its integrated memory reclamation mechanism, which is able to reliably and deterministically de-allocate nodes as soon as the final operation with a reference has concluded, similar to reference counting. This differentiates our approach from most other lock-free data structures, which usually require external (generic) memory reclamation or garbage collection mechanisms such as hazard pointers. Our deterministic memory reclamation mechanism completely prevents the build up of memory awaiting reclamation and is hence very memory efficient, yet it does not introduce any substantial performance overhead. By utilizing concrete knowledge about the internal structure and access patterns of our queue, we are able to construct and constrain the reclamation mechanism in such a way that keeps the overhead for memory management almost entirely out of the common fast path. The presented algorithm is portable to all modern 64-bit processor architectures, as it only relies on the commonly available and lock-free atomic synchronization primitives compare-and-swap and fetch-and-add."",""1558-2183"","""",""10.1109/TPDS.2021.3097901"",""Bundesministerium für Bildung und Forschung(grant numbers:01IS18072)"; German Science Foundation(grant numbers:625/7-2);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490347"",""Concurrent algorithms";lock-free/non-blocking data structures;memory reclamation;first in-first out (FIFO) queues;"shared memory"",""Memory management";Instruction sets;Hazards;Indexes;Runtime;Arrays;"Synchronization"","""",""1"","""",""33"",""IEEE"",""19 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Fast Post-Hoc Normalization for Brain Inspired Sparse Coding on a Neuromorphic Device,""K. Henke"; G. T. Kenyon;" B. Migliori"",""Computer, Computational, and Statistical Sciences (CCS-3), Los Alamos National Laboratory, NM, USA"; Computer, Computational, and Statistical Sciences (CCS-3), Los Alamos National Laboratory, NM, USA;" Computer, Computational, and Statistical Sciences (CCS-3), Los Alamos National Laboratory, NM, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Aug 2021"",""2022"",""33"",""2"",""302"",""309"",""Exploration of novel computational platforms is critical for the advancement of artificial intelligence as we approach the physical limitations of traditional hardware. Biologically accurate, energy efficient neuromorphic systems are particularly promising for enabling future breakthroughs because of their ability to process information in parallel and to scale using extremely low power. Sparse coding is a signal processing technique which has been known to model the information encoding in the primary visual cortex. When sparse solutions are solved using local neuron competition along with the unsupervised dictionary learning that mimics cortical development, we can build an end to end, hardware to software, brain inspired solution to a machine learning problem. In this article, we perform a detailed comparison of sparse coding solutions generated classically by orthogonal matching pursuit (OMP) implemented on a conventional digital processor with spike-based solutions obtained using the Intel Loihi neuromorphic processor. A novel “post-hoc” normalization technique to shorten simulation time for Loihi is presented along with analysis of optimal parameter selection, reconstruction errors, and unsupervised dictionary learning for Loihi approaches and their classical counterparts. Preliminary results show that both the Loihi full simulation approach and the post-hoc normalization approach are well suited to neuromorphic processors and operate in a size, weight and power regime that is not accessible by classical approaches. Ultimately, the use of this normalization technique allows for faster and, often, better solutions than demonstrated previously."",""1558-2183"","""",""10.1109/TPDS.2021.3068777"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9385919"",""Neuromorphic computing";machine learning;artificial intelligence;neurocomputers;computer vision;"signal processing"",""Encoding";Neurons;Neuromorphics;Matching pursuit algorithms;Dictionaries;Image reconstruction;"Heuristic algorithms"","""",""1"","""",""17"",""IEEE"",""24 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Fast Proactive Repair in Erasure-Coded Storage: Analysis, Design, and Implementation,""X. Li"; K. Cheng; Z. Shen;" P. P. C. Lee"",""School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong; School of Informatics, Xiamen University, Xiamen, Fujian, China;" Department of Computer Science and Engineering, The Chinese University of Hong Kong, Shatin, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2022"",""2022"",""33"",""12"",""3400"",""3414"",""Erasure coding offers a storage-efficient redundancy mechanism for maintaining data availability guarantees in large-scale storage clusters, yet it also incurs high performance overhead in failure repair. Recent developments in accurate disk failure prediction allow soon-to-fail (STF) nodes to be repaired in advance, thereby opening new opportunities for accelerating failure repair in erasure-coded storage. To this end, we present a fast proactive repair solution called ${{\sf FastPR}}$FastPR, which carefully couples two repair methods, namely migration (i.e., relocating the chunks of an STF node) and reconstruction (i.e., decoding the chunks of an STF node through erasure coding), so as to fully parallelize the repair operation across the storage cluster. ${{\sf FastPR}}$FastPR solves a bipartite maximum matching problem and schedules both migration and reconstruction in a parallel fashion. We show that ${{\sf FastPR}}$FastPR significantly reduces the repair time over the baseline repair approaches for both Reed-Solomon codes and Azure's Local Reconstruction Codes via mathematical analysis, large-scale simulation, and Amazon EC2 experiments."",""1558-2183"","""",""10.1109/TPDS.2022.3152817"",""National Key R&D Program of China(grant numbers:2021YFF0704001)"; National Natural Science Foundation of China(grant numbers:62072381); CCF-Huawei Innovation Research Plan(grant numbers:CCF2021-admin-270-202102); Xiamen Youth Innovation Fund(grant numbers:3502Z20206052);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721141"",""Erasure coding";repair;"distributed storage"",""Maintenance engineering";Codes;Encoding;Redundancy;Production;Bandwidth;"Schedules"","""","""","""",""54"",""IEEE"",""24 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Federated Learning With Nesterov Accelerated Gradient,""Z. Yang"; W. Bao; D. Yuan; N. H. Tran;" A. Y. Zomaya"",""School of Computer Science, The University of Sydney, Sydney, NSW, Australia"; School of Computer Science, The University of Sydney, Sydney, NSW, Australia; School of Electrical and Information Engineering, University of Sydney, Sydney, NSW, Australia; School of Computer Science, The University of Sydney, Sydney, NSW, Australia;" School of Computer Science, The University of Sydney, Sydney, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Oct 2022"",""2022"",""33"",""12"",""4863"",""4873"",""Federated learning (FL) is a fast-developing technique that allows multiple workers to train a global model based on a distributed dataset. Conventional FL (FedAvg) employs gradient descent algorithm, which may not be efficient enough. Momentum is able to improve the situation by adding an additional momentum step to accelerate the convergence and has demonstrated its benefits in both centralized and FL environments. It is well-known that Nesterov Accelerated Gradient (NAG) is a more advantageous form of momentum, but it is not clear how to quantify the benefits of NAG in FL so far. This motives us to propose FedNAG, which employs NAG in each worker as well as NAG momentum and model aggregation in the aggregator. We provide a detailed convergence analysis of FedNAG and compare it with FedAvg. Extensive experiments based on real-world datasets and trace-driven simulation are conducted, demonstrating that FedNAG increases the learning accuracy by 3–24% and decreases the total training time by 11–70% compared with the benchmarks under a wide range of settings."",""1558-2183"","""",""10.1109/TPDS.2022.3206480"",""Australian Research Council(grant numbers:DP200103718,DP200103494)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9891808"",""Edge computing";federated learning;"nesterov accelerated gradient"",""Convergence";Training;Computational modeling;Collaborative work;Quantization (signal);Servers;"Internet of Things"","""",""5"","""",""43"",""IEEE"",""14 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FedGraph: Federated Graph Learning With Intelligent Sampling,""F. Chen"; P. Li; T. Miyazaki;" C. Wu"",""University of Aizu, Aizuwakamatsu, Japan"; University of Aizu, Aizuwakamatsu, Japan; University of Aizu, Aizuwakamatsu, Japan;" University of Electro-Communications, Chofu, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Dec 2021"",""2022"",""33"",""8"",""1775"",""1786"",""Federated learning has attracted much research attention due to its privacy protection in distributed machine learning. However, existing work of federated learning mainly focuses on Convolutional Neural Network (CNN), which cannot efficiently handle graph data that are popular in many applications. Graph Convolutional Network (GCN) has been proposed as one of the most promising techniques for graph learning, but its federated setting has been seldom explored. In this article, we propose FedGraph for federated graph learning among multiple computing clients, each of which holds a subgraph. FedGraph provides strong graph learning capability across clients by addressing two unique challenges. First, traditional GCN training needs feature data sharing among clients, leading to risk of privacy leakage. FedGraph solves this issue using a novel cross-client convolution operation. The second challenge is high GCN training overhead incurred by large graph size. We propose an intelligent graph sampling algorithm based on deep reinforcement learning, which can automatically converge to the optimal sampling policies that balance training speed and accuracy. We implement FedGraph based on PyTorch and deploy it on a testbed for performance evaluation. The experimental results of four popular datasets demonstrate that FedGraph significantly outperforms existing work by enabling faster convergence to higher accuracy."",""1558-2183"","""",""10.1109/TPDS.2021.3125565"",""Okawa Foundation for Information and Telecommunications"; G-7 Scholarship Foundation; JSPS KAKENHI(grant numbers:21H03424,19K20258);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9606516"",""Federated learning";graph learning;graph sampling;"reinforcement learning"",""Training";Convolution;Collaborative work;Servers;Privacy;Computational modeling;"Convolutional neural networks"","""",""26"","""",""43"",""IEEE"",""8 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FenceKV: Enabling Efficient Range Query for Key-Value Separation,""C. Tang"; J. Wan;" C. Xie"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China;" Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2022"",""2022"",""33"",""12"",""3375"",""3386"",""LSM-tree is widely used in key-value stores for big data storage, but it suffers from write amplification brought by frequent compaction operations. An effective solution for this problem is key-value separation, which decouples values from the LSM-tree and stores them in a separate value log. However, existing key-value separation schemes achieve poor range query performance, especially for small key-value pairs, because they focus on mitigating write amplification but neglect access characteristics of the SSD. In this article, we propose FenceKV, which aims to achieve better range query performance while maintaining reasonable update performance for update-intensive workloads. FenceKV employs a new partition method to map values to the storage space based on the key-range to achieve efficient update and range query. Moreover, it adopts a key-range garbage collection policy to mitigate the garbage collection overhead and maintain sequential access for range queries. We compare FenceKV with modern key-value stores with various workloads, and results show that FenceKV can improve the range query performance significantly, while maintaining reasonable update performance compared to the existing designs of key-value separation."",""1558-2183"","""",""10.1109/TPDS.2022.3149003"",""National Natural Science Foundation of China(grant numbers:62072196)"; Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2021B0101400003); National Natural Science Foundation of China(grant numbers:61821003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9706279"",""LSM-tree";key-value store;key-value separation;range query;"SSD"",""Compaction";Performance evaluation;Throughput;Metadata;Space exploration;Optimization;"Machine learning algorithms"","""",""7"","""",""45"",""IEEE"",""7 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FFNLFD: Fault Diagnosis of Multiprocessor Systems at Local Node With Fault-Free Neighbors Under PMC Model and MM* Model,""L. Lin"; Y. Huang; Y. Lin; S. -Y. Hsieh;" L. Xu"",""Key Laboratory of Network Security and Cryptology, and Center for Applied Mathematics of Fujian Province (Fujian Normal University), College of Computer and Cyber Security, Fujian Normal University, Fuzhou, Fujian, China"; School of Computer Science and Mathematics, Fujian University of Technology, Fuzhou, Fujian, China; Key Laboratory of Network Security and Cryptology, and Center for Applied Mathematics of Fujian Province (Fujian Normal University), College of Computer and Cyber Security, Fujian Normal University, Fuzhou, Fujian, China; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan;" Key Laboratory of Network Security and Cryptology, and Center for Applied Mathematics of Fujian Province (Fujian Normal University), College of Computer and Cyber Security, Fujian Normal University, Fuzhou, Fujian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Nov 2021"",""2022"",""33"",""7"",""1739"",""1751"",""Fault diagnosability is utilized as a significant measure that reflects the reliability of a multiprocessor system. However, people frequently pay close attention to the entire system’s diagnosability while ignoring the system’s important local information. The $m$m-fault-free-neighbor local fault diagnosability (for short, $m$m-FFNLFD) is a novel indicator, which describes the diagnosability of a system at a local node with $m$m fault-free neighbors. In this paper, we propose the $m$m-FFNLFD of general networks at local node under the Preparata Metze Chien model. Moreover, we also characterize some important properties of $m$m-FFNLFD of a multiprocessor system under the comparison model. Furthermore, we apply our proposed conclusions to directly obtain the $m$m-FFNLFD of 11 well-known networks under PMC-M and MM*-M, including hypercubes, locally twisted cubes, $k$k-ary $n$n-cubes, crossed cubes, twisted hypercubes, exchanged hypercubes, star graphs, $(n,k)$(n,k)-star graphs, $(n,k)$(n,k)-arrangement graphs, data center network DCells and BCDCs. Finally, we compare the $m$m-FFNLFD with both diagnosability and conditional diagnosability, and it is shown that the $m$m-FFNLFD is greater than all the other fault diagnosabilities."",""1558-2183"","""",""10.1109/TPDS.2021.3126257"",""National Natural Science Foundation of China(grant numbers:62171132,62102088,U1905211,61771140)"; Fok Ying Tung Education Foundation(grant numbers:171061); Natural Science Foundation of Fujian Province(grant numbers:2021J05228); Fujian University of Technology(grant numbers:GJ-YB-20-06);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609660"",""Interconnection networks";reliability;fault diagnosis;"fault-free-neighbor local fault diagnosability"",""Multiprocessing systems";Hypercubes;Data models;Fault diagnosis;Data centers;Computer science;"Computer crime"","""",""3"","""",""45"",""IEEE"",""9 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Flexible Clustered Federated Learning for Client-Level Data Distribution Shift,""M. Duan"; D. Liu; X. Ji; Y. Wu; L. Liang; X. Chen; Y. Tan;" A. Ren"",""Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, Chongqing University, Chongqing, China"; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, Chongqing University, Chongqing, China; School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, China; School of Computer Science and Engineering, Xi'an Jiaotong University, Xi'an, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, Chongqing University, Chongqing, China;" Key Laboratory of Dependable Service Computing in Cyber Physical Society, Ministry of Education, Chongqing University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2661"",""2674"",""Federated Learning (FL) enables the multiple participating devices to collaboratively contribute to a global neural network model while keeping the training data locally. Unlike the centralized training setting, the non-IID, imbalanced (statistical heterogeneity) and distribution shifted training data of FL is distributed in the federated network, which will increase the divergences between the local models and the global model, further degrading performance. In this paper, we propose a flexible clustered federated learning (CFL) framework named FlexCFL, in which we 1) group the training of clients based on the similarities between the clients’ optimization directions for lower training divergence"; 2) implement an efficient newcomer device cold start mechanism for framework scalability and practicality;" 3) flexibly migrate clients to meet the challenge of client-level data distribution shift. FlexCFL can achieve improvements by dividing joint optimization into groups of sub-optimization and can strike a balance between accuracy and communication efficiency in the distribution shift environment. The convergence and complexity are analyzed to demonstrate the efficiency of FlexCFL. We also evaluate FlexCFL on several open datasets and made comparisons with related CFL frameworks. The results show that FlexCFL can significantly improve absolute test accuracy by $+10.6\%$+10.6% on FEMNIST compared with FedAvg, $+3.5\%$+3.5% on FashionMNIST compared with FedProx, $+8.4\%$+8.4% on MNIST compared with FeSEM, $+4.7\%$+4.7% on Sentiment140 compare with IFCA. The experiment results show that FlexCFL is also communication efficient in the distribution shift environment."",""1558-2183"","""",""10.1109/TPDS.2021.3134263"",""National Natural Science Foundation of China(grant numbers:61672116,61601067,61802038,61672115)"; Chongqing High-Tech Research Key Program(grant numbers:cstc2019jscx-mbdx0063); Fundamental Research Funds for the Central Universities(grant numbers:0214005207005,2019CDJGFJSJ001); Chongqing Youth Talent Support Program(grant numbers:cstc2020jcyj-jqX0012); China Postdoctoral Science Foundation(grant numbers:2017M620412);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647969"",""Federated learning";distributed machine learning;"neural networks"",""Training";Servers;Convergence;Optimization;Data models;Collaborative work;"Training data"","""",""11"","""",""47"",""IEEE"",""13 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Flexible Performant GEMM Kernels on GPUs,""T. Faingnaert"; T. Besard;" B. De Sutter"",""Department of Electronics and Information Systems, Ghent University, Ghent, Belgium"; Julia Computing, Ghent University, Ghent, Belgium;" Department of Electronics and Information Systems, Ghent University, Ghent, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Feb 2022"",""2022"",""33"",""9"",""2230"",""2248"",""General Matrix Multiplication or GEMM kernels take centre place in high performance computing and machine learning. Recent NVIDIA GPUs include GEMM accelerators, such as NVIDIA’s Tensor Cores. Their exploitation is hampered by the two-language problem: it requires either low-level programming which implies low programmer productivity or using libraries that only offer a limited set of components. Because rephrasing algorithms in terms of established components often introduces overhead, the libraries’ lack of flexibility limits the freedom to explore new algorithms. Researchers using GEMMs can hence not enjoy programming productivity, high performance, and research flexibility at once. In this paper we solve this problem. We present three sets of abstractions and interfaces to program GEMMs within the scientific Julia programming language. The interfaces and abstractions are co-designed for researchers’ needs and Julia’s features to achieve sufficient separation of concerns and flexibility to easily extend basic GEMMs in many different ways without paying a performance price. Comparing our GEMMs to state-of-the-art libraries cuBLAS and CUTLASS, we demonstrate that our performance is in the same ballpark of the libraries, and in some cases even exceeds it, without having to write a single line of code in CUDA C++ or assembly, and without facing flexibility limitations."",""1558-2183"","""",""10.1109/TPDS.2021.3136457"",""Fonds Wetenschappelijk Onderzoek(grant numbers:3G051318)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9655458"",""Matrix multiplication";graphics processors;"high-level programming languages"",""Libraries";Kernel;Graphics processing units;Codes;Programming;Instruction sets;"Productivity"","""",""1"","""",""52"",""IEEE"",""17 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"FlitZip: Effective Packet Compression for NoC in MultiProcessor System-on-Chip,""D. Deb"; R. M.K.;" J. Jose"",""Department of Computer Science and Engineering, Indian Institute of Technology Guwahati, Guwahati, Assam, India"; Department of Electronics and Communication Engineering, R. V. College of Engineering, Bangalore, India;" Department of Computer Science and Engineering, Indian Institute of Technology Guwahati, Guwahati, Assam, India"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jul 2021"",""2022"",""33"",""1"",""117"",""128"",""Applications running on Network on Chip (NoC) based multicore systems demand increased on-chip network bandwidth that can cater to the need for intensive communication among the cores and caches. Due to strict area and power budget, the bandwidth offered by NoC is very limited. Data-intensive and communication-centric applications on encountering a cache miss lead to a considerable burden on the underlying network for transferring blocks from multiple cache hierarchies to the requesting core as packets. This increases the packet transmission latency, thereby slowing down the system performance. Also, NoC being the highest component of power consumption after the cores, an increase in packets increases the dynamic power consumption of NoC. The article proposes FlitZip that addresses the problem by reducing on-chip traffic through compressing network packets. Hence, the compressed packet requires less bandwidth during its transfer, reducing the network's power consumption. Experimental analysis shows that FlitZip achieves a better compression ratio of 52 percent, reduces packet latency and bandwidth utilization by 19.28 and 27 percent, respectively. It also reduces the area and power consumption of the de/compression units by 53.33 and 62.3 percent, respectively, compared to the state-of-the-art packet compression technique, NoΔ."",""1558-2183"","""",""10.1109/TPDS.2021.3090315"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459512"",""Network on chip";packet;delta compression;multicore processor;"router"",""Image coding";Bandwidth;Encoding;Power demand;Random access memory;System-on-chip;"Multicore processing"","""",""4"","""",""39"",""IEEE"",""17 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Formulating Cost-Effective Data Distribution Strategies Online for Edge Cache Systems,""X. Xia"; F. Chen; Q. He; J. Grundy; M. Abdelrazek; J. Shen; A. Bouguettaya;" H. Jin"",""School of Computer Science, University of Adelaide, Adelaide, SA, Australia"; School of Information Technology, Deakin University, Burwood, VIC, Australia; Department of Computing Technologies, Swinburne University of Technology, Hawthorn, VIC, Australia; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; School of Information Technology, Deakin University, Burwood, VIC, Australia; School of Computing and Information Technology, University of Wollongong, Wollongong, NSW, Australia; School of Computer Science, University of Sydney, Camperdown, NSW, Australia;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4270"",""4281"",""Edge Computing (EC) enables a new kind of caching system in close geographic proximity to end-users by allowing app vendors to cache popular data on edge servers deployed at base stations. This edge cache system can better support latency-sensitive applications. However, transmitting data from the centralized cloud to the edge servers without proper transmission strategies may cost app vendors dearly. Cost-effective data distribution strategies are of particular importance for applications, whose data to be cached at the edge often changes dynamically. In this paper, we study this Online Edge Data Distribution (OEDD) problem, aiming to minimize app vendors’ total transmission cost, while ensuring low transmission latency in the long term. We first model this problem and prove its $\mathcal {NP}$NP-hardness. We then combine Lyapunov optimization and game theory to propose a novel Latency-Aware Online (LAO) approach for solving this OEDD problem over time in a distributed manner with provable performance guarantees. The evaluation of LAO based on a real-world dataset demonstrates that it can help app vendors formulate cost-effective edge data distribution strategies in an online manner."",""1558-2183"","""",""10.1109/TPDS.2022.3185250"",""Australian Research Council Discovery(grant numbers:DP200102491)"; Laureate Fellowship(grant numbers:FL190100035);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9804351"",""Data distribution";edge cache system;online algorithm;"optimization"",""Servers";Costs;Videos;Optimization;Data communication;Australia;"Cloud computing"","""",""3"","""",""35"",""IEEE"",""22 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"FRuDA: Framework for Distributed Adversarial Domain Adaptation,""S. Gan"; A. Mathur; A. Isopoussu; F. Kawsar; N. Berthouze;" N. D. Lane"",""ETH Zurich, Zrich, Switzerland"; Nokia Bell Labs, Cambridge, U.K.; Invenia Labs, Cambridge, U.K.; Nokia Bell Labs, Cambridge, U.K.; University College London, London, U.K.;" University of Cambridge, Cambridge, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3153"",""3164"",""Breakthroughs in unsupervised domain adaptation (uDA) can help in adapting models from a label-rich source domain to unlabeled target domains. Despite these advancements, there is a lack of research on how uDA algorithms, particularly those based on adversarial learning, can work in distributed settings. In real-world applications, target domains are often distributed across thousands of devices, and existing adversarial uDA algorithms – which are centralized in nature – cannot be applied in these settings. To solve this important problem, we introduce FRuDA: an end-to-end framework for distributed adversarial uDA. Through a careful analysis of the uDA literature, we identify the design goals for a distributed uDA system and propose two novel algorithms to increase adaptation accuracy and training efficiency of adversarial uDA in distributed settings. Our evaluation of FRuDA with five image and speech datasets show that it can boost target domain accuracy by up to 50% and improve the training efficiency of adversarial uDA by at least $11\times$11×."",""1558-2183"","""",""10.1109/TPDS.2021.3136673"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9656647"",""Distributed domain adaptation";domain shift;"adversarial learning"",""Training";Feature extraction;Adaptation models;Costs;Electronics packaging;Data models;"Classification algorithms"","""","""","""",""46"",""IEEE"",""20 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"funcX: Federated Function as a Service for Science,""Z. Li"; R. Chard; Y. Babuji; B. Galewsky; T. J. Skluzacek; K. Nagaitsev; A. Woodard; B. Blaiszik; J. Bryan; D. S. Katz; I. Foster;" K. Chard"",""Department of Computer Science and Engineering, Research Institute of Trustworthy Autonomous Systems, Southern University of Science and Technology, Shenzhen, China"; Argonne National Laboratory, Lemont, IL, USA; University of Chicago, Chicago, IL, USA; NCSA, University of Illinois, Urbana, IL, USA; University of Chicago, Chicago, IL, USA; Northwestern University, Evanston, IL, USA; University of Chicago, Chicago, IL, USA; University of Chicago, Chicago, IL, USA; University of Chicago, Chicago, IL, USA; NCSA, CS, ECE, and the iSchool, University of Illinois, Urbana, IL, USA; University of Chicago, Chicago, IL, USA;" University of Chicago, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Oct 2022"",""2022"",""33"",""12"",""4948"",""4963"",""funcX is a distributed function as a service (FaaS) platform that enables flexible, scalable, and high performance remote function execution. Unlike centralized FaaS systems, funcX decouples the cloud-hosted management functionality from the edge-hosted execution functionality. funcX's endpoint software can be deployed, by users or administrators, on arbitrary laptops, clouds, clusters, and supercomputers, in effect turning them into function serving systems. funcX's cloud-hosted service provides a single location for registering, sharing, and managing both functions and endpoints. It allows for transparent, secure, and reliable function execution across the federated ecosystem of endpoints—enabling users to route functions to endpoints based on specific needs. funcX uses containers (e.g., Docker, Singularity, and Shifter) to provide common execution environments across endpoints. funcX implements various container management strategies to execute functions with high performance and efficiency on diverse funcX endpoints. funcX also integrates with an in-memory data store and Globus for managing data that may span endpoints. We motivate the need for funcX, present our prototype design and implementation, and demonstrate, via experiments on two supercomputers, that funcX can scale to more than 130000 concurrent workers. We show that funcX's container warming-aware routing algorithm can reduce the completion time for 3,000 functions by up to 61% compared to a randomized algorithm and the in-memory data store can speed up data transfers by up to 3x compared to a shared file system."",""1558-2183"","""",""10.1109/TPDS.2022.3208767"",""National Science Foundation(grant numbers:2004894,2004932)"; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9899739"",""Function-as-a-service";cyberinfrastructure;"distributed computing"",""Containers";Computational modeling;Task analysis;Registers;Python;Cloud computing;"Supercomputers"","""",""3"","""",""82"",""IEEE"",""22 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"G-SLIDE: A GPU-Based Sub-Linear Deep Learning Engine via LSH Sparsification,""Z. Pan"; F. Zhang; H. Li; C. Zhang; X. Du;" D. Deng"",""Key Laboratory of Data Engineering and Knowledge Engineering (MOE), Beijing, China"; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), Beijing, China; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), Beijing, China;" Computer Science Department, Rutgers University, New Brunswick, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3015"",""3027"",""Deep learning has been one of the trendiest research topics. However, as data quantities rise exponentially, training large neural networks can become prohibitively expensive with billions of parameters. Fortunately, recent research has discovered that not all of the computations in traditional network training are necessary. By selectively sparsifying the majority of the neurons during training, we can still obtain acceptable accuracy. SLIDE, a C++ OpenMP-based sub-linear deep learning engine, has been developed in this situation. SLIDE uses the algorithm of locality sensitive hashing (LSH) to query neurons with high activation in sub-linear time. It achieves a remarkable speedup in training large fully-connected networks by making use of the network sparsity as well as multi-core parallelism. However, SLIDE is limited to CPUs, ignoring the popular GPU devices with greater parallel potential and computational capability. In this article, we propose G-SLIDE, a GPU-based sub-linear deep learning engine, which combines the benefits of SLIDE’s adaptive sparsification algorithms with GPUs’ high performance. The main challenges in developing G-SLIDE are efficiently using LSH to sparsify networks and training the special sparse neural networks on the GPU. To address these challenges, we propose several novel solutions, such as specific data formats and appropriate workload partitioning for threads to fully utilize the GPU resources. We evaluate G-SLIDE on two extremely sparse datasets with a 2080 Ti GPU, and the results demonstrate that for the time of one training epoch, G-SLIDE can achieve more than 16.4× speedup over SLIDE on a 32-core/64-thread CPU. Furthermore, on the same platform, G-SLIDE can earn an average of 16.2× speedup over TensorFlow-GPU and 30.8× speedup over TensorFlow-CPU."",""1558-2183"","""",""10.1109/TPDS.2021.3132493"",""National Key Research and Development Program of China(grant numbers:2018YFB1004401)"; National Natural Science Foundation of China(grant numbers:61732014,62172419,61802412); GHfund A(grant numbers:20210701); Alibaba Innovative Research;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635657"",""GPU";machine learning system;adaptive sparsity;sparse neural network;"LSH"",""Graphics processing units";Training;Deep learning;Neurons;Biological neural networks;Engines;"Message systems"","""",""1"","""",""51"",""IEEE"",""3 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Gaviss : Boosting the Performance of GPU-Accelerated NFV Systems via Data Sharing,""L. Guo"; K. Zhang;" X. S. Wang"",""School of Computer Science and Technology, Fudan University, Shanghai, China"; School of Computer Science and Technology, Fudan University, Shanghai, China;" School of Computer Science and Technology, Fudan University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4472"",""4483"",""GPUs have demonstrated the capability of significantly improving the performance of network functions (NF). In an Network Function Virtualization (NFV) system, multiple NFs form a service chain to provide services. However, NFs in state-of-the-art GPU-accelerated NFV systems still utilize a GPU independently where each NF needs to transfer data to the GPU memory for acceleration. As a result, a packet might be transferred into the GPU memory by each NF when it passes through the service chain. We find these expensive and repetitive transfers are the main factor that limits the overall performance of an NFV system. We propose Gaviss, a GPU-accelerated NFV system with effective data sharing. By sharing packets in the GPU memory among network functions, a packet needs to be transferred to the GPU only once, eliminating the performance overhead caused by repetitive transfers. Extensive experimental results show that Gaviss can improve the overall throughput by 2.6-13.2× and reduce the latency by up to 37.9%, when compared with state-of-the-art approaches. Moreover, Gaviss also demonstrates up to 2.5× higher price-performance ratio than CPU-based implementations, making GPUs competitive for building NFV systems."",""1558-2183"","""",""10.1109/TPDS.2022.3193368"",""VMware(grant numbers:10.13039/100016682)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9839394"",""Communications technology";computer networks;"network function virtualization"",""Graphics processing units";Data transfer;Noise measurement;Synchronization;Kernel;Throughput;"Logic gates"","""",""2"","""",""37"",""IEEE"",""25 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GIRAF: General Purpose In-Storage Resistive Associative Framework,""L. Yavits"; R. Kaplan;" R. Ginosar"",""Department of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel"; Department of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel;" Department of Electrical Engineering, Technion-Israel Institute of Technology, Haifa, Israel"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Aug 2021"",""2022"",""33"",""2"",""276"",""287"",""GIRAF is a General purpose In-storage Resistive Associative Framework based on resistive content addressable memory (RCAM), which functions simultaneously as a storage and a massively parallel associative processor. GIRAF alleviates the bandwidth wall by connecting every memory bit to processing transistors and keeping computing inside the storage arrays, thus implementing deep in-data, rather than near-data, processing. We show that GIRAF outperformed a reference computer architecture with a bandwidth-limited external storage access on a variety of data-intensive workloads. The performance of GIRAF Dot Product and Sparse Matrix-Vector multiplication exceeds the attainable performance of a reference architecture by 1200 × and 130 ×, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3065448"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376647"",""Memristors";processing in memory;processing in storage;resistive content addressable memory;"associative processing"",""Computer architecture";Random access memory;Three-dimensional displays;Bandwidth;Arrays;Memory architecture;"Central Processing Unit"","""",""10"","""",""90"",""IEEE"",""11 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GOSH: Task Scheduling Using Deep Surrogate Models in Fog Computing Environments,""S. Tuli"; G. Casale;" N. R. Jennings"",""Department of Computing, Imperial College London, London, U.K."; Department of Computing, Imperial College London, London, U.K.;" Department of Computing, Imperial College London, London, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2821"",""2833"",""Recently, intelligent scheduling approaches using surrogate models have been proposed to efficiently allocate volatile tasks in heterogeneous fog environments. Advances like deterministic surrogate models, deep neural networks (DNN) and gradient-based optimization allow low energy consumption and response times to be reached. However, deterministic surrogate models, which estimate objective values for optimization, do not consider the uncertainties in the distribution of the Quality of Service (QoS) objective function that can lead to high Service Level Agreement (SLA) violation rates. Moreover, the brittle nature of DNN training and the limited exploration with low agility in gradient-based optimization prevent such models from reaching minimal energy or response times. To overcome these difficulties, we present a novel scheduler that we call GOSH for Gradient Based Optimization using Second Order derivatives and Heteroscedastic Deep Surrogate Models. GOSH uses a second-order gradient based optimization approach to obtain better QoS and reduce the number of iterations to converge to a scheduling decision, subsequently lowering the scheduling time. Instead of a vanilla DNN, GOSH uses a Natural Parameter Network (NPN) to approximate objective scores. Further, a Lower Confidence Bound (LCB) optimization approach allows GOSH to find an optimal trade-off between greedy minimization of the mean latency and uncertainty reduction by employing error-based exploration. Thus, GOSH and its co-simulation based extension GOSH*, can adapt quickly and reach better objective scores than baseline methods. We show that GOSH* reaches better objective scores than GOSH, but it is suitable only for high resource availability settings, whereas GOSH is apt for limited resource settings. Real system experiments for both GOSH and GOSH* show significant improvements against the state-of-the-art in terms of energy consumption, response time and SLA violations by up to 18, 27 and 82 percent, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3136672"",""Imperial College London";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9656655"",""DL for PDC";fog computing;scheduling;heteroscedastic models;lower confidence bound;QoS optimization;"second-order optimization"",""Quality of service";Adaptation models;Optimization;Task analysis;Uncertainty;Computational modeling;"Time factors"","""",""13"","""",""56"",""IEEE"",""20 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"GraphOpt: Constrained-Optimization-Based Parallelization of Irregular Graphs,""N. Shah"; W. Meert;" M. Verhelst"",""Department of Electrical Engineering - MICAS, KU Leuven, Leuven, Belgium"; Department of Computer Science - DTAI, KU Leuven, Leuven, Belgium;" Department of Electrical Engineering - MICAS, KU Leuven, Leuven, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2022"",""2022"",""33"",""12"",""3321"",""3332"",""Sparse, irregular graphs show up in various applications like linear algebra, machine learning, engineering simulations, robotic control, etc. These graphs have a high degree of parallelism, but their execution on parallel threads of modern platforms remains challenging due to the irregular data dependencies. The execution performance can be improved by efficiently partitioning the graphs such that the communication and thread synchronization overheads are minimized without hurting the utilization of the threads. To achieve this, this article proposes GraphOpt, a tool that models the graph parallelization as a constrained optimization problem and uses the open Google OR-Tools solver to find good partitions. Several scalability techniques are developed to handle large real-world graphs with millions of nodes and edges. Extensive experiments are performed on the graphs of sparse matrix triangular solves (linear algebra) and sum-product networks (machine learning), respectively, showing a mean speedup of 2.0× and 1.8× over previous state-of-the-art libraries, demonstrating the effectiveness of the constrained-optimization-based graph parallelization."",""1558-2183"","""",""10.1109/TPDS.2022.3151194"",""H2020 European Research Council(grant numbers:ERC-2016-STG-715037)"; Intel; Huawei;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9713729"",""Graph parallelization";partitioning;constrained optimization;sparse matrix triangular solves;"CPU multithreading"",""Parallel processing";Task analysis;Instruction sets;Synchronization;Hardware;Optimization;"Scalability"","""",""2"","""",""48"",""IEEE"",""14 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"gSoFa: Scalable Sparse Symbolic LU Factorization on GPUs,""A. Gaihre"; X. S. Li;" H. Liu"",""Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA"; Lawrence Berkeley National Laboratory, Berkeley, CA, USA;" Department of Electrical and Computer Engineering, Stevens Institute of Technology, Hoboken, NJ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Oct 2021"",""2022"",""33"",""4"",""1015"",""1026"",""Decomposing a matrix $\mathbf {A}$A into a lower matrix $\mathbf {L}$L and an upper matrix $\mathbf {U}$U, which is also known as LU decomposition, is an essential operation in numerical linear algebra. For a sparse matrix, LU decomposition often introduces more nonzero entries in the $\mathbf {L}$L and $\mathbf {U}$U factors than in the original matrix. A symbolic factorization step is needed to identify the nonzero structures of $\mathbf {L}$L and $\mathbf {U}$U matrices. Attracted by the enormous potentials of the Graphics Processing Units (GPUs), an array of efforts have surged to deploy various LU factorization steps except for the symbolic factorization, to the best of our knowledge, on GPUs. This article introduces gSoFa, the first GPU-based symbolic factorization design with the following three optimizations to enable scalable LU symbolic factorization for nonsymmetric pattern sparse matrices on GPUs. First, we introduce a novel fine-grained parallel symbolic factorization algorithm that is well suited for the Single Instruction Multiple Thread (SIMT) architecture of GPUs. Second, we tailor supernode detection into a SIMT friendly process and strive to balance the workload, minimize the communication and saturate the GPU computing resources during supernode detection. Third, we introduce a three-pronged optimization to reduce the excessive space consumption problem faced by multi-source concurrent symbolic factorization. Taken together, gSoFa achieves up to 31× speedup from 1 to 44 Summit nodes (6 to 264 GPUs) and outperforms the state-of-the-art CPU project, on average, by 5×. Notably, gSoFa also achieves up to 47 percent of the peak memory throughput of a V100 GPU in the Summit Supercomputer."",""1558-2183"","""",""10.1109/TPDS.2021.3090316"",""National Science Foundation(grant numbers:2000722)"; CAREER(grant numbers:2046102); Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459541"",""Sparse linear algebra";sparse linear solvers;LU decomposition;"static symbolic factorization on GPU"",""Sparse matrices";Matrix decomposition;Graphics processing units;Memory management;Data structures;Parallel processing;"Image edge detection"","""",""2"","""",""35"",""IEEE"",""17 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Hamiltonian Paths of $k$k-ary $n$n-cubes Avoiding Faulty Links and Passing Through Prescribed Linear Forests,""Y. Yang";" L. Zhang"",""School of Mathematics and Information Science, Henan Normal University, Xinxiang, Henan, China";" School of Mathematics and Information Science, Henan Normal University, Xinxiang, Henan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""24 Nov 2021"",""2022"",""33"",""7"",""1752"",""1760"",""The $k$k-ary $n$n-cube $Q_n^k$Qnk is one of the most attractive interconnection networks for parallel and distributed systems. Let $F$F be a set of faulty links in $Q_n^k$Qnk and let $L$L be a linear forest in $Q_n^k-F$Qnk-F such that $|E(L)|+|F|\leq 2n-3$|E(L)|+|F|≤2n-3. For any two distinct nodes $u$u and $v$v of $Q_n^k$Qnk with $n\geq 2$n≥2 and odd $k\geq 3$k≥3, we prove that $Q_n^k-F$Qnk-F admits a Hamiltonian path between $u$u and $v$v passing through $L$L if and only if none of the paths in $L$L has $u$u or $v$v as internal nodes or both of them as end-nodes. The upper bound $2n-3$2n-3 on $|E(L)|+|F|$|E(L)|+|F| is optimal in the worst case. The main results in this paper generalized some known results."",""1558-2183"","""",""10.1109/TPDS.2021.3126254"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609537"",""Interconnection networks"; $k$   k    -ary  $n$   n    -cubes;fault tolerance;prescribed linear forests;"hamiltonian paths"",""Forestry";Fault tolerant systems;Fault tolerance;Program processors;Routing;Upper bound;"Information science"","""",""12"","""",""28"",""IEEE"",""9 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Harnessing the Potential of Function-Reuse in Multimedia Cloud Systems,""C. Denninnart";" M. A. Salehi"",""Center of Advanced Computer Study, University of Louisiana at Lafayette, Lafayette, LA, USA";" Center of Advanced Computer Study, University of Louisiana at Lafayette, Lafayette, LA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2021"",""2022"",""33"",""3"",""617"",""629"",""Cloud-based computing systems can get oversubscribed due to the budget constraints of their users or limitations in certain resource types. The oversubscription can, in turn, degrade the users perceived Quality of Service (QoS). The approach we investigate to mitigate both the oversubscription and the incurred cost is based on smart reusing of the computation needed to process the service requests (i.e., tasks). We propose a reusing paradigm for the tasks that are waiting for execution. This paradigm can be particularly impactful in serverless platforms where multiple users can request similar services simultaneously. Our motivation is a multimedia streaming engine that processes the media segments in an on-demand manner. We propose a mechanism to identify various types of “mergeable” tasks and aggregate them to improve the QoS and mitigate the incurred cost. We develop novel approaches to determine when and how to perform task aggregation such that the QoS of other tasks is not affected. Evaluation results show that the proposed mechanism can improve the QoS by significantly reducing the percentage of tasks missing their deadlines and reduce the overall time (and subsequently the incurred cost) of utilizing cloud services by more than 9 percent."",""1558-2183"","""",""10.1109/TPDS.2021.3097911"",""National Science Foundation(grant numbers:CNS-2007209,CNS-2047144)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9490337"",""Task aggregation";over-subscription;serverless computing;cloud computing;"video stream processing"",""Streaming media";Task analysis;Cloud computing;Containers;Engines;Delays;"Admission control"","""",""4"","""",""41"",""IEEE"",""19 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"HEROv2: Full-Stack Open-Source Research Platform for Heterogeneous Computing,""A. Kurth"; B. Forsberg;" L. Benini"",""Integrated Systems Laboratory, Zurich, Switzerland"; D-ITET, Eidgenossische Technische Hochschule Zurich Departement Informationstechnologie und Elektrotechnik, Zurich, Switzerland;" DEIS, University of Bologna, Bologna, BO, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4368"",""4382"",""Heterogeneous computers integrate general-purpose host processors with domain-specific accelerators to combine versatility with efficiency and high performance. To realize the full potential of heterogeneous computers, however, many hardware and software design challenges have to be overcome. While architectural and system simulators can be used to analyze heterogeneous computers, they are faced with unavoidable compromises between simulation speed and performance modeling accuracy. In this work we present HEROv2, an FPGA-based research platform that enables accurate and fast exploration of heterogeneous computers consisting of accelerators based on clusters of 32-bit RISC-V cores and an application-class 64-bit ARMv8 or RV64 host processor. HEROv2 allows to seamlessly share data between 64-bit host s and 32-bit accelerators and comes with a fully open-source on-chip network, a unified heterogeneous programming interface, and a mixed-data-model, mixed-ISA heterogeneous compiler based on LLVM. We evaluate HEROv2 in four case studies from the application level over toolchain and system architecture down to accelerator microarchitecture. We demonstrate how HEROv2 enables effective research and development on the full stack of heterogeneous computing. For instance, the compiler can tile loops and infer data transfers to and from the accelerators, which leads to a speedup of up to 4.4 × compared to the original program and in most cases is only 15% slower than a handwritten implementation, which requires 2.6 × more code."",""1558-2183"","""",""10.1109/TPDS.2022.3189390"",""FRACTAL(grant numbers:877056)"; Croatian-Swiss Research Programme; Heterogeneous Computing Systems with Customized Accelerators(grant numbers:180625);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9822980"",""Heterogeneous (Hybrid) systems";parallel processors;shared memory;hardware/software interfaces;"system architectures"",""Hardware";Program processors;Codes;Computers;System-on-chip;Programming;"Engines"","""",""3"","""",""72"",""IEEE"",""8 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Heterogeneous Multi-Agent System for Brain-Computer Interaction in Routing and Forwarding With Memristive Neuron Networks,""Y. Zhou"; W. Chen; L. Li; L. Gong;" T. Zhang"",""College of Communication Engineering, Jilin University, Changchun, Jilin, China"; College of Communication Engineering, Jilin University, Changchun, Jilin, China; College of Communication Engineering, Jilin University, Changchun, Jilin, China; College of Communication Engineering, Jilin University, Changchun, Jilin, China;" College of Communication Engineering, Jilin University, Changchun, Jilin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3233"",""3248"",""In this research, we aimed to design a novel brain-computer interaction (BCI) approach in routing and forwarding mechanisms to realize a networking multi-modal and multi-brain linked control. In recent years, the field programmable gate array (FPGA) has become a popular choice to construct a heterogeneous network for routing and forwarding processes due to its ability to complete a high performance computation in single node. Conventional BCI mode focus on capturing the dynamic EEG signals from multiple electrode channels and this concept neglects the existing complexity and diversity of connecting information between two brain regions. To this moment, we build a heterogeneous multi-agent system (HMAS) architecture to simulate the memristive neuron networks (MNN) with a set of logic units for multi-agent in the Internet. Specifically, we first feed the non-linear features into an intelligent router to adjust their routing and forwarding processes. Subsequently, at one-time step, the output of all testing nodes is written into a heterogeneous logic space, which mainly consists of the sub-agent, forward port, and memory memristive neuron. In the FPGA, each programmable cell selectively integrates and stores a huge number of motion information between multiple interacting channels. We can achieve the networking cooperative functions of forward and route through FPGA large-scale computation. According to the extensive experimental results on several data, we validate the effectiveness of the proposed HMAS-BCI architecture to compare the optimization method of the multiple simulations."",""1558-2183"","""",""10.1109/TPDS.2021.3137837"",""Department of Science and Technology of Jilin Province(grant numbers:20190302034GX)"; Domaion Foundation of Equipment Advance Research of 13th Five-year Plan(grant numbers:ZX7Y201902120-06001); China Postdoctoral Science Foundation(grant numbers:2020M670856);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661451"",""Heterogeneous multi-agent system";brain-computer interaction;routing and forwarding mechanism;"MNN"",""Brain modeling";Routing;Computational modeling;Multi-layer neural network;Task analysis;Multi-agent systems;"Flexible printed circuits"","""",""2"","""",""69"",""IEEE"",""23 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Heterogeneous Systolic Array Architecture for Compact CNNs Hardware Accelerators,""R. Xu"; S. Ma; Y. Wang; Y. Guo; D. Li;" Y. Qiao"",""Institute of Microelectronics, National University of Defense Technology, Changsha, Hunan, China"; Science and Technology on Parallel and Distributed Processing Laboratory, National University of Defense Technology, Changsha, Hunan, China; Institute of Microelectronics, National University of Defense Technology, Changsha, Hunan, China; Institute of Microelectronics, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Parallel and Distributed Processing Laboratory, National University of Defense Technology, Changsha, Hunan, China;" Institute of Microelectronics, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2860"",""2871"",""Compact convolutional neural networks have become a hot research topic. However, we find that the systolic array accelerators are extremely inefficient in dealing with compact models, especially when processing depthwise convolutional layers in the neural networks. To make systolic arrays more efficient for compact convolutional neural networks, we propose the heterogeneous systolic array (HeSA) architecture. It introduces heterogeneous processing elements that support multiple dataflows, which can further exploit the reuse data chance of depthwise convolutional layers and without changing the structure of the naÃ¯ve systolic array. By increasing the utilization rate of processing elements in the array, the HeSA improves the performance, throughput, and energy efficiency compared to the standard baseline. In addition, we design the flexible buffer structure for the HeSA. Through configuring it, the HeSA can allocate bandwidth flexibly to maintaining high performance and low communication cost. Based on our evaluation with typical workloads, the HeSA improves the utilization rate of the computing resource in depthwise convolutional layers by 4.5× - 11.2× and acquires 1.6 - 3.1× total performance speedup compared to the standard systolic array architecture. In the large-scale array design, the HeSA can reduce the data traffic by 40% while maintaining the same performance as the scaling-out method. By improving the on-chip data reuse opportunities and reducing data traffic, the HeSA saves over 20% in energy consumption. Meanwhile, the area of the HeSA is basically unchanged compared to the baseline due to its simple design."",""1558-2183"","""",""10.1109/TPDS.2021.3129647"",""NSFC(grant numbers:61802420,62025208,62172430)"; Natural Science Foundation of Hunan Province(grant numbers:2021JJ10052); STIP of Hunan Province(grant numbers:2019RS2027);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623517"",""Hardware accelerator";architecture;convolutional neural network;depthwise separable convolution;"systolic array"",""Arrays";Systolic arrays;Convolution;Computer architecture;Standards;Scalability;"Kernel"","""",""2"","""",""35"",""CCBY"",""22 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"High Performance Evaluation of Helmholtz Potentials Using the Multi-Level Fast Multipole Algorithm,""M. P. Lingg"; S. M. Hughey; B. Shanker;" H. M. Aktulga"",""Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA"; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, USA; Department of Electrical and Computer Engineering, Michigan State University, East Lansing, MI, USA;" Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3651"",""3666"",""Evaluation of pair potentials is critical in a number of areas of physics. The classical $N$N-body problem has its root in evaluating the Laplace potential, and has spawned tree-algorithms, the fast multipole method (FMM), as well as kernel independent approaches. Over the years, FMM for Laplace potential has had a profound impact on a number of disciplines as it has been possible to develop highly scalable parallel versions of these algorithms. This is in stark contrast to parallel algorithms for oscillatory potentials such as the Helmholtz potential. The principal bottlenecks to scalable parallelism are the computation and communication costs of operations necessary to traverse up, across, and down the tree. In this article, we analyze asymptotic costs for both computation and communication in a parallel implementation, and describe techniques to overcome bottlenecks and achieve high performance evaluation of the Helmholtz potential for different distributions of particles. We demonstrate that the resulting implementation has a load balancing effect that significantly reduces the time-to-solution and enhances the scale of problems that can be treated using full wave physics."",""1558-2183"","""",""10.1109/TPDS.2022.3165649"",""National Science Foundation(grant numbers:CCF-1822932)"; National Energy Research Scientific Computing Center; U.S. Department of Energy(grant numbers:DE-AC02-05CH11231); High Performance Computing Center;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9751386"",""Helmholtz equation";multilevel fast multipole method;tree algorithm;electromagnetic interaction;"global interpolation"",""Costs";Parallel algorithms;Electric potential;Mathematical models;Octrees;Machine-to-machine communications;"Kernel"","""","""","""",""60"",""IEEE"",""7 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Highly Accurate Clock Synchronization With Drift Correction for the Controller Area Network,""M. Akpınar"; E. G. Schmidt;" K. W. Schmidt"",""Microelectronics, Guidance, and Electro-Optics, Aselsan Inc., Ankara, Turkey"; Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey;" Department of Electrical and Electronics Engineering, Middle East Technical University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4071"",""4082"",""Modern vehicles, that have to be considered as safety-critical cyber-physical systems, require highly accurate clock synchronization (CS) among their distributed computing devices. Since Controller Area Network (CAN) is the predominant in-vehicle communication bus, it is highly relevant to support CS for CAN. This article proposes an original CS method for distributed in-vehicle networks based on CAN with both offset and drift correction. While offset correction is performed based on timestamps in periodic reference messages (RMs), our new method benefits from the re-synchronization mechanism of the CAN bit timing to apply highly accurate drift correction. Our algorithm does not make any modifications to the CAN protocol but requires the measurement of the phase error from the CAN controller. We derive analytical bounds for the expected clock differences and further validate the practicability of the proposed method by comprehensive experiments. As the main result, our method achieves a clock accuracy below $2\";"\mu$2μs independent of important parameters such as the bit rate, RM period, bus utilization and time-varying clock drifts."",""1558-2183"","""",""10.1109/TPDS.2022.3179316"",""Türkiye Bilimsel ve Teknolojik Araştırma Kurumu(grant numbers:119E277)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785903"",""Controller area network";clock synchronization;offset correction;drift correction;"phase error"",""Silicon";Clocks;Synchronization;Protocols;Real-time systems;Time-frequency analysis;"Hardware"","""",""1"","""",""45"",""IEEE"",""31 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"HiTDL: High-Throughput Deep Learning Inference at the Hybrid Mobile Edge,""J. Wu"; L. Wang; Q. Pei; X. Cui; F. Liu;" T. Yang"",""National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; VU Amsterdam, Amsterdam, The Netherlands; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" Peng Cheng Laboratory, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4499"",""4514"",""Deep neural networks (DNNs) have become a critical component for inference in modern mobile applications, but the efficient provisioning of DNNs is non-trivial. Existing mobile- and server-based approaches compromise either the inference accuracy or latency. Instead, a hybrid approach can reap the benefits of the two by splitting the DNN at an appropriate layer and running the two parts separately on the mobile and the server respectively. Nevertheless, the DNN throughput in the hybrid approach has not been carefully examined, which is particularly important for edge servers where limited compute resources are shared among multiple DNNs. This article presents HiTDL, a runtime framework for managing multiple DNNs provisioned following the hybrid approach at the edge. HiTDL's mission is to improve edge resource efficiency by optimizing the combined throughput of all co-located DNNs, while still guaranteeing their SLAs. To this end, HiTDL first builds comprehensive performance models for DNN inference latency and throughout with respect to multiple factors including resource availability, DNN partition plan, and cross-DNN interference. HiTDL then uses these models to generate a set of candidate partition plans with SLA guarantees for each DNN. Finally, HiTDL makes global throughput-optimal resource allocation decisions by selecting partition plans from the candidate set for each DNN via solving a fairness-aware multiple-choice knapsack problem. Experimental results based on a prototype implementation show that HiTDL improves the overall throughput of the edge by $4.3\times$4.3× compared with the state-of-the-art."",""1558-2183"","""",""10.1109/TPDS.2022.3195664"",""National Natural Science Foundation of China(grant numbers:61761136014,61520106005)"; National Key Research & Development (R&D) Plan(grant numbers:2017YFB1001703); DFG;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847073"",""Deep learning inference";edge computing;resource allocation;"systems for machine learning"",""Throughput";Graphics processing units;Servers;Resource management;Task analysis;Mobile handsets;"Mobile applications"","""",""7"","""",""65"",""IEEE"",""1 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Horus: Interference-Aware and Prediction-Based Scheduling in Deep Learning Systems,""G. Yeung"; D. Borowiec; R. Yang; A. Friday; R. Harper;" P. Garraghan"",""School of Computing & Communications, Lancaster University, Lancaster, U.K"; School of Computing & Communications, Lancaster University, Lancaster, U.K; School of Computing, University of Leeds, Leeds, U.K; School of Computing & Communications, Lancaster University, Lancaster, U.K; School of Computing & Communications, Lancaster University, Lancaster, U.K;" School of Computing & Communications, Lancaster University, Lancaster, U.K"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Jun 2021"",""2022"",""33"",""1"",""88"",""100"",""To accelerate the training of Deep Learning (DL) models, clusters of machines equipped with hardware accelerators such as GPUs are leveraged to reduce execution time. State-of-the-art resource managers are needed to increase GPU utilization and maximize throughput. While co-locating DL jobs on the same GPU has been shown to be effective, this can incur interference causing slowdown. In this article we propose Horus: an interference-aware and prediction-based resource manager for DL systems. Horus proactively predicts GPU utilization of heterogeneous DL jobs extrapolated from the DL model's computation graph features, removing the need for online profiling and isolated reserved GPUs. Through micro-benchmarks and job co-location combinations across heterogeneous GPU hardware, we identify GPU utilization as a general proxy metric to determine good placement decisions, in contrast to current approaches which reserve isolated GPUs to perform online profiling and directly measure GPU utilization for each unique submitted job. Our approach promotes high resource utilization and makespan reduction";" via real-world experimentation and large-scale trace driven simulation, we demonstrate that Horus outperforms other DL resource managers by up to 61.5 percent for GPU resource utilization, 23.7-30.7 percent for makespan reduction and 68.3 percent in job wait time reduction."",""1558-2183"","""",""10.1109/TPDS.2021.3079202"",""Engineering and Physical Sciences Research Council(grant numbers:EP/P031617/1)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9428512"",""Distributed systems";deep learning;interference;GPU utilization;cloud computing;"workload prediction"",""Graphics processing units";Interference;Kernel;Predictive models;Computational modeling;Production;"Load modeling"","""",""25"","""",""67"",""CCBY"",""11 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"HSA-Net: Hidden-State-Aware Networks for High-Precision QoS Prediction,""Z. Wang"; X. Zhang; M. Yan; L. Xu;" D. Yang"",""Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, Chongqing, China"; Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, Chongqing, China;" Key Laboratory of Dependable Service Computing in Cyber Physical Society (Chongqing University), Ministry of Education, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Oct 2021"",""2022"",""33"",""6"",""1421"",""1435"",""The high-precision QoS (quality of service) prediction is based on the comprehensive perception of state information of users and services. However, the current QoS prediction approaches have limited accuracy, for most state information of users and services (i.e., network speed, latency, network type, and more) are hidden due to privacy protection. Therefore, this article proposes a hidden-state-aware network (HSA-Net) that includes three steps called hidden state initialization, hidden state perception, and QoS prediction. A hidden state initialization approach is developed first based on the latent dirichlet allocation (LDA). After that, a hidden-state perception approach is proposed to abstract the initialized hidden state by fusing the known information (e.g., service ID and user location). The perception approach consists of four hidden-state perception (HSP) modes (i.e., known mode, object mode, hybrid mode and overall mode) implemented to generate explainable and fused features through four adaptive convolutional kernels. Finally, the relationship between the fused features and the QoS is discovered through a fully connected network to complete the high-precision QoS prediction process. The proposed HSA-Net is evaluated on two real-world datasets. According to the results, the HSA-Net's mean absolute error (MAE) index reduced by 3.67% and 28.84%, whereas the root mean squared error (RMSE) index decreased by 3.07% and 7.14% compared with ten baselines on average in the two datasets."",""1558-2183"","""",""10.1109/TPDS.2021.3111810"",""National Key Research and Development Program of China(grant numbers:2018YFB2101200)"; Special Funds for the Central Government(grant numbers:YDZX20195000004725); National Natural Science Foundation of China(grant numbers:61772093); Key Project of Technology Innovation and Application Development of Chongqing(grant numbers:cstc2019jscx-mbdxX0020); Chongqing Science and Technology Plan Project(grant numbers:cstc2018jszx-cyztzxX0037); Chongqing Technology Innovation and Application Development Project(grant numbers:cstc2019jszx- mbdxX0064);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9535236"",""Service recommendation";convolutional neural network;QoS prediction;hidden states;"feature fusion"",""Quality of service";Time factors;Deep learning;Predictive models;Kernel;Collaboration;"Web services"","""",""6"","""",""66"",""IEEE"",""10 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Hydra: A Decentralized File System for Persistent Memory and RDMA Networks,""S. Zheng"; J. Wang; D. Xue; J. Shu;" L. Huang"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4192"",""4206"",""Emerging byte-addressable persistent memory (PM) has the potential to disrupt the boundary between memory and storage. Combined with high-speed RDMA networks, distributed PM-based storage systems offer the opportunity to provide huge increases in storage performance by closely coupling PM and RDMA features. However, existing distributed file systems adopt the conventional centralized client-server architecture designed for traditional disks, leading to excessive access latency, limited scalability, and high recovery overhead. In this paper, we propose a fully decentralized PM-based file system, Hydra. By exploiting the performance advantages of local PM, Hydra leverages data access locality to achieve high performance. To accelerate file transmission among Hydra nodes, file metadata and data are decoupled and updated differentially through one-sided RDMA reads. Hydra also batches RDMA requests and classifies RPCs into synchronous and asynchronous types to minimize network overhead. Decentralization enables Hydra to tolerate node failures and achieve load balancing. Experimental results show that Hydra outperforms existing distributed file systems by a large margin, and shows good scalability on multi-threaded and parallel workloads."",""1558-2183"","""",""10.1109/TPDS.2022.3180369"",""Natural Science Foundation of Shanghai(grant numbers:22ZR1435400)"; Shanghai Municipal Science and Technology Major Project(grant numbers:2021SHZDZX0102);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9792291"",""Persistent memory";file system;RDMA;distributed system;"decentralization"",""Servers";Metadata;Scalability;Distributed databases;Random access memory;Tail;"File systems"","""","""","""",""61"",""IEEE"",""9 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"iBalancer: Load-Aware in-Server Flow Scheduling for Sub-Millisecond Tail Latency,""Q. Zhang"; Y. Liu;" T. Liu"",""Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China"; Sino-German Joint Software Institute, School of Computer Science, Beihang University, Beijing, China;" Shandong Provincial Key Laboratory of Computer Networks, Shandong Computer Science Center (National Supercomputer Center in Jinan), Shandong Academy of Sciences, Qilu University of Technology, Jinan, Shandong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Dec 2021"",""2022"",""33"",""8"",""1761"",""1774"",""Achieving microsecond-scale tail latency poses an extreme challenge to the conventional architecture of “NIC-OS-Application” in the face of high concurrent requests. Existing kernel-bypass network systems improve this situation significantly. Still, they cannot achieve load-aware in-server requests distribution, which in turn not only harms resource efficiency but, more importantly, beats the goal of squeezing tail latency. This paper proposes iBalancer, an in-server proactive load balancer for the kernel-bypass system, which aggressively handles NIC-side flow scheduling according to the load of threads on the processor-side. Furthermore, we propose a novel metric, “polling time interval (PTI),” to quantify the load of worker threads, which not only indicates utilization of the core bound to the worker thread but also reflects the differences in the processing time of different flows. By scheduling flows according to the metric PTI, iBalancer tends to average the queueing latencies of different flows, such as Set & Get operations for an in-memory key-value store. In addition, by decoupling flow scheduling from packet steering, iBalancer achieves a tail latency aware flow-to-core binding and preserves hardware-based request distribution among cores. The proposed system is evaluated and compared to mTCP and Shenango using two representative microsecond-scale network applications: Memcached KVS and a real-time deep-learning-based financial fraud identification application. Experimental results show that iBalancer can process up to 4.75$ \times $× and 1.55$ \times \ $× higher load over mTCP and Shenango under 500μs 99th percentile tail latency limit on Memcached. For the financial fraud identification application, iBalancer is able to process 4.56$ \times $× and 1.16$ \times $× higher load than mTCP and Shenango considering 900μs tail latency."",""1558-2183"","""",""10.1109/TPDS.2021.3120021"",""National Key Research and Development Program of China(grant numbers:2016YFB0200100)"; National Natural Science Foundation of China(grant numbers:61732002,62002186);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573311"",""Quality of service";parallel systems;"performance measurements"",""Instruction sets";Servers;Dispatching;Measurement;Load management;Task analysis;"Network systems"","""","""","""",""56"",""IEEE"",""14 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving Cache Utilization of Nested Parallel Programs by Almost Deterministic Work Stealing,""S. Shiina";" K. Taura"",""Department of Information and Communication Engineering, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, Japan";" Department of Information and Communication Engineering, Graduate School of Information Science and Technology, University of Tokyo, Bunkyo-ku, Tokyo, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4530"",""4546"",""Nested (fork-join) parallelism eases parallel programming by enabling high-level expression of parallelism and leaving the mapping between parallel tasks and hardware to the runtime scheduler. A challenge in dynamic scheduling of nested parallelism is how to exploit data locality, which has become more demanding in the deep cache hierarchies of modern processors with a large number of cores. This paper introduces almost deterministic work stealing (ADWS), which efficiently exploits data locality by deterministically planning a cache-hierarchy-aware schedule, while allowing a little scheduling variety to facilitate dynamic load balancing. Furthermore, we propose an extension of our prior work on ADWS to achieve better shared cache utilization. The improved version of the scheduler is called multi-level ADWS. The idea is that only part of a computation whose working set size is small enough to fit into a shared cache is scheduled by ADWS within the cache recursively, thus avoiding excessive capacity misses. Our evaluation on a benchmark of parallel decision tree construction demonstrated that multi-level ADWS outperformed the conventional random work stealing of Cilk Plus by 61% and it showed a 40% performance improvement over the previous ADWS design."",""1558-2183"","""",""10.1109/TPDS.2022.3196192"",""JSPS KAKENHI(grant numbers:21J22305)"; New Energy and Industrial Technology Development Organization(grant numbers:JPNP16007);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9848990"",""Dynamic load balancing";locality;nested parallelism;task parallelism;task scheduling;"work stealing"",""Task analysis";Parallel processing;Dynamic scheduling;Decision trees;Program processors;Processor scheduling;"Runtime"","""","""","""",""57"",""CCBY"",""3 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving Fairness for SSD Devices through DRAM Over-Provisioning Cache Management,""R. Liu"; Z. Tan; L. Long; Y. Wu; Y. Tan;" D. Liu"",""College of Computer Science, Chongqing University of Posts and Telecommunications, Chongqing, China"; College of Computer Science, Chongqing University of Posts and Telecommunications, Chongqing, China; College of Computer Science, Chongqing University of Posts and Telecommunications, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China; College of Computer Science, Chongqing University, Chongqing, China;" College of Computer Science, Chongqing University, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Mar 2022"",""2022"",""33"",""10"",""2444"",""2454"",""Modern NVMe SSDs have been widely deployed in multi-tenant cloud computing environments or multi-programming systems. When multiple applications concurrently access one SSD hardware, unfairness within the shared SSD will slow down the application significantly and lead to a violation of service level objectives. However, traditional data cache management within SSDs mainly focuses on improving cache hit ratio, which causes data cache contention and sacrifices fairness among multiple applications. In this paper, we propose a DRAM-based Over-Provisioning (OP) cache management mechanism, named Justitia, to reduce data cache contention and improve fairness for modern SSDs. Justitia consists of two stages including Static-OP stage and Dynamic-OP stage. Through the novel OP mechanism in the two stages, Justitia reduces the max slowdown by $4.5\times$4.5× on average. At the same time, Justitia increases fairness by $20.6\times$20.6× and buffer hit ratio by $19.6\%$19.6% averagely, compared with the traditional shared mechanism."",""1558-2183"","""",""10.1109/TPDS.2022.3143295"",""National Natural Science Foundation of China(grant numbers:61902045,62072059)"; Chongqing High-Tech Research Key Program(grant numbers:cstc2021jcyj-msxmX0981);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9682597"",""Solid state drives (SSDs)";multiple applications;fairness;"data cache management"",""Ash";Random access memory;Nonvolatile memory;Hardware;Writing;Time factors;"Organizations"","""",""4"","""",""33"",""IEEE"",""14 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Improving Federated Learning With Quality-Aware User Incentive and Auto-Weighted Model Aggregation,""Y. Deng"; F. Lyu; J. Ren; Y. -C. Chen; P. Yang; Y. Zhou;" Y. Zhang"",""Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China"; School of Computer Science and Engineering, Central South University, Changsha, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4515"",""4529"",""Federated learning enables distributed model training over various computing nodes, e.g., mobile devices, where instead of sharing raw user data, computing nodes can solely commit model updates without compromising data privacy. The quality of federated learning relies on the model updates contributed by computing nodes training with their local data. However, with various factors (e.g., training data size, mislabeled data samples, skewed data distributions), the model update qualities of computing nodes can vary dramatically, while inclusively aggregating low-quality model updates can deteriorate the global model quality. To achieve efficient federated learning, in this paper, we propose a novel framework named FAIR, i.e., Federated leArning with qualIty awaReness. Particularly, FAIR integrates three major components: 1) learning quality estimation: we adopt the model aggregation weight (learned in the third component) to reversely quantify the individual learning quality of nodes in a privacy-preserving manner, and leverage the historical learning records to infer the next-round learning quality"; 2) quality-aware incentive mechanism: within the recruiting budget, we model a reverse auction problem to stimulate the participation of high-quality and low-cost computing nodes, and the method is proved to be truthful, individually rational, and computationally efficient;" and 3) auto-weighted model aggregation: based on the gradient descent method, we devise an auto-weighted model aggregation algorithm to automatically learn the optimal aggregation weights to further enhance the global model quality. Based on real-world datasets and learning tasks, extensive experiments are conducted to demonstrate the efficacy of FAIR."",""1558-2183"","""",""10.1109/TPDS.2022.3195207"",""National Key R&D Program of China(grant numbers:2022YFC2009800)"; National Natural Science Foundation of China(grant numbers:62002389,62001180,62122095,62072472,U19A2067); Guoqiang Institute, Tsinghua University; Young Elite Scientists Sponsorship Program(grant numbers:YESS20200238); Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010137005); Natural Science Foundation of Hunan Province(grant numbers:2020JJ2050,2021JJ20079); Key R&D Program of Hunan Province of China(grant numbers:2022GK2013); Young Talents Plan of Hunan Province of China(grant numbers:2021RC3004);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847055"",""Edge computing";incentive mechanism;learning quality;mobile computing;model aggregation;"federated learning"",""Task analysis";Computational modeling;Collaborative work;Data models;Training;Resource management;"Training data"","""",""8"","""",""48"",""IEEE"",""1 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Improving I/O Performance for Exascale Applications Through Online Data Layout Reorganization,""L. Wan"; A. Huebl; J. Gu; F. Poeschel; A. Gainaru; R. Wang; J. Chen; X. Liang; D. Ganyushin; T. Munson; I. Foster; J. -L. Vay; N. Podhorszki; K. Wu;" S. Klasky"",""Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA"; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Center for Advanced Systems Understanding (CASUS), Görlitz, Germany; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Missouri University of Science and Technology, Rolla, MO, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA;" Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""878"",""890"",""The applications being developed within the U.S. Exascale Computing Project (ECP) to run on imminent Exascale computers will generate scientific results with unprecedented fidelity and record turn-around time. Many of these codes are based on particle-mesh methods and use advanced algorithms, especially dynamic load-balancing and mesh-refinement, to achieve high performance on Exascale machines. Yet, as such algorithms improve parallel application efficiency, they raise new challenges for I/O logic due to their irregular and dynamic data distributions. Thus, while the enormous data rates of Exascale simulations already challenge existing file system write strategies, the need for efficient read and processing of generated data introduces additional constraints on the data layout strategies that can be used when writing data to secondary storage. We review these I/O challenges and introduce two online data layout reorganization approaches for achieving good tradeoffs between read and write performance. We demonstrate the benefits of using these two approaches for the ECP particle-in-cell simulation WarpX, which serves as a motif for a large class of important Exascale applications. We show that by understanding application I/O patterns and carefully designing data layouts we can increase read performance by more than 80 percent."",""1558-2183"","""",""10.1109/TPDS.2021.3100784"",""Exascale Computing(grant numbers:17-SC-20-SC)"; U.S. Department of Energy; National Nuclear Security Administration; Center of Advanced Systems Understanding; Germany's Federal Ministry of Education and Research; Saxon Ministry for Science, Culture and Tourism;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501495"",""Parallel IO";data layout;IO performance;WarpX;"data access optimization"",""Layout";Arrays;Heuristic algorithms;Computational modeling;Performance evaluation;Optimization;"Distributed databases"","""",""10"","""",""38"",""IEEE"",""29 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"Incentive-Aware Autonomous Client Participation in Federated Learning,""M. Hu"; D. Wu; Y. Zhou; X. Chen;" M. Chen"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computing, Faculty of Science and Engineering, Macquarie University, Macquarie Park, NSW, Australia; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Apr 2022"",""2022"",""33"",""10"",""2612"",""2627"",""Federated learning (FL) emerges as a promising paradigm to enable a federation of clients to train a machine learning model in a privacy-preserving manner. Most existing works assumed that the central parameter server (PS) determines the participation of clients implying that clients cannot make autonomous participation decisions. The above assumption is unrealistic because the participation in FL training may incur various cost and clients also have strong desire to be rewarded for participation. To address this problem, we design a novel autonomous client participation scheme to incentivize clients. Specifically, the PS provides a certain reward shared among participating clients for each training round. Clients decide whether to participate each FL training round or not based on their own utilities (i.e., reward minus cost). The process can be modeled as a minority game (MG) with incomplete information and clients end up in the minority side win after each training round because the reward of each participating client may not cover its cost if too many clients participate and vice verse. The challenge of autonomous participation schemes lies in lowering the volatility of participating clients in each round due to the lack of coordination among clients. Through solid analysis, we prove that: 1) The volatility of participating clients in each round is very high under the standard MG scheme. 2) The volatility of participating clients can be reduced significantly under the stochastic MG scheme. 3) A coalition based MG is proposed, which can further reduce the volatility in each round. By conducting extensive experiments in real settings, we demonstrate that the stochastic MG-based scheme outperforms other state-of-the-art algorithms in terms of utility and volatility, and the coalition MG-based client participation scheme can further boost the utility by 39%-48% and reduce the volatility by 51%–100%. Moreover, our algorithms can achieve almost the same model accuracy as that obtained by centralized client participation algorithms."",""1558-2183"","""",""10.1109/TPDS.2022.3148113"",""National Natural Science Foundation of China(grant numbers:62072486,U1911201,U2001209,61972432,62176101)"; Science and Technology Planning Project of Guangdong Province(grant numbers:2021A0505110008); Natural Science Foundation of Guangdong Province(grant numbers:2021A1515011369); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Guangdong Provincial Pearl River Talents Program(grant numbers:2017GC010465); Australian Research Council(grant numbers:DE180100950);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9705080"",""Client participation";federated learning;minority game;"volatility"",""Training";Games;Stochastic processes;Collaborative work;Servers;Costs;"Standards"","""",""6"","""",""51"",""IEEE"",""4 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Increasing the Efficiency of Massively Parallel Sparse Matrix-Matrix Multiplication in First-Principles Calculation on the New-Generation Sunway Supercomputer,""X. Chen"; Y. Gao; H. Shang; F. Li; Z. Xu; X. Liu;" D. Chen"",""National Research Center of Parallel Computer Engineering and Technology, Beijing, China"; National Supercomputer Center in Tianjin, Tianjin, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Research Center of Parallel Computer Engineering and Technology, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Research Center of Parallel Computer Engineering and Technology, Beijing, China;" National Research Center of Parallel Computer Engineering and Technology, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Sep 2022"",""2022"",""33"",""12"",""4752"",""4766"",""The first-principles approach based on density-functional theory (DFT)/density-functional perturbation theory (DFPT) is widely used in calculations of the systems’ ground state energy, response properties (e.g., polarizability, phonon dispersions) and is playing an increasingly important role in chemistry, physics and materials science. For the large-scale calculations, the computation of the density matrix/response density matrix in DFT/DFPT has become the main performance bottleneck. One of the solutions is using the linear scaling method to get the density matrix and response density matrix. Here a massively parallel medium sparse matrix-matrix multiplication algorithm is designed for first-principle calculations and implemented on the new-generation Sunway supercomputer. Experiments show that the proposed method has obvious performance advantages compared to the original parallel version under moderate sparsity. The computing cores scale to 3,900,000 with strong scalability of 77.3$\%$%."",""1558-2183"","""",""10.1109/TPDS.2022.3202518"",""National Natural Science Foundation of China(grant numbers:22003073)"; National Science and Technology Major(grant numbers:2017-I-0004-0004); National Key R&D Program of China(grant numbers:2019YFA0709402);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869723"",""Density matrix";density-functional theory;new-generation sunway supercomputer;"parallel sparse matrix-matrix multiplication"",""Sparse matrices";Purification;Supercomputers;Discrete Fourier transforms;Tensors;Table lookup;"Perturbation methods"","""",""1"","""",""58"",""IEEE"",""29 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Inferring the Dynamics of the State Evolution During Quantum Annealing,""E. Pelofske"; G. Hahn;" H. Djidjev"",""Los Alamos National Laboratory, CCS-3 Information Sciences, Los Alamos, NM, USA"; T.H. Chan School of Public Health, Harvard University, Cambridge, MA, USA;" Los Alamos National Laboratory, CCS-3 Information Sciences, Los Alamos, NM, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Aug 2021"",""2022"",""33"",""2"",""310"",""321"",""To solve an optimization problem using a commercial quantum annealer, one has to represent the problem of interest as an Ising or a quadratic unconstrained binary optimization (QUBO) problem and submit its coefficients to the annealer, which then returns a user-specified number of low-energy solutions. It would be useful to know what happens in the quantum processor during the anneal process so that one could design better algorithms or suggest improvements to the hardware. However, existing quantum annealers are not able to directly extract such information from the processor. Hence, in this article we propose to use advanced features of D-Wave 2000Q to indirectly infer information about the dynamics of the state evolution during the anneal process. Specifically, D-Wave 2000Q allows the user to customize the anneal schedule, that is, the schedule with which the anneal fraction is changed from the start to the end of the anneal. Using this feature, we design a set of modified anneal schedules whose outputs can be used to generate information about the states of the system at user-defined time points during a standard anneal. With this process, called slicing, we obtain approximate distributions of lowest-energy anneal solutions as the anneal time evolves. We use our technique to obtain a variety of insights into the annealer, such as the state evolution during annealing, when individual bits in an evolving solution flip during the anneal process and when they stabilize, and we introduce a technique to estimate the freeze-out point of both the system as well as of individual qubits."",""1558-2183"","""",""10.1109/TPDS.2020.3044846"",""U.S. Department of Energy(grant numbers:20190065DR,20180267ER)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9293405"",""D-Wave 2000Q";quantum annealing;quenching,slicing;state evolution;qubits;"freezeout point"",""Annealing";Qubit;Schedules;Standards;Visualization;Optimization;"NP-hard problem"","""",""3"","""",""28"",""IEEE"",""14 Dec 2020"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Jdebug: A Fast, Non-Intrusive and Scalable Fault Locating Tool for Ten-Million-Scale Parallel Applications,""D. Peng"; Y. Feng; Y. Liu; X. Liu; W. Xue; D. Chen; J. Song;" Z. Chen"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; National Supercomputer Center in Wuxi, Wuxi, China; National Research Centre of Parallel Computer Engineering and Technology, Beijing, China; National Research Centre of Parallel Computer Engineering and Technology, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; National Supercomputer Center in Wuxi, Wuxi, China;" National Research Centre of Parallel Computer Engineering and Technology, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Jun 2022"",""2022"",""33"",""12"",""3491"",""3504"",""This article presents Jdebug, a fast, non-intrusive and scalable fault locating tool for extreme-scale parallel applications. Large-scale debugging has drawn more attention with the increasing scale of supercomputers and applications. To eliminate program intrusion caused by traditional instrumentation or interception during debugging information acquisition, we introduce the out-of-band management into large-scale debugging. We propose a rapid information gathering scheme that separates user and debugging traffic to solve scalability problem and to eliminate program interference during merging data. Observations of Program Counters (PC) and performance characteristics in suspended applications find abnormalities and help locate abnormal threads caused by software errors or hardware failures effectively. Evaluation shows that Jdebug collects PCs of over 20 million cores on the new Sunway supercomputer within 1.97 seconds, and can locate the abnormal threads in 1.4 seconds with an accuracy of 92.5%. In the running test of three fundamental benchmarks (HPL, HPCG, Graph500) and seventeen real-world applications, Jdebug quickly and accurately locates abnormal threads to help find scalability errors and hardware failures including memory access failures, communication failures, and execution component failures, which validates its effectiveness."",""1558-2183"","""",""10.1109/TPDS.2022.3157690"",""National Key Research and Development Program of China(grant numbers:2017YFB0202001)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730097"",""Distributed debugging";software engineering;software/software engineering;"testing and debugging"",""Debugging";Servers;Supercomputers;Hardware;Maintenance engineering;Instruction sets;"Task analysis"","""",""1"","""",""52"",""IEEE"",""8 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Joint Application Placement and Request Routing Optimization for Dynamic Edge Computing Service Management,""R. Li"; Z. Zhou; X. Zhang;" X. Chen"",""School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China;" School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""2 Sep 2022"",""2022"",""33"",""12"",""4581"",""4596"",""As mobile edge computing (MEC) hosting applications at the network edge with limited capacities, service providers are facing the new challenge of how to make full use of the scarce edge resources to maximize the system performance. Accommodating this challenge requires careful application placement and request routing to coordinate diverse MEC nodes. However, frequent application re-placement would greatly increase the system reconfiguration cost, indicating a performance-cost trade-off. In response, in this paper, we study the problem of joint optimization on application placement and request routing to maximize the system performance, under a long-term budget of the application reconfiguration cost. Solving this problem is non-trivial since the long-term budget is coupled with the future system states (e.g., user request arrivals) that are typically unpredictable. To address this challenge, we first advocate an approximated dynamic optimization framework to decompose the long-term optimization problem into a series of one-shot problems which do not require the future system states. Moreover, since the decomposed problem is a mixed integer linear program (MILP) which is proven to be NP-hard, we then devise an efficient dependent rounding based approximation algorithm, which can achieve the near-optimal performance in a fast manner. Both rigorous theoretical analysis and extensive trace-driven evaluations demonstrate the proposed framework can achieve superior performance gain over existing schemes."",""1558-2183"","""",""10.1109/TPDS.2022.3195205"",""National Natural Science Foundation of China(grant numbers:61972432,U20A20159,62172455)"; Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021B151520008); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355); Pearl River Talent Program(grant numbers:2017GC010465); National Natural Science Foundation of China(grant numbers:62172454); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2021A1515011912);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9847031"",""Application placement";approximated dynamic optimization;request routing;"edge computing"",""Servers";Cloud computing;Optimization;Routing;Costs;Heuristic algorithms;"Delays"","""",""4"","""",""51"",""IEEE"",""1 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Joint Coverage-Reliability for Budgeted Edge Application Deployment in Mobile Edge Computing Environment,""L. Zhao"; B. Li; W. Tan; G. Cui; Q. He; X. Xu; L. Xu;" Y. Yang"",""School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China"; Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia; School of Computer and Software, Nanjing University of Information Science and Technology, Nanjing, China; Department of Information Technology and Decision Sciences, Old Dominion University, Norfolk, VA, USA;" Department of Computing Technologies, Swinburne University of Technology, Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3760"",""3771"",""Mobile edge computing (MEC), as an emerging technology, allows application vendors to deploy application instances on edge servers to deliver low-latency services to nearby end-users. However, due to hardware faults, software exceptions, or cyberattacks, edge servers are prone to failures in the highly distributed and dynamic MEC environment. Hence service reliability must be ensured when failures occur. This raises a critical and open problem - improving service reliability when deploying application instances in the MEC environment. In this article, we jointly consider both user coverage and service reliability when deploying application instances on edge servers with a given application deployment budget $\mathcal {K}$K. We formally define this joint Coverage-Reliability for $\mathcal {K}$K-Budgeted Edge Application Deployment (CR-BEAD) problem and model it as a constrained optimization problem. Next, we propose an optimal approach (named BEAD-O) based on integer programming to find optimal solutions to small-scale CR-BEAD problems. We also propose a greedy approach named BEAD-G with a constant approximation ratio of $1 - 1/e$1-1/e to solve large-scale CR-BEAD problems efficiently. Extensive experimental evaluation against three representative approaches illustrates the effectiveness and efficiency of our approaches."",""1558-2183"","""",""10.1109/TPDS.2022.3166163"",""National Natural Science Foundation of China(grant numbers:61672022,U1904186)"; Key Disciplines of Computer Science and Technology of Shanghai Polytechnic University(grant numbers:XXKZD1604); Australian Research Council Discovery(grant numbers:DP180100212,DP200102491);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763314"",""Mobile edge computing";service reliability;user coverage;application deployment;"approximation approach"",""Servers";Reliability;Social networking (online);Cloud computing;Software;Computer crashes;"Multi-access edge computing"","""",""7"","""",""39"",""IEEE"",""26 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Kokkos 3: Programming Model Extensions for the Exascale Era,""C. R. Trott"; D. Lebrun-Grandié; D. Arndt; J. Ciesko; V. Dang; N. Ellingwood; R. Gayatri; E. Harvey; D. S. Hollman; D. Ibanez; N. Liber; J. Madsen; J. Miles; D. Poliakoff; A. Powell; S. Rajamanickam; M. Simberg; D. Sunderland; B. Turcksin;" J. Wilke"",""Sandia National Laboratories, Albuquerque, NM, USA"; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Argonne National Laboratory, Lemont, IL, USA; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Swiss National Supercomputing Centre, Lugano, Switzerland; Sandia National Laboratories, Albuquerque, NM, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA;" Sandia National Laboratories, Albuquerque, NM, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""805"",""817"",""As the push towards exascale hardware has increased the diversity of system architectures, performance portability has become a critical aspect for scientific software. We describe the Kokkos Performance Portable Programming Model that allows developers to write single source applications for diverse high-performance computing architectures. Kokkos provides key abstractions for both the compute and memory hierarchy of modern hardware. We describe the novel abstractions that have been added to Kokkos version 3 such as hierarchical parallelism, containers, task graphs, and arbitrary-sized atomic operations to prepare for exascale era architectures. We demonstrate the performance of these new features with reproducible benchmarks on CPUs and GPUs."",""1558-2183"","""",""10.1109/TPDS.2021.3097283"",""Exascale Computing(grant numbers:17-SC-20-SC)"; U.S. Department of Energy; National Nuclear Security Administration;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9485033"",""Performance portability";programming models;high-performance computing;heterogeneous computing;"exascale"",""Programming";Hardware;Kernel;Graphics processing units;Layout;Laboratories;"Benchmark testing"","""",""91"","""",""24"",""CCBY"",""14 Jul 2021"","""","""",""IEEE"",""IEEE Journals"""
"LB4OMP: A Dynamic Load Balancing Library for Multithreaded Applications,""J. H. M. Korndörfer"; A. Eleliemy; A. Mohammed;" F. M. Ciorba"",""Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland"; Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland; HPE's HPC/AI EMEA Research Lab (ERL), Switzerland;" Department of Mathematics and Computer Science, University of Basel, Basel, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""830"",""841"",""Exascale computing systems will exhibit high degrees of hierarchical parallelism, with thousands of computing nodes and hundreds of cores per node. Efficiently exploiting hierarchical parallelism is challenging due to load imbalance that arises at multiple levels. OpenMP is the most widely-used standard for expressing and exploiting the ever-increasing node-level parallelism. The scheduling options in OpenMP are insufficient to address the load imbalance that arises during the execution of multithreaded applications. The limited scheduling options in OpenMP hinder research on novel scheduling techniques which require comparison with others from the literature. This work introduces LB4OMP, an open-source dynamic load balancing library that implements successful scheduling algorithms from the literature. LB4OMP is a research infrastructure designed to spur and support present and future scheduling research, for the benefit of multithreaded applications performance. Through an extensive performance analysis campaign, we assess the effectiveness and demystify the performance of all loop scheduling techniques in the library. We show that, for numerous applications-systems pairs, the scheduling techniques in LB4OMP outperform the scheduling options in OpenMP. Node-level load balancing using LB4OMP leads to reduced cross-node load imbalance and to improved MPI+OpenMP applications performance, which is critical for Exascale computing."",""1558-2183"","""",""10.1109/TPDS.2021.3107775"",""Swiss National Science Foundation(grant numbers:169123)"; Swiss Platform for Advanced Scientific Computing; European Union's Horizon 2020 research and innovation programme(grant numbers:957407);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524500"",""Hierarchical parallelism";dynamic load balancing;self-scheduling;runtime library;OpenMP;multithreaded programming;"shared-memory systems"",""Dynamic scheduling";Processor scheduling;Parallel processing;Optimal scheduling;Libraries;Standards;"Load management"","""",""5"","""",""45"",""CCBY"",""27 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Leveraging Code Snippets to Detect Variations in the Performance of HPC Systems,""J. Zhai"; L. Zheng; J. Sun; F. Zhang; X. Tang; X. Qian; B. He; W. Xue; W. Chen;" W. Zheng"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science, University of Illinois Urbana-Champaign, Champaign, IL, USA; Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Ming Hsieh Department of Electrical Engineering, University of Southern California, Los Angeles, CA, USA; School of Computing, National University of Singapore, Singapore; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3558"",""3574"",""Variations in the performance of parallel and distributed systems are becoming increasingly challenging. The runtimes of different executions can vary greatly even with a fixed number of computing nodes. Many HPC applications on supercomputers exhibit such variance. This not only leads to unpredictable execution times, but also renders the system’s behavior unintuitive. The efficient online detection of variations in performance is an open problem in HPC research. To solve it, we propose an approach, called vSensor, to detect variations in the performance of systems. The key finding of this study is that the source code of programs can better represent performance at runtime than an external detector. Specifically, many HPC applications contain code snippets that are fixed workload patterns of execution, e.g., the workload of an invariant quantity and a linearly growing workload. This observation allows us to automatically identify these snippets of workload-related code and use them to detect variations in performance. We evaluate vSensor on the Tianhe-2A system with a large number of parallel applications, and the results indicate that it can efficiently identify variations in system performance. The average overhead of 4,096 processes is less than 6% for fixed-workload v-sensors. We identify a problematic node with slow memory by using vSensor that degrades the performance of the program by 21%. A serious issue with network performance is also detected that slows down the Tianhe-2A system by 3.37 times for an HPC kernel."",""1558-2183"","""",""10.1109/TPDS.2022.3158742"",""National Key R&D Program of China(grant numbers:2017YFA0604500)"; National Natural Science Foundation of China(grant numbers:U20A20226); Beijing Natural Science Foundation(grant numbers:4202031,L192027); Tsinghua University-Peking Union Medical College Hospital Initiative Scientific Research Program(grant numbers:20191080594);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9735307"",""Parallel computing";performance variance;HPC;performance analysis;"performance optimization"",""Codes";Runtime;Sensors;Benchmark testing;Instruments;Performance analysis;"Supercomputers"","""",""1"","""",""47"",""IEEE"",""15 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"libEnsemble: A Library to Coordinate the Concurrent Evaluation of Dynamic Ensembles of Calculations,""S. Hudson"; J. Larson; J. -L. Navarro;" S. M. Wild"",""Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA"; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA;" Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""977"",""988"",""Almost all applications stop scaling at some point";" those that don't are seldom performant when considering time to solution on anything but aspirational/unicorn resources. Recognizing these tradeoffs as well as greater user functionality in a near-term exascale computing era, we present libEnsemble, a library aimed at particular scalability- and capability-stretching uses. libEnsemble enables running concurrent instances of an application in dynamically allocated ensembles through an extensible Python library. We highlight the structure, execution, and capabilities of the library on leading pre-exascale environments as well as advanced capabilities for exascale environments and beyond."",""1558-2183"","""",""10.1109/TPDS.2021.3082815"",""U.S. Department of Energy(grant numbers:17-SC-20-SC)"; ComPASS and NUCLEI SciDAC; Advanced Scientific Computing Research(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9439163"",""Dynamic ensembles";simulation support systems;workflow management;"HPC Python"",""Generators";Resource management;Libraries;Optimization;Arrays;History;"Concurrent computing"","""",""9"","""",""38"",""IEEE"",""21 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"LightFed: An Efficient and Secure Federated Edge Learning System on Model Splitting,""J. Guo"; J. Wu; A. Liu;" N. N. Xiong"",""School of Computer Science and Engineering, Central South University, Changsha, Hunan, China"; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA; School of Computer Science and Engineering, Central South University, Changsha, Hunan, China;" Department of Computer Science and Mathematics, Sul Ross State University, Alpine, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2701"",""2713"",""With the integration of Artificial Intelligence (AI) and Internet of Things (IoT), the Federated Edge Learning (FEL), a promising computing framework is developing. However, there are still unsolved issues on communication efficiency and data security due to the huge models and unreliable transmission links. To address these issues, this paper proposes a novel federated edge learning system, called LightFed, where the edge nodes upload only vital partial local models, and successfully achieve lightweight communication and model aggregation. First, a novel model aggregation method Model Splitting and Splicing (MSS) and a Selective Parameter Transmission (SPT) scheme are proposed. By detecting the updating gradients of local parameters and filtering significant parameters, selective rotated transmission and efficient aggregation of local models are achieved. Second, a Training Filling Model (TFM) is proposed to infer the total data distribution of edge nodes, and train a filling model to mitigate the unbalanced training data without violating the data privacy of individual users. Moreover, a blockchain-powered confusion transmission mechanism is proposed for defending the attacks from external adversaries and protecting the model information. Finally, extensive experimental results demonstrate that our LightFed significantly outperforms the existing FEL systems in terms of communication efficiency and privacy security."",""1558-2183"","""",""10.1109/TPDS.2021.3127712"",""National Natural Science Foundation of China(grant numbers:62072475,61772554)"; Graduate Students of Central South University(grant numbers:2021zzts0750);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613755"",""Federated edge learning";communication efficiency;privacy protection;"deep neural network"",""Data models";Computational modeling;Training;Mathematical models;Servers;Security;"Data privacy"","""",""8"","""",""38"",""IEEE"",""12 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Lightweight and Accurate DNN-Based Anomaly Detection at Edge,""Q. Zhang"; R. Han; G. Xin; C. H. Liu; G. Wang;" L. Y. Chen"",""Beijing Institute of Technology, Beijing, P.R. China"; Beijing Institute of Technology, Beijing, P.R. China; Beijing Institute of Technology, Beijing, P.R. China; Beijing Institute of Technology, Beijing, P.R. China; Beijing Institute of Technology, Beijing, P.R. China;" TU Delft, Delft, The Netherlands"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2927"",""2942"",""Deep neural networks (DNNs) have been showing significant success in various anomaly detection applications such as smart surveillance and industrial quality control. It is increasingly important to detect anomalies directly on edge devices, because of high responsiveness requirements and tight latency constraints. The accuracy of DNN-based solutions rely on large model capacity and thus long training and inference time, making them inapplicable on resource strenuous edge devices. It is hence imperative to scale DNN model sizes in correspondence to the run-time system requirements, i.e., meeting deadlines with minimal accuracy losses, which are highly dependent on the platforms and real-time system status. Existing scaling techniques either take long training time to pre-generate scaling options or disturb the unsteady training process of anomaly detection DNNs, lacking the adaptability to heterogeneous edge systems and incurring low inference accuracies. In this article, we present LightDNN to scale DNN models for anomaly detection applications at edge, featuring high detection accuracies with lightweight training and inference time. To this end, LightDNN quickly extracts and compresses blocks in a DNN, and provides large scaling space (e.g., 1 million options) by dynamically combining these compressed blocks online. At run-time, LightDNN predicts the DNN’s inference latency according to the monitored system status, and optimizes the combination of blocks to maximize its accuracy under deadline constraints. We implement and extensively evaluate LightDNN on both CPU and GPU edge platforms using 8 popular anomaly detection workloads. Comparative experiments with state-of-the-art methods show that our approach provides 145.8 to 0.56 trillion times more scaling options without increasing training and inference overheads, thus achieving as much as 15.05% increase in accuracy under the same deadlines."",""1558-2183"","""",""10.1109/TPDS.2021.3137631"",""National Natural Science Foundation of China(grant numbers:61872337,62132019)"; National Research and Development Program of China(grant numbers:2019YQ1700); Swiss National Science Foundation NRP75(grant numbers:407540_167266);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665270"",""Anomaly detection";edge inference;DNN;model scaling;"predictable latency"",""Training";Anomaly detection;Videos;Computational modeling;Predictive models;Image edge detection;"Graphics processing units"","""","""","""",""58"",""IEEE"",""29 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Liquid: Intelligent Resource Estimation and Network-Efficient Scheduling for Deep Learning Jobs on Distributed GPU Clusters,""R. Gu"; Y. Chen; S. Liu; H. Dai; G. Chen; K. Zhang; Y. Che;" Y. Huang"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Group, Hangzhou, Zhejiang, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2808"",""2820"",""Deep learning (DL) is becoming increasingly popular in many domains, including computer vision, speech recognition, self-driving automobiles, etc. GPU can train DL models efficiently but is expensive, which motivates users to share GPU resource to reduce money costs in practice. To ensure efficient sharing among multiple users, it is necessary to develop efficient GPU resource management and scheduling solutions. However, existing ones have several shortcomings. First, they require the users to specify the job resource requirement which is usually quite inaccurate and leads to cluster resource underutilization. Second, when scheduling DL jobs, they rarely take the cluster network characteristics into consideration, resulting in low job execution performance. To overcome the above issues, we propose Liquid, an efficient GPU resource management platform for DL jobs with intelligent resource requirement estimation and scheduling. First, we propose a regression model based method for job resource requirement estimation to avoid users over-allocating computing resources. Second, we propose intelligent cluster network-efficient scheduling methods in both immediate and batch modes based on the above resource requirement estimation techniques. Third, we further propose three system-level optimizations, including pre-scheduling data transmission, fine-grained GPU sharing, and event-driven communication. Experimental results show that our Liquid can accelerate the job execution speed by 18% on average and shorten the average job completion time (JCT) by 21% compared with cutting-edge solutions. Moreover, the proposed optimization methods are effective in various scenarios."",""1558-2183"","""",""10.1109/TPDS.2021.3138825"",""China National Science Foundation(grant numbers:62072230)"; Alibaba Group; China National Science Foundation(grant numbers:U1811461,61832005,61702254); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664375"",""Job scheduling";resource management;deep learning;"GPU clusters"",""Graphics processing units";Processor scheduling;Resource management;Estimation;Liquids;Optimization;"Training"","""",""26"","""",""37"",""IEEE"",""28 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"LOCUS: User-Perceived Delay-Aware Service Placement and User Allocation in MEC Environment,""Y. Chen"; S. Zhang; Y. Jin; Z. Qian; M. Xiao; J. Ge;" S. Lu"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Computer Science and Technology/Suzhou Institute for Advanced Study, University of Science and Technology of China, Hefei, China; State Key Laboratory for Novel Software Technology, Software Institute, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Nov 2021"",""2022"",""33"",""7"",""1581"",""1592"",""In the multi-access edge computing environment, app vendors deploy their services and applications at the network edges, and edge users offload their computation tasks to edge servers. We study the user-perceived delay-aware service placement and user-allocation problem in edge environment. We model the MEC-enabled network, where the user-perceived delay consists of computing delay and transmission delay. The total cost in the offloading system is defined as the sum of service placement, edge server usage and energy consumption cost, and we need to minimize the total cost by determining the overall service-placing decision and user-allocation decision, while guaranteeing that the user-perceived delay requirement of each user is fulfilled. Our considered problem is formulated as a Mixed Integer Linear Programming problem, and we prove its NP-hardness. Due to the intractability of the considered problem, we propose a LOCal-search based algorithm for USer-perceived delay-aware service placement and user-allocation in edge environment, named LOCUS, which starts with a feasible solution and then repeatedly reduces the total cost by performing local-search steps. After that, we analyze the time complexity of LOCUS and prove that it achieves provable guaranteed performance. Finally, we compare LOCUS with other existing methods and show its good performance through experiments."",""1558-2183"","""",""10.1109/TPDS.2021.3119948"",""National Key Research and Development Program of China(grant numbers:2017YFB1001801)"; National Natural Science Foundation of China(grant numbers:61872175,61832008); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9573423"",""User-perceived delay";service placement;user allocation;edge computing;"local search"",""Servers";Task analysis;Resource management;Delays;Costs;Cloud computing;"Heuristic algorithms"","""",""13"","""",""46"",""IEEE"",""14 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"LOFS: A Lightweight Online File Storage Strategy for Effective Data Deduplication at Network Edge,""G. Cheng"; D. Guo; L. Luo; J. Xia;" S. Gu"",""Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China"; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China;" Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2022"",""2022"",""33"",""10"",""2263"",""2276"",""Edge computing responds to users’ requests with low latency by storing the relevant files at the network edge. Various data deduplication technologies are currently employed at edge to eliminate redundant data chunks for space saving. However, the lookup for the global huge-volume fingerprint indexes imposed by detecting redundancies can significantly degrade the data processing performance. Besides, we envision a novel file storage strategy that realizes the following rationales simultaneously: 1) space efficiency, 2) access efficiency, and 3) load balance, while the existing methods fail to achieve them at one shot. To this end, we report LOFS, a Lightweight Online File Storage strategy, which aims at eliminating redundancies through maximizing the probability of successful data deduplication, while realizing the three design rationales simultaneously. LOFS leverages a lightweight three-layer hash mapping scheme to solve this problem with constant-time complexity. To be specific, LOFS employs the Bloom filter to generate a sketch for each file, and thereafter feeds the sketches to the Locality Sensitivity hash (LSH) such that similar files are likely to be projected nearby in LSH tablespace. At last, LOFS assigns the files to real-world edge servers with the joint consideration of the LSH load distribution and the edge server capacity. Trace-driven experiments show that LOFS closely tracks the global deduplication ratio and generates a relatively low load std compared with the comparison methods."",""1558-2183"","""",""10.1109/TPDS.2021.3133098"",""National Natural Science Foundation of China(grant numbers:U19B2024,62002378)"; Research Funding of NUDT(grant numbers:ZK20-30);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645269"",""Data deduplication";locality sensitivity hash;edge computing;"storage systems"",""Servers";Indexes;Costs;Redundancy;Throughput;Image edge detection;"Resource management"","""",""6"","""",""47"",""CCBY"",""10 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Look-up-Table Based Processing-in-Memory Architecture With Programmable Precision-Scaling for Deep Learning Applications,""P. R. Sutradhar"; S. Bavikadi; M. Connolly; S. Prajapati; M. A. Indovina; S. M. P. Dinakarrao;" A. Ganguly"",""Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA"; Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA; Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Microelectronic Engineering, Rochester Institute of Technology, Rochester, NY, USA; Department of Electrical and Computer Engineering, George Mason University, Fairfax, VA, USA;" Department of Computer Engineering, Rochester Institute of Technology, Rochester, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Aug 2021"",""2022"",""33"",""2"",""263"",""275"",""Processing in memory (PIM) architecture, with its ability to perform ultra-low-latency parallel processing, is regarded as a more suitable alternative to von Neumann computing architectures for implementing data-intensive applications such as Deep Neural Networks (DNN) and Convolutional Neural Networks (CNN). In this article, we present a Look-up Table (LUT) based PIM architecture aimed at CNN/DNN acceleration that replaces logic-based processing with pre-calculated results stored inside the LUTs in order to perform complex computations on the DRAM memory platform. Our LUT-based DRAM-PIM architecture offers superior performance at a significantly higher energy-efficiency compared to the more conventional bit-wise parallel PIM architectures, while at the same time avoids fabrication challenges associated with the in-memory implementation of logic circuits. Alongside, the processing elements can be programmed and re-programmed to perform virtually any operation, including operations of Convolutional, Fully Connected, Pooling, and Activating Layers of CNN/DNN. Furthermore, it is capable of operating on several combinations of bit-widths of the operand data and thereby offers a wider range of flexibility across performance, precision, and efficiency. Transmission Gate (TG) realization of the circuitry ensures minimal footprint from the PIM architecture. Our simulations demonstrate that the proposed architecture can perform AlexNet inference at a nearly 13× faster rate and 125× more efficiency compared to state-of-the-art GPU and also provides 1.35× higher throughput at 2.5× higher energy-efficiency than another recent DRAM-implemented LUT-based PIM architecture in its baseline operation mode. Moreover, it offers 12× higher frame-rate at 9× more efficiency per frame for the lowest operand precision setting, with respect to its own baseline operation mode."",""1558-2183"","""",""10.1109/TPDS.2021.3066909"",""National Science Foundation(grant numbers:CNS-1553264)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9380930"",""Processing in memory (PIM)";look-up table (LUT);deep neural networks (DNN);"convolutional neural networks (CNN)"",""Computer architecture";Random access memory;Table lookup;Performance evaluation;Registers;Parallel processing;"Optimization"","""",""7"","""",""43"",""IEEE"",""17 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"LoomIO: Object-Level Coordination in Distributed File Systems,""Y. Hua"; X. Shi; K. He; H. Jin; W. Xie; L. He;" Y. Chen"",""National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China"; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; VMware Inc., Palo Alto, CA, USA; University of Warwick, Coventry, U.K;" Texas Tech University, Lubbock, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""8 Dec 2021"",""2022"",""33"",""8"",""1799"",""1810"",""Device-level interference is recognized as a major cause of the performance degradation in distributed file systems. Although the approaches of mitigating interference through coordination at application-level, middleware-level, and server-level have shown beneficial results in previous studies, we find their effectiveness is largely reduced since I/O requests are re-arranged by underlying object file systems. In this research study, we prove that object-level coordination is critical and often the key to address the interference issue, as the scheduling of object requests determines the device-level accesses and thus determines the actual I/O bandwidth and latency. This article proposes an object-level coordination system, LoomIO, which uses an OBOP (One-Broadcast-One-Propagate) method and a time-limited coordination process to deliver highly efficient coordination service. Specifically, LoomIO enables object requests to achieve an optimized scheduling decision within a few milliseconds and largely mitigates the device-level interference. We have implemented a LoomIO prototye and integrated it into Ceph file system. The evaluation results show that LoomIO achieved the considerable improvements in resource utilization (by up to 35%), in I/O throughput (by up to 31%), and in 99th percentile latency (by up to 54%) compared to the K-optimal method which uses the same scheduling algorithm as LoomIO but does not have the coordination support."",""1558-2183"","""",""10.1109/TPDS.2021.3126260"",""National Key Research and Development Program of China(grant numbers:2020AAA0108501)"; National Natural Science Foundation of China(grant numbers:61772218); Key R&D Program of Hubei(grant numbers:2020BAA020);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609610"",""Distributed object file system";I/O coordination;performance;"erasure-coding"",""Interference";Layout;Performance evaluation;Throughput;Redundancy;Receivers;"Encoding"","""","""","""",""36"",""IEEE"",""9 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"LosaTM: A Hardware Transactional Memory Integrated With a Low-Overhead Scenario-Awareness Conflict Manager,""C. Fu"; L. Wan;" J. Han"",""State Key Laboratory of ASIC and System, Fudan University, Shanghai, China"; State Key Laboratory of ASIC and System, Fudan University, Shanghai, China;" State Key Laboratory of ASIC and System, Fudan University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Sep 2022"",""2022"",""33"",""12"",""4849"",""4862"",""The vigorous development of high compute-intensive applications has led to the demand for maximizing the concurrency of multicore processors. The best-effort hardware transactional memory(HTM) is an important technology adopted by vendors to improve the potential concurrency of multicore processors, but the HTM implementations on commercial products have some drawbacks for its simplicity and need some further optimizations to enable more exploitation of concurrency. In this article, we propose and evaluate a novel design of HTM, called LosaTM, which can provide a scenario-awareness conflict management strategy. By leveraging the proposed feature of multiple-grained coherency maintenance in the coherence protocol, LosaTM resolves most false conflicts at a half-cache-line granularity. Furthermore, we design a winner/aborter vector conflict management algorithm to improve the efficiency of LosaTM in handling friendly-fire and unfairness competition that we have newly defined. In order to coordinate these integrated conflict management strategies, a scheduling strategy is also proposed to adaptively select the appropriate management according to the specific conflict scenario. We use gem5 to simulate LosaTM in detail on an 8-core tiled CMP system, and the simulation result shows that it only causes 0.7% of the L1 cache size hardware overhead while achieving a 38% average execution time reduction on the native STAMP. The speedup also demonstrates that LosaTM outperforms the state-of-the-art designs in previous works."",""1558-2183"","""",""10.1109/TPDS.2022.3206777"",""National Natural Science Foundation of China(grant numbers:61934002)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9893383"",""Scenario-specific conflict management strategies";hardware transactional memory;cache coherence protocols;"false sharing"",""Hardware";Coherence;Program processors;Optimization;Memory management;Concurrent computing;"Protocols"","""","""","""",""33"",""IEEE"",""15 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Lossy Compression of Communication Traces Using Recurrent Neural Networks,""J. Sun"; T. Yan; H. Sun; H. Lin;" G. Sun"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3106"",""3116"",""In high performance computing (HPC) systems, collecting and replaying communication traces are fundamental approaches to analyze performance. With increasingly large-scale HPC systems and applications, tracing tools can produce huge trace data that is costly and challenging to store and analyze. Due to the inherent repetition of behaviors of HPC applications, domain-aware data compression methods can effectively reduce the storage cost of trace data. This study proposes LCR (Lossy Compression and Replay), a framework that aggressively compresses and replays MPI communication traces. Differing from existing trace compression methods, which explicitly identify loop and synchronization structures of communication events, LCR models traces as time series and compactly represents them by lightweight recurrent neural networks. Experimental results demonstrate that LCR can further reduce the size of irregular traces by three orders of magnitude at most, compared with existing structural methods. Meanwhile, LCR accurately reproduces performance and communication patterns of original MPI programs."",""1558-2183"","""",""10.1109/TPDS.2021.3132417"",""NSF of China(grant numbers:61772485)"; Youth Innovation Promotion Association of CAS;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635662"",""High performance computing";performance modeling;MPI trace;"machine learning"",""Recurrent neural networks";Sun;Codes;Time series analysis;Runtime;Pattern matching;"Libraries"","""",""2"","""",""44"",""IEEE"",""3 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Mapping-Aware Kernel Partitioning Method for CGRAs Assisted by Deep Learning,""T. Kojima"; A. Ohwada;" H. Amano"",""Keio University, Yokohama, Kanagawa, Japan"; Keio University, Yokohama, Kanagawa, Japan;" Keio University, Yokohama, Kanagawa, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1213"",""1230"",""Coarse-grained reconfigurable architectures (CGRAs) provide high energy efficiency with word-level programmability rather than bit-level ones such as FPGAs. The coarser reconfigurability brings about higher energy efficiency and reduces the complexity of compiler tasks compared to the FPGAs. However, application mapping process for CGRAs is still time-consuming. When the compiler tries to map a large and complicated application data-flow-graph(DFG) onto the reconfigurable fabric, it tends to result in inefficient resource use or to fail in mapping. In case of failure, the compiler must divide it into several sub-DFGs and goes back to the same flow. In this work, we propose a novel partitioning method based on a genetic algorithm to eliminate the unmappable DFGs and improve the mapping quality. In order not to generate unmappable sub-DFGs, we also propose an estimation model which predicts the mappability and resource requirements using a DGCNN (Deep Graph Convolutional Neural Network). The genetic algorithm with this model can seek the most resource-efficient mapping without the back-end mapping process. Our model can predict the mappability with more than 98% accuracy and resource usage with a negligible error for two studied CGRAs. Besides, the proposed partitioning method demonstrates 53-75% of memory saving, 1.28-1.39x higher throughput, and better mapping quality over three comparative approaches."",""1558-2183"","""",""10.1109/TPDS.2021.3107746"",""JSPS KAKENHI(grant numbers:19J21493)"; JST CREST(grant numbers:JPMJCR19K1);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524520"",""Coarse-grained reconfigurable architecture";CGRA;deep learning;genetic algorithm;"graph partitioning"",""Registers";Partitioning algorithms;Arrays;Routing;Optimization;Estimation;"Pipelines"","""",""5"","""",""64"",""IEEE"",""27 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Max-Tree Computation on GPUs,""N. Blin"; E. Carlinet; F. Lemaitre; L. Lacassagne;" T. Géraud"",""EPITA Research and Development Laboratory (LRDE), Paris, France"; EPITA Research and Development Laboratory (LRDE), Paris, France; LIP6, ALSOC team, Sorbonne University, CNRS, Paris, France; LIP6, ALSOC team, Sorbonne University, CNRS, Paris, France;" EPITA Research and Development Laboratory (LRDE), Paris, France"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jun 2022"",""2022"",""33"",""12"",""3520"",""3531"",""In Mathematical Morphology, the max-tree is a region-based representation that encodes the inclusion relationship of the threshold sets of an image. This tree has proved useful in numerous image processing applications. For the last decade, work has led to improving the construction time of this structure";" mixing algorithmic optimizations, parallel and distributed computing. Nevertheless, there is still no algorithm that benefits from the computing power of the massively parallel architectures. In this work, we propose the first GPU algorithm to compute the max-tree. The proposed approach leads to significant speed-ups, and is up to one order of magnitude faster than the current State-of-the-Art parallel CPU algorithms. This work paves the way for a max-tree integration in image processing GPU pipelines and real-time image processing based on Mathematical Morphology. It is also a foundation for porting other image representations from Mathematical Morphology on GPUs."",""1558-2183"","""",""10.1109/TPDS.2022.3158488"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732653"",""Mathematical morphology";hierarchical image representation;component-trees;max-tree;"graph algorithms"",""Image processing";Pipelines;Morphology;Optimization;Shape;Level set;"Encoding"","""",""1"","""",""49"",""IEEE"",""10 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Maximizing User Service Satisfaction for Delay-Sensitive IoT Applications in Edge Computing,""J. Li"; W. Liang; W. Xu; Z. Xu; X. Jia; W. Zhou;" J. Zhao"",""School of Computing, The Australian National University, Canberra, ACT, Australia"; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong, China; College of Computer Science, Sichuan University, Chengdu, Sichuan, China; School of Software, Dalian University of Technology, Dalian, Liaoning, China; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong, China; Institute of Data Science, City University of Macau, Macau, China;" School of Computer Science, Fudan University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1199"",""1212"",""The Internet of Things (IoT) technology provisions unprecedented opportunities to evolve the interconnection among human beings. However, the latency brought by unstable wireless networks and computation failures caused by limited resources on IoT devices prevents users from experiencing high efficiency and seamless user experience. To address these shortcomings, the integrated Mobile Edge Computing (MEC) with remote clouds is a promising platform to enable delay-sensitive service provisioning for IoT applications, where edge-clouds (cloudlets) are co-located with wireless access points in the proximity of IoT devices. Thus, computation-intensive and sensing data from IoT devices can be offloaded to the MEC network immediately for processing, and the service response latency can be significantly reduced. In this paper, we first formulate two novel optimization problems for delay-sensitive IoT applications, i.e., the total utility maximization problems under both static and dynamic offloading task request settings, with the aim to maximize the accumulative user satisfaction on the use of the services provided by the MEC, and show the NP-hardness of the defined problems. We then devise efficient approximation and online algorithms with provable performance guarantees for the problems in a special case where the bandwidth capacity constraint is negligible. We also develop efficient heuristic algorithms for the problems with the bandwidth capacity constraint. We finally evaluate the performance of the proposed algorithms through experimental simulations. Experimental results demonstrate that the proposed algorithms are promising in reducing service delays and enhancing user satisfaction, and the proposed algorithms outperform their counterparts by at least 10.8 percent."",""1558-2183"","""",""10.1109/TPDS.2021.3107137"",""Australian Research Council(grant numbers:DP200101985)"; National Natural Science Foundation of China(grant numbers:61602330); Sichuan Science and Technology Program(grant numbers:2018GZDZX0010,2017GZDZX0003); National Key Research and Development Program of China(grant numbers:2017YFB0202403); National Natural Science Foundation of China(grant numbers:61802048); Dalian University of Technology; Research Grants Council of Hong Kong(grant numbers:CityU 11214316);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9521690"",""Cost modeling";resource optimization and allocation;service provisioning;delay-sensitive IoT applications;maximum profit generalized assignment problems;approximation algorithms;online algorithms;task offloading and scheduling;service delay;user satisfaction of using services;"mobile edge computing (MEC)"",""Internet of Things";Task analysis;Cloud computing;Delays;Heuristic algorithms;Approximation algorithms;"Bandwidth"","""",""25"","""",""27"",""IEEE"",""24 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"MCDS: AI Augmented Workflow Scheduling in Mobile Edge Cloud Computing Systems,""S. Tuli"; G. Casale;" N. R. Jennings"",""Department of Computing, Imperial College London, London, U.K."; Loughborough University, Loughborough, U.K.;" Loughborough University, Loughborough, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2794"",""2807"",""Workflow scheduling is a long-studied problem in parallel and distributed computing (PDC), aiming to efficiently utilize compute resources to meet user's service requirements. Recently proposed scheduling methods leverage the low response times of edge computing platforms to optimize application Quality of Service (QoS). However, scheduling workflow applications in mobile edge-cloud systems is challenging due to computational heterogeneity, changing latencies of mobile devices and the volatile nature of workload resource requirements. To overcome these difficulties, it is essential, but at the same time challenging, to develop a long-sighted optimization scheme that efficiently models the QoS objectives. In this work, we propose MCDS: Monte Carlo Learning using Deep Surrogate Models to efficiently schedule workflow applications in mobile edge-cloud computing systems. MCDS is an Artificial Intelligence (AI) based scheduling approach that uses a tree-based search strategy and a deep neural network-based surrogate model to estimate the long-term QoS impact of immediate actions for robust optimization of scheduling decisions. Experiments on physical and simulated edge-cloud testbeds show that MCDS can improve over the state-of-the-art methods in terms of energy consumption, response time, SLA violations and cost by at least 6.13, 4.56, 45.09 and 30.71 percent respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3135907"",""Imperial College London";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653818"",""AI for PDC";edge computing;cloud computing;deep learning;monte carlo learning;"workflow scheduling"",""Quality of service";Task analysis;Processor scheduling;Optimization;Time factors;Optimal scheduling;"Costs"","""",""8"","""",""56"",""IEEE"",""16 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Mechanisms for Resource Allocation and Pricing in Mobile Edge Computing Systems,""T. Bahreini"; H. Badri;" D. Grosu"",""Department of Computer Science, Wayne State University, Detroit, MI, USA"; Department of Computer Science, Wayne State University, Detroit, MI, USA;" Department of Computer Science, Wayne State University, Detroit, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2021"",""2022"",""33"",""3"",""667"",""682"",""In this article, we address the resource allocation and monetization challenges in Mobile Edge Computing (MEC) systems, where users have heterogeneous demands and compete for high quality services. We formulate the Edge Resource Allocation Problem (ERAP) as a Mixed-Integer Linear Program (MILP) and prove that ERAP is NP-hard. To solve the problem efficiently, we propose two resource allocation mechanisms. First, we develop an auction-based mechanism and prove that the proposed mechanism is individually-rational and produces envy-free allocations. We also propose an LP-based approximation mechanism that does not guarantee envy-freeness, but it provides solutions that are guaranteed to be within a given distance from the optimal solution. We evaluate the performance of the proposed mechanisms by conducting an extensive experimental analysis on ERAP instances of various sizes. We use the optimal solutions obtained by solving the MILP model using a commercial solver as benchmarks to evaluate thequality of solutions. Our analysis shows that the proposed mechanisms obtain near optimal solutions for fairly large size instances of the problem in a reasonable amount of time."",""1558-2183"","""",""10.1109/TPDS.2021.3099731"",""National Science Foundation(grant numbers:IIS-1724227)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9496271"",""Edge computing";resource allocation;pricing;envy-free mechanism;"approximation algorithm"",""Resource management";Servers;Pricing;Edge computing;Cloud computing;Cost accounting;"Computational modeling"","""",""27"","""",""43"",""IEEE"",""26 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Meet: Rack-Level Pooling Based Load Balancing in Datacenter Networks,""J. Dong"; L. Tan; C. Tian; Y. Zhou; Y. Wang; W. Dou;" G. Chen"",""State Key Laboratory of Media Convergence and Communication, Communication University of China, Beijing, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Sustech Institute of Future Networks, Southern University of Science and Technology, Shenzhen, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3628"",""3639"",""Datacenter networks enable multiple paths between hosts to provide large bisection bandwidth. It requires load balancers to cope with network uncertainties such as traffic dynamics and topology asymmetry. Existing edge-based load balancing schemes are usually faced with the problem of limited network visibility. This article proposes Meet, a rack-level pooling based load-balancer deployed at the edge that can handle the aformentioned uncertainties. Meet utilizes both passive information as well as active probing to comprehensively sense the network conditions with relatively low cost. Meet dynamically reroutes flows effectively based on the visibility of the network condition. Meet has been tested with extensive flow-level simulations against state-of-the-art load balancers. It outperforms Hermes by up to 10% in the experiments, and outperforms others solutions such as DRILL by up to 50%. Meet requires no modifications to the switches and is feasible to deploy at the edge."",""1558-2183"","""",""10.1109/TPDS.2022.3162297"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101390001)"; National Natural Science Foundation of China(grant numbers:62072228,61971382); Fundamental Research Funds for the Central Universities; Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9743293"",""Load balancing";"datacenter network"",""Load management";Bandwidth;Uncertainty;Topology;Routing;Software;"Market research"","""",""1"","""",""46"",""IEEE"",""25 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"MemXCT: Design, Optimization, Scaling, and Reproducibility of X-Ray Tomography Imaging,""M. Hidayetoğlu"; T. Biçer; S. G. de Gonzalo; B. Ren; D. Gürsoy; R. Kettimuthu; I. T. Foster;" W. -M. W. Hwu"",""University of Illinois at Urbana-Champaign, Champaign, IL, USA"; Argonne National Laboratory, Lemont, IL, USA; University of Illinois at Urbana-Champaign, Champaign, IL, USA; College of Willam & Mary, Williamsburg, VA, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA;" University of Illinois at Urbana-Champaign, Champaign, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Jan 2022"",""2022"",""33"",""9"",""2014"",""2031"",""This work extends our previous research entitled “MemXCT: Memory-centric X-ray CT Reconstruction with Massive Parallelization” that was originally published at SC19 conference (Hidayetoğlu et al., 2019) with reproducibility of the computational imaging performance. X-ray computed tomography (XCT) is regularly used at synchrotron light sources to study the internal morphology of materials at high resolution. However, experimental constraints, such as radiation sensitivity, can result in noisy or undersampled measurements. Further, depending on the resolution, sample size and data acquisition rates, the resulting noisy dataset can be in the order of terabytes. Advanced iterative reconstruction techniques can produce high-quality images from noisy measurements, but their computational requirements have made their use an exception rather than the rule. We propose a novel memory-centric approach that avoids redundant computations at the expense of additional memory complexity. We develop a memory-centric iterative reconstruction system, MemXCT, that uses an optimized SpMV implementation with two-level pseudo-Hilbert ordering and multi-stage input buffering. We evaluate MemXCT on various supercomputer architectures involving KNL and GPU. MemXCT can reconstruct a large (11K×11K) mouse brain tomogram in 10 seconds using 4096 KNL nodes (256K cores). The results presented in our original article at the SC19 were based on large-scale supercomputing resources. The MemXCT application was selected for the Student Cluster Competition (SCC) Reproducibility Challenge and evaluated on a variety of cloud computing resources by universities around the world in the SC20 conference. We summarize the results of the top-ranked SCC Reproducibility Challenge teams and identify the most pertinent measures for ensuring the reproducibility of our experiments in this article."",""1558-2183"","""",""10.1109/TPDS.2021.3128032"",""U.S. Department of Energy(grant numbers:DE-AC02-06CH11357)"; National Science Foundation(grant numbers:OCI-0725070,ACI-1238993); XPACC Center for Exascale Simulation of Plasma-Coupled Combustion and Department of Energy(grant numbers:DE-NA0002374);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9625857"",""X-ray tomography";space-filling curves;cache utilization;knights landing;GPU computing;SpMV;"reproducibility"",""Image reconstruction";X-ray imaging;Reproducibility of results;Noise measurement;Optimization;Computed tomography;"Mice"","""",""3"","""",""68"",""IEEE"",""23 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Microservice Deployment in Edge Computing Based on Deep Q Learning,""W. Lv"; Q. Wang; P. Yang; Y. Ding; B. Yi; Z. Wang;" C. Lin"",""School of Computer Science and Technology, Xidian University, Xi'an, China"; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China; School of Computer Science and Technology, Xidian University, Xi'an, China;" School of Computer Science and Technology, Xidian University, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2968"",""2978"",""The microservice deployment strategy is promising in reducing the overall service response time in the microservice-oriented edge computing platform. However, existing works ignore the effect of different interaction frequencies among microservices and the decrease in service execution performance caused by the increased node loads. In this article, we first model the invocation relationships among microservices as an undirected and weighted interaction graph to characterize the communication overhead. Then, we propose a multi-objective microservice deployment problem (MMDP) in edge computing. MMDP aims to minimize the communication overhead while achieving load balance between edge nodes. Without the requirement for domain experts, we propose Reward Sharing Deep Q Learning (RSDQL), a learning-based algorithm, to solve MMDP and obtain the optimal deployment strategy. In addition, to improve the scalability of the services, we propose an Elastic Scaling algorithm (ES) based on heuristics to deal with the dynamic pressure of requests. Finally, we conduct a series of experiments in Kubernetes to evaluate the performance of our approach. Experimental results indicate that, compared with interaction-aware strategy and Kubernetes default strategy, RSDQL has shorter response times, more balanced resource loads, and makes services scale elastically according to the request pressure."",""1558-2183"","""",""10.1109/TPDS.2022.3150311"",""National Natural Science Foundation of China(grant numbers:61972302,61962019)"; Shaanxi Key Technology R&D Program(grant numbers:2021ZDLGY07-01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712168"",""Microservice";interaction awareness;load balancing;multi-objective model;deep Q learning;"elastic scaling"",""Microservice architectures";Containers;Load modeling;Edge computing;Time factors;Load management;"Scalability"","""",""9"","""",""49"",""IEEE"",""11 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Min-Max Cost Optimization for Efficient Hierarchical Federated Learning in Wireless Edge Networks,""J. Feng"; L. Liu; Q. Pei;" K. Li"",""State Key Laboratory of ISN, School of Telecommunication Engineering, Xidian University, Xi'an, Shaanxi, China"; State Key Laboratory of ISN, School of Telecommunication Engineering, Xidian University, Xi'an, Shannxi, China; State Key Laboratory of ISN, School of Telecommunication Engineering, Xidian University, Xi'an, Shannxi, China;" Department of Computer Science, State University of New York, New Paltz, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2687"",""2700"",""Federated learning is a distributed machine learning technology that can protect users’ data privacy, so it has attracted more and more attention in the industry and academia. Nonetheless, most of the existing works focused on the cost optimization of the entire process, while the cost of individual participants cannot be considered. In this article, we explore a min-max cost-optimal problem to guarantee the convergence rate of federated learning in terms of cost in wireless edge networks. In particular, we minimize the cost of the worst-case participant subject to the delay, local CPU-cycle frequency, power allocation, local accuracy, and subcarrier assignment constraints. Considering that the formulated problem is a mixed-integer nonlinear programming problem, we decompose it into several sub-problems to derive its solutions, in which the subcarrier assignment and power allocation are obtained by utilizing the Lagrangian dual decomposition method, the CPU-cycle frequency is obtained by a heuristic algorithm, and the local accuracy is obtained by an iteration algorithm. Simulation results show the convergence of the proposed algorithm and reveal that the proposed scheme can accomplish a tradeoff between the cost and fairness by comparing the proposed scheme with the existing schemes."",""1558-2183"","""",""10.1109/TPDS.2021.3131654"",""National Key Research and Development Program of China(grant numbers:2020YFB1807500)"; National Natural Science Foundation of China(grant numbers:62102297,62001357); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2020A1515110496,2020A1515110079); China Postdoctoral Science Foundation(grant numbers:2021M692501); Fundamental Research Funds for the Central Universities(grant numbers:XJS210105,XJS210107); Open Project of Shaanxi Key Laboratory of Information Communication Network and Security(grant numbers:ICNS202005);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9629331"",""CPU-cycle frequency";federated learning;local accuracy;min-max cost;"wireless edge networks"",""Computational modeling";Servers;Smart devices;Collaborative work;Training;Data models;"Resource management"","""",""51"","""",""28"",""IEEE"",""30 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"MIPD: An Adaptive Gradient Sparsification Framework for Distributed DNNs Training,""Z. Zhang";" C. Wang"",""Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, Hong Kong";" Department of Computer Science, The University of Hong Kong, Hong Kong, SAR, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3053"",""3066"",""Asynchronous training based on the parameter server architecture is widely used for scaling up the DNN training over large datasets and DNN models. Communication has been identified as the major bottleneck when deploying the DNN training over the large-scale distributed deep learning systems. Recent studies try to reduce the communication traffic through gradient sparsification and quantization approaches. We identify three limitations in previous studies. First, the fundamental guideline for gradient sparsification of their work is the magnitude of the gradient. However, the gradients’ magnitude represents the current optimization direction while it cannot indicate the significance of the parameters, which potentially results in delayed updating for the significant parameters. Second, their gradient quantization methods based on the entire model often lead to error accumulation for gradients aggregation since the gradients from different layers of the DNN model follow different distributions. Third, previous quantization approaches are CPU intensive, which generates strong overhead for the server. We propose MIPD, an adaptive and layer-wise gradient sparsification framework that compresses the gradients based on model interpretability and probability distribution of gradients. MIPD compresses the gradients according to the corresponding significance of its parameters, which is defined by model interpretability. An Exponential Smoothing method is also proposed to compensate for the dropped gradients on the server to reduce the gradients error. MIPD proposes to update half of the parameters for each training step to reduce the CPU overhead of the server. It encodes the gradients based on their probability distribution, thereby minimizing the approximated errors. Extensive experimental results generated on the GPU cluster indicate that the proposed framework effectively improves the training performance of DNNs by up to 36.2%, which ensures high accuracy as compared to state-of-art solutions. Accordingly, the CPU and network usage of the server dropped by up to 42.0% and 32.7% respectively."",""1558-2183"","""",""10.1109/TPDS.2022.3154387"",""Hong Kong RGC Collaborative Research Fund(grant numbers:C5026-18G)"; Research Impact Fund(grant numbers:R5060-19); RGC Collaborative Research Fund(grant numbers:C6021-19EF);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721697"",""Exponential smoothing prediction";gradients sparsification;model interpretability;probability distribution;"quantization"",""Training";Servers;Quantization (signal);Convergence;Probability distribution;Degradation;"Adaptation models"","""",""3"","""",""35"",""IEEE"",""25 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Mixing Activations and Labels in Distributed Training for Split Learning,""D. Xiao"; C. Yang;" W. Wu"",""Guangdong Key Laboratory of Big Data Analysis and Processing, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"; Guangdong Key Laboratory of Big Data Analysis and Processing, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China;" Key Laboratory of Machine Intelligence and Advanced Computing, Ministry of Education, School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3165"",""3177"",""Split Learning (SL) is a distributed machine learning setting that allows several nodes to train neural networks based on model parallelism. Since SL avoids sharing raw data among training nodes, it can protect data privacy by nature. However, recent studies show that, raw data may be reconstructed from activations in training, which may cause data privacy leakage. Besides raw data, label sharing in SL may also cause privacy problems. In order to address these issues, we propose a novel mechanism called multiple activations and labels mix (MALM). By taking advantage of the diversity of sample categories, MALM generates mixed activations that preserve a low distance correlation with the raw data so as to reduce the risk of reconstruction attacks. To protect label information, MALM creates obfuscated labels associated with the raw data so as to prevent adversaries from inferring ground-truth labels. Since clients with few sample categories may not effectively generate mixed activations and obfuscated labels, we propose a bipartite graph based assistant client match technique for MALM, which lets clients with a large number of categories provide mixed activations and obfuscated labels for clients with few categories. Those clients with few categories can mix the obtained mixed activations and obfuscated labels with their own activations and labels. Experimental results show that, compared with baselines, MALM can reduce the risk of raw data and label information leakage with lower cost, while achieving comparable even better model performance."",""1558-2183"","""",""10.1109/TPDS.2021.3139191"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101090005)"; National Natural Science Foundation of China(grant numbers:U1711263,U1801266); Guangdong Provincial Natural Science Foundation of China(grant numbers:2018B030312002);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665231"",""Distributed training";split learning;privacy preservation;model parallelism;"neural networks"",""Training";Privacy;Servers;Data models;Parallel processing;Distributed databases;"Data privacy"","""",""4"","""",""31"",""IEEE"",""29 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Mobility-Aware Offloading and Resource Allocation for Distributed Services Collaboration,""H. Chen"; S. Deng; H. Zhu; H. Zhao; R. Jiang; S. Dustdar;" A. Y. Zomaya"",""College of Computer Science and Technology, Zhejiang University, Hangzhou, China"; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Institute of Intelligence Applications, Yunnan University of Finance and Economics, Kunming, China; Distributed Systems Group, TU Wien, Vienna, Austria;" High Performance Computing & Networking, School of Computer Science, University of Sydney, Camperdown, NSW, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Mar 2022"",""2022"",""33"",""10"",""2428"",""2443"",""In mobile edge computing (MEC) systems, mobile users (MUs) are capable of allocating local resources (CPU frequency and transmission power) and offloading tasks to edge servers in the vicinity in order to enhance their computation capabilities and reduce back-and-forth transmission over backhaul link. Nevertheless, mobile environment makes it hard to draw offloading and resource allocation decisions under dynamical wireless channel state and users’ locations. In real life, social relationship is also provably a significant factor affecting integral performance in collaborative work, which results in MUs decisions strongly coupled and renders this problem further intractable. Most of previous works ignore the impact of inter-user dependency (or data dependency among IoT devices). To bridge this gap, we study the service collaboration with master-slave dependency among service chains of MUs and formulate this combinational optimization problem as a mixed integer non-linear programming (MINLP) problem. To this end, we derive the closed-form expression of resource allocation solution by convex optimization and transform it to integer linear programming (ILP) problem. Subsequently, we propose a distributed algorithm based on Markov approximation which has polynomial computation complexity. Experimental result on real-world dataset substantiates the usefulness and superiority of our scheme, in terms of reducing latency and energy consumption."",""1558-2183"","""",""10.1109/TPDS.2022.3142314"",""Key Research Project of Zhejiang Province(grant numbers:2022C01145)"; National Natural Science Foundation of China(grant numbers:U20A20173,62125206); Zhejiang University Deqing Institute of Advanced technology and Industrilization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9681614"",""Mobile edge computing";task offloading;resource allocation;dependency;"collaborative computing"",""Resource management";Task analysis;Servers;Optimization;Energy consumption;Collaboration;"Collaborative work"","""",""9"","""",""55"",""IEEE"",""13 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Modeling Speedup in Multi-OS Environments,""B. R. Tauro"; C. Liu;" K. C. Hale"",""Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA"; Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA;" Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1436"",""1450"",""For workloads that place strenuous demands on system software, novel operating system designs like unikernels, library OSes, and hybrid runtimes offer a promising path forward. However, while these systems can outperform general-purpose OSes, they have limited ability to support legacy applications. Multi-OS environments, where the application’s execution is split between a control plane and a data plane operating system, can address this challenge, but reasoning about the performance of applications that run in such a split execution environment is currently guided only by expert intuition and empirical analysis. As the level of specialization in system software and hardware continues to increase, there is both a pressing need and ripe opportunity for investigating analytical models that can predict application performance and guide programmers’ intuition when considering multi-OS environments. In this paper we present such a model to place bounds on application speedup, beginning with a simple, intuitive formulation, and progressing to a more refined model. We present an analysis of the model for a diverse set of benchmarks, as well as a prototype tool to project multi-OS speedups for applications on existing systems. Finally, we validate our model on state-of-the-art multi-OS systems, demonstrating that it reliably predicts speedup with 96% average accuracy."",""1558-2183"","""",""10.1109/TPDS.2021.3114984"",""National Science Foundation(grant numbers:CNS-1718252,CNS-1730689,>CNS-1763612,CCF-2028958,CCF-2029014)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9547731"",""Operating systems";multi-kernels;speedup models;"performance modeling"",""Kernel";Computational modeling;Hardware;Analytical models;Tools;Linux;"Benchmark testing"","""","""","""",""54"",""IEEE"",""24 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Monodirectional Evolutional Symport Tissue P Systems With Promoters and Cell Division,""B. Song"; K. Li;" X. Zeng"",""College of Information Science and Engineering, Hunan University, Changsha, Hunan, China"; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China;" College of Information Science and Engineering, Hunan University, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Aug 2021"",""2022"",""33"",""2"",""332"",""342"",""Monodirectional tissue P systems with promoters are natural inspired parallel computing paradigms, where only symport rules are permitted, and with the restriction of “monodirectionality”, objects for two given regions are transferred in one direction. In this article, a novel kind of P systems, monodirectional evolutional symport tissue P systems with promoters (MESTP P systems) is raised, where objects may be revised during the movement between two regions. The computational theory of MESTP P systems that rules are employed in a flat maximally parallel pattern is investigated. We prove that finite natural number sets are created by MESTP P systems applying one cell, at most 1 promoter and all evolutional symport rules having a maximal length 2 or with arbitrary number of cells, promoters and all evolutional symport rules having a maximal length 2. MESTP P systems are Turing universal when two cells, at most 1 promoter and all evolutional symport rules having a maximal length 2 are employed. In addition, with the help of cell division mechanism, monodirectional evolutional symport tissue P systems with promoters and cell division (MESTPD P systems) are employed to solve NP-complete (the SAT) problem, where system uses at most 1 promoter and all evolutional symport rules having a maximal length 3. These results show that MESTP(D) P systems are still computationally powerful even if monodirectionality control mechanism is imposed, thereby developing membrane algorithms for MESTP(D) P systems is theoretically possible as well as potentially exploitable."",""1558-2183"","""",""10.1109/TPDS.2021.3065397"",""National Natural Science Foundation of China(grant numbers:61972138,61872309)"; Fundamental Research Funds for the Central Universities(grant numbers:531118010355); Natural Science Foundation of Hunan Province(grant numbers:2020JJ4215); Key Research and Development Program of Changsha(grant numbers:kq2004016);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376640"",""Bio-inspired computing";membrane computing;tissue-like network;universality;"NP-complete problem"",""Biomembranes";Computational modeling;Skin;NP-complete problem;Micromechanical devices;Biological systems;"Biological system modeling"","""",""18"","""",""71"",""IEEE"",""11 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Multi-Swarm Co-Evolution Based Hybrid Intelligent Optimization for Bi-Objective Multi-Workflow Scheduling in the Cloud,""H. Li"; D. Wang; M. Zhou; Y. Fan;" Y. Xia"",""Key Laboratory of Complex System Intelligent Control and Decision, Beijing Institute of Technology, Beijing, China"; Key Laboratory of Complex System Intelligent Control and Decision, Beijing Institute of Technology, Beijing, China; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Department of Automation, Tsinghua University, Beijing, China;" Key Laboratory of Complex System Intelligent Control and Decision, Beijing Institute of Technology, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Feb 2022"",""2022"",""33"",""9"",""2183"",""2197"",""Many scientific applications can be well modelled as large-scale workflows. Cloud computing has become a suitable platform for hosting and executing them. Workflow scheduling has gained much attention in recent years. However, since cloud service providers must offer services for multiple users with various QoS demands, scheduling multiple applications with different QoS requirements is highly challenging. This work proposes a Multi-swarm Co-evolution-based Hybrid Intelligent Optimization (MCHO) algorithm for multiple-workflow scheduling to minimize total makespan and cost while meeting the deadline constraint of each workflow. First, we design a multi-swarm co-evolutionary mechanism where three swarms are adopted to sufficiently search for various elite solutions. Second, to improve global search and convergence performance, we embed local and global guiding information into the updating process of a Particle Swarm Optimizer, and develop a swarm cooperation technique. Third, we propose a Genetic Algorithm-based elite enhancement strategy to exploit more non-dominated individuals, and apply the Metropolis Acceptance rule of Simulated Annealing to update the local guiding solution for each swarm so as to prevent it from being stuck into a local optimum at an early stage. Extensive experimental results demonstrate that MCHO outperforms the state-of-art scheduling algorithms with better distributed non-dominated solutions."",""1558-2183"","""",""10.1109/TPDS.2021.3122428"",""National Key Research and Development Program of China(grant numbers:2018YFB1003700)"; National Natural Science Foundation of China(grant numbers:61836001); Ministry of Science and Higher Education of the Russian Federation(grant numbers:075-15-2020-903);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9585437"",""Cloud computing";intelligent optimization;multi-swarm;multiple workflows;Quality of Service (QoS);"scheduling"",""Task analysis";Cloud computing;Costs;Optimization;Heuristic algorithms;Quality of service;"Dynamic scheduling"","""",""22"","""",""54"",""IEEE"",""26 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Multi-Task Federated Learning for Personalised Deep Neural Networks in Edge Computing,""J. Mills"; J. Hu;" G. Min"",""Department of Computer Science, University of Exeter, Exeter, U.K."; Department of Computer Science, University of Exeter, Exeter, U.K.;" Department of Computer Science, University of Exeter, Exeter, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2021"",""2022"",""33"",""3"",""630"",""641"",""Federated Learning (FL) is an emerging approach for collaboratively training Deep Neural Networks (DNNs) on mobile devices, without private user data leaving the devices. Previous works have shown that non-Independent and Identically Distributed (non-IID) user data harms the convergence speed of the FL algorithms. Furthermore, most existing work on FL measures global-model accuracy, but in many cases, such as user content-recommendation, improving individual User model Accuracy (UA) is the real objective. To address these issues, we propose a Multi-Task FL (MTFL) algorithm that introduces non-federated Batch-Normalization (BN) layers into the federated DNN. MTFL benefits UA and convergence speed by allowing users to train models personalised to their own data. MTFL is compatible with popular iterative FL optimisation algorithms such as Federated Averaging (FedAvg), and we show empirically that a distributed form of Adam optimisation (FedAvg-Adam) benefits convergence speed even further when used as the optimisation strategy within MTFL. Experiments using MNIST and CIFAR10 demonstrate that MTFL is able to significantly reduce the number of rounds required to reach a target UA, by up to $5\times$5× when using existing FL optimisation strategies, and with a further $3\times$3× improvement when using FedAvg-Adam. We compare MTFL to competing personalised FL algorithms, showing that it is able to achieve the best UA for MNIST and CIFAR10 in all considered scenarios. Finally, we evaluate MTFL with FedAvg-Adam on an edge-computing testbed, showing that its convergence and UA benefits outweigh its overhead."",""1558-2183"","""",""10.1109/TPDS.2021.3098467"",""EPSRC DTP Studentship"; EU Horizon 2020 INITIATE(grant numbers:101008297);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492755"",""Federated learning";multi-task learning;deep learning;edge computing;"adaptive optimization"",""Training";Servers;Data models;Optimization;Convergence;Computational modeling;"Adaptation models"","""",""75"","""",""34"",""IEEE"",""21 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Near-Zero Downtime Recovery From Transient-Error-Induced Crashes,""C. Chen"; G. Eisenhauer;" S. Pande"",""Amazon Science, Santa Clara, CA, USA"; School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA;" School of Computer Science, Georgia Institute of Technology, Atlanta, GA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""765"",""778"",""Due to the system scaling, transient errors caused by external noise, e.g., heat fluxes and particle strikes, have become a growing concern for the current and upcoming exa-scale high-performance-computing (HPC) systems. Applications running on these systems are expected to experience transient errors more frequently than ever before, which will either lead them to generate incorrect outputs or cause them to crash. However, since such errors are still quite rare as compared to no-fault cases, desirable solutions call for low/no-overhead systems that do not compromise the performance under no-fault conditions and also allow very fast fault recovery to minimize downtime. In this article, we present IterPro, a light-weight compiler-assisted resilience technique to quickly and accurately recover processes from transient-error-induced crashes. During the compilation of applications, IterPro constructs a set of recovery kernels for crash-prone instructions. These recovery kernels are executed to repair the corrupted process states on-the-fly upon occurrences of errors, enabling applications to continue their executions instead of being terminated. When constructing recovery kernels, IterPro exploits side effects introduced by induction variable based code optimization techniques based on loop unrolling and strength reduction to improve its recovery capability. To this end, two new code transformation passes are introduced to expose the side effects for resilience purposes. We evaluated IterPro with 4 scientific workloads as well as the NPB benchmarks suite. During their normal execution, IterPro incurs almost zero runtime overhead and a small, fixed 27MB memory overhead. Meanwhile, IterPro can recover on an average 83.55 percent of crash-causing errors within dozens of milliseconds with negligible downtime. We also evaluated IterPro with parallel jobs running on 3072 cores and showed that IterPro can successfully mask the impact of crash-causing errors by providing almost uninterrupted execution. Finally, we present our preliminary evaluation result for BLAS, which shows that IterPro is capable of recovering failures in libraries with a very high coverage rate of 83 percent and negligible overheads. With such an effective recovery mechanism, IterPro could tremendously mitigate the overheads and resource requirements of the resilience subsystem in future exa-scale systems."",""1558-2183"","""",""10.1109/TPDS.2021.3096055"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9479798"",""Resiliency";transient fault;soft error;fault tolerance;exa-scale computing;failure;crash;segment fault;"compiler"",""Kernel";Computer crashes;Transient analysis;Runtime;Indexes;Resilience;"Optimization"","""","""","""",""33"",""IEEE"",""9 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Necessary Feasibility Analysis for Mixed-Criticality Real-Time Embedded Systems,""H. S. Chwa"; H. Baek;" J. Lee"",""Department of Information and Communication Engineering, DGIST, Daegu, South Korea"; Department of Computer Science and Engineering, Incheon National University, Incheon, South Korea;" Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Nov 2021"",""2022"",""33"",""7"",""1520"",""1537"",""As multiple software components with different safety-criticality levels are integrated on a shared computing platform, a real-time embedded system becomes a mixed-criticality (MC) system, which should provide timing guarantees at all different levels of assurance to software components with different criticality levels. In the real-time systems community, the concept of an MC system is regarded as a promising, emerging solution to solve an inherent challenge of real-time systems: pessimistic reservation of computing resources, which yields a low resource-utilization for the sake of guaranteeing timing requirements. Since a timing guarantee should be provided before a real-time system starts to operate, its feasibility has been extensively studied for single-criticality systems";" however, the same cannot be said for MC systems. In this article, we develop necessary feasibility tests for MC real-time embedded systems, which is the first study that yields non-trivial results for MC necessary feasibility on both uniprocessor and multiprocessor platforms. To this end, we investigate characteristics of MC necessary feasibility conditions, and identify new challenges posed by the characteristics. By addressing those challenges, we develop two collective necessary feasibility tests and their simplified versions, which are able to exploit a tradeoff between capability in finding infeasible task sets and time-complexity. The simulation results demonstrate that the proposed tests find a number of additional infeasible task sets for both uniprocessor and multiprocessor platforms, which have been proven neither feasible nor infeasible by any existing studies."",""1558-2183"","""",""10.1109/TPDS.2021.3118610"",""National Research Foundation of Korea(grant numbers:2020R1F1A1076058,2021R1A2B5B02001758,2019R1F1A1059663,2017M3A9G8084463)"; Institute for Information & communications Technology Planning & Evaluation(grant numbers:2014-3-00065); DGIST R&D Program of MSIT(grant numbers:20-CoE-IT-01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563206"",""Real-time embedded systems";mixed-criticality systems;necessary feasibility analysis;timing guarantees;"uniprocessor and multiprocessor platforms"",""Task analysis";Real-time systems;Timing;Scheduling algorithms;Embedded systems;Complexity theory;"Program processors"","""",""6"","""",""30"",""IEEE"",""7 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"NeoFlow: A Flexible Framework for Enabling Efficient Compilation for High Performance DNN Training,""S. Zheng"; R. Chen; Y. Jin; A. Wei; B. Wu; X. Li; S. Yan;" Y. Liang"",""School of EECS, Peking University, Beijing, China"; School of EECS, Peking University, Beijing, China; School of EECS, Peking University, Beijing, China; School of EECS, Peking University, Beijing, China; School of EECS, Peking University, Beijing, China; SenseTime Research & Shanghai AI Lab, Beijing, China; SenseTime Research, Beijing, China;" School of EECS, Peking University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3220"",""3232"",""Deep neural networks (DNNs) are increasingly deployed in various image recognition and natural language processing applications. The continuous demand for accuracy and high performance has led to innovations in DNN design and a proliferation of new operators. However, existing DNN training frameworks such as PyTorch and TensorFlow only support a limited range of operators and rely on hand-optimized libraries to provide efficient implementations for these operators. To evaluate novel neural networks with new operators, the programmers have to either replace the holistic new operators with existing operators or provide low-level implementations manually. Therefore, a critical requirement for DNN training frameworks is to provide high-performance implementations for the neural networks containing new operators automatically in the absence of efficient library support. In this article, we introduce NeoFlow, which is a flexible framework for enabling efficient compilation for high-performance DNN training. NeoFlow allows the programmers to directly write customized expressions as new operators to be mapped to graph representation and low-level implementations automatically, providing both high programming productivity and high performance. First, NeoFlow provides expression-based automatic differentiation to support customized model definitions with new operators. Then, NeoFlow proposes an efficient compilation system that partitions the neural network graph into subgraphs, explores optimized schedules, and generates high-performance libraries for subgraphs automatically. Finally, NeoFlow develops an efficient runtime system to combine the compilation and training as a whole by overlapping their execution. In the experiments, we examine the numerical accuracy and performance of NeoFlow. The results show that NeoFlow can achieve similar or even better performance at the operator and whole graph level for DNNs compared to deep learning frameworks. Especially, for novel networks training, the geometric mean speedups of NeoFlow to PyTorch, TensorFlow, and CuDNN are 3.16X, 2.43X, and 1.92X, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3138862"",""National Natural Science Foundation of China(grant numbers:U21B2017)"; Shanghai Committee of Science and Technology, China(grant numbers:20DZ1100800);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664259"",""Deep learning";training;code generation;compiler optimization;"automatic differentiation"",""Training";Libraries;Convolution;Codes;Deep learning;Tensors;"Schedules"","""",""4"","""",""54"",""CCBY"",""28 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"NetEC: Accelerating Erasure Coding Reconstruction With In-Network Aggregation,""Y. Qiao"; M. Zhang; Y. Zhou; X. Kong; H. Zhang; M. Xu; J. Bi;" J. Wang"",""Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China"; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Alibaba Inc., Hangzhou, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, and the Department of Computer Science and Technology, Tsinghua University, Beijing, China; Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China;" Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Apr 2022"",""2022"",""33"",""10"",""2571"",""2583"",""In distributed storage systems, Erasure Coding (EC) is a crucial technology to enable high data availability. By downloading parity data from survived machines, EC can reconstruct lost data with much lower storage overheads than data replication. However, this reduction in storage cost comes at the expense of extra performance problems: low reconstruction rate, high degraded read latency, and high host CPU utilization. Our analysis shows that these performance problems are deeply rooted in the host-based EC processing. To resolve these problems, we present NetEC, an in-network accelerating framework that fully offloads EC to the new generation programmable switching ASICs. We propose Explicit Buffer Size Notification (EBSN) to constrain decoding buffer usage, and design an on-switch one-to-many TCP proxy to integrate EBSN with TCP. We also design two parallel Galois Field (GF) offloading methods—table lookup and bitmatrix methods—to maximize parsable bytes. We implement NetEC on programmable switches and integrate it with HDFS. Extensive evaluations show that NetEC improves the reconstruction rate by 2.7x-6.8x, reduces the degraded read latency significantly, and removes the host CPU overhead completely. We also emulate multi-rack scenarios and show that NetEC is able to support $\sim$∼GB/s reconstruction rate and tens of concurrent tasks."",""1558-2183"","""",""10.1109/TPDS.2022.3145836"",""Joint Research on IPv6 Network Governance: Research, Development and Demonstration(grant numbers:2020YFE0200500)"; National Natural Science Foundation of China(grant numbers:62002009);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693136"",""Erasure coding";distributed storage sytems;programmable switch;"software-defined networks"",""Encoding";Decoding;Codes;Throughput;Task analysis;Switches;"Bandwidth"","""",""2"","""",""48"",""IEEE"",""25 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"NetSHa: In-Network Acceleration of LSH-Based Distributed Search,""P. Zhang"; H. Pan; Z. Li; P. Cui; R. Jia; P. He; Z. Zhang; G. Tyson;" G. Xie"",""Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; ByteDance Inc., Beijing, China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; Queen Mary University of London, London, U.K.;" Computer Network Information Center, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Feb 2022"",""2022"",""33"",""9"",""2213"",""2229"",""Locality Sensitive Hashing (LSH) is widely adopted to index similar data in high-dimensional space for approximate nearest neighbor search. Demanding applications (e.g. web search) mean that LSH must exhibit low response times and high throughput. To achieve this, they tend to load balance between multiple machines. However, as the scale of concurrent queries and the volume of data grow, large numbers of index messages are required. Hence, the network is a key bottleneck. To address this gap, we propose NetSHa, which exploits the computational capacity of programmable switches. Specifically, we introduce a heuristic sort-reduce approach to drop potentially poor candidate answers while preserving search quality. Then, NetSHa aggregates good candidate answers from different index messages when transmitting them. Through this, it reduces the network communication cost. Furthermore, we introduce a best-effort replacement mechanism to improve its concurrency. We implement NetSHa on a Barefoot Tofino programmable switch and evaluate it using 7 real-world datasets. The experimental results show that NetSHa reduces the packet volume by $4\sim 10$4∼10 times and improves the search efficiency by least 3× in comparison with typical LSH-based distributed search frameworks."",""1558-2183"","""",""10.1109/TPDS.2021.3135842"",""National Key Research and Development Program of China(grant numbers:2020YFB1805600)"; National Natural Science Foundation of China(grant numbers:61725206,U20A20180,62002344); Informatization Plan of Chinese Academy of Sciences(grant numbers:CAS-WX2021SF-0506); CAS-Austria Joint Project(grant numbers:171111KYSB20200001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653850"",""Local sensitive hashing";distributed search;"in-network computation"",""Servers";Indexes;Costs;Task analysis;Hash functions;Concurrent computing;"Aggregates"","""",""3"","""",""57"",""IEEE"",""16 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"NetSync: A Network Adaptive and Deduplication-Inspired Delta Synchronization Approach for Cloud Storage Services,""W. Xia"; C. Wei; Z. Li; X. Wang;" X. Zou"",""Harbin Institute of Technology, Shenzhen, Guangdong, China"; Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China; Tsinghua University, Beijing, China; Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China;" Department of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Apr 2022"",""2022"",""33"",""10"",""2554"",""2570"",""Delta sync (synchronization) is a key bandwidth-saving technique for cloud storage services. The representative delta sync utility, rsync, matches data chunks by sliding a search window byte-by-byte to maximize the redundancy detection for bandwidth efficiency. However, it is difficult for this process to cater to the forthcoming high-bandwidth cloud storage services which require lightweight delta sync that can well support large files. Moreover, rsync employs invariant chunking and compression methods during the sync process, making it unable to cater to services from various network environments which require the sync approach to perform well under different network conditions. Inspired by the Content-Defined Chunking (CDC) technique used in data deduplication, we propose NetSync, a network adaptive and CDC-based lightweight delta sync approach with less computing and protocol (metadata) overheads than the state-of-the-art delta sync approaches. Besides, NetSync can choose appropriate compressing and chunking strategies for different network conditions. The key idea of NetSync is (1) to simplify the process of chunk matching by proposing a fast weak hash called FastFP that is piggybacked on the rolling hashes from CDC, and redesigning the delta sync protocol by exploiting deduplication locality and weak/strong hash properties";" (2) to minimize the sync time by adaptively choosing chunking parameters and compression methods according to the current network conditions. Our evaluation results driven by both benchmark and real-world datasets suggest NetSync performs $2\times$2×–$10\times$10× faster and supports $30\%$30%–$80\%$80% more clients than the state-of-the-art rsync-based WebR2sync+ and deduplication-based approach."",""1558-2183"","""",""10.1109/TPDS.2022.3145025"",""NSFC(grant numbers:61972441)"; Shenzhen Science and Technology(grant numbers:JCYJ20190806143405318,JCYJ20200109113427092,GXWD20201230155427003-20200821172511002); Guangdong Basic and Applied Basic Research Foundation(grant numbers:2021A1515012634); State Key Laboratory of Computer Architecture(grant numbers:CARCHA202006);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9693219"",""rsync";content-defined chunking;network adaptive;"cloud storage"",""Synchronization";Servers;Cloud computing;Bandwidth;Adaptive systems;Redundancy;"Power capacitors"","""",""3"","""",""44"",""IEEE"",""25 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Network Cost-Aware Geo-Distributed Data Analytics System,""K. Oh"; M. Zhang; A. Chandra;" J. Weissman"",""Department of Computer Science, University of Nebraska Omaha, Omaha, NE, USA"; Department of Computer Science, University of Nebraska Omaha, Omaha, NE, USA; Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, MN, USA;" Department of Computer Science and Engineering, University of Minnesota Twin Cities, Minneapolis, MN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1407"",""1420"",""Many geo-distributed data analytics (GDA) systems have focused on the network performance-bottleneck: inter-data center network bandwidth to improve performance. Unfortunately, these systems may encounter a cost-bottleneck (${\$}$$) because they have not considered data transfer cost (${\$}$$), one of the most expensive and heterogeneous resources in a multi-cloud environment. In this article, we present Kimchi, a network cost-aware GDA system to meet the cost-performance tradeoff by exploiting data transfer cost heterogeneity to avoid the cost-bottleneck. Kimchi determines cost-aware task placement decisions for scheduling tasks given inputs including data transfer cost, network bandwidth, input data size and locations, and desired cost-performance tradeoff preference. In addition, Kimchi is also mindful of data transfer cost in the presence of dynamics. Kimchi has been applied to two common GDA MapReduce models: synchronous barrier and asynchronous push-based shuffle. A Kimchi prototype has been implemented on Spark, and experiments show that it reduces cost by 5% $\scriptstyle \sim$∼ 24% without impacting performance and reduces query execution time by 45% $\scriptstyle \sim$∼ 70% without impacting cost compared to other baseline approaches centralized, vanilla Spark, and bandwidth-aware (e.g., Iridium). More importantly, Kimchi allows applications to explore a much richer cost-performance tradeoff space in a multi-cloud environment."",""1558-2183"","""",""10.1109/TPDS.2021.3108893"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9527073"",""Geo-distributed data";multi-DCs;multi cloud providers;"data analytics system"",""Task analysis";Data transfer;Bandwidth;Wide area networks;Sparks;Iridium;"Distributed databases"","""",""2"","""",""49"",""IEEE"",""1 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"NITI: Training Integer Neural Networks Using Integer-Only Arithmetic,""M. Wang"; S. Rasoulinezhad; P. H. W. Leong;" H. K. . -H. So"",""ACCESS – AI Chip Center for Emerging Smart Systems, InnoHK Centers, Hong Kong Science Park, Hong Kong, China"; School of Electrical and Information Engineering, The University of Sydney, Camperdown, NSW, Australia; School of Electrical and Information Engineering, The University of Sydney, Camperdown, NSW, Australia;" Department of Electrical and Electronic Engineering, University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3249"",""3261"",""Low bitwidth integer arithmetic has been widely adopted in hardware implementations of deep neural network inference applications. However, despite the promised energy-efficiency improvements demanding edge applications, the use of low bitwidth integer arithmetic for neural network training remains limited. Unlike inference, training demands high dynamic range and numerical accuracy for high quality results, making the use of low-bitwidth integer arithmetic particularly challenging. To address this challenge, we present a novel neural network training framework called NITI that exclusively utilizes low bitwidth integer arithmetic. NITI stores all parameters and accumulates intermediate values as 8-bit integers while using no more than 5 bits for gradients. To provide the necessary dynamic range during the training process, a per-layer block scaling exponentiation scheme is utilized. By deeply integrating with the rounding procedures and integer entropy loss calculation, the proposed scaling scheme incurs only minimal overhead in terms of storage and additional computation. Furthermore, a hardware-efficient pseudo-stochastic rounding scheme that eliminates the need for external random number generation is proposed to facilitate conversion from wider intermediate arithmetic results to lower precision for storage. Since NITI operates only with standard 8-bit integer arithmetic and storage, it is possible to accelerate it using existing low bitwidth operators originally developed for inference in commodity accelerators. To demonstrate this, an open-source software implementation of end-to-end training, using native 8-bit integer operations in modern GPUs is presented. In addition, experiments have been conducted on an FPGA-based training accelerator to evaluate the hardware advantage of NITI. When compared with an equivalent training setup implemented with floating point storage and arithmetic, NITI has no accuracy degradation on the MNIST and CIFAR10 datasets. On ImageNet, NITI achieves similar accuracy as state-of-the-art integer training frameworks without relying on full-precision floating-point first and last layers."",""1558-2183"","""",""10.1109/TPDS.2022.3149787"",""Innovation Award"; ACCESS – AI Chip Center for Emerging Smart Systems; Innovation and Technology Fund;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9709160"",""Neural network training";integer arithmetic;NITI;GPU;tensor core;"hardware accelerator"",""Training";Arithmetic;Neural networks;Dynamic range;Degradation;Tensors;"Standards"","""",""9"","""",""41"",""IEEE"",""9 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"OCTOPUS: Overcoming Performance and Privatization Bottlenecks in Distributed Learning,""S. Wang"; S. Nepal; K. Moore; M. Grobler; C. Rudolph;" A. Abuadbba"",""CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia"; CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia; CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia; CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia; Faculty of Information Technology, Monash University, Melbourne, VIC, Australia;" CSIRO's Data61 and Cybersecurity CRC, Sydney, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2022"",""2022"",""33"",""12"",""3460"",""3477"",""The diversity and quantity of data warehouses, gathering data from distributed devices such as mobile devices, can enhance the success and robustness of machine learning algorithms. Federated learning enables distributed participants to collaboratively learn a commonly shared model while holding data locally. However, it is also faced with expensive communication and limitations due to the heterogeneity of distributed data sources and lack of access to global data. In this paper, we investigate a practical distributed learning scenario where multiple downstream tasks (e.g., classifiers) could be efficiently learned from dynamically updated and non-iid distributed data sources while providing local data privatization. We introduce a new distributed/collaborative learning scheme to address communication overhead via latent compression, leveraging global data while providing privatization of local data without additional cost due to encryption or perturbation. This scheme divides learning into (1) informative feature encoding, and transmitting the latent representation of local data to address communication overhead";" (2) downstream tasks centralized at the server using the encoded codes gathered from each node to address computing overhead. Besides, a disentanglement strategy is applied to address the privatization of sensitive components of local data. Extensive experiments are conducted on image and speech datasets. The results demonstrate that downstream tasks with the compact latent representations with the privatization of local data can achieve comparable accuracy to centralized learning."",""1558-2183"","""",""10.1109/TPDS.2022.3157258"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729632"",""Distributed learning";data collection;representation learning;disentanglement;"privatization"",""Distributed databases";Task analysis;Servers;Privatization;Data models;Dictionaries;"Training"","""",""1"","""",""70"",""IEEE"",""7 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"On Mixing Eventual and Strong Consistency: Acute Cloud Types,""M. Kokociński"; T. Kobus;" P. T. Wojciechowski"",""Institute of Computing Science, Poznan University of Technology, Poznań, Poland"; Institute of Computing Science, Poznan University of Technology, Poznań, Poland;" Institute of Computing Science, Poznan University of Technology, Poznań, Poland"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1338"",""1356"",""In this article we study the properties of distributed systems that mix eventual and strong consistency. We formalize such systems through acute cloud types (ACTs), abstractions similar to conflict-free replicated data types (CRDTs), which by default work in a highly available, eventually consistent fashion, but which also feature strongly consistent operations for tasks which require global agreement. Unlike other mixed-consistency solutions, ACTs can rely on efficient quorum-based protocols, such as Paxos. Hence, ACTs gracefully tolerate machine and network failures also for the strongly consistent operations. We formally study ACTs and demonstrate phenomena which are neither present in purely eventually consistent nor strongly consistent systems. In particular, we identify temporary operation reordering, which implies interim disagreement between replicas on the relative order in which the client requests were executed. When not handled carefully, this phenomenon may lead to undesired anomalies, including circular causality. We prove an impossibility result which states that temporary operation reordering is unavoidable in mixed-consistency systems with sufficiently complex semantics. Our result is startling, because it shows that apparent strengthening of the semantics of a system (by introducing strongly consistent operations to an eventually consistent system) results in the weakening of the guarantees on the eventually consistent operations."",""1558-2183"","""",""10.1109/TPDS.2021.3090318"",""Fundacja na rzecz Nauki Polskiej"; European Commission; European Regional Development Fund(grant numbers:POIR.04.04.00-00-5C5B/17-00); Narodowe Centrum Nauki(grant numbers:DEC-2012/07/B/ST6/01230); Politechnika Poznańska;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459528"",""Eventual consistency";mixed consistency;fault-tolerance;acute cloud types;"ACT"",""Semantics";Protocols;Synchronization;Data structures;Reactive power;Servers;"Scalability"","""",""1"","""",""79"",""IEEE"",""17 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"On the Analysis of Cache Invalidation With LRU Replacement,""Q. Zheng"; T. Yang; Y. Kan; X. Tan; J. Yang;" X. Jiang"",""Department of Automation, University of Science and Technology of China, Hefei, China"; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China; Department of Automation, University of Science and Technology of China, Hefei, China;" Department of Automation, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2021"",""2022"",""33"",""3"",""654"",""666"",""Caching contents close to end-users can improve the network performance, while causing the problem of guaranteeing consistency. Specifically, solutions are classified into validation and invalidation, the latter of which can provide strong cache consistency strictly required in some scenarios. To date, little work on the analysis of cache invalidation has been covered. In this work, by using conditional probability to characterize the interactive relationship between existence and validity, we develop an analytical model that evaluates the performance (hit probability and server load) of four different invalidation schemes with LRU replacement under arbitrary invalidation frequency distribution. The model allows us to theoretically identify some key parameters that affect our metrics of interest and gain some common insights on parameter settings to balance the performance of cache invalidation. Compared with other cache invalidation models, our model can achieve higher accuracy in predicting the cache hit probability. We also conduct extensive simulations that demonstrate the achievable performance of our model."",""1558-2183"","""",""10.1109/TPDS.2021.3098459"",""National Key Research and Development Program of China(grant numbers:2020YFA0711400)"; Key Technologies Research and Development Program of Anhui Province(grant numbers:202004a05020078); CETC Joint Advanced Reasearch Foundation(grant numbers:6141B08080101);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492756"",""LRU";cache;consistency;invalidation;hit probability;"server load"",""Servers";Analytical models;Load modeling;Computational modeling;Mathematical model;Predictive models;"Numerical models"","""",""5"","""",""30"",""IEEE"",""21 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"On the Benefits of Multiple Gossip Steps in Communication-Constrained Decentralized Federated Learning,""A. Hashemi"; A. Acharya; R. Das; H. Vikalo; S. Sanghavi;" I. Dhillon"",""School of Electrical and Computer Engineering, Purdue University, West Lafayette, IN, USA"; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA; Department of Computer Science, University of Texas at Austin, Austin, TX, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA; Department of Electrical and Computer Engineering, University of Texas at Austin, Austin, TX, USA;" Department of Computer Science, University of Texas at Austin, Austin, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2727"",""2739"",""Federated learning (FL) is an emerging collaborative machine learning (ML) framework that enables training of predictive models in a distributed fashion where the communication among the participating nodes are facilitated by a central server. To deal with the communication bottleneck at the server, decentralized FL (DFL) methods advocate rely on local communication of nodes with their neighbors according to a specific communication network. In DFL, it is common algorithmic practice to have nodes interleave (local) gradient descent iterations with gossip (i.e., averaging over the network) steps. As the size of the ML models grows, the limited communication bandwidth among the nodes does not permit communication of full-precision messages";" hence, it is becoming increasingly common to require that messages be lossy, compressed versions of the local parameters. The requirement of communicating compressed messages gives rise to the important question: given a fixed communication budget, what should be our communication strategy to minimize the (training) loss as much as possible? In this article, we explore this direction, and show that in such compressed DFL settings, there are benefits to having multiple gossip steps between subsequent gradient iterations, even when the cost of doing so is appropriately accounted for, e.g., by means of reducing the precision of compressed information. In particular, we show that having ${\mathcal O}(\log \frac{1}{\epsilon })$O(log1ε) gradient iterations with constant step size - and ${\mathcal O}(\log \frac{1}{\epsilon })$O(log1ε) gossip steps between every pair of these iterations - enables convergence to within $\epsilon$ε of the optimal value for a class of non-convex problems that arise in the training of deep learning models, namely, smooth non-convex objectives satisfying Polyak-Łojasiewicz condition. Empirically, we show that our proposed scheme bridges the gap between centralized gradient descent and DFL on various machine learning tasks across different network topologies and compression operators."",""1558-2183"","""",""10.1109/TPDS.2021.3138977"",""National Science Foundation(grant numbers:ECCS-1809327,CCF-1564000,IIS-1546452,HDR-1934932)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664349"",""Federated learning";decentralized learning;communication-constrained distributed optimization;compressed communication;"nonconvex optimization"",""Optimization";Convergence;Task analysis;Training;Signal processing algorithms;Linear programming;"Collaborative work"","""",""6"","""",""35"",""IEEE"",""28 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Online Learning for Distributed Computation Offloading in Wireless Powered Mobile Edge Computing Networks,""X. Wang"; Z. Ning; L. Guo; S. Guo; X. Gao;" G. Wang"",""School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China"; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Communication and Information Engineering, Chongqing University of Posts and Telecommunications, Chongqing, China; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong, China; Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and Telecommunications, Chongqing, China;" Chongqing Key Laboratory of Computational Intelligence, Chongqing University of Posts and Telecommunications, Chongqing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Dec 2021"",""2022"",""33"",""8"",""1841"",""1855"",""A novel paradigm named Wireless Powered Mobile Edge Computing (WP-MEC) emerges recently, which integrates Mobile Edge Computing (MEC) and Wireless Power Transfer (WPT) technologies. It enables mobile clients to both extend their computing capacities by task offloading, and charge from edge servers via energy transmission. Existing studies generally focus on the centralized design of task scheduling and energy charging in WP-MEC networks. To meet the decentralization requirement of the near-coming 6G network, we propose an online learning algorithm for computation offloading in WP-MEC networks with a distributed execution manner. Specifically, we first define the delay minimization problem by considering task deadline and energy constraints. Then, we transform it into a primal-dual optimization problem based on the Bellman equation. After that, we design a novel neural model that learns both offloading and time division decisions in each time slot to solve the formulated optimization problem. To train and execute the designed algorithm distributivity, we form multiple learning models decentralized on edge servers and they work coordinately to achieve parameter synchronization. At last, both theoretical and performance analyses show that the designed algorithm has significant advantages in comparison with other representative schemes."",""1558-2183"","""",""10.1109/TPDS.2021.3129618"",""National Natural Science Foundation of China(grant numbers:61872310,61971084,62025105,62001073)"; Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,15220320/20E); Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:R2020A045); Natural Science Foundation of Chongqing(grant numbers:cstc2019jcyj-cxttX0002,cstc2021ycjh-bgzxm0013,cstc2021ycjh-bgzxm0039,cstc2021jcyj-msxmX0031); Chongqing Municipal Education Commission(grant numbers:HZ2021008,CXQT21019); Support Program for Overseas Students to Return to China for Entrepreneurship and Innovation(grant numbers:cx2021003,cx2021053);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623474"",""Wireless powered mobile edge computing";delay minimization;distributed execution;"online learning"",""Task analysis";Servers;Processor scheduling;Delays;Optimization;Resource management;"Multi-access edge computing"","""",""26"","""",""31"",""IEEE"",""22 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Online Orchestration of Collaborative Caching for Multi-Bitrate Videos in Edge Computing,""S. Yang"; L. Jiao; R. Yahyapour;" J. Cao"",""School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China"; Department of Computer and Information Science, University of Oregon, Eugene, OR, USA; Gesellschaft für wissenschaftliche Datenverarbeitung mbH Göttingen (GWDG), and Institute of Computer Science, University of Göttingen, Göttingen, Germany;" Department of Computing, The Hong Kong Polytechnic University, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4207"",""4220"",""In the traditional video streaming service provisioning paradigm, users typically request video contents through nearby Content Delivery Network (CDN) server(s). However, because of the uncertain wide area networks delays, the (remote) users usually suffer from long video streaming delay, which affects the quality of experience. Multi-Access Edge Computing (MEC) offers caching infrastructures in closer proximity to end users than conventional Content Delivery Networks (CDNs). Yet, for video caching, MEC's potential has not been fully unleashed as it overlooks the opportunities of collaborative caching and multi-bitrate video transcoding. In this paper, we model and formulate an Integer Linear Program (ILP) to capture the long-term cost minimization problem for caching videos at MEC, allowing joint exploitation of MEC with CDN and real-time video transcoding to satisfy arbitrary user demands. While this problem is intractable and couples the caching decisions for adjacent time slots, we design a polynomial-time online orchestration framework which first relaxes and carefully decomposes the problem into a series of subproblems solvable in each individual time slot and then converts the fractional solutions into integers without violating constraints. We have formally proved a parameterized-constant competitive ratio as the performance guarantee for our approach, and also conducted extensive evaluations to confirm its superior practical performance. Simulation results demonstrate that our proposed algorithm outperforms the state-of-the-art algorithms, with 13.6% improvement on average in terms of total cost."",""1558-2183"","""",""10.1109/TPDS.2022.3182022"",""National Natural Science Foundation of China(grant numbers:62172038,61802018)"; Beijing Institute of Technology Research Fund Program for Young Scholars; National Science Foundation(grant numbers:CNS-2047719); Hong Kong RGC TRS(grant numbers:T-41-603/20R); Hong Kong GRC GRF PolyU(grant numbers:15217919);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795154"",""Multi-Access edge computing";multi-bitrate video;online caching;"quality of experience"",""Videos";Costs;Quality of experience;Delays;Cloud computing;Bit rate;"Servers"","""",""6"","""",""39"",""IEEE"",""13 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Online Power Management for Multi-Cores: A Reinforcement Learning Based Approach,""Y. Wang"; W. Zhang; M. Hao;" Z. Wang"",""School of Cyberspace Science, Harbin Institute of Technology, Harbin, China"; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China;" School of Computing, University of Leeds, Leeds, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""751"",""764"",""Power and energy is the first-class design constraint for multi-core processors and is a limiting factor for future-generation supercomputers. While modern processor design provides a wide range of mechanisms for power and energy optimization, it remains unclear how software can make the best use of them. This article presents a novel approach for runtime power optimization on modern multi-core systems. Our policy combines power capping and uncore frequency scaling to match the hardware power profile to the dynamically changing program behavior at runtime. We achieve this by employing reinforcement learning (RL) to automatically explore the energy-performance optimization space from training programs, learning the subtle relationships between the hardware power profile, the program characteristics, power consumption and program running times. Our RL framework then uses the learned knowledge to adapt the chip's power budget and uncore frequency to match the changing program phases for any new, previously unseen program. We evaluate our approach on two computing clusters by applying our techniques to 11 parallel programs that were not seen by our RL framework at the training stage. Experimental results show that our approach can reduce the system-level energy consumption by 12 percent, on average, with less than 3 percent of slowdown on the application performance. By lowering the uncore frequency to leave more energy budget to allow the processor cores to run at a higher frequency, our approach can reduce the energy consumption by up to 17 percent while improving the application performance by 5 percent for specific workloads."",""1558-2183"","""",""10.1109/TPDS.2021.3092270"",""National Key Research and Development Program of China(grant numbers:2020YFB1406902)"; Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101360001); Shenzhen Science and Technology Research and Development Fundation(grant numbers:JCYJ20190806143418198); National Natural Science Foundation of China(grant numbers:61872110);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9465707"",""Power management";multi-cores;reinforcement learning;power capping;uncore frequency;"phase change detection"",""Hardware";Optimization;Training;Runtime;Power system management;Power demand;"Energy consumption"","""",""5"","""",""72"",""IEEE"",""25 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Online Pricing and Trading of Private Data in Correlated Queries,""H. Cai"; F. Ye; Y. Yang; Y. Zhu; J. Li;" F. Xiao"",""College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China"; Department of Electrical and Computer Engineering, Stony Brook University, New York, NY, USA; Department of Electrical and Computer Engineering, Stony Brook University, New York, NY, USA; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China;" College of Computer, Nanjing University of Posts and Telecommunications, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""6 Aug 2021"",""2022"",""33"",""3"",""569"",""585"",""With the commoditization of private data, data trading in consideration of user privacy protection has become a fascinating research topic. The trading for private web browsing histories brings huge economic value to data consumers when leveraged by targeted advertising. And the online pricing of these private data further helps achieve more realistic data trading. In this paper, we study the trading and pricing of multiple correlated queries on private web browsing history data at the same time. We propose CTRADE, which is a novel online data CommodiTization fRamework for trAding multiple correlateD queriEs over private data. CTRADE first devises a modified matrix mechanism to perturb query answers. It especially quantifies privacy loss under the relaxation of classical differential privacy and a newly devised mechanism with relaxed matrix sensitivity, and further compensates data owners for their diverse privacy losses in a satisfying manner. CTRADE then proposes an ellipsoid-based query pricing mechanism according to a given linear market value model, which exploits the features of the ellipsoid to explore and exploit the close-optimal dynamic price at each round. In particular, the proposed mechanism produces a low cumulative regret, which is quadratic in the dimension of the feature vector and logarithmic in the number of total rounds. Through real-data based experiments, our analysis and evaluation results demonstrate that CTRADE balances total error and privacy preferences well within acceptable running time, indeed produces a convergent cumulative regret with more rounds, and also achieves all desired economic properties of budget balance, individual rationality, and truthfulness."",""1558-2183"","""",""10.1109/TPDS.2021.3095238"",""2030 National Key AI Program of China(grant numbers:2018AAA0100503,2018AAA0100500)"; National Natural Science Foundation of China(grant numbers:61772341,61472254,61772338,61672240); Science and Technology Commission of Shanghai Municipality(grant numbers:18511103002,19510760500,19511101500); Program for Changjiang Young Scholars in University of China; Program for China Top Young Talents; Program for Shanghai Top Young Talents; SJTU Global Strategic Partnership Fund 2019 SJTU-HKUST; NUPTSF(grant numbers:NY220134);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477119"",""Data trading";data pricing;web browsing history;"data privacy"",""Pricing";Privacy;Distributed databases;History;Economics;Correlation;"Time complexity"","""",""8"","""",""43"",""Crown"",""7 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Online Reconfiguration of IoT Applications in the Fog: The Information-Coordination Trade-Off,""B. Donassolo"; A. Legrand; P. Mertikopoulos;" I. Fajjari"",""Orange Labs, Châtillon, France"; CNRS, Inria, Grenoble INP, LIG, Université Grenoble Alpes, Grenoble, France; CNRS, Inria, Grenoble INP, LIG, Université Grenoble Alpes, Grenoble, France;" Orange Labs, France"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1156"",""1172"",""The evolution of the Internet of Things (IoT) is driving an extraordinary growth of traffic and processing demands, persuading 5G players to change their infrastructures. In this context, Fog computing emerges as a potential solution, providing nearby resources to run IoT applications. However, the Fog raises several challenges which hinders its adoption. In this article, we consider the reconfiguration problem, i.e., how to dynamically adapt the placement of IoT applications running on the Fog, depending on application needs and evolution of resource usage. We propose and evaluate a series of reconfiguration algorithms, based on both online scheduling and online learning approaches. Through an extensive set of experiments in a realistic testbed, we demonstrate that the performance strongly depends on the quality and availability of information from both Fog infrastructure and IoT applications. This information mainly concerns the application’s resource usage (estimated by the user during the design of the application) and the availability of resources in the infrastructure (collected by commercial off-the-shelf monitoring tools). Finally, we show that a reactive and greedy strategy, which relies on this additional information, can overcome the performance of state-of-the-art online learning algorithms, even in a scenario with inaccurate information."",""1558-2183"","""",""10.1109/TPDS.2021.3097281"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9485041"",""Fog computing";Internet of Things;reconfiguration;online learning;"online scheduling"",""Internet of Things";Sensors;Cloud computing;Quality of service;Performance evaluation;Automobiles;"Sensor phenomena and characterization"","""",""3"","""",""32"",""IEEE"",""14 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Online Thread Auto-Tuning for Performance Improvement and Resource Saving,""G. Luan"; P. Pang; Q. Chen; S. Xue; Z. Song;" M. Guo"",""Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China; Alibaba Cloud, Hangzhou, Zhejiang, China; Alibaba Cloud, Hangzhou, Zhejiang, China;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Jul 2022"",""2022"",""33"",""12"",""3746"",""3759"",""Multi-threading is a common way for programs to benefit from the multi/many-core design. However, the performance of some parallel programs does not increase/even decrease as the number of cores/threads increases. Our study shows that the performance of a parallel program is impacted by the number of cores/threads, the thread placement, the inputs of the program. It is nontrivial to identify the optimal number of cores and the corresponding thread placement to maximize the performance, when the input of a program is determined online and the workload of different iterations may not be identical. To resolve the above problem, we propose Otter, a thread auto-tuning system at runtime for iterative parallel programs. Otter collects the runtime information in the first few iterations and makes decisions on the number of threads and thread placement policy to achieve the goal of improving performance or saving resources. It considers the characteristics of dynamic workload in the iteration process and reduces the time overhead through a migration method. Experiments on a 96-core machine show that Otter improves the performance of the benchmarks by 20.7% and reduces core hours by 51.3% on average compared to the case of running them with all the CPU cores."",""1558-2183"","""",""10.1109/TPDS.2022.3169410"",""National Natural Science Foundation of China(grant numbers:62022057,61832006,61872240)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9762963"",""Iteration";runtime;thread placement;thread scalability;"parallel"",""Optimization";Instruction sets;Scalability;Market research;Benchmark testing;Tuning;"Runtime"","""",""2"","""",""44"",""IEEE"",""26 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimal Checkpointing Strategies for Iterative Applications,""Y. Du"; L. Marchal; G. Pallez;" Y. Robert"",""Tongji University, Shanghai, China"; CNRS & Inria, LIP, École Normale Supérieure de Lyon, Lyon, France; Inria & Université de Bordeaux, Talence, France;" CNRS & Inria, LIP, École Normale Supérieure de Lyon, Lyon, France"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Aug 2021"",""2022"",""33"",""3"",""507"",""522"",""This work provides an optimal checkpointing strategy to protect iterative applications from fail-stop errors. We consider a general framework, where the application repeats the same execution pattern by executing consecutive iterations, and where each iteration is composed of several tasks. These tasks have different execution lengths and different checkpoint costs. Assume that there are n tasks and that task ai, where 0 ≤ i <";" n, has execution time ti and checkpoint cost ci. A naive strategy would checkpoint after each task. Another naive strategy would checkpoint at the end of each iteration. A strategy inspired by the Young/Daly formula would work for √{2 μcave } seconds, where μ is the application MTBF and cave is the average checkpoint time, and checkpoint at the end of the current task (and repeat). Another strategy, also inspired by the Young/Daly formula, would select the task amin with smallest checkpoint cost cmin and would checkpoint after every pth instance of that task, leading to a checkpointing period p T, where T = Σi=0n-1 ai is the time per iteration. One would choose the period so that p T ≈ √{2 μcmin} to obey the Young/Daly formula. All these naive and Young/Daly strategies are suboptimal. Our main contribution is to show that the optimal checkpoint strategy is globally periodic, and to design a dynamic programming algorithm that computes the optimal checkpointing pattern. This pattern may well checkpoint many different tasks, and this across many different iterations. We show through simulations, both from synthetic and real-life application scenarios, that the optimal strategy outperforms the naive and Young/Daly strategies."",""1558-2183"","""",""10.1109/TPDS.2021.3099440"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9495174"",""Iterative application";checkpoint strategy;fail-stop error;"resilience"",""Task analysis";Checkpointing;Iterative methods;Heuristic algorithms;Fault tolerant systems;Fault tolerance;"Dynamic programming"","""",""2"","""",""49"",""IEEE"",""26 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimal Convex Hull Formation on a Grid by Asynchronous Robots With Lights,""R. Hector"; R. Vaidyanathan; G. Sharma;" J. L. Trahan"",""Division of Electrical & Computer Engineering Louisiana State University, Baton Rouge, LA, USA"; Division of Electrical & Computer Engineering Louisiana State University, Baton Rouge, LA, USA; Department of Computer Science, Kent State University, Kent, OH, USA;" Division of Electrical & Computer Engineering Louisiana State University, Baton Rouge, LA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2022"",""2022"",""33"",""12"",""3532"",""3545"",""We consider the distributed setting of $n$n autonomous mobile robots that operate in Look-Compute-Move cycles and communicate with other robots using a constant number of colored lights (the robots with lights model). We assume obstructed visibility where collinear robots do not see each other. In addition, we consider a grid-based terrain embedded in the 2-dimensional euclidean plane. The Convex Hull Formation problem is to relocate the $n$n robots (starting at arbitrary, but distinct, initial positions) so that each robot is positioned on a vertex of a convex hull. In this article, we provide a framework for solving Convex Hull Formation. We then provide four asynchronous algorithms under this framework. Key measures of the algorithms’ performance include the time taken and the space occupied. The presented algorithms are randomized and their time bounds hold with high probability. The first $O(\max \lbrace n^{2},D\rbrace)$O(max{n2,D})-time, $O({n^{2}})$O(n2)-perimeter, and $O({n^{3}})$O(n3)-area algorithm serves to introduce key ideas, where $D$D is the diameter of the initial configuration. The subsequent algorithms, differing in computational requirements, run in $O(\max \lbrace n^{\frac{3}{2}},D\rbrace)$O(max{n32,D}) time with a perimeter of $O(n^{\frac{3}{2}})$O(n32) and area of $O(n^{3})$O(n3). We also prove lower bounds of $\Omega (n^{\frac{3}{2}})$Ω(n32) for time and perimeter and $\Omega (n^{3})$Ω(n3) for area, for any Convex Hull Formation algorithm";" i.e., our $O(\max \lbrace n^{\frac{3}{2}},D\rbrace)-$O(max{n32,D})-time algorithm is optimal in time, perimeter, and area."",""1558-2183"","""",""10.1109/TPDS.2022.3158202"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9732663"",""Autonomous robots";robots with lights;complete visibility;convex hull;grid;area;perimeter;time;"colors"",""Robots";Robot kinematics;Robot sensing systems;Color;Computational modeling;Service robots;"Voting"","""",""3"","""",""30"",""IEEE"",""10 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimal Embedding of Aggregated Service Function Tree,""D. Guo"; B. Ren; G. Tang; L. Luo; T. Chen;" X. Fu"",""Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China"; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, Hunan, China;" University of Göttingen, Göttingen, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Apr 2022"",""2022"",""33"",""10"",""2584"",""2596"",""Many hardware-based security middleboxes have been deployed in the networks to defend against different threats. However, these hardware middleboxes are hard to upgrade or migrate. The emergence of network functions virtualization (NFV), which realizes various security functions in the form of virtual network functions (VNFs), brings many benefits to network security. To improve the security level further, several VNFs are coordinated in a pre-defined order to form service function chains (SFCs). It is expected that the SFCs are embedded properly with low cost, including the VNF setup cost and the flow routing cost. In this paper, we find that when an SFC is required by multiple flows for the identical network security threats, the total cost could be reduced by embedding an aggregated service function tree (ASFT) instead of multiple independent SFCs. We formally characterize the integer programming model of this problem and prove that it is NP-hard. Then we propose a performance-guaranteed approximation algorithm and prove that the algorithm could find the optimal solution in a special case. Extensive experiments indicate that our method can reduce the total cost by $22.0\%$22.0% and $24.1\%$24.1% against two compared algorithms, respectively."",""1558-2183"","""",""10.1109/TPDS.2022.3147870"",""National Key Research and Development Program of China(grant numbers:2018YFE0207600,2020YFE0200500)"; National Natural Science Foundation of China(grant numbers:U19B2024,61802421); European Union’s Horizon 2020 Research and Innovation Program; Marie Sklodowska-Curie(grant numbers:824019);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9698997"",""Network functions virtualization";service function chain;"approximation algorithm"",""Costs";Security;Approximation algorithms;Routing;Network security;Middleboxes;"Computer crime"","""","""","""",""33"",""IEEE"",""1 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimal Repair-Scaling Trade-off in Locally Repairable Codes: Analysis and Evaluation,""S. Wu"; Z. Shen; P. P. C. Lee;" Y. Xu"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"; College of Informatics, Xiamen University, Xiamen, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Jun 2021"",""2022"",""33"",""1"",""56"",""69"",""How to improve the repair performance of erasure-coded storage is a critical issue for maintaining high reliability of modern large-scale storage systems. Locally repairable codes (LRC) are one popular family of repair-efficient erasure codes that mitigate the repair bandwidth and are deployed in practice. To adapt to the changing demands of access efficiency and fault tolerance, modern storage systems also conduct frequent scaling operations on erasure-coded data. In this article, we analyze the optimal trade-off between the repair and scaling performance of LRC in clustered storage systems. Specifically, we focus on two optimal repair-scaling trade-offs, and design placement strategies that operate along the two optimal repair-scaling trade-off curves subject to the fault tolerance constraints. We prototype and evaluate our placement strategies on a LAN testbed, and show that they outperform the conventional placement schemes in repair and scaling operations."",""1558-2183"","""",""10.1109/TPDS.2021.3087352"",""NSFC(grant numbers:61832011,62072381)"; CCF-Tencent Open Fund WeBank Special Fund(grant numbers:2021KF0AB01); Fundamental Research Funds for the Central Universities(grant numbers:WK2150110022);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9448455"",""LRC";repair;scaling;"clustered storage"",""Maintenance engineering";Encoding;Fault tolerant systems;Bandwidth;Redundancy;Switches;"Computer science"","""",""2"","""",""36"",""IEEE"",""8 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimization of Reactive Force Field Simulation: Refactor, Parallelization, and Vectorization for Interactions,""P. Gao"; X. Duan; B. Schmidt; W. Zhang; L. Gan; H. Fu; W. Xue; W. Liu;" G. Yang"",""School of Software, Shandong University, Jinan, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Institute for Computer Science, Johannes Gutenberg University, Mainz, Germany; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Earth System Science, Ministry of Education Key Laboratory for Earth System Modeling, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; School of Software, Shandong University, Jinan, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2021"",""2022"",""33"",""2"",""359"",""373"",""Molecular dynamics (MD) simulations are playing an increasingly important role in many areas ranging from chemical materials to biological molecules. With the continuing development of MD models, the potentials are getting larger and more complex. In this article, we focus on the reactive force field (ReaxFF) potential from LAMMPS to optimize the computation of interactions. We present our efforts on refactoring for neighbor list building, bond order computation, as well as valence angles and torsion angles computation. After redesigning these kernels, we develop a vectorized implementation for non-bonded interactions, which is nearly 100 × faster than the management processing element (MPE) on the Sunway TaihuLight supercomputer. Furthermore, we have implemented the three-body-list free torsion angles computation, and propose a line-locked software cache method to eliminate write conflicts in the torsion angle and valence angle interactions resulting in an order-of-magnitude speedup on a single Sunway TaihuLight node. In addition, we achieve a speedup of up to 3.5 compared to the KOKKOS package on an Intel Xeon Gold 6148 core. When executed on 1,024 processes, our implementation enables the simulation of 21,233,664 atoms on 66,560 cores with a performance of 0.032 ns/day and a weak scaling efficiency of 95.71 percent."",""1558-2183"","""",""10.1109/TPDS.2021.3091408"",""National Key Research and Development Program of China(grant numbers:2020YFB0204700)"; National Natural Science Foundation of China(grant numbers:61972231,U1806205); Key Project of Joint Fund of Shandong Province(grant numbers:ZR2019LZH007); Key Technology Research and Development Program of Shandong(grant numbers:2018CXGC1211); CSC and DAAD; Center for High Performance Computing and System Simulation; Pilot National Laboratory for Marine Science and Technology; Engineering Research Center of Digital Media Technology, Ministry of Education;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462358"",""High performance computing";molecular dynamics;computational science;"Sunway TaihuLight supercomputer (TaihuLight)"",""Mathematical model";Optimization;Software;Force;Supercomputers;Computational modeling;"Chemicals"","""",""3"","""",""34"",""IEEE"",""22 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Optimized Page Fault Handling During RDMA,""A. Psistakis"; N. Chrysos; F. Chaix; M. Asiminakis; M. Gianioudis; P. Xirouchakis; V. Papaefstathiou;" M. Katevenis"",""Department of Computer Science, University of Illinois at Urbana-Champaign (UIUC), Urbana, IL, USA"; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece; Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece;" Institute of Computer Science, Foundation for Research and Technology-Hellas (ICS-FORTH), Heraklion, Crete, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""3990"",""4005"",""Remote Direct Memory Access (RDMA) is widely used in High-Performance Computing (HPC) while making inroads in datacenters and accelerators. State-of-the-art RDMA engines typically do not endure page faults, therefore users are forced to pin their buffers, which complicates the programming model, limits the memory utilization, and moves the pressure to the Network Interface Cards (NICs). In this article we introduce a mechanism for handling dynamic page faults during RDMA, named PART, suitable for emerging processors that also integrate the Network Interface. PART leverages the IOMMU already present in modern processors for translations. PART avoids the pinning overheads, allows any buffer to be used for communication, and enables overlapping page fault handling with serving subsequent RDMA transfers. We implement and optimize PART for a cluster of ARMv8 cores with tightly-coupled network interfaces. Handling a minor page-fault of a small transfer at the destination takes approximately 38 $\mu$μsecs, while there is no performance degradation when running three full MPI applications in 16 nodes and 64 cores. Detailed breakdown uncovers the hardware and system software components of this overhead and was used to further optimize the system. A 4MB RDMA transfer performs 1.46x better over pinning."",""1558-2183"","""",""10.1109/TPDS.2022.3175666"",""European Commission(grant numbers:671553,754337)"; European High-Performance Computing Joint Undertaking(grant numbers:955776); European Union's Horizon 2020 Research and Innovation Programme;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779430"",""Page fault";RDMA;IOMMU;MPI;low-power ARM processors;"pinning avoidance"",""Program processors";Costs;Network interfaces;Memory management;Pins;Programming;"Optimization"","""","""","""",""45"",""CCBY"",""20 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Depthwise Separable Convolution Operations on GPUs,""G. Lu"; W. Zhang;" Z. Wang"",""School of Cyberspace Science, Harbin Institute of Technology, Harbin, China"; School of Cyberspace Science, Harbin Institute of Technology, Harbin, China;" School of Computing, University of Leeds, Leeds, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""28 Jun 2021"",""2022"",""33"",""1"",""70"",""87"",""The depthwise separable convolution is commonly seen in convolutional neural networks (CNNs), and is widely used to reduce the computation overhead of a standard multi-channel 2D convolution. Existing implementations of depthwise separable convolutions target accelerating model training with large batch sizes with a large number of samples to be processed at once. Such approaches are inadequate for small-batch-sized model training and the typical scenario of model inference where the model takes in a few samples at once. This article aims to bridge the gap of optimizing depthwise separable convolutions by targeting the GPU architecture. We achieve this by designing two novel algorithms to improve the column and row reuse of the convolution operation to reduce the number of memory operations performed on the width and the height dimensions of the 2D convolution. Our approach employs a dynamic tile size scheme to adaptively distribute the computational data across GPU threads to improve GPU utilization and to hide the memory access latency. We apply our approach on two GPU platforms: an NVIDIA RTX 2080Ti GPU and an embedded NVIDIA Jetson AGX Xavier GPU, and two data types: 32-bit floating point (FP32) and 8-bit integer (INT8). We compared our approach against cuDNN that is heavily tuned for the NVIDIA GPU architecture. Experimental results show that our approach delivers over 2× (up to 3×) performance improvement over cuDNN. We show that, when using a moderate batch size, our approach averagely reduces the end-to-end training time of MobileNet and EfficientNet by 9.7 and 7.3 percent respectively, and reduces the end-to-end inference time of MobileNet and EfficientNet by 12.2 and 11.6 percent respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3084813"",""National Key Research and Development Program of China(grant numbers:2017YFB0202901)"; Key-Area Research and Development Program of Guangdong Province(grant numbers:2019B010136001); National Natural Science Foundation of China(grant numbers:61672186,61872294); Shenzhen Technology Research and Development(grant numbers:JCYJ20190806143418198);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444208"",""Performance optimization";convolution;depthwise;pointwise;memory optimization;"GPU utilization"",""Convolution";Graphics processing units;Instruction sets;Kernel;Standards;Training;"Registers"","""",""19"","""",""55"",""IEEE"",""28 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing DNN Compilation for Distributed Training With Joint OP and Tensor Fusion,""X. Yi"; S. Zhang; L. Diao; C. Wu; Z. Zheng; S. Fan; S. Wang; J. Yang;" W. Lin"",""Computer Science, University of Hong Kong, Hong Kong, Hong Kong"; Computer Science, University of Hong Kong, Hong Kong, Hong Kong; Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China; Computer Science, University of Hong Kong, Hong Kong, Hong Kong; Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China; Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China; Compute Arch, NVIDIA Corp, Beijing, China;" Alibaba Cloud Intelligent, Alibaba Group, Hangzhou, Zhejiang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Sep 2022"",""2022"",""33"",""12"",""4694"",""4706"",""This article proposes DisCo, an automatic deep learning compilation module for data-parallel distributed training. Unlike most deep learning compilers that focus on training or inference on a single device, DisCo optimizes a DNN model for distributed training over multiple GPU machines. Existing single-device compilation strategies do not work well in distributed training, due mainly to communication inefficiency that they incur. DisCo generates optimized, joint computation operator and communication tensor fusion strategies to enable highly efficient distributed training. A GNN-based simulator is built to effectively estimate per-iteration training time achieved by operator/tensor fusion candidates. A backtracking search algorithm is driven by the simulator, navigating efficiently in the large strategy space to identify good operator/tensor fusion strategies that minimize distributed training time. We compare DisCo with existing DL fusion schemes and show that it achieves good training speed-up close to the ideal, full computation-communication overlap case."",""1558-2183"","""",""10.1109/TPDS.2022.3201531"",""Alibaba Group"; Hong Kong RGC contracts HKU(grant numbers:17204619,17208920);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9866830"",""Distributed systems";"machine learning"",""Training";Optimization;Tensors;Computational modeling;Codes;Performance evaluation;"Hardware"","""",""2"","""",""59"",""IEEE"",""25 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Error-Bounded Lossy Compression for Scientific Data With Diverse Constraints,""Y. Liu"; S. Di; K. Zhao; S. Jin; C. Wang; K. Chard; D. Tao; I. Foster;" F. Cappello"",""Department of Computer Science, University of Chicago, Chicago, IL, USA"; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Department of Computer Science and Engineering, University of California Riverside, Riverside, CA, USA; School of EECS, Washington State University, Pullman, WA, USA; Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA; School of EECS, Washington State University, Pullman, WA, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA;" Mathematics and Computer Science Division, Argonne National Laboratory, Lemont, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4440"",""4457"",""Vast volumes of data are produced by today's scientific simulations and advanced instruments. These data cannot be stored and transferred efficiently because of limited I/O bandwidth, network speed, and storage capacity. Error-bounded lossy compression can be an effective method for addressing these issues: not only can it significantly reduce data size, but it can also control the data distortion based on user-defined error bounds. In practice, many scientific applications have specific requirements or constraints for lossy compression, in order to guarantee that the reconstructed data are valid for post hoc analysis. For example, some datasets contain irrelevant data that should be isolated in particular and users often have intuition regarding value ranges, geospatial regions, and other data subsets that are crucial for subsequent analysis. Existing state-of-the-art error-bounded lossy compressors, however, do not consider these constraints during compression, resulting in inferior compression ratios with respect to user's post hoc analysis, due to the fact that the data itself provides little or no value for post hoc analysis. In this work we address this issue by proposing an optimized framework that can preserve diverse constraints during the error-bounded lossy compression, e.g., cleaning the irrelevant data, efficiently preserving different precision for multiple value intervals, and allowing users to set diverse precision over both regular and irregular regions. We perform our evaluation on a supercomputer with up to 2,100 cores. Experiments with six real-world applications show that our proposed diverse constraints based error-bounded lossy compressor can obtain a higher visual quality or data fidelity on reconstructed data with the same or even higher compression ratios compared with the traditional state-of-the-art compressor SZ. Our experiments also demonstrate very good scalability in compression performance compared with the I/O throughput of the parallel file system."",""1558-2183"","""",""10.1109/TPDS.2022.3194695"",""Exascale Computing Project(grant numbers:17-SC-20-SC)"; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); National Science Foundation(grant numbers:OAC-2003709,OAC-2003624/2042084,CSSI-2104023/2104024);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844293"",""Big data";error-bounded lossy compression;data reduction;"large-scale scientific simulation"",""Data models";Compressors;Quantization (signal);Predictive models;Analytical models;Encoding;"Dark matter"","""",""2"","""",""40"",""USGov"",""28 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Network Transfers for Data Analytic Jobs Across Geo-Distributed Datacenters,""L. Chen"; S. Liu;" B. Li"",""School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA"; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada;" Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2021"",""2022"",""33"",""2"",""403"",""414"",""It has become a recent trend that large volumes of data are generated, stored, and processed across geographically distributed datacenters. When popular data parallel frameworks, such as MapReduce and Spark, are employed to process such geo-distributed data, optimizing the network transfer in communication stages becomes increasingly crucial to application performance, as the inter-datacenter links have much lower bandwidth than intra-datacenter links. In this article, we focus on exploiting the flexibility of multi-path routing for inter-datacenter flows of data analytic jobs, with the hope of better utilizing inter-datacenter links and thus improve job performance. We design an optimal multi-path routing and scheduling strategy to achieve the best possible network performance for all concurrent jobs, based on our formulation of an optimization problem that can be transformed into an equivalent linear programming (LP) problem to be efficiently solved. As a highlight of this article, we have implemented our proposed algorithm in the controller of an application-layer software-defined inter-datacenter overlay testbed, designed to provide transfer optimization service for Spark jobs. With extensive evaluations of our real-world implementation on Google Cloud, we have shown convincing evidence that our optimal multi-path routing and scheduling strategies have achieved significant improvements in terms of job performance."",""1558-2183"","""",""10.1109/TPDS.2021.3093232"",""Louisiana Board of Regents(grant numbers:LEQSF(2019-22)-RD-A-21,LEQSF(2021-22)-RD-D-07)"; National Science Foundation(grant numbers:OIA-2019511);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468370"",""Geo-distributed datacenters";network transfer;data analytics;"optimization"",""Data analysis";Routing;Bandwidth;Task analysis;Sparks;Optimization;"Internet"","""",""7"","""",""39"",""IEEE"",""29 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Optimizing Video Caching at the Edge: A Hybrid Multi-Point Process Approach,""X. Zhang"; Y. Zhou; D. Wu; M. Hu; X. Zheng; M. Chen;" S. Guo"",""Department of Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China"; School of Computing, FSE, Macquarie University, Macquarie Park, NSW, Australia; Department of Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; Department of Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Computing, FSE, Macquarie University, Macquarie Park, NSW, Australia; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, Hubei, China;" Department of Computing, The Polytechnic University of Hong Kong, Hung Hom, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Apr 2022"",""2022"",""33"",""10"",""2597"",""2611"",""It is always a challenging problem to deliver a huge volume of videos over the Internet. To meet the high bandwidth and stringent playback demand, one feasible solution is to cache video contents on edge servers based on predicted video popularity. Traditional caching algorithms (e.g., LRU, LFU) are too simple to capture the dynamics of video popularity, especially long-tailed videos. Recent learning-driven caching algorithms (e.g., DeepCache) show promising performance, however, such black-box approaches are lack of explainability and interpretability. Moreover, the parameter tuning requires a large number of historical records, which are difficult to obtain for videos with low popularity. In this paper, we optimize video caching at the edge using a white-box approach, which is highly efficient and also completely explainable. To accurately capture the evolution of video popularity, we develop a mathematical model called HRS model, which is the combination of multiple point processes, including Hawkes’ self-exciting, reactive and self-correcting processes. The key advantage of the HRS model is its explainability, and much less number of model parameters. In addition, all its model parameters can be learned automatically through maximizing the Log-likelihood function constructed by past video request events. Next, we further design an online HRS-based video caching algorithm. To verify its effectiveness, we conduct a series of experiments using real video traces collected from Tencent Video, one of the largest online video providers in China. Experiment results demonstrate that our proposed algorithm outperforms the state-of-the-art algorithms, with 15.5% improvement on average in terms of cache hit rate under realistic settings."",""1558-2183"","""",""10.1109/TPDS.2022.3147240"",""National Natural Science Foundation of China(grant numbers:U1911201,U2001209,62072486,61872310,62176101)"; Science and Technology Planning Project of Guangdong Province(grant numbers:2021A0505110008); Australian Research Council(grant numbers:DE180100950); Major Key Project of PCL(grant numbers:PCL2021A08); Australian Research Council(grant numbers:LP190100676); Hong Kong RGC Research Impact Fund(grant numbers:R5060-19); General Research Fund(grant numbers:152221/19E,152203/20E,152244/21E); Shenzhen Science and Technology Innovation Commission(grant numbers:R2020A045);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9699396"",""Video caching";edge servers;point process;Monte Carlo;"gradient descent"",""Streaming media";Servers;Mathematical models;Prediction algorithms;Internet;Heuristic algorithms;"Computational modeling"","""",""5"","""",""55"",""IEEE"",""1 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"OptZConfig: Efficient Parallel Optimization of Lossy Compression Configuration,""R. Underwood"; J. C. Calhoun; S. Di; A. Apon;" F. Cappello"",""Argonne National Laboratory Mathematics and Computer Science Division, Lemont, IL, USA"; Clemson University, Clemson, SC, USA; Argonne National Laboratory Mathematics and Computer Science Division, Lemont, IL, USA; Clemson University, Clemson, SC, USA;" Argonne National Laboratory Mathematics and Computer Science Division, Lemont, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2022"",""2022"",""33"",""12"",""3505"",""3519"",""Lossless compressors have very low compression ratios that do not meet the needs of today’s large-scale scientific applications that produce vast volumes of data. Error-bounded lossy compression (EBLC) is considered a critical technique for the success of scientific research. Although EBLC allows users to set an error bound for the compression, users have been unable to specify the requirements on the compression quality, limiting practical use. Our contributions are: (1) We formulate the problem of configuring EBLC to preserve a user-defined metric as an optimization problem. This allows many classes of new metrics to be preserved, which improves over current practices. (2) We present a framework, OptZConfig, that can adapt to improvements in the search algorithm, compressor, and metrics with minimal changes, enabling future advancements in this area. (3) We demonstrate the advantages of our approach against the leading methods to configure compressors to preserve specific metrics. Our approach improves compression ratios against a specialized compressor by up to $3\times$3×, has a 56× speedup over FRaZ, 1000× speedup over MGARD-QOI post tuning, and 110× speedup over systematic approaches which had not been bounded by compressors before."",""1558-2183"","""",""10.1109/TPDS.2022.3154096"",""National Science Foundation(grant numbers:NRTDESE 1633608,1619253,1910197,OAC-2003709,OAC-2104023)"; Exascale Computing(grant numbers:17-SC-20-SC); U.S. Department of Energy; National Nuclear Security Administration(grant numbers:DE-AC02-06CH11357);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9730115"",""Error bounded lossy compression";LibPressio;non-linear optimization;"parallel computing"",""Measurement";Compressors;Optimization;Distortion;Meteorology;Quantization (signal);"PSNR"","""",""6"","""",""38"",""IEEE"",""8 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"OSM: Off-Chip Shared Memory for GPUs,""S. Darabi"; E. Yousefzadeh-Asl-Miandoab; N. Akbarzadeh; H. Falahati; P. Lotfi-Kamran; M. Sadrosadati;" H. Sarbazi-Azad"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; School of Computer Science, Institute for Researches in Fundamental Sciences (IPM), Tehran, Iran; School of Computer Science, Institute for Researches in Fundamental Sciences (IPM), Tehran, Iran; School of Computer Science, Institute for Researches in Fundamental Sciences (IPM), Tehran, Iran;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2022"",""2022"",""33"",""12"",""3415"",""3429"",""Graphics Processing Units (GPUs) employ a shared memory, a software-managed cache for programmers, in each streaming multiprocessor to accelerate data sharing among the threads in a thread block. Although 60% of the shared memory space is underutilized, on average, there are some workloads that demand higher shared memory capacities. Therefore, improving shared memory utilization while satisfying the needs of shared memory intensive workloads is challenging. We make a key observation that the lifetime of each shared memory address is significantly shorter than the execution time of a thread block. In this paper, we first propose Off-Chip Shared Memory (OSM) that allocates shared memory space in the off-chip memory, and accelerates accesses to it via a small on-chip cache. Using an 8 KB cache for shared memory addresses, OSM provides almost the same performance as the baseline GPU that uses 96 KB on-chip shared memory. OSM improves GPU performance in two ways. First, it allocates higher shared memory capacities in the off-chip memory, and improves thread-level parallelism (TLP). Second, it designs a unified cache for shared memory and global address spaces, providing more caching space for global memory address space even for the workloads with high shared memory utilization. Our experimental results show an average 21% and 18% IPC improvement compared to the baseline and the state-of-the-art architectures."",""1558-2183"","""",""10.1109/TPDS.2022.3154315"",""Iran National Science Foundation";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9721067"",""Cache memory";GPUs;lifetime;off-chip memory;"shared memory"",""Instruction sets";Graphics processing units;System-on-chip;Memory management;Proposals;Bandwidth;"Registers"","""",""3"","""",""76"",""IEEE"",""24 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Outperforming Sequential Full-Word Long Addition With Parallelization and Vectorization,""A. Chusov"",""Far-Eastern Federal University, Vladivostok, Russia"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Oct 2022"",""2022"",""33"",""12"",""4974"",""4985"",""The article presents algorithms for parallel and vectorized full-word addition of big unsigned integers with carry propagation. Because of the propagation, software parallelization and vectorization of non-polynomial addition of big integers have long been considered impractical due to data dependencies between digits of the operands. The presented algorithms are based upon parallel and vectorized detection of carry origins within elements of vector operands, masking bits which correspond to those elements and subsequent scalar addition of the resulting integers. The acquired bits can consequently be taken into account to adjust the sum using the proposed generalization of the Kogge-Stone method. Essentially, the article formalizes and experimentally verifies parallel and vectorized implementation of carry-lookahead adders applied at arbitrary granularity of data. This approach is noticeably beneficial for manycore, CUDA and vectorized implementation using AVX-512 with masked instructions. Experiments show that the parallel and vectorized implementations of the proposed algorithms can be multiple times faster compared to a sequential ripple-carry adder or adders based on redundant number systems such as one used in the GNU Multiple Precision library."",""1558-2183"","""",""10.1109/TPDS.2022.3211937"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910379"",""AVX-512";CUDA;full-word addition;Long arithmetic;parallel arithmetic;SIMD;SMP;"vectorization"",""Adders";Arithmetic;Signal processing algorithms;Additives;Complexity theory;Energy efficiency;"Vector processors"","""",""1"","""",""30"",""IEEE"",""4 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallel and Asynchronous Smart Contract Execution,""J. Liu"; P. Li; R. Cheng; N. Asokan;" D. Song"",""Zhejiang University, Hangzhou, China"; Tsinghua University, Beijing, China; University of San Francisco, San Francisco, CA, USA; University of Waterloo, Waterloo, ON, Canada;" University of California, Berkeley, Berkeley, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1097"",""1108"",""Today's blockchains suffer from low throughput and high latency, which impedes their widespread adoption of more complex applications like smart contracts. In this article, we propose a novel paradigm for smart contract execution. It distinguishes between consensus nodes and execution nodes: different groups of execution nodes can execute transactions in parallel";" meanwhile, consensus nodes can asynchronously order transactions and process execution results. Moreover, it requires no coordination among execution nodes and can effectively prevent livelocks. We show two ways of applying this paradigm to blockchains. First, we show how we can make Ethereum support parallel and asynchronous contract execution without hard-forks. Then, we propose a new public, permissionless blockchain. Our benchmark shows that, with a fast consensus layer, it can provide a high throughput even for complex transactions like Cryptokitties gene mixing. It can also protect simple transactions from being starved by complex transactions."",""1558-2183"","""",""10.1109/TPDS.2021.3095234"",""Zhejiang Key R&D Plans(grant numbers:2021C01116,2019C03133)"; National Natural Science Foundation of China(grant numbers:62002319,U20A20222); China Zheshang Bank;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477197"",""Blockchain";smart contract;parallel execution;"asynchronous execution"",""Blockchain";Smart contracts;Servers;Cryptography;Protocols;Throughput;"Bitcoin"","""",""9"","""",""41"",""IEEE"",""7 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallel and Distributed Structured SVM Training,""J. Jiang"; Z. Wen; Z. Wang; B. He;" J. Chen"",""The University of Western Australia, Crawley, WA, Australia"; The University of Western Australia, Crawley, WA, Australia; Zhejiang University, Hangzhou, Zhejiang, China; National University of Singapore, Singapore;" South China University of Technology, Guangzhou, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1084"",""1096"",""Structured Support Vector Machines (structured SVMs) are a fundamental machine learning algorithm, and have solid theoretical foundation and high effectiveness in applications such as natural language parsing and computer vision. However, training structured SVMs is very time-consuming, due to the large number of constraints and inferior convergence rates, especially for large training data sets. The high cost of training structured SVMs has hindered its adoption to new applications. In this article, we aim to improve the efficiency of structured SVMs by proposing a parallel and distributed solution (namely FastSSVM) for training structured SVMs building on top of MPI and OpenMP. FastSSVM exploits a series of optimizations (e.g., optimizations on data storage and synchronization) to efficiently use the resources of the nodes in a cluster and the cores of the nodes. Moreover, FastSSVM tackles the large constraint set problem by batch processing and addresses the slow convergence challenge by adapting stop conditions based on the improvement of each iteration. We theoretically prove that our solution is guaranteed to converge to a global optimum. A comprehensive experimental study shows that FastSSVM can achieve at least four times speedup over the existing solutions, and in some cases can achieve two to three orders of magnitude speedup."",""1558-2183"","""",""10.1109/TPDS.2021.3101155"",""National Key Research and Development Program of China(grant numbers:2020AAA0103800)"; MoE AcRF Tier 1(grant numbers:T1 251RES1824); National Natural Science Foundation of China(grant numbers:62072186); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2019B1515130001); Oracle for Research;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502528"",""Parallel and distributed training";structured machine learning;"support vector machines"",""Training";Optimization;Support vector machines;Task analysis;Convergence;Synchronization;"Natural languages"","""",""5"","""",""50"",""IEEE"",""30 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Parallel Dynamic Sparse Approximate Inverse Preconditioning Algorithm on GPU,""J. Gao"; X. Chu; X. Wu; J. Wang;" G. He"",""School of Computer and Electronic Information, Nanjing Normal University, Nanjing, China"; School of Computer and Electronic Information, Nanjing Normal University, Nanjing, China; School of Computer and Electronic Information, Nanjing Normal University, Nanjing, China; Futurewei Technologies, Santa Clara, CA, USA;" Zhijiang College, Zhejiang University of Technology, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Sep 2022"",""2022"",""33"",""12"",""4723"",""4737"",""The dynamic sparse approximate inverse (SPAI) preconditioner has proven to be effective in accelerating the convergence of iterative methods for large linear systems. Recently, accelerating it on graphics processing unit (GPU) has attracted considerable attention due to the fact that the cost of constructing the preconditioner is high. However, the existing parallel dynamic SPAI preconditioning algorithms on GPU are usually ineffective because of the out-of-memory error for large matrices. This motivates us to investigate how to accelerate the construction of dynamic SPAI preconditioners on GPU. In this article, we propose an efficient dynamic SPAI preconditioning algorithm on GPU, called GDSPAI. For our proposed GDSPAI, there are the following novelties: (1) a well-known dynamic SPAI preconditioning algorithm is substantially modified to address the main challenges of parallelization on GPU, (2) a parallel framework of constructing the dynamic SPAI preconditioner on GPU is presented on the basis of the modified dynamic SPAI preconditioning algorithm";" and (3) each component of the preconditioner is computed in parallel inside a group of threads. Experimental results show that the proposed GDSPAI is effective for large matrices, and outperforms the popular preconditioning algorithms in three public libraries, as well as a recent parallel static SPAI preconditioning algorithm."",""1558-2183"","""",""10.1109/TPDS.2022.3202214"",""National Natural Science Foundation of China(grant numbers:61872422)"; Natural Science Foundation of Zhejiang Province(grant numbers:LY19F020028);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869300"",""Sparse approximate inverse";preconditioning;dynamic;CUDA;"GPU"",""Heuristic algorithms";Graphics processing units;Sparse matrices;Approximation algorithms;Message systems;Symmetric matrices;"Libraries"","""",""3"","""",""45"",""IEEE"",""29 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PayDebt: Reduce Buffer Occupancy Under Bursty Traffic on Large Clusters,""K. Liu"; C. Tian; Q. Wang; Y. Chen; B. Tian; W. Sun; K. Meng; L. Yan; L. Han; J. Fu; W. Dou;" G. Chen"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Huawei Technologies Co., Ltd, Shenzhen, China; Huawei Technologies Co., Ltd, Shenzhen, China; Huawei Technologies Co., Ltd, Shenzhen, China; Huawei Technologies Co., Ltd, Shenzhen, China; Huawei Technologies Co., Ltd, Shenzhen, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Sep 2022"",""2022"",""33"",""12"",""4707"",""4722"",""The average/tail Flow Completion Times (FCTs) are critical to many datacenter applications. Congestion control plays a central role in optimizing FCT. Inappropriate congestion control can exacerbate buffer occupancy, thus hurting the flow performance. Our observations are that current approaches are too aggressive in injecting packets into underlying networks. Instead of handling buffer explosion afterward, we reduce buffer occupancy in the first place. We propose PayDebt, a novel and readily-deployable proactive congestion control protocol. At its heart, a debt mechanism provides bandwidth coordination between the already-buffered and the forthcoming packets. We evaluate PayDebt both in a testbed and large-scale simulations. The buffer occupancy can be decreased by up to 8.0×-35.9× compared to DCQCN and Homa."",""1558-2183"","""",""10.1109/TPDS.2022.3202504"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101390001)"; National Natural Science Foundation of China(grant numbers:92067206,62072228,61972222); Fundamental Research Funds for the Central Universities; Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869721"",""Token";proactive congestion control;"datacenter networks"",""Protocols";Receivers;Delays;Bandwidth;Topology;Network topology;"Channel allocation"","""","""","""",""70"",""IEEE"",""29 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Performance and Cost-Efficient Spark Job Scheduling Based on Deep Reinforcement Learning in Cloud Computing Environments,""M. T. Islam"; S. Karunasekera;" R. Buyya"",""Cloud Computing and Distributed Systems (CLOUDS) Lab, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia"; Cloud Computing and Distributed Systems (CLOUDS) Lab, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia;" Cloud Computing and Distributed Systems (CLOUDS) Lab, School of Computing and Information Systems, University of Melbourne, Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2021"",""2022"",""33"",""7"",""1695"",""1710"",""Big data frameworks such as Spark and Hadoop are widely adopted to run analytics jobs in both research and industry. Cloud offers affordable compute resources which are easier to manage. Hence, many organizations are shifting towards a cloud deployment of their big data computing clusters. However, job scheduling is a complex problem in the presence of various Service Level Agreement (SLA) objectives such as monetary cost reduction, and job performance improvement. Most of the existing research does not address multiple objectives together and fail to capture the inherent cluster and workload characteristics. In this article, we formulate the job scheduling problem of a cloud-deployed Spark cluster and propose a novel Reinforcement Learning (RL) model to accommodate the SLA objectives. We develop the RL cluster environment and implement two Deep Reinforce Learning (DRL) based schedulers in TF-Agents framework. The proposed DRL-based scheduling agents work at a fine-grained level to place the executors of jobs while leveraging the pricing model of cloud VM instances. In addition, the DRL-based agents can also learn the inherent characteristics of different types of jobs to find a proper placement to reduce both the total cluster VM usage cost and the average job duration. The results show that the proposed DRL-based algorithms can reduce the VM usage cost up to 30%."",""1558-2183"","""",""10.1109/TPDS.2021.3124670"",""Australian Research Council";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9599497"",""Cloud computing";cost-efficiency;performance improvement;"deep reinforcement learning"",""Sparks";Cloud computing;Costs;Task analysis;Service level agreements;Big Data;"Reinforcement learning"","""",""37"","""",""40"",""IEEE"",""2 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Performant, Multi-Objective Scheduling of Highly Interleaved Task Graphs on Heterogeneous System on Chip Devices,""J. Mack"; S. E. Arda; U. Y. Ogras;" A. Akoglu"",""Electrical and Computer Engineering Department, University of Arizona, Tucson, AZ, USA"; School of Electrical, Computer and Energy Engineering, Arizona State University, Tempe, AZ, USA; Electrical and Computer Engineering Department, University of Wisconsin-Madison, Madison, WI, USA;" Electrical and Computer Engineering Department, University of Arizona, Tucson, AZ, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""3 Feb 2022"",""2022"",""33"",""9"",""2148"",""2162"",""Performance-, power-, and energy-aware scheduling techniques play an essential role in optimally utilizing processing elements (PEs) of heterogeneous systems. List schedulers, a class of low-complexity static schedulers, have commonly been used in static execution scenarios. However, list schedulers are not suitable for runtime decision making, particularly when multiple concurrent applications are interleaved dynamically. For such cases, the static task execution times and expectation of idle PEs assumed by list schedulers lead to inefficient system utilization and poor performance. To address this problem, we present techniques for optimizing execution of list scheduling algorithms in dynamic runtime scenarios via a family of algorithms inspired by the well-known heterogeneous earliest finish time (HEFT) list scheduler. Through dynamically arriving, realistic workload scenarios that are simulated in an open-source discrete event heterogeneous SoC simulator, we exhaustively evaluate each of the proposed algorithms across two SoCs modeled after the Xilinx Zynq Ultrascale+ ZCU102 and O-Droid XU3 development boards. Altogether, depending on the chosen variant in this family of algorithms, we are able to achieve an up to 39% execution time improvement, up to 7.24x algorithmic speedup, or up to 30% energy consumption improvement compared to the baseline HEFT implementation."",""1558-2183"","""",""10.1109/TPDS.2021.3135876"",""Air Force Research Laboratory"; Defense Advanced Research Projects Agency(grant numbers:FA8650-18-2-7860);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9653796"",""Scheduling and task partitioning";heterogeneous (hybrid) systems;energy-aware systems;hardware simulation;"HEFT"",""Task analysis";Heuristic algorithms;Runtime;Dynamic scheduling;Scheduling algorithms;Schedules;"Optimal scheduling"","""",""8"","""",""49"",""IEEE"",""16 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PhaST: Hierarchical Concurrent Log-Free Skip List for Persistent Memory,""Z. Li"; B. Jiao; S. He;" W. Yu"",""College of Computer Science and Technology, Zhejiang University, Hangzhou, China"; Department of Computer Science, Florida State University, Tallahassee, FL, USA; College of Computer Science and Technology, Zhejiang University, Hangzhou, China;" Department of Computer Science, Florida State University, Tallahassee, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""3929"",""3941"",""Skip list (skiplist) is a competitive index structure that offers superior concurrency and excellent performance but with high memory overhead and low access locality. Emerging persistent memory (PM) technologies present an opportunity to mitigate the capacity constraint of DRAM. However, data consistency on PM typically results in excessive write overhead. In addition, fast concurrent access to an index is critical to the throughput on high-end contemporary computer systems. In this article, we propose a Partitioned HierArchical SkiplisT called PhaST, which can simultaneously reduce the skiplist height and improve its access locality, through its hierarchy of component structures, while enabling fast parallel recovery in case of failure. To ensure high concurrency and fast data consistency, we also have developed writelock-free concurrent insert and log-free atomic split. Furthermore, we have developed a durable lock-free concurrent search that can discern transient structural inconsistencies and deliver highly concurrent read operations. We have conducted an extensive evaluation of PhaST compared to state-of-the-art studies such as NV-Skiplist, wB+-Tree, FPTree, and FAST-FAIR. Our evaluation results show PhaST outperforms other indexing structures by up to 4.05× and 2.87× in single-threaded inserts and searches, and 1.56× and 2.62× in concurrent inserts and searches."",""1558-2183"","""",""10.1109/TPDS.2022.3173707"",""National Key Research and Development Program of China(grant numbers:2021ZD0110700)"; National Natural Science Foundation of China(grant numbers:62172361); Zhejiang Lab Research Project(grant numbers:2020KC0AC01); Alibaba Innovative Research Project;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9772399"",""Skiplist";persistent memory;"concurrency"",""Indexing";Random access memory;Concurrent computing;Vegetation;Performance evaluation;Concurrency control;"Bandwidth"","""",""4"","""",""53"",""IEEE"",""10 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Pistis: Issuing Trusted and Authorized Certificates With Distributed Ledger and TEE,""Z. Li"; H. Wu; L. H. Lao; S. Guo; Y. Yang;" B. Xiao"",""Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong"; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong; College of Computer Science, Chongqing University, Chongqing, China; Department of Computer Science, Stony Brook University, Stony Brook, NY, USA;" Department of Computing, The Hong Kong Polytechnic University, Kowloon, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Nov 2021"",""2022"",""33"",""7"",""1636"",""1649"",""The security of HTTPS fundamentally relies on SSL/TLS certificates issued by Certificate Authorities (CAs), which, however, are vulnerable to be compromised to issue unauthorized certificates (i.e., certificates issued without domains’ permission). Current countermeasures such as Certificate Transparency (CT) can only detect unauthorized certificates rather than preventing them. In this article, we present Pistis, a framework for issuing authorized and trusted certificates with the distributed ledger and Trusted Execution Environment (TEE) technology. In Pistis, TEE nodes validate whether the domain in a requested certificate passes the domain ownership validation (i.e., under corresponding applicants’ control) and submit attested results to a smart contract in the distributed ledger. The smart contract issues a certificate to the applicant when an attested result shows a pass. Therefore, Pistis can ensure its issued certificates are authorized due to the domain ownership validation mechanism in the TEE. Furthermore, as the issued certificates are stored in a Merkle Patricia Tree (MPT) in Pistis, they are trusted and can be verified by a normal user easily. The security of Pistis is formally proved in the Universally Composable (UC) framework. Compared with state-of-the-art, Pistis avoids potential damages by preventing unauthorized certificates from issuing."",""1558-2183"","""",""10.1109/TPDS.2021.3121562"",""HK RGC GRF PolyU(grant numbers:15217321,15216220)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582795"",""Distributed ledger";blockchain;smart contract;trusted execution environment (TEE);"certificate issuance"",""Smart contracts";Blockchains;Distributed ledger;Public key;Servers;Internet;"Software"","""",""6"","""",""37"",""IEEE"",""20 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PLVER: Joint Stable Allocation and Content Replication for Edge-Assisted Live Video Delivery,""H. Wang"; G. Tang; K. Wu;" J. Wang"",""Department of Computer Science, University of Victoria, Victoria, BC, Canada"; Peng Cheng Laboratory, Shenzhen, Guangdong, China; Department of Computer Science, University of Victoria, Victoria, BC, Canada;" Department of Computer Science, City University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jul 2021"",""2022"",""33"",""1"",""218"",""230"",""Live streaming services have gained extreme popularity in recent years. Due to the spiky traffic patterns of live videos, utilizing distributed edge servers to improve viewers' quality of experience (QoE) has become a common practice nowadays. Nevertheless, the current client-driven content caching mechanism does not support pre-caching from the cloud to the edge, resulting in a considerable amount of cache misses in live video delivery. By jointly considering the features of live videos and edge servers, we propose PLVER, a proactive live video push scheme to address the cache miss problem in live video delivery. Specifically, PLVER first conducts a one-to-multiple stable allocation between edge clusters and user groups to balance the load of live traffic over the edge servers. It then adopts proactive video replication algorithms to speed up video replication among the edge servers. We conduct extensive trace-driven evaluation, covering 0.3 million Twitch viewers and more than 300 Twitch channels. The results demonstrate that with PLVER, edge servers can carry 28 and 82 percent more traffic than the auction-based replication (ABR) method and the caching on requested time (CORT) method, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3090784"",""Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN-2018-03896)"; National Natural Science Foundation of China(grant numbers:61802421); China Postdoctoral Science Foundation(grant numbers:2019M663017); Hong Kong Research Grant Council(grant numbers:RIF R5060-19);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462365"",""Live video streaming";stable one-to-multiple allocation;proactive video replication;"edge computing"",""Streaming media";Servers;Resource management;Quality of experience;Social networking (online);Real-time systems;"Clustering algorithms"","""",""5"","""",""38"",""IEEE"",""22 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"POCLib: A High-Performance Framework for Enabling Near Orthogonal Processing on Compression,""F. Zhang"; J. Zhai; X. Shen; O. Mutlu;" X. Du"",""Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, ETH Zurich, Zurich, Switzerland;" Key Laboratory of Data Engineering and Knowledge Engineering (MOE), School of Information, Renmin University of China, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2021"",""2022"",""33"",""2"",""459"",""475"",""Parallel technology boosts data processing in recent years, and parallel direct data processing on hierarchically compressed documents exhibits great promise. The high-performance direct data processing technique brings large savings in both time and space by removing the need for decompressing data. However, its benefits have been limited to data traversal operations";" for random accesses, direct data processing is several times slower than the state-of-the-art baselines. This article proposes a novel concept, orthogonal processing on compression (orthogonal POC), which means that text analytics can be efficiently supported directly on compressed data, regardless of the type of the data processing – that is, the type of data processing is orthogonal to its capability of conducting POC. Previous proposals, such as TADOC, are not orthogonal POC. This article presents a set of techniques that successfully eliminate the limitation, and for the first time, establishes the near orthogonal POC feasibility of effectively handling both data traversal operations and random data accesses on hierarchically-compressed data. The work focuses on text data and yields a unified high-performance library, called POCLib. In a ten-node distributed Spark cluster on Amazon EC2, POCLib achieves 3.1× speedup over the state-of-the-art on random data accesses to compressed data, while preserving the capability of supporting traversal operations efficiently and providing large (3.9×) space savings."",""1558-2183"","""",""10.1109/TPDS.2021.3093234"",""National Key Research and Development Program of China(grant numbers:2018YFB1004401)"; National Natural Science Foundation of China(grant numbers:61732014,U20A20226,61802412); Natural Science Foundation of Beijing Municipality(grant numbers:4202031); Beijing Academy of Artificial Intelligence; State Key Laboratory of Computer Architecture(grant numbers:CARCHA202007);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468343"",""Near orthogonal processing on compression";direct processing on compressed data;TADOC;"orthogonal POC"",""Task analysis";Indexing;Data analysis;Technological innovation;Dictionaries;Data structures;"Standards"","""",""59"","""",""57"",""IEEE"",""29 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PostMan: Rapidly Mitigating Bursty Traffic via On-Demand Offloading of Packet Processing,""Y. Niu"; P. Jin; J. Guo; Y. Xiao; R. Shi; F. Liu; C. Qian;" Y. Wang"",""National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, Ohio State University, Columbus, OH, USA; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Laboratory, Cluster and Grid Computing Laboratory, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; Department of Computer Science and Engineering, University of California Santa Cruz, Santa Cruz, CA, USA;" Department of Computer Science and Engineering, Ohio State University, Columbus, OH, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2021"",""2022"",""33"",""2"",""374"",""387"",""Unexpected bursty traffic brought by certain sudden events, such as news in the spotlight on a social network or discounted items on sale, can cause severe load imbalance in backend services. Migrating hot data - the standard approach to achieve load balance - meets a challenge when handling such unexpected load imbalance, because migrating data will slow down the server that is already under heavy pressure. This article proposes PostMan, an alternative approach to rapidly mitigate load imbalance for services processing small requests. Motivated by the observation that processing large packets incurs far less CPU overhead than processing small ones, PostMan deploys a number of middleboxes called helpers to assemble small packets into large ones for the heavily-loaded server. This approach essentially offloads the overhead of packet processing from the heavily-loaded server to helpers. To minimize the overhead, PostMan activates helpers on demand, only when bursty traffic is detected. The heavily-loaded server determines when clients connect/disconnect to/from helpers based on the real-time load statistics. To tolerate helper failures, PostMan can migrate connections across helpers and can ensure packet ordering despite such migration. Driven by real-world workloads, our evaluation shows that, with the help of PostMan, a Memcached server can mitigate bursty traffic within hundreds of milliseconds, while migrating data takes tens of seconds and increases the latency during migration."",""1558-2183"","""",""10.1109/TPDS.2021.3092266"",""National Natural Science Foundation of China(grant numbers:61722206,61761136014,392046569)"; NSFC-DFG(grant numbers:61520106005); National Key Research and Development Program of China(grant numbers:2017YFB1001703); Fundamental Research Funds for the Central Universities(grant numbers:2017KFKJXX009,3004210116); National Program for Support of Top-notch Young Professionals; National Science Foundation(grant numbers:CNS-1566403);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9464666"",""Bursty traffic";packet offloading;packet batching;"high-performance network stack"",""Servers";Bandwidth;Throughput;Redundancy;Load management;Linux;"Computer science"","""",""4"","""",""54"",""IEEE"",""24 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Power Log’n’Roll: Power-Efficient Localized Rollback for MPI Applications Using Message Logging Protocols,""K. Dichev"; D. De Sensi; D. S. Nikolopoulos; K. W. Cameron;" I. Spence"",""Electronics, Electrical Engineering and Computer Science, Queen's University Belfast, Belfast, United Kingdom of Great Britain and Northern Ireland"; University of Pisa, Pisa, Italy; Computer Science, Virginia Tech, Blacksburg, Virginia, USA; Computer Science, Virginia Tech, Blacksburg, Virginia, USA;" Electronics, Electrical Engineering and Computer Science, Queen's University Belfast, Belfast, United Kingdom of Great Britain and Northern Ireland"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1276"",""1288"",""In fault tolerance for parallel and distributed systems, message logging protocols have played a prominent role in the last three decades. Such protocols enable local rollback to provide recovery from fail-stop errors. Global rollback techniques can be straightforward to implement but at times lead to slower recovery than local rollback. Local rollback is more complicated but can offer faster recovery times. In this work, we study the power and energy efficiency implications of global and local rollback. We propose a power-efficient version of local rollback to reduce power consumption for non-critical, blocked processes, using Dynamic Voltage and Frequency Scaling (DVFS) and clock modulation (CM). Our results for 3 different MPI codes on 2 parallel systems show that power-efficient local rollback reduces CPU energy waste up to 50% during the recovery phase, compared to existing global and local rollback techniques, without introducing significant overheads. Furthermore, we show that savings manifest for all blocked processes, which grow linearly with the process count. We estimate that for settings with high recovery overheads the total energy waste of parallel codes is reduced with the proposed local rollback."",""1558-2183"","""",""10.1109/TPDS.2021.3107745"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9524502"",""Fault tolerance";local rollback;fail-stop errors;MPI;message logging;"power/energy savings"",""Protocols";Fault tolerant systems;Fault tolerance;Runtime;Resilience;Payloads;"Topology"","""",""1"","""",""53"",""IEEE"",""27 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Power-Aware Checkpointing for Multicore Embedded Systems,""M. Ansari"; S. Safari; H. Khdr; P. Gohari-Nazari; J. Henkel; A. Ejlali;" S. Hessabi"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; School of Computer Science, Institute for Research in Fundamental Sciences (IPM), Tehran, Iran; Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4410"",""4424"",""Increasing the number of cores integrated on a single chip offers a great potential for the implementation of fault-tolerant techniques to achieve high reliability in real-time embedded systems. Checkpointing with rollback-recovery is a well-established technique to tolerate transient faults in multicore platforms. To consider the worst-case fault occurrence scenario, checkpointing technique requires to re-execute some parts of the tasks, and that might lead to simultaneous execution of task parts with high power consumptions, which eventually might result in a peak power increase beyond the thermal design power (TDP). Exceeding TDP can elevate on-chip temperatures beyond safe limits, and thereby triggering countermeasures that throttle down the voltage and frequency levels or power gate the cores. Such countermeasures might lead to violating task deadlines and degrading the system's reliability. To avoid such severe scenarios, it is inevitable to consider the impact of applying fault-tolerant techniques on the power consumption and prevent violating the power constraint of the chip, i.e., TDP. This article presents for the first time, a peak-power-aware checkpointing (PPAC) technique that tolerates a given number of faults, k, while at the same time meets the power constraint in hard real-time embedded systems. To do this, our proposed technique (PPAC) adjusts the timing of the checkpoints, which have lower power consumption than the tasks to the execution time points that have power spikes beyond TDP. Moreover, PPAC exploits the available slack times on the cores to delay the execution of some tasks to avoid the remaining power spikes beyond TDP, which could not be mitigated by solely adjusting checkpoints. To evaluate our technique, we extend the state-of-the-art system-level simulator, gem5, with the state-of-the-art checkpointing module in Linux. Our experimental results show that our proposed technique is able to tolerate a given number of faults without exceeding the timing and power constraints in hard real-time embedded systems. The resulting peak power reduction achieved by our technique compared to state-of-the-art techniques is an average of 23%. Moreover, our technique employs the Dynamic Power Management (DPM) during the slack times resulting at runtime in the case of fault-free scenarios, which provides energy savings with an average of 17.28% and up to 61.1%."",""1558-2183"","""",""10.1109/TPDS.2022.3188568"",""Deutsche Forschungsgemeinschaft"; German Research Foundation, Invasive Computing Project(grant numbers:146371743);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815557"",""Peak power consumption";checkpointing;multicore platforms;"embedded systems"",""Task analysis";Checkpointing;Timing;Power demand;Real-time systems;Embedded systems;"Reliability"","""",""3"","""",""48"",""IEEE"",""5 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PPOAccel: A High-Throughput Acceleration Framework for Proximal Policy Optimization,""Y. Meng"; S. Kuppannagari; R. Kannan;" V. Prasanna"",""Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA"; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA; U.S. Army Research Lab, Los Angeles, CA, USA;" Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Feb 2022"",""2022"",""33"",""9"",""2066"",""2078"",""Reinforcement Learning (RL) is a major branch of AI that enables agents to learn optimal decision making via interaction with the environment. Proximal Policy Optimization (PPO) is the state-of-the-art policy optimization based RL algorithm which achieves superior overall performance on various benchmarks. A PPO agent iteratively optimizes its policy - a function which chooses optimal actions approximated by a DNN, with each iteration consisting of two computationally intensive phases: Sample Generation - where agents inference on its policy and interact with the environment to collect data, and Model Update - where the policy is trained using the collected data. In this paper, we develop the first high-throughput PPO accelerator on CPU-FPGA heterogeneous platform. Our unified systolic-array based design accelerates both the inference and the training of the deep neural network used in a RL algorithm, and is generalizable to various MLP and CNN models across a wide range of RL applications. We develop novel optimizations to simultaneously reduce data access and computation latencies, specifically: (a) optimal data flow mapping to systolic array, (b) novel memory-blocked data layout to enable streaming stall-free data access in both forward and backward propagations, and, (c) a systolic array compute sharing technique to mitigate load imbalance in the training of two networks. We evaluate our design on widely used robotics and gaming benchmarks, achieving 1.4×–26× and 1.3×–2.7× improvements in throughput, respectively, when compared with state-of-the-art CPU/CPU-GPU implementations."",""1558-2183"","""",""10.1109/TPDS.2021.3134709"",""National Science Foundation(grant numbers:2009057)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647962"",""Reinforcement learning";hardware accelerators;"FPGA"",""Conferences";Portable document format;Indexes;Typesetting;Loading;Web sites;"Warranties"","""",""3"","""",""49"",""IEEE"",""13 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Predicting Throughput of Distributed Stochastic Gradient Descent,""Z. Li"; M. Paolieri; L. Golubchik; S. -H. Lin;" W. Yan"",""Department of Computer Science, University of Southern California, Los Angeles, CA, USA"; Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Department of Computer Science, University of Southern California, Los Angeles, CA, USA; Meta, Menlo Park, CA, USA;" Department of Computer Science, University of Southern California, Los Angeles, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2900"",""2912"",""Training jobs of deep neural networks (DNNs) can be accelerated through distributed variants of stochastic gradient descent (SGD), where multiple nodes process training examples and exchange updates. The total throughput of the nodes depends not only on their computing power, but also on their networking speeds and coordination mechanism (synchronous or asynchronous, centralized or decentralized), since communication bottlenecks and stragglers can result in sublinear scaling when additional nodes are provisioned. In this paper, we propose two classes of performance models to predict throughput of distributed SGD: fine-grained models, representing many elementary computation/communication operations and their dependencies";" and coarse-grained models, where SGD steps at each node are represented as a sequence of high-level phases without parallelism between computation and communication. Using a PyTorch implementation, real-world DNN models and different cloud environments, our experimental evaluation illustrates that, while fine-grained models are more accurate and can be easily adapted to new variants of distributed SGD, coarse-grained models can provide similarly accurate predictions when augmented with ad hoc heuristics, and their parameters can be estimated with profiling information that is easier to collect."",""1558-2183"","""",""10.1109/TPDS.2022.3151739"",""National Science Foundation(grant numbers:CCF-1763747,CNS-1816887)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9714870"",""Distributed machine learning";stochastic gradient descent;performance prediction;scalability;"PyTorch"",""Computational modeling";Predictive models;Training;Throughput;Servers;Computer architecture;"Uplink"","""","""","""",""37"",""IEEE"",""16 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Privacy-Preserving Deduplication of Sensor Compressed Data in Distributed Fog Computing,""C. Zhang"; Y. Miao; Q. Xie; Y. Guo; H. Du;" X. Jia"",""Department of Computer Science, City University of Hong Kong, Hong Kong, China"; School of Cyber Engineering, Xidian University, Xi'an, China; Center for Computer Science and Information Technology, City University of Hong Kong, Hong Kong; School of Artificial Intelligence, Beijing Normal University, Beijing, China; Department of Computer Science and Technology, Harbin Institute of Technology Shenzhen, Shenzhen, China;" Department of Computer Science, City University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4176"",""4191"",""Distributed fog computing has received wide attention recently. It enables distributed computing and data management on the network nodes within the close vicinity of IoT devices. An important service of fog-cloud based systems is data deduplication. With the increasing concern of privacy, some privacy-preserving data deduplication schemes have been proposed. However, they cannot support lossless deduplication of encrypted similar data in the fog-cloud network. Meanwhile, no existing design can protect message equality information while resisting brute-force and frequency analysis attacks. In this paper, we propose a privacy-preserving and compression-based data deduplication system under the fog-cloud network, which supports lossless deduplication of similar data in the encrypted domain. Specifically, we first use the generalized deduplication technique and cryptographic primitives to implement secure deduplication over similar data. Then, we devise a two-level deduplication protocol that can perform secure and efficient deduplication at distributed fog nodes and the cloud. The proposed system can not only resist brute-force and frequency analysis attacks but also ensure that only the data operator can capture the message equality information. We formally analyze the security of our design. Performance evaluations demonstrate that our proposed design is efficient in computing, storage, and communication."",""1558-2183"","""",""10.1109/TPDS.2022.3179992"",""Research Grants Council of Hong Kong(grant numbers:CityU 11213920)"; National Natural Science Foundation of China(grant numbers:62102035,62072361,62172124); Fundamental Research Funds for the Central Universities(grant numbers:2020NTST32); Shenzhen Basic Research Program(grant numbers:JCYJ20190806143011274); Shenzhen Colleges and Universities Stable Support Program(grant numbers:GXWD20201230155427003-20200822080602001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787776"",""Brute-force attacks";fog-cloud network;frequency analysis;message equality information;"similar data deduplication"",""Cloud computing";Cryptography;Sensors;Internet of Things;Data models;Costs;"Computer science"","""",""5"","""",""55"",""IEEE"",""3 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"Privacy-Preserving Efficient Federated-Learning Model Debugging,""A. Li"; L. Zhang; J. Wang; F. Han;" X. -Y. Li"",""School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China;" School of Computer Science, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Mar 2022"",""2022"",""33"",""10"",""2291"",""2303"",""Federated learning allows large amounts of mobile clients to jointly construct a global model without sending their private data to a central server. A fundamental issue in this framework is the susceptibility to the erroneous training data. This problem is especially challenging due to the invisibility of clients’ local training data and training process, as well as the resource constraints. In this paper, we aim to solve this issue by introducing the first FL debugging framework, FLDebugger, for mitigating test error caused by erroneous training data. The proposed solution traces the global model’s bugs (test errors), jointly through the training log and the underlying learning algorithm, back to first identify the clients and subsequently their training samples that are most responsible for the errors. In addition, we devise an influence-based participant selection strategy to fix bugs as well as to accelerate the convergence of model retraining. The performance of the identification algorithm is evaluated via extensive experiments on a real AIoT system (50 clients, including 20 edge computers, 20 laptops and 10 desktops) and in larger-scale simulated environments. The evaluation results attest to that our framework achieves accurate, privacy-preserving and efficient identification of negatively influential clients and samples, and significantly improves the model performance by fixing bugs."",""1558-2183"","""",""10.1109/TPDS.2021.3137321"",""National Key Research and Development Program of China(grant numbers:2018YFB0803400)"; National Natural Science Foundation of China(grant numbers:61822209,61625205,61932016,62132018); Key Research Program of Frontier Science, Chinese Academy of Sciences(grant numbers:QYZDY-SSW-JSC002); Tencent Marketing Solution Rhino-Bird Focused Research Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661312"",""Federated learning";Influence function;"data quality assessment"",""Training";Data models;Adaptation models;Debugging;Computational modeling;Training data;"Predictive models"","""",""9"","""",""45"",""IEEE"",""23 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Propagation Pattern for Moment Representation of the Lattice Boltzmann Method,""J. Gounley"; M. Vardhan; E. W. Draeger; P. Valero-Lara; S. V. Moore;" A. Randles"",""Computational Sciences and Engineering Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA"; Department of Biomedical Engineering, Duke University, Durham, NC, USA; Lawrence Livermore National Laboratory, Center for Applied Scientific Computing, Livermore, CA, USA; Oak Ridge National Laboratory, Computer Science and Mathematics Division, Oak Ridge, TN, USA; Department of Computer Science, University of Texas at El Paso, El Paso, TX, USA;" Department of Biomedical Engineering, Duke University, Durham, NC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2021"",""2022"",""33"",""3"",""642"",""653"",""A propagation pattern for the moment representation of the regularized lattice Boltzmann method (LBM) in three dimensions is presented. Using effectively lossless compression, the simulation state is stored as a set of moments of the lattice Boltzmann distribution function, instead of the distribution function itself. An efficient cache-aware propagation pattern for this moment representation has the effect of substantially reducing both the storage and memory bandwidth required for LBM simulations. This article extends recent work with the moment representation by expanding the performance analysis on central processing unit (CPU) architectures, considering how boundary conditions are implemented, and demonstrating the effectiveness of the moment representation on a graphics processing unit (GPU) architecture."",""1558-2183"","""",""10.1109/TPDS.2021.3098456"",""Oak Ridge National Laboratory"; National Institutes of Health(grant numbers:1U01CA253511); American Heart Association Predoctoral Fellowship and ACM/IEEE-CS George Michael Memorial High Performance Computing Fellowship; Oak Ridge Leadership Computing Facility; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725,DE-AC52-07NA27344);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9492751"",""Lattice Boltzmann methods";high-performance computing;"fluid dynamics"",""Mathematical model";Lattice Boltzmann methods;Kernel;Memory management;Tensors;Boundary conditions;"Bandwidth"","""",""5"","""",""52"",""IEEE"",""21 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Protein Structured Reservoir Computing for Spike-Based Pattern Recognition,""K. -A. Tsakalos"; G. C. Sirakoulis; A. Adamatzky;" J. Smith"",""Department of Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, Greece"; Department of Electrical and Computer Engineering, Democritus University of Thrace, Xanthi, Greece; Unconventional Computing Laboratory, University of the West of England, Bristol, U.K;" Computer Science Research Centre, University of the West of England, Bristol, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""27 Aug 2021"",""2022"",""33"",""2"",""322"",""331"",""Nowadays we witness a miniaturisation trend in the semiconductor industry backed up by groundbreaking discoveries and designs in nanoscale characterisation and fabrication. To facilitate the trend and produce ever smaller, faster and cheaper computing devices, the size of nanoelectronic devices is now reaching the scale of atoms or molecules - a technical goal undoubtedly demanding for novel devices. Following the trend, we explore an unconventional route of implementing reservoir computing on a single protein molecule and introduce neuromorphic connectivity with a small-world networking property. We have chosen Izhikevich spiking neurons as elementary processors, corresponding to the atoms of verotoxin protein, and its molecule as a `hardware' architecture of the communication networks connecting the processors. We apply on a single readout layer, various training methods in a supervised fashion to investigate whether the molecular structured Reservoir Computing (RC) system is capable to deal with machine learning benchmarks. We start with the Remote Supervised Method, based on Spike-Timing-Dependent-Plasticity, and carry on with linear regression and scaled conjugate gradient back-propagation training methods. The RC network is evaluated as a proof-of-concept on the handwritten digit images from the standard MNIST and the extended MNIST datasets and demonstrates acceptable classification accuracies in comparison with other similar approaches."",""1558-2183"","""",""10.1109/TPDS.2021.3068826"",""Hellenic Foundation for Research and Innovation(grant numbers:1228)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9387584"",""Molecular networks";reservoir computing;liquid state machine;izhikevich model;remote supervised learning;"pattern recognition"",""Reservoirs";Proteins;Training;Topology;Three-dimensional displays;Neurons;"Nanoscale devices"","""",""4"","""",""64"",""IEEE"",""26 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PS+: A Simple yet Effective Framework for Fast Training on Parameter Server,""A. -L. Jin"; W. Xu; S. Guo; B. Hu;" K. Yeung"",""University of Hong Kong, Hong Kong, China"; Hong Kong Polytechnic University, Hong Kong, China; Hong Kong Polytechnic University, Hong Kong, China; Zhejiang University, Hangzhou, China;" University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""1 Sep 2022"",""2022"",""33"",""12"",""4625"",""4637"",""In distributed training, workers collaboratively refine the global model parameters by pushing their updates to the Parameter Server and pulling fresher parameters for the next iteration. This introduces high communication costs for training at scale, and incurs unproductive waiting time for workers. To minimize the waiting time, existing approaches overlap communication and computation for deep neural networks. Yet, these techniques not only require the layer-by-layer model structures, but also need significant efforts in runtime profiling and hyperparameter tuning. To make the overlapping optimization simple and generic, in this article, we propose a new Parameter Server framework. Our solution decouples the dependency between push and pull operations, and allows workers to eagerly pull the global parameters. This way, both push and pull operations can be easily overlapped with computations. Besides, the overlapping manner offers a different way to address the straggler problem, where the stale updates greatly retard the training process. In the new framework, with adequate information available to workers, they can explicitly modulate the learning rates for their updates. Thus, the global parameters can be less compromised by stale updates. We implement a prototype system in PyTorch and demonstrate its effectiveness on both CPU/GPU clusters. Experimental results show that our prototype saves up to 54% less time for each iteration and up to 37% fewer iterations for model convergence, achieving up to 2.86× speedup over widely-used synchronization schemes."",""1558-2183"","""",""10.1109/TPDS.2022.3200518"",""Hong Kong RGC(grant numbers:PolyU 15222621,RIF R5060-19,GRF 152221/19E,GRF 152203/20E,GRF 152244/21E)"; National Natural Science Foundation of China(grant numbers:61872310); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2021B0101400003); Shenzhen Science and Technology Innovation Commission(grant numbers:R2020A045);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864063"",""Machine learning";distributed training;"parameter server"",""Training";Servers;Synchronization;Computational modeling;Hardware;Data models;"Computational efficiency"","""","""","""",""46"",""IEEE"",""22 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"PushBox: Making Use of Every Bit of Time to Accelerate Completion of Data-Parallel Jobs,""C. Tian"; Y. Wang; B. Tian; Y. Zhao; Y. Zhou; C. Wang; H. Guan; W. Dou;" G. Chen"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"; School of Modern Posts, Nanjing University of Posts and Telecommunications, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; School of Computer Science, University of Sydeny, Sydney, NSW, Australia; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4256"",""4269"",""To minimize a job's completion time, we need to minimize the completion time of its final stage's last task. Scheduling of machine slots and networks largely dominates the variable part of each task's duration. Finding an optimal schedule is NP-hard even for offline and simplified scenarios. Previous work does lead to improved performance with various strategies. State-of-the-art task placement and network scheduling efforts are largely disjunctive. Without joint optimization, they are sub-optimal and myopic in many scenarios. Task placement usually treats the network as a black box. Thus, we use prioritized bandwidth allocation among tasks making the network both predictable and efficient to achieve joint scheduling. With this feature, joint scheduling can be transformed into a special bin-packing problem. Over this minimal yet power-enough abstraction, we propose PushBox to schedule data-parallel jobs in multi-tenant clusters. When designing the joint scheduling algorithm, we not only embrace the wisdom of prior art but also respect administrators’ fairness intent, which is so far largely ignored. We implement PushBox on Hadoop 3. PushBox performs persistently well on both a small testbed and a trace-driven simulator."",""1558-2183"","""",""10.1109/TPDS.2022.3182037"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2020B0101390001)"; National Natural Science Foundation of China(grant numbers:92067206,62072228,61972222); Fundamental Research Funds for the Central Universities; Collaborative Innovation Center of Novel Software Technology and Industrialization; Jiangsu Innovation and Entrepreneurship;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9798860"",""Task scheduling";distributed system;"datacenter"",""Task analysis";Processor scheduling;Optimal scheduling;Semantics;Computational modeling;Schedules;"Resource management"","""","""","""",""50"",""IEEE"",""17 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"QoS-Aware Co-Scheduling for Distributed Long-Running Applications on Shared Clusters,""J. Zhu"; R. Yang; X. Sun; T. Wo; C. Hu; H. Peng; J. Xiao; A. Y. Zomaya;" J. Xu"",""Beihang University, Beijing, China"; School of Computing, University of Leeds, Leeds, U.K.; School of Computing, University of Leeds, Leeds, U.K.; Beihang University, Beijing, China; Beihang University, Beijing, China; Beihang University, Beijing, China; Alibaba Group, Hangzhou, Zhejiang, China; University of Sydney, Camperdown, NSW, Australia;" School of Computing, University of Leeds, Leeds, U.K."",""IEEE Transactions on Parallel and Distributed Systems"",""29 Sep 2022"",""2022"",""33"",""12"",""4818"",""4834"",""To achieve a high degree of resource utilization, production clusters need to co-schedule diverse workloads – including both batch analytic jobs with short-lived tasks and long-running applications (LRAs) that execute for a long time frame from hours to months – onto the shared resources. Microservice architecture advances the manifestation of distributed LRAs (DLRAs), comprising multiple interconnected microservices that are executed in long-lived distributed containers and serve massive user requests. Detecting and mitigating QoS violation become even more intractable due to the network uncertainties and latency propagation across dependent microservices. However, current resource managers are only responsible for resource allocation among applications/jobs but agnostic to runtime QoS such as latency at application level. The state-of-the-art QoS-aware scheduling approaches are dedicated for monolithic applications, without considering the temporal-spatio performance variability across distributed microservices. In this paper, we present Toposch, a new scheduling and execution framework to prioritize the QoS of DLRAs whilst balancing the performance of batch jobs and maintaining high cluster utilization through harvesting idle resources. Toposch tracks footprints of every single request across microservices and uses critical path analysis, based on the end-to-end latency graph, to identify microservices that have high risk of QoS violation. Based on microservice and node level risk assessment, we intervene the batch scheduling by adaptively reducing the visible resources to batch tasks and thus delaying their execution to give way to DLRAs. We propose a prediction-based vertical resource auto-scaling mechanism, with the aid of resource-performance modeling and fine-grained resource inference and access control, for prompt recovery of QoS violation. A cost-effective task preemption is leveraged to ensure a low-cost task preemption and resource reclamation during the auto-scaling. Toposch is integrated with Apache YARN and experiments show that Toposch outperforms other baselines in terms of performance guarantee of DLRAs, at an acceptable cost of batch job slowdown. The tail latency of DLRAs is merely 1.12x of the case of executing alone on average in Toposch with a 26% JCT increase of Spark analytic jobs."",""1558-2183"","""",""10.1109/TPDS.2022.3202493"",""MIIT of China(grant numbers:2105-370171-07-02-860873)"; S&T Program of Hebei(grant numbers:20310101D); Fundamental Research Funds for the Central Universities(grant numbers:20226941); UK EPSRC(grant numbers:EP/T01461X/1); Alan Turing Pilot Project; Alan Turing PDEA Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9869329"",""Resource scheduling";cluster management;QoS;tail latency;"datacenters"",""Quality of service";Microservice architectures;Task analysis;Runtime;Containers;Resource management;"Databases"","""",""6"","""",""65"",""CCBY"",""29 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"QoS-Aware Scheduling of Remote Rendering for Interactive Multimedia Applications in Edge Computing,""R. Xie"; J. Fang; J. Yao; K. Liu; X. Jia;" K. Wu"",""College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China"; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; College of Computer Science, Chongqing University, Chongqing, China; Department of Computer Science, City University of Hong Kong, Hong Kong, China;" College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3816"",""3832"",""Leveraging emerging edge computing and 5G networks, researchers proposed to offload the 3D rendering of interactive multimedia applications (e.g., virtual reality and cloud gaming) onto edge servers. For high resource utilization, multiple rendering tasks run in the same GPU server and compete against each other for the computation resource. Each task has its requirement for performance, i.e., QoS target. A significant problem is how to schedule tasks so that each preset QoS is met and the performance of all tasks are maximized. We make the following contributions. First, we formulate the problem into a QoS constrained max-min utility problem. Second, we find that using the common natural logarithm as a utility function overly promotes one performance but demotes another. To avoid this phenomenon, we design a special utility function. Third, we propose an efficient scheduling algorithm, consisting of a resolution adjustment algorithm and a frame rate fair scheduling algorithm, both of which interact with each other. The former selects resolutions for tasks and the latter decides which task to process. We evaluate our method with actual rendering data, and the simulations demonstrate that our method can effectively improve task performance as well as satisfy QoS simultaneously."",""1558-2183"","""",""10.1109/TPDS.2022.3172121"",""China NSFC(grant numbers:61802263,62072317,62172064)"; Research Grants Council of Hong Kong(grant numbers:CityU 11202419); Faculty Research Fund of Shenzhen University(grant numbers:860/000002110325); China NSFC(grant numbers:U2001207,61872248); Guangdong NSF(grant numbers:2017A030312008); Shenzhen Science and Technology Foundation(grant numbers:ZDSYS20190902092853047,R2020A045); Department of Education of Guangdong Province(grant numbers:2019KCXTD005,2021ZDZX1068); Guangdong(grant numbers:2019ZT08X603);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9767696"",""Edge computing";task scheduling;"remote rendering"",""Rendering (computer graphics)";Task analysis;Servers;Quality of service;Streaming media;Graphics processing units;"Three-dimensional displays"","""",""1"","""",""37"",""IEEE"",""3 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Real-Time Scheduling of Parallel Task Graphs With Critical Sections Across Different Vertices,""X. Jiang"; N. Guan; M. Yang; Y. Wang; Y. Tang;" W. Yi"",""Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, China"; City University of Hong Kong, Hong Kong; University of Electronic Science and Technology of China, Chengdu, China; Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, China; Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, China;" Key Laboratory of Intelligent Computing in Medical Image, Ministry of Education, Northeastern University, Shenyang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4117"",""4133"",""All existing work on real-time scheduling of parallel task graph models with shared resources assumes that a critical section must be contained inside a single vertex. However, this assumption does not hold in many realistic parallel real-time software. In this work, we conduct the first study on real-time scheduling and analysis of parallel task graphs where critical sections are allowed to cross different vertices. We show that allowing this may potentially lead to deadlocks and the so-called resource unrelated blocking time problem. We formalize the conditions for the deadlocks and resource unrelated blocking time to happen, and propose two different solutions to address them and develop corresponding schedulability analysis techniques. We conduct comprehensive experiments to evaluate our method. The results indicate that there is a significant impact to the system schedulability when tasks incur deadlock and resource unrelated blocking. Moreover, the schedulability can benefit from the execution of workload in parallel with critical sections if tasks can be carefully designed so that all deadlocks and resource unrelated blocking time can be avoided, and our methods are efficient to determine the schedulability of systems where critical sections across different vertices exist."",""1558-2183"","""",""10.1109/TPDS.2022.3179328"",""National Natural Science Foundation of China(grant numbers:NSFC 62102072)"; Research Grants Council of Hong Kong(grant numbers:GRF 15206221);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785844"",""Multi-core";parallel tasks;critical section;"real-time scheduling"",""Task analysis";Real-time systems;Protocols;System recovery;Analytical models;Heuristic algorithms;"Computational modeling"","""","""","""",""41"",""IEEE"",""31 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Redesigning and Optimizing UCSF DOCK3.7 on Sunway TaihuLight,""K. Xu"; J. Zhang; X. Duan; X. Wan; N. Huang; B. Schmidt; W. Liu;" G. Yang"",""School of Software, Shandong University, Jinan, China"; School of Software, Shandong University, Jinan, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Computational Medicine (Beijing) Co. Ltd, Beijing, China; National Institute of Biological Sciences, Beijing, China; Institute for Computer Science, Johannes Gutenberg University, Mainz, Germany; School of Software, Shandong University, Jinan, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4458"",""4471"",""Molecular docking is the process of posing, scoring, and ranking small molecules at the binding sites of proteins to prioritize compounds for experimental testing. It is a widely-used computational method in the drug discovery process. However, it is a highly time-consuming procedure since a receptor may need to find favorable ligand orientations in billions of ligands. UCSF DOCK3.7 is one of the most widely used molecular docking applications. In this paper, we port and optimize UCSF DOCK3.7 on the Sunway TaihuLight supercomputer. To avoid the impact of load imbalance, we employ a producer-consumer strategy that can overlap I/O and computation in order to achieve high performance. Furthermore, we present a new binary file format to replace the mol2db2 file format for ligand storage and adopt xzip rather than gzip to compress ligand files. We show that our file format can reduce I/O time significantly while xzip saves significant storage. For the routines which determine the orientation of a ligand relative to the receptor, we present an improved algorithm to discard geometrically similar orientations. Furthermore, we fuse loops and compress memory usage to store data in fast Local Device Memory (LDM) in order to score ligand orientations with high efficiency. In addition, we propose a number of architecture-specific optimizations. Asynchronous data transfer and vectorization of computation are implemented to take full advantage of the SW26010 processor. Our experiments show that a speedup of 167 can be achieved by using the proposed strategies. Compared to a core of an Intel(R) Core(TM) i9-10900K CPU, our approach achieves speedups of 15 on a SW26010 core group. Furthermore, our implementation achieves strong scalability to hundreds of thousands of heterogeneous cores on the next-generation Sunway supercomputer."",""1558-2183"","""",""10.1109/TPDS.2022.3194916"",""National Natural Science Foundation of China(grant numbers:61972231,U1806205)"; Key Project of Joint Fund of Shandong Province(grant numbers:ZR2019LZH00); Ministry of Education, China;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9845705"",""Molecular docking";drug discovery;heterogeneous architecture;"high-performance computing"",""Memory management";Optimization;Supercomputers;Mathematical models;Graphics processing units;Field programmable gate arrays;"Drugs"","""","""","""",""42"",""IEEE"",""1 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"ReHy: A ReRAM-Based Digital/Analog Hybrid PIM Architecture for Accelerating CNN Training,""H. Jin"; C. Liu; H. Liu; R. Luo; J. Xu; F. Mao;" X. Liao"",""National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" National Engineering Research Center for Big Data Technology and System, Services Computing Technology and System Lab, Cluster and Grid Computing Lab, School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2872"",""2884"",""Processing-In-Memory(PIM) has emerged as a high-performance and energy-efficient computing paradigm for accelerating convolutional neural network (CNN) applications. Resistive random access memory (ReRAM) has been widely used in PIM architectures due to its extremely high efficiency for accelerating matrix-vector multiplications through analog computing. However, because CNN training usually requires high-precision computation in the backward propagation (BP) stage, the limited precision of analog PIM accelerators impedes their adoption in CNN training. In this article, we propose ReHy, a hybrid PIM accelerator to support CNN training in ReRAM arrays. It is composed of Analog PIM (APIM) and Digital PIM (DPIM) modules. ReHy uses APIM to accelerate the feed-forward propagation (FP) stage for high performance, and DPIM to process the BP stage for high accuracy. We exploit the capability of ReRAM for Boolean logic operations to design the DPIM architecture. Particularly, we design floating-point multiplication and addition operators to support matrix multiplications in ReRAM arrays. We also propose a performance model to offload high-precision matrix multiplications to DPIM according to the data parallelism. Experimental results show that ReHy can speed up CNN training by 48.8× and 2.4×, and reduce energy consumption by 35.1× and 2.33×, compared with CPU/GPU architectures (baseline) and the state-of-the-art FloatPIM, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3138087"",""National Natural Science Foundation of China(grant numbers:62072198,61832006,61825202,61929103)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9663036"",""Resistive random access memory";convolutional neural network training;"digital-analog hybrid accelerator"",""Training";Convolutional neural networks;Computer architecture;Parallel processing;Resistance;Memristors;"Arrays"","""",""5"","""",""28"",""CCBYNCND"",""24 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Reliable Wide-Area Data Transfers for Streaming Workflows,""H. Sapkota";" E. Arslan"",""Department of Computer Science & Engineering, University of Nevada, Reno, NV, USA";" Department of Computer Science & Engineering, University of Nevada, Reno, NV, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3546"",""3557"",""Many large science projects rely on remote clusters for (near) real-time data processing, thus they demand reliable wide-area data transfer performance for smooth end-to-end workflow executions. However, data transfers are often exposed to performance variations due to the changing network (e.g., background traffic) and dataset (e.g., average file size) conditions, necessitating adaptive solutions to meet stringent performance requirements of delay-sensitive streaming workflows. In this article, we propose FStream++ to provide reliable transfer performance for large streaming science applications by dynamically adjusting transfer settings to adapt to changing transfer conditions. FStream++ combines three optimization methods as dynamic tuning, online profiling, and historical analysis to swiftly and accurately discover optimal transfer settings that can meet workflow requirements. Dynamic tuning uses a heuristic model to predict the values of transfer parameters based on dataset characteristics and network settings. Since heuristic models fall short to incorporate many important factors such as I/O throughput and resource interference, we complement it with online profiling to execute a real-time search for a subset of transfer settings. Finally, historical analysis takes advantage of the long-running nature of streaming workflows by storing and analyzing previous performance observations to shorten the execution time of online profiling. We evaluate the performance of FStream++ by transferring several synthetic and real-world workloads in high-performance production networks and show that it offers up to $3.6x$3.6x performance improvement over legacy transfer applications and up to 24% over our previous work FStream."",""1558-2183"","""",""10.1109/TPDS.2022.3158673"",""National Science Foundation(grant numbers:1850353,2007789,2019164)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9733263"",""Distributed workflows";high-speed networks;online profiling;streaming science applications;"throughput optimization"",""Throughput";Reliability;Real-time systems;Data transfer;Pipeline processing;Concurrent computing;"Optimization"","""","""","""",""40"",""IEEE"",""11 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Reproducibility: Performance Evaluation of MemXCT on Azure CycleCloud Platform,""Y. Liu"; Y. Meng; K. Xu; Z. Xu; T. Wu; Y. Yang;" S. Yin"",""School of Information Science and Technology, ShanghaiTech University, Shanghai, China"; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China;" School of Information Science and Technology, ShanghaiTech University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""28 Jan 2022"",""2022"",""33"",""9"",""2047"",""2049"",""Memory-Centric X-ray Computational Tomography(CT) is an iterative reconstruction technique that trades compute simplifications with higher memory accesses. MemXCT implements a sparse matrix-vector multiplication(SpMV) with multi-stage buffering and two-level pseudo-Hilbert ordering for optimization. Motivated by the need to validate conclusions from previous work, we reproduce the numerical results, the algorithm’s performance, and the scaling behavior of the algorithms as the number of MPI processes increases on Azure. Digital artifacts from these experiments are available at: 10.5281/zenodo.5598108"",""1558-2183"","""",""10.1109/TPDS.2021.3127450"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9612082"",""SpStudent cluster challenge";reproducibility;scalability;"MemXCT"",""Graphics processing units";Scalability;Bandwidth;Performance evaluation;Computer architecture;Sparse matrices;"Reproducibility of results"","""","""","""",""6"",""IEEE"",""11 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Repurposing GPU Microarchitectures with Light-Weight Out-Of-Order Execution,""K. Iliakis"; S. Xydis;" D. Soudris"",""National Technical University of Athens, Athens, Greece"; Harokopio University of Athens, Athens, Greece;" National Technical University of Athens, Athens, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2021"",""2022"",""33"",""2"",""388"",""402"",""GPU is the dominant platform for accelerating general-purpose workloads due to its computing capacity and cost-efficiency. GPU applications cover an ever-growing range of domains. To achieve high throughput, GPUs rely on massive multi-threading and fast context switching to overlap computations with memory operations. We observe that among the diverse GPU workloads, there exists a significant class of kernels that fail to maintain a sufficient number of active warps to hide the latency of memory operations, and thus suffer from frequent stalling. We argue that the dominant Thread-Level Parallelism model is not enough to efficiently accommodate the variability of modern GPU applications. To address this inherent inefficiency, we propose a novel micro-architecture with lightweight Out-Of-Order execution capability enabling Instruction-Level Parallelism to complement the conventional Thread-Level Parallelism model. To minimize the hardware overhead, we carefully design our extension to highly re-use the existing micro-architectural structures and study various design trade-offs to contain the overall area and power overhead, while providing improved performance. We show that the proposed architecture outperforms traditional platforms by 23 percent on average for low-occupancy kernels, with an area and power overhead of 1.29 and 10.05 percent, respectively. Finally, we establish the potential of our proposal as a micro-architecture alternative by providing 16 percent speedup over a wide collection of 60 general-purpose kernels."",""1558-2183"","""",""10.1109/TPDS.2021.3093231"",""European Union's Horizon 2020 Research and Innovation programme(grant numbers:825061)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468415"",""General purpose GPU";micro-architecture;out-of-order execution;instruction level parallelism;"parallel systems"",""Graphics processing units";Kernel;Out of order;Computer architecture;Parallel processing;Context;"Resource management"","""","""","""",""56"",""IEEE"",""29 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Reputation-Aware Hedonic Coalition Formation for Efficient Serverless Hierarchical Federated Learning,""J. S. Ng"; W. Y. B. Lim; Z. Xiong; X. Cao; J. Jin; D. Niyato; C. Leung;" C. Miao"",""Alibaba Group and Alibaba-NTU Joint Research Institute (JRI), Nanyang Technological University (NTU), Singapore"; Alibaba Group and Alibaba-NTU Joint Research Institute (JRI), Nanyang Technological University (NTU), Singapore; Information Systems Technology and Design (ISTD) Pillar, Singapore University of Technology and Design (SUTD), Singapore; School of Electronic and Information Engineering, Beihang University, Beijing, China; TuSimple, Beijing, China; School of Computer Science and Engineering (SCSE), NTU, Singapore; The University of British Columbia, Vancouver, BC, Canada;" SCSE, NTU, Singapore, Alibaba-NTU JRI, LILY, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2675"",""2686"",""Amid growing concerns on data privacy, Federated Learning (FL) has emerged as a promising privacy preserving distributed machine learning paradigm. Given that the FL network is expected to be implemented at scale, several studies have proposed system architectures towards improving the network scalability and efficiency. Specifically, the Hierarchical FL (HFL) network utilizes cluster heads, e.g., base stations, for the intermediate aggregation and relay of model parameters. Serverless FL is also proposed recently, in which the data owners, i.e., workers, exchange the local model parameters among a neighborhood of workers. This decentralized approach reduces the risk of a single point of failure but inevitably incurs significant communication overheads. To achieve the best of both worlds, we propose the Serverless Hierarchical Federated Learning (SHFL) framework in this article. The SHFL framework adopts a two-layer system architecture. In the lower layer, the FL workers are grouped into clusters under cluster heads. In the upper layer, the cluster heads exchange the intermediate parameters with their one-hop neighbors without the aid of a central server. To improve the sustainable efficiency of the FL system while taking into account the incentive design for workers’ marginal contributions in the system, we propose the reputation-aware hedonic coalition formation game in this article. Specifically, the workers are rewarded for their marginal contribution to the cluster, whereas the reputation opinions of each cluster head is updated in a decentralized manner, thereby deterring malicious behaviors by the cluster head. This improves the performance of the network since cluster heads with higher reputation scores are more reliable in relaying the intermediate model parameters. The simulation results show that our proposed hedonic coalition formation algorithm converges to a Nash-stable partition and improves the network efficiency."",""1558-2183"","""",""10.1109/TPDS.2021.3139039"",""National Research Foundation, Prime Minister's Office, Singapore"; Alibaba Innovative Research; National Research Foundation Singapore(grant numbers:AISG2-RP-2020-019); WASP/NTU(grant numbers:M4082187 (4080)); Singapore Ministry of Education(grant numbers:RG16/20); SUTD(grant numbers:SRG-ISTD-2021-165); SUTD-ZJU IDEA(grant numbers:SUTD-ZJU (VP) 202102); SUTD-ZJU IDEA Seed(grant numbers:SUTD-ZJU SD 202101); NSFC(grant numbers:61827901,62071343);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9665214"",""Federated learning";serverless federated learning;decentralized edge intelligence;"hedonic coalition formation"",""Magnetic heads";Servers;Training;Base stations;Costs;Computational modeling;"Collaborative work"","""",""7"","""",""40"",""IEEE"",""29 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Resilient Real-Valued Consensus in Spite of Mobile Malicious Agents on Directed Graphs,""Y. Wang"; H. Ishii; F. Bonnet;" X. Défago"",""Division of Decision and Control Systems, KTH Royal Institute of Technology, Stockholm, Sweden"; Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan; Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan;" Department of Computer Science, Tokyo Institute of Technology, Tokyo, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2021"",""2022"",""33"",""3"",""586"",""603"",""This article addresses novel real-valued consensus problems in the presence of malicious adversaries that can move within the network and induce faulty behaviors in the attacked agents. By adopting several mobile adversary models from the computer science literature, we develop protocols which can mitigate the influence of such malicious agents. The algorithms follow the class of mean subsequence reduced (MSR) algorithms, under which agents ignore the suspicious values received from neighbors during their state updates. Different from the static adversary models, even after the adversaries move away, the infected agents may remain faulty in their values, whose effects must be taken into account. We develop conditions on the network structures for both the complete and non-complete directed graph cases, under which the proposed algorithms are guaranteed to attain resilient consensus. The tolerance bound for network conditions becomes more strict as the adversaries are allowed to have more power. Extensive simulations are carried out over random graphs to verify the effectiveness of our approach when the information of the adversarial agents in terms of their models and numbers is unknown to the agents."",""1558-2183"","""",""10.1109/TPDS.2021.3096074"",""JST CREST(grant numbers:JPMJCR15K3)"; JST-Mirai Program(grant numbers:18077648); Scientific Research(grant numbers:18H01460);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9479723"",""Fault-tolerant distributed algorithms";multi-agent systems;resilient real-valued consensus;mobile adversary agents;malicious agents;"network connectivity"",""Protocols";Directed graphs;Computer science;Computational modeling;Voltage measurement;Timing;"Security"","""",""9"","""",""40"",""IEEE"",""9 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Robustness of Subsystem Reliability of $k$k-Ary $n$n-Cube Networks Under Probabilistic Fault Model,""X. Liu"; S. Zhou; S. -Y. Hsieh;" H. Zhang"",""Center for Applied Mathematics of Fujian Province and School of Mathematics and Statistics, Fujian Normal University, Fuzhou, China"; Center for Applied Mathematics of Fujian Province and School of Mathematics and Statistics, Fujian Normal University, Fuzhou, China; Department of Computer Science and Information Engineering, National Cheng Kung University, Tainan, Taiwan;" Center for Applied Mathematics of Fujian Province and School of Mathematics and Statistics, Fujian Normal University, Fuzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Sep 2022"",""2022"",""33"",""12"",""4684"",""4693"",""With the emergence of the Big Data era, as multiprocessor systems consisting of multiple processors play a vital role in big data analytics, we are prompted to explore the qualitative and quantitative metric to characterize the reliability of the systems. As the size of the multiprocessor systems grows, the probability of the occurrence of failing processors increases. One metric of the macroscopic reliability of a system is the measure of the collective effect when its subsystems are out of function. The subsystem reliability of a system is the quantitative metric that a fault-free subsystem of specific size is operational as before with the occurrence of individual faults. Although some networks have the same order and similar topologies, there are differences in their subsystem reliabilities. In this work, we focus on the comparison of two distinct topologies of $k$k-ary $n$n-cube networks with the same order and calculate the robustness of reliability bounds of $k$k-ary $n$n-cube networks. We analytically show that the subsystem reliability is negatively correlated with the dimension $n$n, even if two subsystems of $Q_{n}^{k}$Qnk are of the same order. That is, the smaller $n$n is, the larger subsystem reliability of $Q_{n}^{k}$Qnk will be. This work provides a theoretical methodology to choose the more dependable topology of $k$k-ary $n$n-cube networks with the same order. Finally, we apply some numerical simulations to validate the results we established."",""1558-2183"","""",""10.1109/TPDS.2022.3199251"",""National Natural Science Foundation of China(grant numbers:61977016,61572010)"; Natural Science Foundation of Fujian Province(grant numbers:2020J01164,2017J01738);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858633"",""  $k$   k     -ary   $n$   n     -cube networks";subsystem reliability;robustness;"principle of inclusion-exclusion (PIE)"",""Reliability";Topology;Probabilistic logic;Computer network reliability;Robustness;Program processors;"Multiprocessing systems"","""","""","""",""32"",""IEEE"",""17 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Run-Time Remapping Algorithm of Dataflow Actors on NoC-Based Heterogeneous MPSoCs,""M. Rizk"; K. J. M. Martin;" J. -P. Diguet"",""Lab-STICC UMR CNRS 6285, CNRS, Brest, France"; Lab-STICC UMR CNRS 6285, Université de Bretagne-Sud (UBS), Lorient, France;" CROSSING, IRL CNRS 2010, CNRS, Adelaide, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""3959"",""3976"",""Multiprocessor system-on-chip (MPSoC) platforms have been emerging as the main solution to cope with processor frequency ceiling and power density issues while still improving performances. Then, network-on-chip (NoC) has been adopted to provide the increasing number of processors with the required communication bandwidth as well as with the necessary flexibility. Video processing and streaming applications are adopting dynamic dataflow model of computation as the need for high performance parallel computing is growing. Dataflow applications executed on modern MPSoC-based architectures are becoming increasingly dynamic and more data-dependent. Different tasks execute concurrently with significant modifications in their workloads and resource demanding over time depending on the input data. Hence, adopting any static or offline dynamic scheduling for mapping tasks will not cope with the computation variations. This article introduces an original run-time mapping algorithm based on the Move Based (MB) method targeting a dedicated heterogeneous NoC-based MPSoC architecture to achieve workload balancing and optimized communication traffic. The performance of the proposed algorithm is verified by conducting cycle-accurate SystemC simulations of the adopted NoC implementing a real MPEG4-SP decoder. The obtained results reveal the effectiveness of our proposed algorithm. For various real-life videos, the proposed algorithm systematically succeeded to enhance significantly the performance."",""1558-2183"","""",""10.1109/TPDS.2022.3177957"",""Région Bretagne";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9782520"",""NoC";heterogeneous MPSoC;run-time remapping;dataflow actor;"move-based algorithm"",""Computer architecture";Task analysis;Program processors;Computational modeling;Behavioral sciences;Streaming media;"Monitoring"","""","""","""",""36"",""IEEE"",""26 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"SaPus: Self-Adaptive Parameter Update Strategy for DNN Training on Multi-GPU Clusters,""Z. Zhang";" C. Wang"",""Department of Computer Science, The University of Hong Kong, Hong Kong";" Department of Computer Science, The University of Hong Kong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Nov 2021"",""2022"",""33"",""7"",""1569"",""1580"",""Parameter server architecture has been identified as an efficient framework for scaling DNNs training on clusters. For large-scale deployment, communication becomes the bottleneck, and the parameter updating strategy strongly impacts the training performance and accuracy. Recent state-of-art solutions have adopted the local SGD approach, which enables workers to update their local version of models and only aggregate them to update the global parameters after finishing a number of iterations, to alleviate heavy communication pressure on the parameter server and improving the training performance. We identify three limitations of these works. First, these works do not provide an approach for determining when the worker is to update the parameter with the server under asynchronous communication strategies that can guarantee the training performance. Second, local SGD suffers from the problem of unbounded gradient delay. Previous work works well for a short delay while can not guarantee the performance with an increase of gradient delay. Third, they do not consider the system performance when determining the update interval of the local SGD, including the CPU, memory, and network, which affects the training performance extremely. We provide a self-adaptive parameter updating strategy called SaPus, which allows each worker to detect their training results through quantification of the accumulated gradient updates and determine when to update the parameter with the server adaptively and individually. Theoretical lower and upper bound of the update interval is also provided. We also propose a weighted aggregation algorithm based on a global-loss window, which is used to collect the most recent loss value of other workers to calculate a weight for the accumulated gradients of each worker to solve the unbounded delay problem in asynchronous local SGD. To increase the robustness of our parameter updating strategy, a performance model is built to provide a resource-aware lower bound for the update interval. Extensive experimental results generated on GPU cluster indicate that our model improves the training performance of DNNs, achieving up to $66.67\%$66.67% speedup as compared with state-of-art solutions. Further, results show the CPU utilization of server dropped by up to $81.1\%$81.1% and network bandwidth usage reduced to less than $1~Gbps$1Gbps on an average during the training."",""1558-2183"","""",""10.1109/TPDS.2021.3118609"",""Hong Kong RGC Research Impact Fund(grant numbers:R5060-19)"; RGC Collaborative Research Fund(grant numbers:C6021-19EF);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563229"",""Distributed DNN training";self-driven;distributed optimization;communication reduction;"local SGD"",""Training";Servers;Delays;Graphics processing units;Computer architecture;Bandwidth;"System performance"","""",""2"","""",""31"",""IEEE"",""7 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scalable Unsupervised ML: Latency Hiding in Distributed Sparse Tensor Decomposition,""N. Abubaker"; M. O. Karsavuran;" C. Aykanat"",""Department of Computer Engineering, Bilkent University, Ankara, Turkey"; Department of Computer Engineering, Bilkent University, Ankara, Turkey;" Department of Computer Engineering, Bilkent University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3028"",""3040"",""Latency overhead in distributed-memory parallel CPD-ALS scales with the number of processors, limiting the scalability of computing CPD of large irregularly sparse tensors. This overhead comes in the form of sparse reduce and expand operations performed on factor-matrix rows via point-to-point messages. We propose to hide the latency overhead through embedding all of the point-to-point messages incurred by the sparse reduce and expand into dense collective operations which already exist in the CPD-ALS. The conventional parallel CPD-ALS algorithm is not amenable for embedding so we propose a computation/communication rearrangement to enable the embedding. We embed the sparse expand and reduce into a hypercube-based ALL-REDUCE operation to limit the latency overhead to $O(\log _2 K)$O(log2K) for a $K$K-processor system. The embedding comes with the cost of increased bandwidth overhead due to the multi-hop routing of factor-matrix rows during the embedded-ALL-REDUCE. We propose an embedding scheme that takes advantage of the expand/reduce properties to reduce this overhead. Furthermore, we propose a novel recursive bipartitioning framework that enables simultaneous hypergraph partitioning and subhypergraph-to-subhypercube mapping to achieve subtensor-to-processor assignment with the objective of reducing the bandwidth overhead during the embedded-ALL-REDUCE. We also propose a bin-packing-based algorithm for factor-matrix row to processor assignment aiming at reducing processors’ maximum send and receive volumes during the embedded-ALL-REDUCE. Experiments on up to 4096 processors show that the proposed framework scales significantly better than the state-of-the-art point-to-point method."",""1558-2183"","""",""10.1109/TPDS.2021.3128827"",""Scientific and Technological Research Council of Turkey(grant numbers:EEEAG-116E043)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9618826"",""Sparse tensor";tensor decomposition;CANDECOMP/PARAFAC;canonical polyadic decomposition;latency hiding;embedded communication;communication cost;concurrent communication;recursive bipartitioning;"hypergraph partitioning"",""Tensors";Program processors;Sparse matrices;Routing;Tools;Matrix decomposition;"Costs"","""",""1"","""",""40"",""IEEE"",""17 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Scalable, Confidential and Survivable Software Updates,""F. Magnanini"; L. Ferretti;" M. Colajanni"",""Department of Engineering “Enzo Ferrari”, University of Modena and Reggio Emilia, Modena, Italy"; Department of Physics, Computer Science and Mathematics, University of Modena and Reggio Emilia, Modena, Italy;" Department of Computer Science and Engineering, University of Bologna, Bologna, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Jul 2021"",""2022"",""33"",""1"",""176"",""191"",""Software update systems must guarantee high availability, integrity and security even in presence of cyber attacks. We propose the first survivable software update framework for the secure distribution of confidential updates that is based on a distributed infrastructure with no single points of failure. Previous works guarantee either survivability or confidentiality of software updates but do not ensure both properties. Our proposal is based on an original application of a multi-authority attribute-based encryption scheme in the context of decentralized access control management that avoids single-point-of-vulnerability. We describe the original framework, propose the protocols to implement it, and demonstrate its feasibility through a security and performance evaluation."",""1558-2183"","""",""10.1109/TPDS.2021.3090330"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459483"",""Software updates";survivability;transparency;"proprietary software"",""Software";Proposals;Encryption;Servers;Authentication;Access control;"Protocols"","""",""1"","""",""28"",""IEEE"",""17 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"ScaleFlux: Efficient Stateful Scaling in NFV,""L. Liu"; H. Xu; Z. Niu; J. Li; W. Zhang; P. Wang; J. Li; J. C. Xue;" C. Wang"",""Zhongguancun Laboratory, Beijing, China"; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong SAR, China; Microsoft Research Asia, Beijing, China; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China; Shandong Provincial Key Laboratory of Computer Networks, Shandong Computer Science Center (NationalSupercomputer Center in Jinan), Qilu University of Technology (Shandong Academy of Sciences), Jinan, Shandong, China; Theory Lab, Huawei Hong Kong Research Center, Hong Kong SAR, China; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China; Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China;" Department of Computer Science, City University of Hong Kong, Hong Kong SAR, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Sep 2022"",""2022"",""33"",""12"",""4801"",""4817"",""Network function virtualization (NFV) enables elastic scaling to middlebox deployment and management. Therefore, efficient stateful scaling is an important task because operators often need to shift traffic and the associated flow states across VNF instances to deal with time-varying loads. Existing NFV scaling methods, however, typically focus on one aspect of the scaling pipeline and does not offer an end-to-end scaling framework. This article presents ScaleFlux, a complete stateful scaling system that efficiently reduces flow-level latency and achieves near-optimal resource usage. ScaleFlux (1) monitors traffic load for each VNF instance and adopts a queue-based mechanism to detect load burstiness timely, (2) deploys a flow bandwidth predictor to predict flow bandwidth time-series with the ABCNN-LSTM model, and (3) schedules the necessary flow and state migration using the simulated annealing algorithm to achieve both flow-level latency guarantee and resource usage minimization. Testbed evaluation with a five-machine cluster shows that ScaleFlux reduces flow completion time by at least 8.7× for all the workloads and achieves near-optimal CPU usage during scaling."",""1558-2183"","""",""10.1109/TPDS.2022.3204209"",""National Natural Science Foundation of China(grant numbers:61802233)"; Qilu University of Technology(grant numbers:2020KJC-ZD02); General Research Fund from Hong Kong Research(grant numbers:11209520); Chinese University of Hong Kong(grant numbers:5501329,5501517,4937007,4937008);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9875954"",""Network function virtualization";network load detection;flow bandwidth prediction;stateful scaling;"service level agreements"",""Logic gates";Bandwidth;Service level agreements;Packet loss;Computer science;Cloud computing;"Wide area networks"","""",""3"","""",""74"",""IEEE"",""5 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Scaling Poisson Solvers on Many Cores via MMEwald,""M. Wu"; Y. Wu; H. Shang; Y. Liu; H. Cui; F. Li; X. Duan; Y. Zhang;" X. Feng"",""State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China; National Supercomputer Center in Wuxi, Wuxi, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China;" State Key Laboratory of Computer Architecture, Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""16 Dec 2021"",""2022"",""33"",""8"",""1888"",""1901"",""The Poisson solver for the calculation of the electrostatic potential is an essential primitive in quantum mechanics calculations. In this article, we adopt the Ewald method and propose a highly-optimized and scalable framework for Poisson solver, MMEwald, on the new generation Sunway supercomputer, capable of utilizing the collection of 390-core accelerators it uses. The MMEwald is based on a grid adapted cut-plane approach to partition the points into batches and distribute the batch to the processors. Furthermore, we propose a set of architecture-specific optimizations to efficiently utilize the memory bandwidth and computation capacity of the supercomputer. Experimental results demonstrate the efficiency of the MMEwald in providing strong and weak scaling performance."",""1558-2183"","""",""10.1109/TPDS.2021.3127138"",""National Natural Science Foundation of China(grant numbers:22003073)"; State Key Laboratory of Computer Architecture Foundation(grant numbers:CARCH 4205,CARCH 4411); National Natural Science Foundation of China(grant numbers:62090024,61872043,61802368);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9611019"",""Poisson solver";architecture-specific optimizations;"many-core processor"",""Optimization";Bandwidth;Supercomputers;Electric potential;Boundary conditions;Electrostatics;"Silicon"","""","""","""",""36"",""IEEE"",""10 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"SciSpot: Scientific Computing On Temporally Constrained Cloud Preemptible VMs,""J. Kadupitiya"; V. Jadhao;" P. Sharma"",""Department of Intelligent Systems Engineering, Indiana University, Bloomington, IN, USA"; Department of Intelligent Systems Engineering, Indiana University, Bloomington, IN, USA;" Department of Intelligent Systems Engineering, Indiana University, Bloomington, IN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3575"",""3588"",""Scientific computing applications are being increasingly deployed on cloud computing platforms. Transient servers such as EC2 spot instances and Google Preemptible VMs, can be used to lower the costs of running applications on the cloud by up to $10\times$10×. However, the frequent preemptions and resource heterogeneity of these transient servers introduces many challenges in their effective and efficient use. In this paper, we develop techniques for modeling and mitigating preemptions of transient servers, and present SciSpot, a software framework that enables low-cost scientific computing on the cloud. SciSpot deploys applications on Google Cloud Preemptible Virtual Machines that exhibit temporally constrained preemptions: VMs are always preempted in a 24 hour interval. Our empirical analysis shows that the preemption rate is generally bathtub shaped, which raises multiple fundamental challenges in performance modeling and policy design. We develop a new reliability model for temporally constrained preemptions, and use statistical mechanics to show why the bathtub shape is generally exhibited. SciSpot’s design is guided by our observation that many emerging scientific computing applications that integrate machine learning with simulations, can be deployed as “bags” of jobs, which represent multiple instantiations of the same computation with different physical model parameters. For a bag of jobs, SciSpot finds the optimal transient server on-the-fly, by taking into account the price, performance, and preemption rates of different servers. SciSpot reduces costs by $5\times$5× compared to conventional cloud deployments, and reduces makespans by up to $10\times$10× compared to conventional high performance computing clusters."",""1558-2183"","""",""10.1109/TPDS.2022.3157272"",""Google"; U.S. Department of Energy(grant numbers:DE-SC0021418); National Science Foundation(grant numbers:OAC-2112606);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739004"",""Distributed systems";cloud computing;"scientific computing"",""Cloud computing";Computational modeling;Transient analysis;Internet;Servers;Costs;"Analytical models"","""","""","""",""65"",""IEEE"",""21 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"SconeKV: A Scalable, Strongly Consistent Key-Value Store,""J. Gonçalves"; M. Matos;" R. Rodrigues"",""Instituto Superior Técnico (ULisboa), Lisboa, Portugal"; Instituto Superior Técnico (ULisboa), Lisboa, Portugal;" Instituto Superior Técnico (ULisboa), Lisboa, Portugal"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4164"",""4175"",""For decades, relational databases provided a strong foundation for constructing applications due to their ACID properties. However, distributed applications reached a scale, both in terms of data volume and number of concurrent clients, that traditional databases cannot accommodate. NoSQL databases addressed this problem by trading consistency for scalability, namely through horizontal scalability schemes supported by optimistic replication protocols, which only guarantee eventual consistency. In this paper, we explore a novel design between the two extremes, which is able to scale to large deployments while still offering strong consistency guarantees in the form of serializable transactions. Our key insight is to leverage recent advances in membership services that provide strongly consistent views at scale. Those assurances from the membership layer simplify building efficient and consistent storage protocols. Our evaluation of the resulting system, SconeKV, in a realistic scenario shows that it scales and performs better than CockroachDB while being competitive with Cassandra."",""1558-2183"","""",""10.1109/TPDS.2022.3179903"",""FCT"; Fundação para a Ciência e a Tecnologia(grant numbers:UIDB/50021/2020,PTDC/CCI-INF/6762/2020,Lisboa-01-0145-FEDER-031456);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9786754"",""Consistency";distributed systems;scalability;"storage"",""Protocols";Scalability;Semantics;Distributed databases;Relational databases;Peer-to-peer computing;"Synchronization"","""","""","""",""40"",""IEEE"",""2 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Secure Deep Neural Network Models Publishing Against Membership Inference Attacks Via Training Task Parallelism,""Y. Mao"; W. Hong; B. Zhu; Z. Zhu; Y. Zhang;" S. Zhong"",""Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China;" Department of Computer Science and Technology, State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""3079"",""3091"",""Vast data and computing resources are commonly needed to train deep neural networks, causing an unaffordable price for individual users. Motivated by the increasing demands of deep learning applications, sharing well-trained models becomes popular. The owner of a pre-trained model can share it by publishing the model directly or providing a prediction interface. Either way, individual users can benefit from deep learning without much cost, and computing resources can be saved. However, recent studies of machine learning security have identified severe threats to these model publishing approaches. This article will focus on the privacy leakage issue of publishing well-trained deep neural network models. To tackle this problem, we propose a series of secure model publishing solutions based on training task parallelism. Specifically, we show how to estimate private model parameters through parallel model training and generate new model parameters in a privacy-preserving manner to replace the original ones for publishing. Based on data parallelism and parameter generating techniques, we design another two solutions concentrating on model quality and parameter privacy, respectively. Through privacy leakage analysis and experimental attack evaluation, we conclude that deep neural network models published with our solutions can provide on-demand model quality guarantees and resist membership inference attacks."",""1558-2183"","""",""10.1109/TPDS.2021.3129612"",""National Key R&D Program of China(grant numbers:2020YFB1005900,NSFC-61902176,BK20190294,NSFC-61872179,NSFC-61872176)"; Leading-edge Technology Program of Jiangsu NSF(grant numbers:BK20202001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623516"",""Machine learning security";membership inference attack;data parallelism;"deep neural network"",""Computational modeling";Data models;Publishing;Training;Privacy;Deep learning;"Analytical models"","""",""3"","""",""52"",""IEEE"",""22 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"SelectiveEC: Towards Balanced Recovery Load on Erasure-Coded Storage Systems,""L. Xu"; M. Lyu; Q. Li; L. Xie; C. Li;" Y. Xu"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Mar 2022"",""2022"",""33"",""10"",""2386"",""2400"",""Erasure coding (EC) has been commonly used to offer high data reliability with low storage cost. Upon failures, the lost blocks are recovered in batches. Due to the limited number of stripes, the data layout within a batch is non-uniform. Together with the random selection of source and replacement nodes for recovery tasks, the recovery workload among live nodes is skewed within a batch, which severely slows down failure recovery. To solve this problem, We present SelectiveEC, a new recovery task scheduling module that provides provable network traffic and recovery load balancing for large-scale EC-based storage systems. It relies on bipartite graphs to model the recovery traffic among live nodes. Then, it intelligently selects tasks to form batches and carefully determines where to read source blocks or to store recovered ones, using theories such as a perfect or maximum matching and $k$k-regular spanning subgraph. SelectiveEC supports single-node failure and multi-node failure recovery, and can be deployed in both homogeneous and heterogeneous network environments. We implement SelectiveEC in HDFS, and evaluate its recovery performance in a local cluster of 18 nodes and AWS EC2 of 50 virtual machine instances. SelectiveEC increases the recovery throughput by up to $30.68\%$30.68% compared with state-of-the-art baselines in homogeneous network environments. It further achieves $1.32\times$1.32× recovery throughput and $1.23\times$1.23× benchmark throughput of HDFS on average in heterogeneous network environments, due to the straggler avoidance by the balanced scheduling."",""1558-2183"","""",""10.1109/TPDS.2021.3129973"",""National Natural Science Foundation of China(grant numbers:61832011,61772486)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9625750"",""Distributed storage system";reliability;erasure coding;recovery traffic;"load balance"",""Task analysis";Codes;Bandwidth;Decoding;Throughput;Encoding;"Load modeling"","""",""4"","""",""52"",""IEEE"",""23 Nov 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"SGCNAX: A Scalable Graph Convolutional Neural Network Accelerator With Workload Balancing,""J. Li"; H. Zheng; K. Wang;" A. Louri"",""Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA"; Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA; Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA;" Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2834"",""2845"",""Convolutional Neural Networks (GCNs) have emerged as promising tools for graph-based machine learning applications. Given that GCNs are both compute- and memory-intensive, this constitutes a major challenge for the underlying hardware to efficiently process large-scale GCNs. In this article, we introduce SGCNAX, a scalable GCN accelerator architecture for the high-performance and energy-efficient acceleration of GCNs. Unlike prior GCN accelerators that either employ limited loop optimization techniques, or determine the design variables based on random sampling, we systematically explore the loop optimization techniques for GCN acceleration and propose a flexible GCN dataflow that adapts to different GCN configurations to achieve optimal efficiency. We further propose two hardware-based techniques to address the workload imbalance problem caused by the unbalanced distribution of zeros in GCNs. Specifically, SGCNAX exploits an outer-product-based computation architecture that mitigates the intra-PE (Processing Elements) workload imbalance, and employs a group-and-shuffle approach to mitigate the inter-PE workload imbalance. Simulation results show that SGCNAX performs 9.2×, 1.6× and 1.2× better, and reduces DRAM accesses by a factor of 9.7×, 2.9× and 1.2× compared to HyGCN, AWB-GCN, and GCNAX, respectively."",""1558-2183"","""",""10.1109/TPDS.2021.3133691"",""National Science Foundation(grant numbers:CCF-1702980,CCF-1812495,CCF-1901165,CCF-2131946)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9645224"",""Graph convolutional neural networks";dataflow accelerators;domain-specific accelerators;"memory access optimization"",""Random access memory";Optimization;Engines;Registers;Neural networks;Computational modeling;"Accelerator architectures"","""",""4"","""",""61"",""IEEE"",""10 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Silent-PIM: Realizing the Processing-in-Memory Computing With Standard Memory Requests,""C. H. Kim"; W. J. Lee; Y. Paik; K. Kwon; S. Y. Kim; I. Park;" S. W. Kim"",""Department of Electrical Engineering, Korea University, Seoul, Korea"; Department of Electrical Engineering, Korea University, Seoul, Korea; Department of Electrical Engineering, Korea University, Seoul, Korea; Department of Electrical Engineering, Korea University, Seoul, Korea; Department of Electrical Engineering, Korea University, Seoul, Korea; K hynix Inc., Icheon, South Korea;" Department of Electrical Engineering, Korea University, Seoul, Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Aug 2021"",""2022"",""33"",""2"",""251"",""262"",""The Deep Neural Network (DNN), Recurrent Neural Network (RNN) applications, rapidly becoming attractive to the market, process a large amount of low-locality data"; thus, the memory bandwidth limits their peak performance. Therefore, many data centers actively adapt high-bandwidth memory like HBM2/HBM2E to resolve the problem. However, this approach would not provide a complete solution since it still transfers the data from the memory to the computing unit. Thus, processing-in-memory (PIM), which performs the computation inside memory, has attracted attention. However, most previous methods require the modification or the extension of core pipelines and memory system components like memory controllers, making the practical implementation of PIM very challenging and expensive in development. In this article, we propose a Silent-PIM that performs the PIM computation with standard DRAM memory requests;" thus, requiring no hardware modifications and allowing the PIM memory device to perform the computation while servicing non-PIM applications’ memory requests. We can achieve our design goal by preserving the standard memory request behaviors and satisfying the DRAM standard timing requirements. In addition, using standard memory requests makes it possible to use DMA as a PIM’s offloading engine, resulting in processing the PIM memory requests fast and making a core perform other tasks. We compared the performance of three Long Short-Term Memory models (LSTM) kernels on real platforms, such as the Silent-PIM modeled on the FPGA, GPU, and CPU. For $(p \times 512) \times (512 \times 2048)$(p×512)×(512×2048) matrix multiplication with a batch size $p$p varying from 1 to 128, the Silent-PIM performed up to 16.9x and 24.6x faster than GPU and CPU, respectively, $p=1$p=1, which was the case without having any data reuse. At $p=128$p=128, the highest data reuse case, the GPU performance was the highest, but the PIM performance was still higher than the CPU execution. Similarly, at $(p \times 2048)$(p×2048) element-wise multiplication and addition, where there was no data reuse, the Silent-PIM always achieved higher than both CPU and GPU. It also showed that when the PIM’s EDP performance was superior to the others in all the cases having no data reuse."",""1558-2183"","""",""10.1109/TPDS.2021.3065365"",""SK hynix Inc."; Korea Institute for Advancement of Technology; Korea Government(grant numbers:N0001883);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9376660"",""Silent-PIM";in-memory processing;standard memory requests;DMA;"LSTM"",""Random access memory";Standards;Performance evaluation;Memory management;Timing;Engines;"Field programmable gate arrays"","""",""10"","""",""29"",""IEEE"",""11 Mar 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"SmartVM: A Smart Contract Virtual Machine for Fast On-Chain DNN Computations,""T. Li"; Y. Fang; Y. Lu; J. Yang; Z. Jian; Z. Wan;" Y. Li"",""College of Computer Science, Nankai University, Tianjin, China"; College of Computer Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; College of Cyber Science, Nankai University, Tianjin, China; College of Computer Science, Nankai University, Tianjin, China; Zhejiang Lab, Hangzhou, Zhejiang, China;" College of Computer Science, Nankai University, Tianjin, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Sep 2022"",""2022"",""33"",""12"",""4100"",""4116"",""Blockchain-based artificial intelligence (BC-AI) has been applied for protecting deep neural network (DNN) data from being tampered with, which is expected to further boost trusted distributed AI applications in many fields. However, due to smart contract execution environment architectural defects, it is challenging for previous BC-AI systems to support computing-intensive tasks on-chain performing such as DNN convolution operations. They have to offload computations and a large amount of data from blockchain to off-chain platforms to execute smart contracts as native code. This failure to take advantage of data locality has become one of the major critical performance bottlenecks in BC-AI system. To this end, in this article, we propose SmartVM with optimization methods to support on-chain DNN inference for BC-AI system. The key idea is to design and optimize the computing mechanism and storage structure of smart contract execution environment according to the characteristics of DNN such as high computational parallelism and large data volume. We decompose SmartVM into three components: 1) a compact DNN-oriented instruction set to describe computations in a short number of instructions to reduce interpretation time. 2) a memory management mechanism to make SmartVM memory dynamic free/allocated according to the size of DNN feature maps. 3) a block-based weight prefetching and parallel computing method to organize each layer's computing and weights prefetching in a pipelined manner. We perform the typical image classification in a private Ethereum blockchain testbed to evaluate SmartVM performance. Experimental results highlight that SmartVM can support DNN inference on-chain with roughly the same efficiency against the native code execution. Compared with the traditional off-chain computing, SmartVM can speed up the overall execution by 70×, 16×, 11×, and 12× over LeNet5, AlexNet, ResNet18, and MobileNet, respectively. The memory footprint can be reduced by 84%, 90.8%, 94.3%, and 93.7% over the above four models, while offering the same level model accuracy. This article sheds light on the design space of the smart contract virtual machine for DNN computation and is promising to further boost BC-AI applications."",""1558-2183"","""",""10.1109/TPDS.2022.3177405"",""CCF-AFSG Research Fund(grant numbers:CCF-AFSG RF20210031)"; Special Funding for Excellent Enterprise Technology Correspondent of Tianjin(grant numbers:21YDTPJC00380); National Natural Science Foundation of China(grant numbers:62002175); Open Project Fund of State Key Laboratory of Computer Architecture; Institute of Computing Technology, Chinese Academy of Sciences(grant numbers:CARCHB202016); Key Research Project of Zhejiang Lab(grant numbers:2021KF0AB04); Natural Science Foundation of Tianjin City(grant numbers:20JCZDJC00610); Foundation of Information Security Evaluation Center of Civil Aviation; Civil Aviation University of China(grant numbers:ISECCA-202102); People's Republic of China ministry of education science and technology development center(grant numbers:2019J02019); Tianjin Graduate Scientific Research Innovation Project(grant numbers:2021YJSB014);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9780577"",""Deep neural network";smart contract;virtual machine;"architectural support technology"",""Smart contracts";Blockchains;Virtual machining;Convolutional neural networks;Convolution;Task analysis;"Random access memory"","""",""6"","""",""52"",""IEEE"",""24 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"SMURF: Efficient and Scalable Metadata Access for Distributed Applications,""B. Zhang";" T. Kosar"",""National Center for Supercomputing Applications, University of Illinois at Urbana-Champaign, Champaign, IL, USA";" Department of Computer Science and Engineering, University at Buffalo (SUNY), Buffalo, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""3915"",""3928"",""In parallel with big data processing and analysis dominating the usage of distributed and Cloud infrastructures, the demand for distributed metadata access and transfer has increased. The volume of data generated by many application domains exceeds petabytes, while the corresponding metadata amounts to terabytes or even more. This article proposes a novel solution for efficient and scalable metadata access for distributed applications across wide-area networks, dubbed SMURF. Our solution combines novel pipelining and concurrent transfer mechanisms with reliability, provides distributed continuum caching and semantic locality-aware prefetching strategies to sidestep fetching latency, and achieves scalable and high-performance metadata fetch/prefetch services in the Cloud. We incorporate the phenomenon of semantic locality awareness for increased prefetch prediction rate using real-life application I/O traces from Yahoo! Hadoop audit logs and propose a novel prefetch predictor. By effectively caching and prefetching metadata based on the access patterns, our continuum caching and prefetching mechanism significantly improves the local cache hit rate and reduces the average fetching latency. We replay approximately 20 Million metadata access operations from real audit traces, where SMURF achieves 90% accuracy during prefetch prediction and reduced the average fetch latency by 50% compared to the state-of-the-art mechanisms."",""1558-2183"","""",""10.1109/TPDS.2022.3175596"",""National Science Foundation(grant numbers:OAC-1724898,CCF-2007829)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9779105"",""Heterogeneity";scalability;metadata access;prefetch prediction;continuum caching;"semantic locality"",""Metadata";Prefetching;Servers;Internet of Things;Protocols;Wide area networks;"Semantics"","""",""1"","""",""52"",""IEEE"",""19 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Solving Consensus in True Partial Synchrony,""S. Srinivasan";" R. Kandukoori"",""Department of Computer Science and Engineering, National Institute of Technology Warangal, Hanamkonda, Telangana, India";" Department of Computer Science and Engineering, National Institute of Technology Warangal, Hanamkonda, Telangana, India"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jun 2022"",""2022"",""33"",""12"",""3478"",""3490"",""The notion of partial synchrony has been introduced to circumvent the FLP impossibility result for solvability of consensus in fault tolerant distributed systems. This notion helps us to evaluate the efficiency of algorithms that needs to solve consensus in the given time interval $ T[\tau _{1} \leq T_{S} \leq \tau _{2}]$T[τ1≤TS≤τ2] where $ T_{S}$TS is the stable time when the system becomes synchronous after a transient period of asynchrony. It has been shown that consensus can be solved in constant time after system enters $ T_{S}$TS with an upper bound of $ T_{S}+17\delta$TS+17δ, and this was later reduced to $ T_{S}+11\delta$TS+11δ, where $ \delta$δ is the upper bound on message delivery. The above result is not trivial as they allowed processes that failed before $ T_{S}$TS to recover after $ T_{S}$TS and participate in consensus. But these algorithms assume that the upper bound for message delivery $\delta$δ holds even for messages sent before $ T_{S}$TS (i.e.,) when the system is in asynchrony. This assumption is limiting as messages can get delayed beyond expected timings in real time distributed systems due to different reasons like network congestion, packet re-transmission, e.t.c. In this work, we overcome this limitation by assuming messages sent before $ T_{S}$TS has no upper bound while also allowing process restarts after $ T_{S}$TS and show that consensus can be solved in constant time after $ T_{S}$TS. Our proposed algorithm solves consensus in $ T_{S}+10\delta$TS+10δ time, which is more efficient than the current upper bound and without the limiting assumption of bounded message delay before $ T_{S}$TS."",""1558-2183"","""",""10.1109/TPDS.2022.3156925"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9729476"",""Paxos";consensus;agreement problem;fault tolerance;"partial synchronous systems"",""Upper bound";Fault tolerant systems;Fault tolerance;Transient analysis;Synchronization;Real-time systems;"Limiting"","""",""2"","""",""12"",""IEEE"",""7 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"SPRINT: A High-Performance, Energy-Efficient, and Scalable Chiplet-Based Accelerator With Photonic Interconnects for CNN Inference,""Y. Li"; A. Louri;" A. Karanth"",""Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA"; Department of Electrical and Computer Engineering, George Washington University, Washington, DC, USA;" School of Electrical Engineering and Computer Science, Ohio University, Athens, OH, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Mar 2022"",""2022"",""33"",""10"",""2332"",""2345"",""Chiplet-based convolution neural network (CNN) accelerators have emerged as a promising solution to provide substantial processing power and on-chip memory capacity for CNN inference. The performance of these accelerators is often limited by inter-chiplet metallic interconnects. Emerging technologies such as photonic interconnects can overcome the limitations of metallic interconnects due to several superior properties including high bandwidth density and distance-independent latency. However, implementing photonic interconnects in chiplet-based CNN accelerators is challenging and requires combined effort of network architectural optimization and CNN dataflow customization. In this article, we propose SPRINT, a chiplet-based CNN accelerator that consists of a global buffer and several accelerator chiplets. SPRINT introduces two novel designs: (1) a photonic inter-chiplet network that can adapt to specific communication patterns in CNN inference through wavelength allocation and waveguide reconfiguration, and (2) a CNN dataflow that can leverage the broadcasting capability of photonic interconnects while minimizing the costly electrical-to-optical and optical-to-electrical signal conversions. Simulations using multiple CNN models show that SPRINT achieves up to 76% and 68% reduction in execution time and energy consumption, respectively, as compared to other state-of-the-art chiplet-based architectures with either metallic or photonic interconnects."",""1558-2183"","""",""10.1109/TPDS.2021.3139015"",""National Science Foundation(grant numbers:CCF-1702980,CCF-1812495,CCF-1901165,CCF-1953980,CCF-1513606,CCF-1703013,CCF-1901192)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664279"",""Convolution neural network";chiplet;accelerator;"photonic interconnects"",""Photonics";Convolutional neural networks;Optical waveguides;Optical switches;Optical filters;Convolution;"Optical network units"","""",""6"","""",""65"",""IEEE"",""28 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"State Space Model and Queuing Network Based Cloud Resource Provisioning for Meshed Web Systems,""Y. Lei"; Z. Cai; X. Li;" R. Buyya"",""School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China"; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China;" Cloud Computing and Distributed Systems (CLOUDS) Laboratory, School of Computing and Information Systems, The University of Melbourne, Melbourne, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3787"",""3799"",""Functions provided by Web applications are increasingly diverse which make their structures complicated and meshed. Cloud computing platforms provide elastic computing capacities for these meshed Web systems to guarantee Service Level Agreement (SLA). Though workloads of meshed Web systems usually change steadily and periodically in total, sometimes there are sudden fluctuations. In this paper, a hybrid State-space-model-and-Queuing-network based Feedback control method (SQF) is developed for auto-scaling Virtual Machines (VMs) allocated to each tier of meshed Web systems. For the case with workloads changing steadily, a State-space-model based static Feedback Control method (SFC) is proposed in SQF to stabilize request response times near the reference time. For unsteadily changing workloads, a Queuing-network based multi-tier collaborative Feedback Control method (QFC) is proposed for effectively eliminating bottlenecks. QFC builds a control system for each tier individually and uses the queuing network to measure the interaction relationships among different tiers. Experimental results show that QFC is able to improve the efficiency of eliminating bottlenecks (decreasing upper-limit SLA violation ratios by 31.99%$\sim$∼56.52%) with similar or a little bit high VM rental costs compared to existing methods while SFC obtains more stable response times for requests with reasonable additional costs."",""1558-2183"","""",""10.1109/TPDS.2022.3170834"",""National Key Research and Development Program of China(grant numbers:2018YFB1402500)"; National Natural Science Foundation of China(grant numbers:61972202,61872186,61872077,61832004); Fundamental Research Funds for the Central Universities(grant numbers:30919011235);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9764612"",""Bottleneck eliminating";cloud computing;feedback control;resource provisioning;"state-space model"",""Queueing analysis";Feedback control;Control theory;Time factors;Cloud computing;Service level agreements;"Predictive models"","""","""","""",""46"",""IEEE"",""27 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Storage-Heterogeneity Aware Task-based Programming Models to Optimize I/O Intensive Applications,""H. Elshazly"; J. Ejarque;" R. M. Badia"",""Barcelona Supercomputing Center (BSC), Barcelona, Spain"; Barcelona Supercomputing Center (BSC), Barcelona, Spain;" Barcelona Supercomputing Center (BSC), Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3589"",""3599"",""Task-based programming models have enabled the optimized execution of the computation workloads of applications. These programming models can take advantage of large-scale distributed infrastructures by allowing the parallel and distributed execution of applications in high-level work components called tasks. Nevertheless, in the era of Big Data and Exascale, the amount of data produced by modern scientific applications has already surpassed terabytes and is rapidly increasing. Hence, I/O performance became the bottleneck to overcome in order to achieve more total performance improvement. New storage technologies offer higher bandwidth and faster solutions than traditional Parallel File Systems (PFS). Such storage devices are deployed in modern day infrastructures to boost I/O performance by offering a fast layer that absorbs the generated data. Therefore, it is necessary for any programming model targeting more performance to manage this heterogeneity and take advantage of it to improve the I/O performance of applications. Towards this goal, we propose in this article a set of programming model capabilities that we refer to as Storage-Heterogeneity Awareness. Such capabilities include: (i) abstracting the heterogeneity of storage systems, and (ii) optimizing I/O performance by supporting dedicated I/O schedulers and an automatic data flushing technique. The evaluation section of this article presents the performance results of different applications on the MareNostrum CTE-Power heterogeneous storage cluster. Our experiments demonstrate that a storage-heterogeneity aware programming model can achieve up to almost 5x I/O performance speedup and 48% total time improvement compared to the reference PFS-based usage of the execution infrastructure."",""1558-2183"","""",""10.1109/TPDS.2022.3161123"",""European Commission(grant numbers:721865)"; Spanish Government(grant numbers:PID2019-107255GB); Generalitat de Catalunya(grant numbers:2014-SGR-1051);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9739916"",""Heterogeneous storage systems";task-based programming models;I/O intensive applications;I/O scheduling;task scheduling;automatic data movement;heterogeneity abstraction;resource pooling;"checkpointing"",""Task analysis";Programming;Performance evaluation;Computational modeling;Bandwidth;Proposals;"Random access memory"","""",""3"","""",""27"",""IEEE"",""22 Mar 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"STRETCH: Virtual Shared-Nothing Parallelism for Scalable and Elastic Stream Processing,""V. Gulisano"; H. Najdataei; Y. Nikolakopoulos; A. V. Papadopoulos; M. Papatriantafilou;" P. Tsigas"",""Chalmers University of Technology, Göteborg, Sweden"; Chalmers University of Technology, Göteborg, Sweden; ZeroPoint Technologies, Gothenburg, Sweden; Mälardalen University, Västerås, Sweden; Chalmers University of Technology, Göteborg, Sweden;" Chalmers University of Technology, Göteborg, Sweden"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4221"",""4238"",""Stream processing applications extract value from raw data through Directed Acyclic Graphs of data analysis tasks. Shared-nothing (SN) parallelism is the de-facto standard to scale stream processing applications. Given an application, SN parallelism ins9tantiates several copies of each analysis task, making each instance responsible for a dedicated portion of the overall analysis, and relies on dedicated queues to exchange data among connected instances. On the one hand, SN parallelism can scale the execution of applications both up and out since threads can run task instances within and across processes/nodes. On the other hand, its lack of sharing can cause unnecessary overheads and hinder the scaling up when threads operate on data that could be jointly accessed in shared memory. This trade-off motivated us in studying a way for stream processing applications to leverage shared memory and boost the scale up (before the scale out) while adhering to the widely-adopted and SN-based APIs for stream processing applications. We introduce STRETCH, a framework that maximizes the scale up and offers instantaneous elastic reconfigurations (without state transfer) for stream processing applications. We propose the concept of Virtual Shared-Nothing (VSN) parallelism and elasticity and provide formal definitions and correctness proofs for the semantics of the analysis tasks supported by STRETCH, showing they extend the ones found in common Stream Processing Engines. We also provide a fully implemented prototype and show that STRETCH's performance exceeds that of state-of-the-art frameworks such as Apache Flink and offers, to the best of our knowledge, unprecedented ultra-fast reconfigurations, taking less than 40 ms even when provisioning tens of new task instances."",""1558-2183"","""",""10.1109/TPDS.2022.3181979"",""Swedish Foundation for Strategic Research(grant numbers:GMT14-0032)"; Vetenskapsrådet(grant numbers:2016-03800,2020-05094,2021-05424,2021-05443); Chalmers AoA frameworks Energy and Production;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795090"",""Stream processing";shared-nothing parallelism;shared-memory;elasticity;"scalability"",""Parallel processing";Watermarking;Task analysis;Semantics;Metadata;Standards;"Prototypes"","""",""3"","""",""48"",""IEEE"",""13 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TaiChi: A Hybrid Compression Format for Binary Sparse Matrix-Vector Multiplication on GPU,""J. Gao"; W. Ji; Z. Tan; Y. Wang;" F. Shi"",""School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China"; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China;" School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3732"",""3745"",""Binary Sparse Matrix-Vector Multiplication (SpMV) is a heavy computational kernel in weblink analysis, integer factorization, compressed sensing, spectral graph theory, and other domains. Testing several popular GPU-based SpMV implementations on 400 sparse matrices, we observed that data transfer to GPU memory accounts for a large part of the total computation time. The transfer of constant value “1”s can be easily eliminated for binary sparse matrices. However, compressing index arrays has always been a great challenge. This article proposes a new compression format TaiChi to further reduce index data copies and improve the performance of SpMV, especially for diagonally dominant binary sparse matrices. Input matrices are first partitioned into relatively dense and ultra-sparse areas. Then the dense areas are encoded inversely by marking “0”s, while the ultra-sparse area is encoded by marking “1”s. We also designed a new SpMV algorithm only using addition and subtraction for binary matrices based on our partition and encoding format. Evaluation results on real-world binary sparse matrices show that our hybrid encoding for binary matrix significantly reduces the data transfer and speeds up the kernel execution. It achieves the highest transfer and kernel execution speedups of 5.63x and 3.84x on GTX 1080 Ti, 3.39x and 3.91x on Tesla V100."",""1558-2183"","""",""10.1109/TPDS.2022.3170501"",""National Natural Science Foundation of China(grant numbers:61972033)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9763312"",""Binary sparse matrix";SpMV;GPU;parallel computing;"mathematical morphology"",""Sparse matrices";Kernel;Graphics processing units;Indexes;Arrays;Encoding;"Partitioning algorithms"","""",""1"","""",""67"",""IEEE"",""26 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Taming System Dynamics on Resource Optimization for Data Processing Workflows: A Probabilistic Approach,""A. C. Zhou"; W. Xue; Y. Xiao; B. He; S. Ibrahim;" R. Cheng"",""College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China"; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; School of Computing, National University of Singapore, Singapore; Inria, Univ. Rennes, CNRS, IRISA, Rennes, France;" Department of Computer Science, University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jul 2021"",""2022"",""33"",""1"",""231"",""248"",""In many data-intensive applications, workflow is often used as an important model for organizing data processing tasks and resource provisioning is an important and challenging problem for improving the performance of workflows. Recently, system variations in the cloud and large-scale clusters, such as those in I/O and network performances and failure events, have been observed to greatly affect the performance of workflows. Traditional resource provisioning methods, which overlook these variations, can lead to suboptimal resource provisioning results. In this article, we provide a general solution for workflow performance optimizations considering system variations. Specifically, we model system dynamics as time-dependent random variables and take their probability distributions as optimization input. Despite its effectiveness, this solution involves heavy computation overhead. Thus, we propose three pruning techniques to simplify workflow structure and reduce the probability evaluation overhead. We implement our techniques in a runtime library, which allows users to incorporate efficient probabilistic optimization into existing resource provisioning methods. Experiments show that probabilistic solutions can improve the performance by up to 65 percent compared to state-of-the-art static solutions, and our pruning techniques can greatly reduce the overhead of our probabilistic approach."",""1558-2183"","""",""10.1109/TPDS.2021.3091400"",""National Natural Science Foundation of China(grant numbers:61802260)"; Shenzhen Science and Technology Foundation(grant numbers:JCYJ20180305125737520); Natural Science Foundation of SZU(grant numbers:000370); Microsoft Research Asia; ANR KerStream(grant numbers:ANR-16-CE25-0014-01); Research Grants Council of HK(grant numbers:17229116,106150091,17205115); University of HK(grant numbers:104004572,102009508,104004129); Innovation&Technology Commission of HK(grant numbers:MRP/029/18);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9462122"",""Cloud dynamics";resource optimization;"data processing workflows"",""Optimization";Task analysis;Probabilistic logic;Dynamic scheduling;Data processing;Cloud computing;"Probability distribution"","""",""1"","""",""51"",""IEEE"",""22 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Taskflow: A Lightweight Parallel and Heterogeneous Task Graph Computing System,""T. -W. Huang"; D. -L. Lin; C. -X. Lin;" Y. Lin"",""Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA"; Department of Electrical and Computer Engineering, University of Utah, Salt Lake City, UT, USA; MathWorks, Natick, MA, USA;" Department of Computer Science, Peking University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1303"",""1320"",""Taskflow aims to streamline the building of parallel and heterogeneous applications using a lightweight task graph-based approach. Taskflow introduces an expressive task graph programming model to assist developers in the implementation of parallel and heterogeneous decomposition strategies on a heterogeneous computing platform. Our programming model distinguishes itself as a very general class of task graph parallelism with in-graph control flow to enable end-to-end parallel optimization. To support our model with high performance, we design an efficient system runtime that solves many of the new scheduling challenges arising out of our models and optimizes the performance across latency, energy efficiency, and throughput. We have demonstrated the promising performance of Taskflow in real-world applications. As an example, Taskflow solves a large-scale machine learning workload up to 29% faster, 1.5× less memory, and 1.9× higher throughput than the industrial system, oneTBB, on a machine of 40 CPUs and 4 GPUs. We have opened the source of Taskflow and deployed it to large numbers of users in the open-source community."",""1558-2183"","""",""10.1109/TPDS.2021.3104255"",""Defense Advanced Research Projects Agency(grant numbers:FA 8650-18-2-7843)"; National Science Foundation(grant numbers:CCF-2126672);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9511796"",""Parallel programming";task parallelism;high-performance computing;"modern C++ programming"",""Task analysis";Parallel processing;Graphics processing units;Computational modeling;Programming;Solid modeling;"Runtime"","""",""27"","""",""57"",""IEEE"",""11 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TDFL: Truth Discovery Based Byzantine Robust Federated Learning,""C. Xu"; Y. Jia; L. Zhu; C. Zhang; G. Jin;" K. Sharif"",""School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China"; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China; School of Cyberspace Science and Technology, Beijing Institute of Technology, Beijing, China;" School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""29 Sep 2022"",""2022"",""33"",""12"",""4835"",""4848"",""Federated learning (FL) enables data owners to train a joint global model without sharing private data. However, it is vulnerable to Byzantine attackers that can launch poisoning attacks to destroy model training. Existing defense strategies rely on the additional datasets to train trustable server models or trusted execution environments to mitigate attacks. Besides, these strategies can only tolerate a small number of malicious users or resist a few types of poisoning attacks. To address these challenges, we design a novel federated learning method TDFL, Truth Discovery based Federated Learning, which can defend against multiple poisoning attacks without additional datasets even when the Byzantine users are $\geq 50\%$≥50%. Specifically, the TDFL considers different scenarios with different malicious proportions. For Honest-majority setting (Byzantine $< 50\%$<50%), we design a special robust truth discovery aggregation scheme to remove malicious model updates, which can assign weights according to users’ contribution";" for Byzantine-majority setting (Byzantine $\geq 50\%$≥50%), we use maximum clique-based filter to guarantee global model quality. To the best of our knowledge, this is the first study that uses truth discovery to defend against poisoning attacks. It is also the first scheme which can achieve strong robustness under multiple kinds of attacks launched by high proportion attackers without root datasets. Extensive comparative experiments are designed with five state-of-the-art aggregation rules under five types of classical poisoning attacks on different datasets. The experimental results demonstrate that TDFL is practical and achieves reasonable Byzantine-robustness."",""1558-2183"","""",""10.1109/TPDS.2022.3205714"",""National Natural Science Foundation of China(grant numbers:61972037,61872041,U1804263)"; China Postdoctoral Science Foundation(grant numbers:2021M700435,2021TQ0042); National Cryptography Development Fund(grant numbers:MMJJ20180412);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9887909"",""Federated learning";truth discovery;"poisoning attack"",""Data models";Collaborative work;Servers;Training;Robustness;Data privacy;"Soft sensors"","""",""2"","""",""35"",""IEEE"",""12 Sep 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Tenant-Grained Request Scheduling in Software-Defined Cloud Computing,""H. Tu"; G. Zhao; H. Xu;" X. Fang"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China;" College of Computer Science and Engineering, Anhui University of Science and Technology, Huainan, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Sep 2022"",""2022"",""33"",""12"",""4654"",""4671"",""Cloud providers host various services for tenants’ requests (e.g., software-as-a-service) and seek to serve as many requests as possible for revenue maximization. Considering a large number of requests, the previous works on fine-grained request scheduling may lead to poor system scalability (or high schedule overhead) and break tenant isolation. In this article, we design a tenant-grained request scheduling framework to conquer the above two disadvantages. We formulate the tenant-grained request scheduling problem as an integer linear programming and prove its NP-hardness. We consider two complementary cases: the offline case (where we know all request demands in advance), and the online case (where we have to make immediate scheduling decisions for requests arriving online). A normalization-based algorithm with an approximation factor of $ {O}(1)$O(1) is proposed to solve the offline problem and a primal-dual-based algorithm with a competitive ratio of $[(1-\epsilon), {O}(\log 3\cdot n+\log (1/\epsilon))]$[(1-ε),O(log3·n+log(1/ε))] is designed for the online scenario, where $\epsilon \in (0,1)$ε∈(0,1) and $n$n is the number of racks in the cloud. We also discuss how to integrate our proposed algorithms with the previous (fine-grained) request scheduling mechanism. Extensive simulation and experiment results show that our algorithms can obtain significant performance gains, e.g., the online algorithm reduces the scheduler's overhead more than $90\%$90% and achieves tenant isolation, while obtaining similar network performance (e.g., throughput) compared with the fine-grained request scheduling methods."",""1558-2183"","""",""10.1109/TPDS.2022.3199031"",""National Natural Science Foundation of China(grant numbers:62132019,62102392)"; Open Research of Projects of Zhejiang Lab(grant numbers:2022QA0AB04); Natural Science Foundation of Jiangsu Province(grant numbers:BK20210121);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857998"",""Software-defined cloud";cloud computing;request scheduling;scalability;"approximation"",""Servers";Job shop scheduling;Cloud computing;Scalability;Approximation algorithms;Scheduling algorithms;"Schedules"","""",""1"","""",""54"",""IEEE"",""16 Aug 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TensorOpt: Exploring the Tradeoffs in Distributed DNN Training With Auto-Parallelism,""Z. Cai"; X. Yan; K. Ma; Y. Wu; Y. Huang; J. Cheng; T. Su;" F. Yu"",""Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong"; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, Guangdong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Huawei Technologies Co. Ltd., Shenzhen, Guangdong, China;" Huawei Technologies Co. Ltd., Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Dec 2021"",""2022"",""33"",""8"",""1967"",""1981"",""Effective parallelization strategies are crucial for the performance of distributed deep neural network (DNN) training. Recently, several methods have been proposed to search parallelization strategies but they all optimize a single objective (e.g., execution time, memory consumption) and produce only one strategy. We propose Frontier Tracking (FT), an efficient algorithm that finds a set of Pareto-optimal parallelization strategies to explore the best trade-off among different objectives. FT can minimize the memory consumption when the number of devices is limited and fully utilize additional resources to reduce the execution time. Based on FT, we develop a user-friendly system, called TensorOpt, which allows users to run their distributed DNN training jobs without caring the details about searching and coding parallelization strategies. Experimental results show that TensorOpt is more flexible in adapting to resource availability compared with existing frameworks."",""1558-2183"","""",""10.1109/TPDS.2021.3132413"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635821"",""Deep learning";distributed systems;"large-scale model training"",""Training";Deep learning;Adaptation models;Memory management;Search problems;"Encoding"","""",""9"","""",""48"",""IEEE"",""6 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Tensorox: Accelerating GPU Applications via Neural Approximation on Unused Tensor Cores,""N. -M. Ho";" W. -F. Wong"",""National University of Singapore, Singapore, Singapore";" National University of Singapore, Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2021"",""2022"",""33"",""2"",""429"",""443"",""Driven by the demands of deep learning, many hardware accelerators, including GPUs, have begun to include specialized tensor processing units to accelerate matrix operations. However, general-purpose GPU applications that have little or no large dense matrix operations cannot benefit from these tensor units. This article proposes Tensorox, a framework that exploits the half-precision tensor cores available on recent GPUs for approximable, non deep learning applications. In essence, a shallow neural network is trained based on the input-output mapping of the function to be approximated. The key innovation in our implementation is the use of the small and dimension-restricted tensor operations in Nvidia GPUs to run multiple instances of the approximation neural network in parallel. With the proper scaling and training methods, our approximation yielded an overall accuracy that is higher than naïvely running the original programs with half-precision. Furthermore, Tensorox allows for the runtime adjustment of the degree of approximation. For the 10 benchmarks we tested, we achieved speedups from 2× to 112× compared to the original in single precision floating point, while maintaining the error caused by the approximation to below 10 percent in most applications."",""1558-2183"","""",""10.1109/TPDS.2021.3093239"",""Ministry of Education - Singapore(grant numbers:T1-251RES1818,MOE2016-T2-2-150)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9468379"",""Graphics processing units";parallel programming;approximate computing;neural networks;tensor processing unit;"GPGPU"",""Hardware";Tensors;Neural networks;Deep learning;Graphics processing units;Task analysis;"Training"","""",""2"","""",""54"",""IEEE"",""29 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"The PetscSF Scalable Communication Layer,""J. Zhang"; J. Brown; S. Balay; J. Faibussowitsch; M. Knepley; O. Marin; R. T. Mills; T. Munson; B. F. Smith;" S. Zampini"",""Argonne National Laboratory, Lemont, IL, USA"; University of Colorado Boulder, Boulder, CO, USA; Argonne National Laboratory, Lemont, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA; University at Buffalo, Buffalo, NY, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne Associate of Global Empire, LLC, Argonne National Laboratory, Lemont, IL, USA;" King Abdullah University of Science and Technology, Thuwal, Saudi Arabia"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""842"",""853"",""PetscSF, the communication component of the Portable, Extensible Toolkit for Scientific Computation (PETSc), is designed to provide PETSc's communication infrastructure suitable for exascale computers that utilize GPUs and other accelerators. PetscSF provides a simple application programming interface (API) for managing common communication patterns in scientific computations by using a star-forest graph representation. PetscSF supports several implementations based on MPI and NVSHMEM, whose selection is based on the characteristics of the application or the target architecture. An efficient and portable model for network and intra-node communication is essential for implementing large-scale applications. The Message Passing Interface, which has been the de facto standard for distributed memory systems, has developed into a large complex API that does not yet provide high performance on the emerging heterogeneous CPU-GPU-based exascale systems. In this article, we discuss the design of PetscSF, how it can overcome some difficulties of working directly with MPI on GPUs, and we demonstrate its performance, scalability, and novel features."",""1558-2183"","""",""10.1109/TPDS.2021.3084070"",""Exascale Computing(grant numbers:17-SC-20-SC)"; U.S. Department of Energy; National Nuclear Security Administration; U.S. Department of Energy(grant numbers:DE-AC02-06CH11357); Office of Science(grant numbers:DE-SC0016140,DE-AC02-0000011838);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442258"",""Communication";GPU;extreme-scale;MPI;"PETSc"",""Libraries";Programming;Graphics processing units;Forestry;Electronics packaging;Arrays;"Scalability"","""",""9"","""",""26"",""IEEE"",""26 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"The State of the Art of Metadata Managements in Large-Scale Distributed File Systems — Scalability, Performance and Availability,""H. Dai"; Y. Wang; K. B. Kent; L. Zeng;" C. Xu"",""Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China"; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Faculty of Computer Science, University of New Brunswick, Fredericton, NB, Canada; Zhejiang Lab, ZJ Lab-Enflame Joint Innovation Research Center, Hangzhou, Zhejiang, China;" Faculty of Science and Technology, University of Macau, Macau, China"",""IEEE Transactions on Parallel and Distributed Systems"",""26 Jul 2022"",""2022"",""33"",""12"",""3850"",""3869"",""File system metadata is the data in charge of maintaining namespace, permission semantics and location of file data blocks. Operations on the metadata can account for up to 80% of total file system operations. As such, the performance of metadata services significantly impacts the overall performance of file systems. A large-scale distributed file system (DFS) is a storage system that is composed of multiple storage devices spreading across different sites to accommodate data files, and in most cases, to provide users with location independent access interfaces. Large-scale DFSs have been widely deployed as a substrate to a plethora of computing systems, and thus their metadata management efficiency is crucial to a massive number of applications, especially with the advent of the Big Data age, which poses tremendous pressure on underlying storage systems. This paper reports the state-of-the-art research on metadata services in large-scale distributed file systems, which is conducted from three indicative perspectives that are always used to characterize DFSs: high-scalability, high-performance, and high-availability, with special focus on their respective major challenges as well as their developed mainstream technologies. Additionally, the paper also identifies and analyzes several existing problems in the research, which could be used as a reference for related studies."",""1558-2183"","""",""10.1109/TPDS.2022.3170574"",""Third Xinjiang Scientific Expedition Program(grant numbers:2021xjkk1300)"; National Natural Science Foundation of China(grant numbers:61672513); Zhejiang provincial(grant numbers:2021R52007); Center-initiated Research Project of Zhejiang Lab(grant numbers:2021DA0AM01); Natural Sciences and Engineering Research Council of Canada; Lockheed-Martin Cybersecurity Fund(grant numbers:LMCRF2020-02); Mitacs(grant numbers:IT24602);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9768784"",""High-availability";high-performance;high-scalability;large-scale distributed file system;"metadata management"",""Metadata";Servers;Distributed databases;Scalability;Computer architecture;Performance evaluation;"Memory"","""",""4"","""",""156"",""IEEE"",""4 May 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"The Supermarket Model With Known and Predicted Service Times,""M. Mitzenmacher";" M. Dell'Amico"",""School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA";" University of Genoa, Genova, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2740"",""2751"",""The supermarket model refers to a system with a large number of queues, where new customers choose $d$d queues at random and join the one with the fewest customers. This model demonstrates the power of even small amounts of choice, as compared to simply joining a queue chosen uniformly at random, for load balancing systems. In this work we perform simulation-based studies to consider variations where service times for a customer are predicted, as might be done in modern settings using machine learning techniques or related mechanisms. Our primary takeaway is that using even seemingly weak predictions of service times can yield significant benefits over blind First In First Out queueing in this context. However, some care must be taken when using predicted service time information to both choose a queue and order elements for service within a queue"; while in many cases using the information for both choosing and ordering is beneficial, in many of our simulation settings we find that simply using the number of jobs to choose a queue is better when using predicted service times to order jobs in a queue. In our simulations, we evaluate both synthetic and real-world workloads–in the latter, service times are predicted by machine learning. Our results provide practical guidance for the design of real-world systems;" moreover, we leave many natural theoretical open questions for future work, validating their relevance to real-world situations."",""1558-2183"","""",""10.1109/TPDS.2022.3146195"",""National Science Foundation(grant numbers:CCF-2101140,DMS-2023528,CCF-1563710,CCF-1535795)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9695323"",""Supermarket model";prediction methods;scheduling;"queueing analysis"",""Queueing analysis";Predictive models;Analytical models;Load modeling;Standards;Prediction algorithms;"Machine learning"","""",""4"","""",""34"",""IEEE"",""27 Jan 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Theoretical Analysis of an Adaptive Periodic Multi Installment Scheduling With Result Retrieval for SAR Image Processing,""G. M. Chinnappan";" B. Veeravalli"",""Department of Electrical and Computer Engineering, National University of Singapore, Singapore";" Department of Electrical and Computer Engineering, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""5 Sep 2022"",""2022"",""33"",""12"",""4672"",""4683"",""Processing a large-scale Synthetic Aperture Radar (SAR) image dataset on a distributed computing infrastructure poses a challenging problem. Large-scale load distribution strategies like multi-installment scheduling (MIS) assume that the size of the result is negligible compared to the input workloads and hence ignore it in their design. Similarly, numerical methods like particle swarm optimization and their variants are not practical for real-time applications, given their run-time complexities. As both the results retrieval and completion time are crucial for SAR image data processing, in this article, we attempt to provide a thorough theoretical analysis of an adaptive MIS that includes the result retrieval phase. We use the periodic nature of the internal installments to keep the strategy simple and fine-tune the last installment to avoid any idle times in the processors. We derive a closed-form solution for the load fractions and hence, the overall processing time, schedule feasibility criteria, and certain other properties that lead to adaptive scheduling. Finally, we validate our theoretical findings through rigorous simulation studies using a loosely connected virtual machines (VMs) topology for the SAR dataset."",""1558-2183"","""",""10.1109/TPDS.2022.3194542"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9844291"",""SAR image";multi-installment scheduling;load distribution;front-end processors;"heterogeneous cluster"",""Program processors";Radar polarimetry;Processor scheduling;Load modeling;Distributed databases;Computational modeling;"Schedules"","""","""","""",""18"",""IEEE"",""28 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TherMa-MiCs: Thermal-Aware Scheduling for Fault-Tolerant Mixed-Criticality Systems,""S. Safari"; H. Khdr; P. Gohari-Nazari; M. Ansari; S. Hessabi;" J. Henkel"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Karlsruhe Institute of Technology, Karlsruhe, Germany; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran;" Karlsruhe Institute of Technology, Karlsruhe, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2021"",""2022"",""33"",""7"",""1678"",""1694"",""Multicore platforms are becoming the dominant trend in designing Mixed-Criticality Systems (MCSs), which integrate applications of different levels of criticality into the same platform. A well-known MCS is the dual-criticality system that is composed of low-criticality and high-criticality tasks. The availability of multiple cores on a single chip provides opportunities to employ fault-tolerant techniques, such as N-Modular Redundancy (NMR), to ensure the reliability of MCSs. However, applying fault-tolerant techniques will increase the power consumption on the chip, and thereby on-chip temperatures might increase beyond safe limits. To prevent thermal emergencies, urgent countermeasures, like Dynamic Voltage and Frequency Scaling (DVFS) or Dynamic Power Management (DPM) will be triggered to cool down the chip. Such countermeasures, however, might not only lead to suspending low-criticality tasks, but also it might lead to violating timing constraints of high-criticality tasks. In order to prevent such severe scenarios, it is indispensable to consider a temperature constraint within the scheduling process of fault-tolerant MCSs. Therefore, this paper presents, for the first time, a thermal-aware scheduling scheme for fault-tolerant MCSs, named TherMa-MiCs. In particular, TherMa-MiCs, satisfies the temperature constraint jointly with the timing constraints of the high-criticality tasks, while attempting to maximize the QoS of low-criticality tasks under the predefined constraints. At the same time, a reliability target is satisfied by employing the well-known N-Modular Redundancy (NMR) fault-tolerant technique. Experimental results show that our proposed scheme meets the temperature and timing constraints, while at the same time, improving the QoS of low-criticality tasks, with an average of 44%."",""1558-2183"","""",""10.1109/TPDS.2021.3123544"",""Deutsche Forschungsgemeinschaft(grant numbers:146371743)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591366"",""Multicores";N Modular Redundancy (NMR);mixed-criticality systems;QoS;"temperature"",""Multicore processing";Fault tolerant systems;Timing;System-on-chip;Power demand;Redundancy;Quality of service;Temperature control;"Thermal management"","""",""10"","""",""76"",""IEEE"",""27 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TianheGraph: Customizing Graph Search for Graph500 on Tianhe Supercomputer,""X. Gan"; Y. Zhang; R. Wang; T. Li; T. Xiao; R. Zeng; J. Liu;" K. Lu"",""National University of Defense Technology, Changsha, Hunan, China"; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China;" National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""941"",""951"",""As the era of exascale supercomputing is coming, it is vital for next-generation supercomputers to find appropriate applications with high social and economic benefit. In recent years, it has been widely accepted that extremely-large graph computation is a promising killer application for supercomputing. Although Tianhe series supercomputers are leading in the world-wide competition of supercomputing (ranked No. 1 in the Top500 list for six times), previously they had been inefficient in graph computation according to the Graph500 list. This is mainly because the previous graph processing system cannot leverage the advanced hardware features of Tianhe supercomputers. To address the problem, in this paper we present our integrated optimizations for improving the graph computation performance on our next-generation Tianhe supercomputing system, mainly including sorting with buffering for heavy vertices, vectorized searching with SVE (Scalable Vector Extension) on matrix2000+ CPUs, and group communication on the proprietary interconnection network. Performance evaluation on a subset of the Tianhe supercomputer (with 512 nodes and 196,608 cores) shows that our customized graph processing system effectively improves the graph search performance and achieves the BFS performance of 2131.98 GTEPS."",""1558-2183"","""",""10.1109/TPDS.2021.3100785"",""National Key Research and Development Program of China(grant numbers:2018YFB2101102)"; National Natural Science Foundation of China(grant numbers:61772541,61872376,61932001); Natural Science Foundation of Hunan Province(grant numbers:2020JJ4669); Foundation of Parallel and Distributed Processing Laboratory(grant numbers:6142110190206);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9502006"",""Graph computation";exascale supercomputer;matrix2000+;group-based communication;Graph500;"BFS"",""Supercomputers";Next generation networking;Benchmark testing;Hardware;Multiprocessor interconnection;Image edge detection;"Performance evaluation"","""",""5"","""",""51"",""IEEE"",""29 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Timed Loops for Distributed Storage in Wireless Networks,""A. Mukherjee"; P. K. Deb;" S. Misra"",""Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India"; Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India;" Department of Computer Science and Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, India"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Aug 2021"",""2022"",""33"",""3"",""698"",""709"",""IoT deployments that have limited memories lack sustained computation power and have limited connectivity to the Internet due to intermittent last-mile connectivity, particularly in rural and remote locations. For maintaining congestion-free operations, most of the collected data from these networks are discarded, instead of being transmitted remotely for further processing. In this article, we propose the paradigm Timed Loop Storage to distribute the data and use the underutilized bandwidth of local network links for sequentially queuing packets of computational data that are being operated on in parts in one of the IoT nodes. While the sequenced packets are executed sequentially on the target IoT device, the remaining packets, which are currently not being operated on, distribute and keep looping over the network links until they are required for processing. A time-synchronized packet deflection mechanism on each node handles data transfer and looping of individual packets. In our implementation, although we observe that the proposed approach requires data rates of 6 Mbps, it incurs only 45 Kb usage of primary storage systems even for sizeable data, ensuring scalability of the connected IoT devices' temporary storage capabilities, thereby making it useful for real-life applications."",""1558-2183"","""",""10.1109/TPDS.2021.3100780"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9501984"",""Wireless networks";Internet of Things;resource allocation;connectivity;"distributed storage networks"",""Internet of Things";Delays;Task analysis;Servers;Performance evaluation;Hardware;"Computer architecture"","""",""4"","""",""18"",""IEEE"",""29 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TODG: Distributed Task Offloading With Delay Guarantees for Edge Computing,""S. Yue"; J. Ren; N. Qiao; Y. Zhang; H. Jiang; Y. Zhang;" Y. Yang"",""School of Computer and Engineering, Central South University, Changsha, Hunan, China"; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China; School of Computer and Engineering, Central South University, Changsha, Hunan, China; School of Computer and Engineering, Central South University, Changsha, Hunan, China; School of Information Science and Engineering, Hunan University, Changsha, Hunan, China; Department of Computer Science and Technology, BNRist, Tsinghua University, Beijing, China;" Department of Electrical and Computer Engineering, Stony Brook University, Stony Brook, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""23 Nov 2021"",""2022"",""33"",""7"",""1650"",""1665"",""Edge computing has been an efficient way to provide prompt and near-data computing services for resource-and-delay sensitive IoT applications via computation offloading. Effective computation offloading strategies need to comprehensively cope with several major issues, including 1) the allocation of dynamic communication and computational resources, 2) delay constraints of heterogeneous tasks, and 3) requirements for computationally inexpensive and distributed algorithms. However, most of the existing works mainly focus on part of these issues, which would not suffice to achieve expected performance in complex and practical scenarios. To tackle this challenge, in this paper, we systematically study a distributed computation offloading problem with delay constraints, where heterogeneous computational tasks require continually offloading to a set of edge servers via a limiting number of stochastic communication channels. The task offloading problem is formulated as a delay-constrained long-term stochastic optimization problem under unknown prior statistical knowledge. To solve this problem, we first provide a technical path to transform and decompose it into several slot-level sub-problems. Then, we devise a distributed online algorithm, namely TODG, to efficiently allocate resources and schedule offloading tasks. Further, we present a comprehensive analysis for TODG in terms of the optimality gap, the worst-case delay, and the impact of system parameters. Extensive simulation results demonstrate the effectiveness and efficiency of TODG."",""1558-2183"","""",""10.1109/TPDS.2021.3123535"",""National Natural Science Foundation of China(grant numbers:62122095,62072472,U19A2067)"; National Key Research and Development Program of China(grant numbers:2019YFA0706403); Natural Science Foundation of Hainan Province(grant numbers:2020JJ2050); Higher Education Discipline Innovation Project(grant numbers:B18059); Young Talents Plan of Hunan Province of China(grant numbers:2019RS2001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9591418"",""distributed task offloading";edge computing;delay guarantee;channel allocation;"stochastic optimization"",""Task analysis";Delays;Servers;Edge computing;Resource management;Mobile handsets;"Optimization"","""",""24"","""",""51"",""IEEE"",""27 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Topology-Aware Neural Model for Highly Accurate QoS Prediction,""J. Li"; H. Wu; J. Chen; Q. He;" C. -H. Hsu"",""School of Information Science and Engineering, Yunnan University, Kunming, Yunnan, China"; School of Information Science and Engineering, Yunnan University, Kunming, Yunnan, China; School of Information Science and Engineering, Yunnan University, Kunming, Yunnan, China; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia;" College of Information and Electrical Engineering, Asia University, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Nov 2021"",""2022"",""33"",""7"",""1538"",""1552"",""With the widespread deployment of various cloud computing and service-oriented systems, there is a rapidly increasing demand for collaborative quality-of-service (QoS) prediction. Existing QoS prediction methods have made great progress in modeling users and services as well as exploiting contexts of service invocations. However, they ignore the completion of service requests/responses relies on the underlying network topology and the complex interactions between Autonomous Systems. To tackle this challenge, we propose a topology-aware neural (TAN) model for collaborative QoS prediction. In the TAN model, the features of users, services, and intermediate nodes on the communication path are projected to a shared latent space as input features. To jointly characterize the invocation process, the path features and end-cross features are captured respectively through an explicit path modeling layer and an implicit cross-modeling layer. After that, a gating layer fuses and transmits these features to the prediction layer for estimating unknown QoS values. In this way, TAN provides a flexible framework that can comprehensively capture the invocation context for making accurate QoS prediction. Experimental results on two real-world datasets demonstrate that TAN significantly outperforms state-of-the-art methods on the tasks of response time, throughput, and reliability prediction. Also, TAN shows better extensibility of using auxiliary information."",""1558-2183"","""",""10.1109/TPDS.2021.3116865"",""National Natural Science Foundation of China(grant numbers:61962061,61562090,61872084)"; Yunnan Provincial Foundation for Leaders of Disciplines in Science and Technology(grant numbers:202005AC160005); Top Young Talents of ”Ten Thousand Plan” in Yunnan Province(grant numbers:YNWR-QNBJ-2019-188); Yunnan University; Guangdong-Hong Kong-Macao Joint Laboratory for Intelligent Micro-Nano Optoelectronic Technology(grant numbers:2020B1212030010);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9555220"",""QoS prediction";deep neural networks;topology-aware modeling;end-to-end interaction;"communication path"",""Quality of service";Cloud computing;Predictive models;Neural networks;Computational modeling;Network topology;"Collaboration"","""",""20"","""",""63"",""IEEE"",""30 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Towards Efficient and Stable K-Asynchronous Federated Learning With Unbounded Stale Gradients on Non-IID Data,""Z. Zhou"; Y. Li; X. Ren;" S. Yang"",""National Engineering Laboratory for Big Data Analytics, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, Shaanxi, China"; National Engineering Laboratory for Big Data Analytics, School of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, Shaanxi, China; National Engineering Laboratory for Big Data Analytics, School of Computer Science and Technology, Xi’an Jiaotong University, Xi’an, Shaanxi, China;" National Engineering Laboratory for Big Data Analytics, the Ministry of Education Key Lab for Intelligent Networks and Network Security, Xi’an Jiaotong University, Xi’an, Shaanxi, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Jun 2022"",""2022"",""33"",""12"",""3291"",""3305"",""Federated learning (FL) is an emerging privacy-preserving paradigm that enables multiple participants collaboratively to train a global model without uploading raw data. Considering heterogeneous computing and communication capabilities of different participants, asynchronous FL can avoid the stragglers effect in synchronous FL and adapts to scenarios with vast participants. Both staleness and non-IID data in asynchronous FL would reduce the model utility. However, there exists an inherent contradiction between the solutions to the two problems. That is, mitigating the staleness requires to select less but consistent gradients while coping with non-IID data demands more comprehensive gradients. To address the dilemma, this paper proposes a two-stage weighted $K$K asynchronous FL with adaptive learning rate (WKAFL). By selecting consistent gradients and adjusting learning rate adaptively, WKAFL utilizes stale gradients and mitigates the impact of non-IID data, which can achieve multifaceted enhancement in training speed, prediction accuracy and training stability. We also present the convergence analysis for WKAFL under the assumption of unbounded staleness to understand the impact of staleness and non-IID data. Experiments implemented on both benchmark and synthetic FL datasets show that WKAFL has better overall performance compared to existing algorithms."",""1558-2183"","""",""10.1109/TPDS.2022.3150579"",""National Key Research and Development Program of China(grant numbers:2020YFA0713900)"; National Natural Science Foundation of China(grant numbers:62172329,61802298,61772410,U21A6005,U1811461,11690011); China Postdoctoral Science Foundation(grant numbers:2020T130513,2019M663726);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712243"",""Federated learning";asynchronous learning;data heterogeneity;prediction accuracy;"training stability"",""Training";Servers;Computational modeling;Data models;Convergence;Distributed databases;"Stability analysis"","""",""9"","""",""56"",""IEEE"",""11 Feb 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Towards Revenue-Driven Multi-User Online Task Offloading in Edge Computing,""Z. Ma"; S. Zhang; Z. Chen; T. Han; Z. Qian; M. Xiao; N. Chen; J. Wu;" S. Lu"",""State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of the Electrical and Computer Engineering, University of New Jersey Institute of Technology, Newark, NJ, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; School of Computer Science and Technology / Suzhou Institute for Advanced Study, University of Science and Technology of China, Hefei, Anhui, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China; Department of Computer and Information Sciences, Temple University, Philadelphia, PA, USA;" State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, Jiangsu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""20 Oct 2021"",""2022"",""33"",""5"",""1185"",""1198"",""Mobile Edge Computing (MEC) has become an attractive solution to enhance the computing and storage capacity of mobile devices by leveraging available resources on edge nodes. In MEC, the arrivals of tasks are highly dynamic and are hard to predict precisely. It is of great importance yet very challenging to assign the tasks to edge nodes with guaranteed system performance. In this article, we aim to optimize the revenue earned by each edge node by optimally offloading tasks to the edge nodes. We formulate the revenue-driven online task offloading (ROTO) problem, which is proved to be NP-hard. We first relax ROTO to a linear fractional programming problem, for which we propose the Level Balanced Allocation (LBA) algorithm. We then show the performance guarantee of LBA through rigorous theoretical analysis, and present the LB-Rounding algorithm for ROTO using the primal-dual technique. The algorithm achieves an approximation ratio of $2(1+\xi)\ln (d+1)$2(1+ξ)ln(d+1) with a considerable probability, where $d$d is the maximum number of process slots of an edge node and $\xi$ξ is a small constant. The performance of the proposed algorithm is validated through both trace-driven simulations and testbed experiments. Results show that our proposed scheme is more efficient compared to baseline algorithms."",""1558-2183"","""",""10.1109/TPDS.2021.3105325"",""National Key Research and Development Program of China(grant numbers:2017YFB1001801)"; National Natural Science Foundation of China(grant numbers:61872175,61832008); Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516964"",""Mobile edge computing";primal-dual technique;online computation offloading;"revenue-optimal"",""Task analysis";Cloud computing;Approximation algorithms;Wireless communication;Software;Resource management;"Optimization"","""",""16"","""",""42"",""IEEE"",""18 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Transparent Asynchronous Parallel I/O Using Background Threads,""H. Tang"; Q. Koziol; J. Ravi;" S. Byna"",""Lawrence Berkeley National Laboratory, Berkeley, CA, USA"; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; NC State University, Raleigh, NC, USA;" Lawrence Berkeley National Laboratory, Berkeley, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""891"",""902"",""Moving toward exascale computing, the size of data stored and accessed by applications is ever increasing. However, traditional disk-based storage has not seen improvements that keep up with the explosion of data volume or the speed of processors. Multiple levels of non-volatile storage devices are being added to handle bursty I/O, however, moving data across the storage hierarchy can take longer than the data generation or analysis. Asynchronous I/O can reduce the impact of I/O latency as it allows applications to schedule I/O early and to check their status later. I/O is thus overlapped with application communication or computation or both, effectively hiding some or all of the I/O latency. POSIX and MPI-I/O provide asynchronous read and write operations, but lack the support for non-data operations such as file open and close. Users also have to manually manage data dependencies and use low-level byte offsets, which requires significant effort and expertise to adopt. In this article, we present an asynchronous I/O framework that supports all types of I/O operations, manages data dependencies transparently and automatically, provides implicit and explicit modes for application flexibility, and error information retrieval. We implemented these techniques in HDF5. Our evaluation of several benchmarks and application workloads demonstrates it effectiveness on hiding the I/O cost from the application."",""1558-2183"","""",""10.1109/TPDS.2021.3090322"",""Exascale Computing(grant numbers:17-SC-20-SC)"; U.S. Department of Energy; National Nuclear Security Administration;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9459479"",""Asynchronous I/O";parallel I/O;"background threads"",""Task analysis";Connectors;Libraries;Instruction sets;Computational modeling;Monitoring;"Middleware"","""",""6"","""",""32"",""IEEE"",""17 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"TriangleKV: Reducing Write Stalls and Write Amplification in LSM-Tree Based KV Stores With Triangle Container in NVM,""C. Ding"; T. Yao; H. Jiang; Q. Cui; L. Tang; Y. Zhang; J. Wan;" Z. Tan"",""Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China"; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, USA; PingCAP, Beijing, China; PingCAP, Beijing, China; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China;" Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System, School of Computer Science and Technology, Huazhong University of Science and Technology, Ministry of Education of China, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4339"",""4352"",""Popular LSM-tree based key-value stores suffer from suboptimal and unpredictable performance due to write amplification and write stalls that cause application performance to periodically drop to nearly zero. Our preliminary experimental studies reveal that (1) write stalls mainly stem from the significantly large amount of data involved in each compaction between $L_{0}$L0-$L_{1}$L1 (i.e., the first two levels of LSM-tree), and (2) write amplification increases with the depth of LSM-trees. Existing work mainly focus on reducing write amplification, while only a couple of them target mitigating write stalls. In this paper, we exploit unique features of non-volatile memory (NVM) to address these two limitations and propose TriangleKV, a new LSM-tree based persistent KV store with multi-tier DRAM-NVM-SSD storage. TriangleKV's design principles include performing smaller and cheaper $L_{0}$L0-$L_{1}$L1 compaction to reduce write stalls while reducing the depth of LSM-trees to mitigate write amplification. To this end, four novel techniques are proposed. First, we relocate and manage the $L_{0}$L0 level in NVM with our proposed triangle container. Second, the new right-angle side compaction is devised to compact $L_{0}$L0 to $L_{1}$L1 at fine-grained key ranges, thus substantially reducing the amount of compaction data. Third, TriangleKV increases the width of each level to decrease the depth of LSM-trees thus mitigating write amplification. Finally, the cross-row hint search is introduced for the triangle container to keep adequate read performance. We implement TriangleKV based on MatrixKV and evaluate it on a hybrid DRAM/NVM/SSD system using Intel's latest 3D Xpoint NVM device Optane DC PMM. Evaluation results show that, with the same amount of NVM, TriangleKV outperforms RocksDB, NoveLSM and MatrixKV in 99th-percentile latencies by $5.5\times$5.5×, $2.1\times$2.1× and $1.1\times$1.1×, and random write throughput by $4.9\times$4.9×, $3.5\times$3.5× and $1.4\times$1.4× respectively."",""1558-2183"","""",""10.1109/TPDS.2022.3188268"",""National Natural Science Foundation of China(grant numbers:62072196)"; Science, Technology and Innovation Commission of Shenzhen Municipality(grant numbers:JCYJ20190809095001781); Special Project for Research and Development in Key areas of Guangdong Province(grant numbers:2021B0101400003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9815150"",""Key-value stores";LSM-tree;"non-volatile memory"",""Nonvolatile memory";Compaction;Random access memory;Throughput;Containers;Tail;"System performance"","""",""2"","""",""54"",""IEEE"",""4 Jul 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"TridentKV: A Read-Optimized LSM-Tree Based KV Store via Adaptive Indexing and Space-Efficient Partitioning,""K. Lu"; N. Zhao; J. Wan; C. Fei; W. Zhao;" T. Deng"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China"; School of Computer Science, Northwestern Polytechnical University, Xi’an, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; SenseTime Research, Shenzhen, China;" SenseTime Research, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Dec 2021"",""2022"",""33"",""8"",""1953"",""1966"",""LSM-tree based key-value (KV) stores suffer severe read performance loss due to the leveled structure of the LSM-tree. Especially, when modern storage devices with high bandwidth and low latency are used, the read performance of KV store is seriously affected by inefficient file indexing. Besides, due to the deletion pattern of inserting tombstones, the KV stores based on LSM-tree are faced with the problem of read performance fluctuations that are caused by large-scale data deletion (also referred to as the Read-After-Delete problem). In this article, TridentKV is proposed to improve the read performance of KV stores. An adaptive learned index structure is first designed to speed up file indexing. Also, a space-efficient partition strategy is proposed to solve the Read-After-Delete problem. Besides, asynchronous reading design is adopted, and SPDK is supported for high concurrency and low latency. TridentKV is implemented on RocksDB and the evaluation results indicate that compared with RocksDB, the read performance of TridentKV is improved by 7× to 12× without loss of write performance and TridentKV provides stable read performance even if a large number of deletions or migrations occur. Instead of RocksDB, TridentKV is exploited to store metadata in Ceph, which improves the read performance of Ceph by 20%$\sim$∼60%."",""1558-2183"","""",""10.1109/TPDS.2021.3118599"",""National Natural Science Foundation of China(grant numbers:61821003)"; National Natural Science Foundation of China(grant numbers:62072196); Shenzhen basic Research(grant numbers:JCYJ20190809095001781); National Key Research and Development Program of China(grant numbers:2018YFB1004401); Beijing Natural Science Foundation(grant numbers:L192027);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9563237"",""Key-value store";read optimization;learned index;SPDK;"partitioned store"",""Indexing";Performance evaluation;Optimization;Metadata;Training;Nonvolatile memory;"Concurrent computing"","""",""4"","""",""52"",""IEEE"",""7 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;
"UFC2: User-Friendly Collaborative Cloud,""M. Zhao"; Z. Li; W. Liu; J. Chen;" X. Li"",""School of Software and BNRist, Tsinghua University, Beijing, China"; School of Software and BNRist, Tsinghua University, Beijing, China; School of Software and BNRist, Tsinghua University, Beijing, China; School of Software and BNRist, Tsinghua University, Beijing, China;" School of Software and BNRist, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Feb 2022"",""2022"",""33"",""9"",""2163"",""2182"",""This article studies how today's cloud storage services support collaborative file editing. As a tradeoff for transparency and user-friendliness, they do not ask collaborators to use version control systems but instead implement their own heuristics for handling conflicts, which however often lead to unexpected and undesired experiences. With specialized measurements and reverse engineering, we unravel a number of their design and implementation issues as the root causes of poor experiences. Driven by the findings, we propose to reconsider the collaboration support of cloud storage services from a novel perspective of operations without using any locks. To enable this idea, we design intelligent and efficient approaches to the inference and transformation of users’ editing operations, as well as optimizations to the maintenance of files’ historical versions and the update of individual files. We build an open-source system UFC2 (User-Friendly Collaborative Cloud) to embody our design, which can avoid most (98%) conflicts with little (2%) overhead."",""1558-2183"","""",""10.1109/TPDS.2021.3132496"",""National Natural Science Foundation of China(grant numbers:61822205,61632020,61632013,61902211)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9635692"",""Cloud storage";collaborative editing;conflict resolution;operation inference;"operation transformation"",""Cloud computing";Collaboration;Matlab;Transforms;Synchronization;Servers;"Codes"","""",""1"","""",""82"",""IEEE"",""3 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Understanding the Impact of Data Staging for Coupled Scientific Workflows,""A. Gainaru"; L. Wan; R. Wang; E. Suchyta; J. Chen; N. Podhorszki; J. Kress; D. Pugmire;" S. Klasky"",""Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA"; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA;" Computer Science and Mathematics Division, Oak Ridge National Laboratory, Oak Ridge, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""4134"",""4147"",""The rate of data generated by cutting-edge experimental science facilities and large-scale simulations enabled by current high-performance computing (HPC) systems has continued to grow at a far greater pace than the development of the network and storage capabilities on which these systems rely. To cope with this challenge, scientist are moving toward the creation of autonomous experiments and HPC simulations using machine learning. However, efficiently moving, storing, and processing large amounts of data away from the point of origin presents an incredible challenge. In-memory computing, in situ analysis, data staging, and data streaming are recognized viable alternatives to traditional file-based methods for transferring data between coupled workflows. However, the performance trade-offs and limitations for these methods are not fully understood when used in HPC applications. This article presents a comprehensive performance assessment of the current solutions for data staging when applied to applications that are not necessary I/O intensive which makes them not ideal candidates for these methods. Our study is based on experiments running at scale on Oak Ridge National Laboratory's Summit supercomputer using applications and simulations that cover typical computational motifs and patterns. We investigated the usability and cost/benefit trade-offs of staging algorithms for HPC applications under different scenarios and highlight opportunities for optimizing the dataflow between coupled simulation workflows."",""1558-2183"","""",""10.1109/TPDS.2022.3179989"",""Exascale Computing Project(grant numbers:17-SC-20-SC)"; U.S. Department of Energy; National Nuclear Security Administration; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787790"",""Data staging";high-performance computing;data management;workflow;coupled simulations;in situ analytics;"data streaming"",""Computational modeling";Data models;Codes;Data visualization;Libraries;Distributed databases;"Analytical models"","""",""2"","""",""46"",""IEEE"",""3 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"VCSR: An Efficient GPU Memory-Aware Sparse Format,""E. Karimi"; N. B. Agostini; S. Dong;" D. Kaeli"",""ECE Department, Northeastern University, Boston, MA, USA"; ECE Department, Northeastern University, Boston, MA, USA; ECE Department, Northeastern University, Boston, MA, USA;" ECE Department, Northeastern University, Boston, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""22 Aug 2022"",""2022"",""33"",""12"",""3977"",""3989"",""The Sparse Matrix-Vector Multiplication (SpMV) kernel is used in a broad class of linear algebra computations. SpMV computations result in a performance bottleneck in many high performance applications, so optimizing SpMV performance is paramount. While implementing this kernel on a GPU can potentially boost performance significantly, current GPU libraries either provide modest performance gains or are burdened with high sparse format conversion overhead. In this paper we introduce the Vertical Compressed Sparse Row (VCSR) format, a novel memory-aware format that out-performs previous proposed formats on a GPU. We first motivate the design of our baseline VCSR format and then step through a series of enhancements that further improve VCSR's memory efficiency (VCSR-MEM) and performance (VCSR-INTRLV), while also considering conversion overhead. VCSR attempts to produce a high degree of thread-level parallelism and memory utilization by exploiting knowledge of GPU memory microarchitecture. VCSR can reduce the number of global memory transactions significantly, an issue not addressed by most other sparse formats. In addition, VCSR provides a novel reordering mechanism. It minimizes the size of the compressed matrix, handles both regular/irregular sparse matrices, and can be customized based on matrix size. VCSR also minimizes conversion overhead, as compared to full or partial row reordering. Our methodology is highly configurable and can be optimized for any sparse matrix. We have evaluated the VCSR format for the SpMV kernel when run on two different NVIDIA GPUs, the Kepler K40 and the Volta V100. We compare VCSR with NVIDIA's cuSPARSE library (the HYB format), a state-of-the-art sparse library. We also compare against other state-of-the-art CSR-based formats, including CSR5, merge-base SpMV and HOLA. We evaluate the benefits of VCSR over the entire University of Florida's SuiteSparse dataset collection. The VCSR-baseline format achieves an average speedup ranging from $1.10\times$1.10× to $1.39\times$1.39× when compared to the performance of the four state-of-the-art formats on an NVIDIA V100. While the VCSR-MEM format can save a significant amount of memory space, it is a bit slower than our VCSR-baseline. VCSR-INTRLV performs much better than the VCSR-baseline, and even when including the conversion overhead, achieves an average speedup of $1.08\times$1.08× as compared to HOLA (the best performing format among the prior schemes)."",""1558-2183"","""",""10.1109/TPDS.2022.3177291"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9787804"",""GPU";memory patterns;SpMV;"sparse matrices"",""Graphics processing units";Sparse matrices;Kernel;Instruction sets;Libraries;Parallel processing;"Indexes"","""",""2"","""",""40"",""IEEE"",""3 Jun 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"ViTrack: Efficient Tracking on the Edge for Commodity Video Surveillance Systems,""L. Cheng"; J. Wang;" Y. Li"",""School of Software and BNrist, Tsinghua University, Beijing, China"; School of Software and BNrist, Tsinghua University, Beijing, China;" School of Software and BNrist, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""17 Aug 2021"",""2022"",""33"",""3"",""723"",""735"",""Nowadays, video surveillance systems are widely deployed in various places, e.g., schools, parks, airports, roads, etc. However, existing video surveillance systems are far from full utilization due to high computation overhead in video processing. In this work, we present ViTrack, a framework for efficient multi-video tracking using computation resource on the edge for commodity video surveillance systems. In the heart of ViTrack lies a two layer spatial/temporal compressed target detection method to significantly reduce the computation overhead by combining videos from multiple cameras. Further, ViTrack derives the video relationship and camera information even in absence of camera location, direction, etc. To alleviate the impact of variant video quality and missing targets, ViTrack leverages a Markov Model based approach to efficiently recover missing information and finally derive the complete trajectory. We implement ViTrack on a real deployed video surveillance system with 110 cameras. The experiment results demonstrate that ViTrack can provide efficient trajectory tracking with processing time 45x less than the existing approach. For 110 video cameras, ViTrack can run on a Dell OptiPlex 390 computer to track given targets in almost real time. We believe ViTrack can enable practical video analysis for widely deployed commodity video surveillance systems."",""1558-2183"","""",""10.1109/TPDS.2021.3081254"",""National Key Research and Development Program of China(grant numbers:2017YFB1003000)"; National Natural Science Foundation of China(grant numbers:61932013);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9452000"",""Video tracking";"edge computing"",""Cameras";Video surveillance;Trajectory;Image edge detection;Target tracking;Object detection;"Trajectory tracking"","""",""5"","""",""32"",""IEEE"",""11 Jun 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Votes-as-a-Proof (VaaP): Permissioned Blockchain Consensus Protocol Made Simple,""X. Fu"; H. Wang;" P. Shi"",""National Key Laboratory of Parallel and Distributed Processing and Key Laboratory of Software Engineering for Complex Systems, College of Computer Science, National University of Defense Technology, Changsha, Hunan, China"; National Key Laboratory of Parallel and Distributed Processing and Key Laboratory of Software Engineering for Complex Systems, College of Computer Science, National University of Defense Technology, Changsha, Hunan, China;" National Key Laboratory of Parallel and Distributed Processing and Key Laboratory of Software Engineering for Complex Systems, College of Computer Science, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""18 Oct 2022"",""2022"",""33"",""12"",""4964"",""4973"",""With the development of Blockchain technology, permissioned Blockchains are getting more and more attention from researchers because applications based on permissioned Blockchains are more practical and easier to be carried out. This paper aims to design a dedicated consensus protocol for permissioned Blockchains. The existing consensus protocols applied to permissioned Blockchains are either derived from public Blockchains such as Proof of Work (PoW) or Proof of Stake (PoS), with full decentralization, resulting in low transaction processing efficiency";" or derived from traditional Byzantine fault-tolerant (BFT) consensus protocols such as Practical BFT (PBFT) or HoneyBadgerBFT, with high communication complexity of the consensus process, resulting in low scalability. Therefore, we propose a dedicated consensus protocol for permissioned Blockchains called Votes-as-a-Proof (VaaP) with high transaction processing efficiency while ensuring high scalability. Every node in VaaP runs a simple consensus process based on voting in parallel. Faulty nodes will only deprive themselves of using consensus service. We present the comparison of VaaP and Sphinx, one of the state-of-the-art consensus protocols, analytically and experimentally (up to 500 nodes). The results indicate that VaaP outperforms Sphinx in throughput, latency and scalability."",""1558-2183"","""",""10.1109/TPDS.2022.3211829"",""National Natural Science Foundation of China(grant numbers:61772030)"; Zhejiang Lab(grant numbers:2021PE0AC01); GF Innovative Research Program;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910415"",""BFT";blockchain;"consensus protocol"",""Consensus protocol";Philosophical considerations;Throughput;Complexity theory;Scalability;Safety;"Prototypes"","""",""2"","""",""28"",""IEEE"",""4 Oct 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"VPIC 2.0: Next Generation Particle-in-Cell Simulations,""R. Bird"; N. Tan; S. V. Luedtke; S. L. Harrell; M. Taufer;" B. Albright"",""Los Alamos National Laboratory, Los Alamos, NM, USA"; University of Tennessee at Knoxville, Knoxville, TN, USA; Los Alamos National Laboratory, Los Alamos, NM, USA; Texas Advanced Computing Center, University of Texas at Austin, Austin, TX, USA; University of Tennessee at Knoxville, Knoxville, TN, USA;" Los Alamos National Laboratory, Los Alamos, NM, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Oct 2021"",""2022"",""33"",""4"",""952"",""963"",""VPIC is a general purpose particle-in-cell simulation code for modeling plasma phenomena such as magnetic reconnection, fusion, solar weather, and laser-plasma interaction in three dimensions using large numbers of particles. VPIC's capacity in both fidelity and scale makes it particularly well-suited for plasma research on pre-exascale and exascale platforms. In this article, we demonstrate the unique challenges involved in preparing the VPIC code for operation at exascale, outlining important optimizations to make VPIC efficient on accelerators. Specifically, we show the work undertaken in adapting VPIC to exploit the portability-enabling framework Kokkos and highlight the enhancements to VPIC's modeling capabilities to achieve performance at exascale. We assess the achieved performance-portability trade-off through a suite of studies on nine different varieties of modern pre-exascale hardware. Our performance-portability study includes weak-scaling runs on three of the top ten TOP500 supercomputers, as well as a comparison of low-level system performance of hardware from four different vendors."",""1558-2183"","""",""10.1109/TPDS.2021.3084795"",""U.S. Department of Energy"; Los Alamos National Laboratory; LANL ASC; Experimental Sciences Programs(grant numbers:LA-UR-21-21453);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9444146"",""Simulation";portability;plasma physics;"particle-in-cell"",""Plasmas";Hardware;Physics;Libraries;Mathematical model;Shape;"Layout"","""",""28"","""",""45"",""CCBY"",""28 May 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;
"vPipe: A Virtualized Acceleration System for Achieving Efficient and Scalable Pipeline Parallel DNN Training,""S. Zhao"; F. Li; X. Chen; X. Guan; J. Jiang; D. Huang; Y. Qing; S. Wang; P. Wang; G. Zhang; C. Li; P. Luo;" H. Cui"",""Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China"; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China; 2012 Labs, Theory Lab, Huawei Technoloies, Co. Ltd, Shenzhen, China; 2012 Labs, Theory Lab, Huawei Technoloies, Co. Ltd, Shenzhen, China; 2012 Labs, Theory Lab, Huawei Technoloies, Co. Ltd, Shenzhen, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China;" Department of Computer Computer Science, The University of Hong Kong, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Aug 2021"",""2022"",""33"",""3"",""489"",""506"",""The increasing computational complexity of DNNs achieved unprecedented successes in various areas such as machine vision and natural language processing (NLP), e.g., the recent advanced Transformer has billions of parameters. However, as large-scale DNNs significantly exceed GPU’s physical memory limit, they cannot be trained by conventional methods such as data parallelism. Pipeline parallelism that partitions a large DNN into small subnets and trains them on different GPUs is a plausible solution. Unfortunately, the layer partitioning and memory management in existing pipeline parallel systems are fixed during training, making them easily impeded by out-of-memory errors and the GPU under-utilization. These drawbacks amplify when performing neural architecture search (NAS) such as the evolved Transformer, where different network architectures of Transformer needed to be trained repeatedly. vPipe is the first system that transparently provides dynamic layer partitioning and memory management for pipeline parallelism. vPipe has two unique contributions, including (1) an online algorithm for searching a near-optimal layer partitioning and memory management plan, and (2) a live layer migration protocol for re-balancing the layer distribution across a training pipeline. vPipe improved the training throughput of two notable baselines (Pipedream and GPipe) by 61.4-463.4 percent and 24.8-291.3 percent on various large DNNs and training settings."",""1558-2183"","""",""10.1109/TPDS.2021.3094364"",""Huawei Innovation Research Program(grant numbers:HK RGC ECS 27200916,HK RGC GRF 17207117,17202318,27208720)"; Croucher Innovation Award; National Natural Science Foundation of China(grant numbers:61802358); USTC Research Funds of Double First-Class Initiative(grant numbers:YD2150002006);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9472938"",""Machine learning";distributed systems;distributed artificial intelligence;pipeline;parallel systems;"memory management"",""Pipelines";Training;Graphics processing units;Throughput;Memory management;Parallel processing;"Tensors"","""",""10"","""",""62"",""CCBYNCND"",""2 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"VQL: Efficient and Verifiable Cloud Query Services for Blockchain Systems,""H. Wu"; Z. Peng; S. Guo; Y. Yang;" B. Xiao"",""Department of Computing, The Hong Kong Polytechnic University, Hong Kong"; Department of Computer Science, Hong Kong Baptist University, Hong Kong; College of Computer Science, Chongqing University, Chongqing, China; Department of Computer Engineering and Computer Science, Stony Brook University, Stony Brook, NY, USA;" Department of Computing, The Hong Kong Polytechnic University, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Oct 2021"",""2022"",""33"",""6"",""1393"",""1406"",""Despite increasingly emerging applications, a primary concern for blockchain to be fully practical is the inefficiency of data query. Direct queries on the blockchain take much time by searching every block, while indirect queries on a blockchain database greatly degrade the authenticity of query results. To conquer the authenticity problem, we propose a Verifiable Query Layer (VQL) that can be deployed in the cloud to provide both efficient and verifiable data query services for blockchain systems. The middleware layer extracts data from the underlying blockchain system and efficiently reorganizes them in databases. To prevent falsified data from being stored in the middleware, a cryptographic fingerprint is calculated based on each constructed database. The database fingerprint will be first verified by miners and then written into the blockchain. Moreover, public users can verify the entire databases or several databases that interest them in the middleware layer. We implement VQL together with the verification schemes and conduct extensive experiments based on a practical blockchain system. The evaluation results demonstrate that VQL can efficiently support various data query services and guarantee the authenticity of query results for blockchain systems."",""1558-2183"","""",""10.1109/TPDS.2021.3113873"",""HK RGC GRF(grant numbers:PolyU 15217321,PolyU 15216220)"; HK ITF(grant numbers:ITS/081/18); Basic and Applied Basic Research Foundation of Guangdong Province(grant numbers:2020A1515111070);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9541060"",""Cloud query service";verifiable query;data authenticity;"blockchain systems"",""Blockchains";Peer-to-peer computing;Middleware;Bitcoin;Data mining;Distributed ledger;"Costs"","""",""35"","""",""33"",""IEEE"",""20 Sep 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"WAMP$^2$2S: Workload-Aware GPU Performance Model Based Pseudo-Preemptive Real-Time Scheduling for the Airborne Embedded System,""Y. Yao"; S. Liu; S. Wu; J. Wang; J. Ni; G. Yang;" Y. Zhang"",""School of Computer Science, Northwestern Polytechnical University, Xi'an, China"; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China; School of Computer Science, Northwestern Polytechnical University, Xi'an, China;" School of Computer Science, Northwestern Polytechnical University, Xi'an, China"",""IEEE Transactions on Parallel and Distributed Systems"",""25 May 2022"",""2022"",""33"",""11"",""2767"",""2780"",""New generation airborne embedded system has deployed Graphical Processing Units (GPUs) to raise processing capability to meet growing computational demands. Comparing with the cloud system, the airborne embedded system usually has a fixed application set, but strict real-time constraints. Unfortunately, the inherent GPU scheduler does not consider the application priority, which cannot provide the sufficient real-time capability to the airborne embedded system. To meet timeliness requirements, it is necessary to predict timing behaviors of those applications and design a real-time scheduling policy based on priority and deadline. We therefore propose WAMP$^2$2S, a workload-aware GPU performance model based pseudo-preemptive real-time scheduling algorithm for the airborne embedded system. The workload-aware GPU performance model can accurately predict the execution time of an application, which is running concurrently with other applications on GPU. The pseudo-preemptive real-time scheduling algorithm can provide the approximate preemption by dynamically adjusting GPU computing resources for active applications. Unlike previous work on GPU performance model and GPU real-time scheduling, WAMP$^2$2S considers the impact of co-executing workload on the execution time estimation and provides a software-only approach for preemption support. In addition, WAMP$^2$2S implements a prototype GPU scheduler without any source code analysis. We evaluate the proposed GPU performance model and real-time scheduling algorithm in both simulated and realistic application sets. Experimental results illustrate that WAMP$^2$2S can achieve low prediction error and high scheduling success ratio."",""1558-2183"","""",""10.1109/TPDS.2021.3134269"",""National Natural Science Foundation of China(grant numbers:61876151,62032018)"; Shanghai Pujiang Program(grant numbers:19PJ1430900); Fundamental Research Funds for the Central Universities(grant numbers:3102019DX1005);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9647957"",""Real-time scheduling";GPU performance model;embedded system;"preemptive scheduling"",""Graphics processing units";Real-time systems;Kernel;Embedded systems;Atmospheric modeling;Predictive models;"Computational modeling"","""",""1"","""",""40"",""IEEE"",""13 Dec 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;
"Work-Stealing Prefix Scan: Addressing Load Imbalance in Large-Scale Image Registration,""M. Copik"; T. Grosser; T. Hoefler; P. Bientinesi;" B. Berkels"",""Department of Computer Science, ETH Zurich, Zürich, Switzerland"; School of Informatics, University of Edinburgh, Edinburgh, U.K; Department of Computer Science, ETH Zurich, Zürich, Switzerland; Department of Computing Science, Umeå University, Umeå, Sweden;" AICES Graduate School, Institute for Geometry and Practical Mathematics, RWTH Aachen University, Aachen, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""27 Jul 2021"",""2022"",""33"",""3"",""523"",""535"",""Parallelism patterns (e.g., map or reduce) have proven to be effective tools for parallelizing high-performance applications. In this article, we study the recursive registration of a series of electron microscopy images - a time consuming and imbalanced computation necessary for nano-scale microscopy analysis. We show that by translating the image registration into a specific instance of the prefix scan, we can convert this seemingly sequential problem into a parallel computation that scales to over thousand of cores. We analyze a variety of scan algorithms that behave similarly for common low-compute operators and propose a novel work-stealing procedure for a hierarchical prefix scan. Our evaluation shows that by identifying a suitable and well-optimized prefix scan algorithm, we reduce time-to-solution on a series of 4,096 images spanning ten seconds of microscopy acquisition from over 10 hours to less than 3 minutes (using 1024 Intel Haswell cores), enabling derivation of material properties at nanoscale for long microscopy image series."",""1558-2183"","""",""10.1109/TPDS.2021.3095230"",""Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung(grant numbers:PZ00P2168016)"; Excellence Initiative of German Federal and State Governments(grant numbers:GSC 111);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9477174"",""Prefix sum";parallel algorithms;work stealing;load balancing;"image registration"",""Microscopy";Image registration;Heuristic algorithms;Scanning electron microscopy;Load management;Standards;"Parallel algorithms"","""",""3"","""",""34"",""IEEE"",""7 Jul 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Workload Balancing via Graph Reordering on Multicore Systems,""Y. Chen";" Y. -C. Chung"",""The Chinese University of Hong Kong, Shenzhen, Guangdong, China";" The Chinese University of Hong Kong, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""19 Oct 2021"",""2022"",""33"",""5"",""1231"",""1245"",""In a shared-memory multicore system, the intrinsic irregular data structure of graphs leads to poor cache utilization, and therefore deteriorates the performance of graph analytics. To address the problem, prior works have proposed a variety of lightweight reordering methods with focus on the optimization of cache locality. However, there is a compromise between cache locality and workload balance. Little insight has been devoted into the issue of workload imbalance for the underlying multicore system, which degrades the effectiveness of parallel graph processing. In this work, a measurement approach is proposed to quantify the imbalance incurred by the concentration of vertices. Inspired by it, we present Cache-aware Reorder (Corder), a lightweight reordering method exploiting the cache hierarchy of multicore systems. At the shared-memory level, Corder promotes even distribution of computation loads amongst multicores. At the private-cache level, Corder facilitates cache efficiency by applying further refinement to local vertex order. Comprehensive performance evaluation of Corder is conducted on various graph applications and datasets. Experimental results show that Corder yields speedup of up to $2.59\times$2.59× and on average $1.45\times$1.45×, which significantly outperforms existing lightweight reordering methods. To identify the root causes of performance boost delivered by Corder, multicore activities are investigated in terms of thread behavior, cache efficiency, and memory utilization. Statistical analysis demonstrates that the issue of imbalanced thread execution time dominates other factors in determining the overall graph processing time. Moreover, Corder achieves remarkable advantages in cross-platform scalability and reordering overhead."",""1558-2183"","""",""10.1109/TPDS.2021.3105323"",""National Key Research and Development Program of China(grant numbers:2018YFB1003505)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9516878"",""Multicore system";cache locality;workload balance;"graph processing"",""Multicore processing";Sorting;Social networking (online);Instruction sets;Blogs;Performance evaluation;"Parallel processing"","""",""1"","""",""50"",""IEEE"",""18 Aug 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"Wukong+G: Fast and Concurrent RDF Query Processing Using RDMA-Assisted GPU Graph Exploration,""Z. Yao"; R. Chen; B. Zang;" H. Chen"",""Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China"; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China;" Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""4 Nov 2021"",""2022"",""33"",""7"",""1619"",""1635"",""RDF graph has been increasingly used to store and represent information shared over the Web, including social graphs and knowledge bases. With the increasing scale of RDF graphs and the concurrency level of SPARQL queries, current RDF systems are confronted with inefficient concurrent query processing on massive data parallelism. The situation becomes more severe in the face of data-intensive queries (aka heavy query), which usually lead to suboptimal response time (latency) as well as throughput collapse. In this article, we present Wukong+G, the first graph-based distributed RDF query processing system that efficiently exploits the hybrid parallelism of CPU and GPU. Wukong+G is made fast and concurrent with four key designs. First, Wukong+G tames massive random memory accesses in graph exploration by efficiently mapping data between CPU and GPU for latency hiding, including a set of techniques like query-aware prefetching, pattern-aware pipelining and fine-grained swapping. Second, Wukong+G scales up by introducing a GPU-friendly RDF store to support RDF graphs exceeding GPU memory size, by using techniques like predicate-based grouping, pairwise caching and look-ahead replacing to narrow the gap between host and device memory scale. Third, Wukong+G scales out through a communication layer that decouples the transferring process for query metadata and intermediate results, and further leverages both native and GPUDirect RDMA to enable efficient communication on a CPU/GPU cluster. Finally, Wukong+G simultaneously runs multiple queries on a single GPU to improve overall throughput and fully exploits hardware heterogeneity (CPU/GPU) by scheduling a single query on CPU and GPU adaptively. We have implemented Wukong+G by extending a state-of-the-art distributed RDF store (i.e., Wukong) with distributed GPU support. Evaluation on a heterogeneous CPU/GPU cluster with RDMA-capable network shows that Wukong+G outperforms Wukong by up to 9.0× (from 2.3×) and scales well on 10 GPU cards for heavy queries. Wukong+G can also improve both latency and throughput by more than one order of magnitude when facing hybrid workloads."",""1558-2183"","""",""10.1109/TPDS.2021.3121568"",""National Key Research and Development Program of China(grant numbers:2020YFB2104100)"; National Natural Science Foundation of China(grant numbers:61772335,61925206); Huawei Technologies;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9582823"",""RDF";SPARQL;graph exploration;hardware heterogeneity;"RDMA"",""Resource description framework";Graphics processing units;History;Hardware;Throughput;Query processing;"Indexes"","""",""3"","""",""84"",""IEEE"",""20 Oct 2021"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;
"zMesh: Theories and Methods to Exploring Application Characteristics to Improve Lossy Compression Ratio for Adaptive Mesh Refinement,""H. Luo"; J. Wang; Q. Liu; J. Chen; S. Klasky;" N. Podhorszki"",""Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA"; Department of Mathematics and Computer Science, Rutgers University, Newark, NJ, USA; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Oak Ridge National Laboratory, Oak Ridge, TN, USA;" Oak Ridge National Laboratory, Oak Ridge, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jul 2022"",""2022"",""33"",""12"",""3702"",""3717"",""Scientific simulations on high-performance computing systems produce vast amounts of data that need to be stored and analyzed efficiently. Lossy compression significantly reduces the data volume by trading accuracy for performance. Despite the recent success of lossy compressions, such as ZFP and SZ, the compression performance is still far from being able to keep up with the exponential growth of data. This article aims to further take advantage of application characteristics, an area that is often under-explored, to improve the compression ratios of adaptive mesh refinement (AMR) - a widely used numerical solver that allows for an improved resolution in limited regions. We propose a level reordering technique zMeshto reduce the storage footprint of AMR applications. In particular, we group the data points that are mapped to the same or adjacent geometric coordinates such that the dataset is smoother and more compressible. Unlike the prior work where the compression performance is affected by the overhead of metadata, this work re-generates the restore recipe using a chained tree structure, thus involving no extra storage overhead for compressed data, which substantially improves the compression ratios. We further derive a mathematical proof that lays the foundation for our method. The results demonstrate that zMesh can improve the smoothness of data by 67.9% and 71.3% for Z-ordering and Hilbert, respectively. Overall, zMesh improves the compression ratios by up to 16.5% and 133.7% for ZFP and SZ, respectively. Despite that zMesh involves additional compute overhead for tree and restore recipe construction, we show that the cost can be amortized as the number of quantities to be compressed increases."",""1558-2183"","""",""10.1109/TPDS.2022.3168386"",""Key-Area Research and Development Program of Guangdong Province(grant numbers:2021B0101190004)"; National Natural Science Foundation of China(grant numbers:62102141); National Science Foundation(grant numbers:CCF-1718297,CCF-1812861); Oak Ridge National Laboratory; U.S. Department of Energy(grant numbers:DE-AC05-00OR22725);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9760099"",""High-performance computing";adaptive mesh refinement (AMR);data reduction;"lossy compression"",""Data models";1/f noise;Layout;Solid modeling;Data compression;Computational modeling;"Compressors"","""",""2"","""",""59"",""IEEE"",""19 Apr 2022"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;;;