"A Bi-layered Parallel Training Architecture for Large-Scale Convolutional Neural Networks,""J. Chen"; K. Li; K. Bilal; x. zhou; K. Li;" P. S. Yu"",""National Supercomputing Center in Changsha, Hunan, Changsha, China"; National Supercomputing Center in Changsha, Hunan, Changsha, China; Qatar University, Doha, Qatar; National Supercomputing Center in Changsha, Hunan, Changsha, China; Department of Computer Science, State University of New York, New Paltz, NY, USA;" Department of Computer Science, University of Illinois at Chicago, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""965"",""976"",""Benefitting from large-scale training datasets and the complex training network, Convolutional Neural Networks (CNNs) are widely applied in various fields with high accuracy. However, the training process of CNNs is very time-consuming, where large amounts of training samples and iterative operations are required to obtain high-quality weight parameters. In this paper, we focus on the time-consuming training process of large-scale CNNs and propose a Bi-layered Parallel Training (BPT-CNN) architecture in distributed computing environments. BPT-CNN consists of two main components: (a) an outer-layer parallel training for multiple CNN subnetworks on separate data subsets, and (b) an inner-layer parallel training for each subnetwork. In the outer-layer parallelism, we address critical issues of distributed and parallel computing, including data communication, synchronization, and workload balance. A heterogeneous-aware Incremental Data Partitioning and Allocation (IDPA) strategy is proposed, where large-scale training datasets are partitioned and allocated to the computing nodes in batches according to their computing power. To minimize the synchronization waiting during the global weight update process, an Asynchronous Global Weight Update (AGWU) strategy is proposed. In the inner-layer parallelism, we further accelerate the training process for each CNN subnetwork on each computer, where computation steps of convolutional layer and the local weight training are parallelized based on task-parallelism. We introduce task decomposition and scheduling strategies with the objectives of thread-level load balancing and minimum waiting time for critical paths. Extensive experimental results indicate that the proposed BPT-CNN effectively improves the training performance of CNNs while maintaining the accuracy."",""1558-2183"","""",""10.1109/TPDS.2018.2877359"",""National Key R&D Program of China(grant numbers:2016YFB0200201)"; National Outstanding Youth Science Program of National Natural Science Foundation of China(grant numbers:61625202); International Postdoctoral Exchange Fellowship Program(grant numbers:2018024); China Postdoctoral Science Foundation(grant numbers:2018T110829); National Science Foundation(grant numbers:IIS-1526499,IIS-1763325,CNS-1626432); NSFC(grant numbers:61672313);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502141"",""Big data";bi-layered parallel computing;convolutional neural networks;deep learning;"distributed computing"",""Training";Computer architecture;Computational modeling;Parallel processing;Task analysis;Distributed computing;"Acceleration"","""",""132"","""",""26"",""IEEE"",""21 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"A Compiler for Agnostic Programming and Deployment of Big Data Analytics on Multiple Platforms,""B. Di Martino"; A. Esposito; S. D'Angelo; S. A. Maisto;" S. Nacchia"",""Department of Engineering, Università degli Studi della Campania “Luigi Vanvitelli”, Caserta, CE, Italy"; Department of Engineering, Università degli Studi della Campania “Luigi Vanvitelli”, Caserta, CE, Italy; Department of Engineering, Università degli Studi della Campania “Luigi Vanvitelli”, Caserta, CE, Italy; Department of Engineering, Università degli Studi della Campania “Luigi Vanvitelli”, Caserta, CE, Italy;" Department of Engineering, Università degli Studi della Campania “Luigi Vanvitelli”, Caserta, CE, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""1920"",""1931"",""To run proper Big Data Analytics, small and medium enterprises (SMEs) need to acquire expertise, hardware and software, which often translates to relevant initial investments for activities not directly connected to the company's business. To reduce such investments, the TOREADOR project proposes a Big Data Analytics framework which supports users in devising their own Big Data solutions by keeping the inherent costs at a minimum, and leveraging pre-existent knowledge and expertise. Among the objectives of the TOREADOR framework is supporting developers in parallelizing and deploying their Big Data algorithms, in order to develop their own analytics solutions. This paper describes the Code-Based approach, adopted within the TOREADOR framework to parallelize users' algorithms and deploy them on distributed platforms, via the annotation of parallelizable code portions with parallelization primitives. The approach, which relies on the guidance of Parallel Patterns to implement the parallelization, and on Skeletons to automatically build execution and deployment templates, is realized through a source-to-source Compiler, also described in the present paper."",""1558-2183"","""",""10.1109/TPDS.2019.2901488"",""European Union's Horizon 2020(grant numbers:688797)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8665909"",""TOREADOR";parallel compiler;skeletons;parallel patterns;parallel primitives;"big data analytics"",""Big Data";Skeleton;Task analysis;Programming;Computational modeling;Program processors;"Parallel processing"","""",""6"","""",""20"",""OAPA"",""12 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"A Correlation-Aware Page-Level FTL to Exploit Semantic Links in Workloads,""J. Zhou"; D. Han; J. Wang; X. Zhou;" C. Jiang"",""College of Information Engineering, Shanghai Maritime University, Shanghai, China"; College of Information Engineering, Shanghai, China; Department of Electrical Engineering and Computer Science, University of Central Florida, Orlando, FL, USA; Computer Science, University of Colorado at Colorado Springs, Colorado Springs, CO, USA;" Department of Computer Science and Technology, Tongji University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Mar 2019"",""2019"",""30"",""4"",""723"",""737"",""NAND Flash based Solid State Disks (SSDs) are gaining tremendous popularity in today's storage market due to their unique erase-before-write feature. The Flash Translation Layer (FTL) in the SSDs redirects the incoming writes to a free physical address and manages a logical to physical address mapping table. However, this induces significant performance degradation to the SSDs. One of the main reasons is that current cache management in FTLs is mainly optimized for the temporal or spatial locality. However, because of multiple levels of data buffers in the whole storage architecture, the locality of internal disk I/O is relatively low. What's more, the increasing capacity of SSD not only generates large mapping tables, but also imposes high pressure on the efficiency of page-level address mapping. To overcome this limitation, we propose Correlation-Aware Page-level FTL, a.k.a CPFTL, which exploits I/O correlations in the workloads. In CPFTL, we develop a correlation-aware mapping table based on the correlation in read operations. We then build a correlation prediction table to support fast mapping entry lookup in the correlation-aware mapping table. Finally, we split read and write caches and build a skew-aware dirty entry index to improve the cache hit ratio and reduce the garbage collection overhead. Our emulator and prototype are open-sourced at: https://github.com/janzhou/SSD-Emulator. The experimental results show that CPFTL can reduce the average response time by 63.4 percent for read dominant workloads and 32.9 percent for transaction workloads."",""1558-2183"","""",""10.1109/TPDS.2018.2871826"",""National Science Foundation(grant numbers:CCF-1337244,1527249,1717388)"; US Army/DURIP(grant numbers:W911NF-17-1-0208); National Natural Science Foundation of China(grant numbers:61672338,61873160);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8470972"",""I/O correlation";solid state drive;"flash translation layer"",""Correlation";Time factors;Prefetching;Random access memory;Bandwidth;Semantics;"Computer science"","""",""9"","""",""35"",""IEEE"",""23 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"A Cost-Efficient Router Architecture for HPC Inter-Connection Networks: Design and Implementation,""Y. Dai"; K. Lu; L. Xiao;" J. Su"",""Department of Computer Science, National University of Defense Technology, Changsha, China"; Department of Computer Science, National University of Defense Technology, Changsha, China; Department of Computer Science, National University of Defense Technology, Changsha, China;" Department of Computer Science, National University of Defense Technology, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""738"",""753"",""High-radix routers with lower latency and higher bandwidth play an increasingly important role in constructing large-scale interconnection networks such as those used in super-computers and datacenters. The tile-based crossbar approach partitions a single large crossbar into many small tiles and can considerably reduce the complexity of arbitration while providing higher throughput than the conventional switch implementation. However, it is not scalable due to power consumption, placement, and routing problems. Inspired by non-saturated throughput theory, this paper proposes a scalable router microarchitecture, termed Multiport Binding Tile-based Router (MBTR). By aggregating multiple physical ports into a single tile a high-radix router can be flexibly organized into different tile arrays, thus the number of tiles and hardware overhead can be considerably reduced. For a radix-64 router MBTR achieves up to $50 \sim 75\%$50∼75%50∼75% reduction in memory consumption as well as wire area compared with a hierarchical switch. We theoretically deduce the sufficient and necessary conditions for the asymmetrical crossbar to achieve un-saturated relative 100 percent throughput. Based on this observation we analyze the MBTR throughput and derive the condition that should be satisfied by the MBTR design parameters to yield 100 percent throughput. We further discuss how to make a trade-off between MBTR parameters based on the constraints of performance, power and area. The simulation results demonstrate MBTR is indistinguishable from the YARC router in terms of throughput and delay, and can even outperform it by reducing potential contention for output ports. We have fabricated a 36-port MBTR chip at 28 nm, providing 100 Gb/s bidirectional bandwidth per port, with a fall-through latency of just 30 ns. Internally it runs at 9.6 Tb/s, thus offering a speedup of $1.34\times$1.34×1.34×."",""1558-2183"","""",""10.1109/TPDS.2018.2873337"",""863 Program of China(grant numbers:2016YFB0200401,2016YFB0200200)"; National Natural Science Foundation of China(grant numbers:61502507,61672526); laboratory pre-research fund(grant numbers:9140C810106150C81001); Foundation for the Author of National Excellent Doctoral Dissertation of the People's Republic of China(grant numbers:201450);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478359"",""High-radix router architecture";non-saturated throughput theory;ASIC implementation;"arbitration logic"",""Throughput";Switches;Microarchitecture;Wires;Bandwidth;Complexity theory;"Memory management"","""",""11"","""",""22"",""IEEE"",""30 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"A Distributed Multilevel Force-Directed Algorithm,""A. Arleo"; W. Didimo; G. Liotta;" F. Montecchiani"",""Technische Universität Wien (TU Wien), Vienna, Austria"; Università degli Studi di Perugia, Perugia, Italy; Università degli Studi di Perugia, Perugia, Italy;" Università degli Studi di Perugia, Perugia, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""754"",""765"",""The use of graph visualization approaches to present and analyze complex data is taking a leading role in conveying information and knowledge to users in many application domains. This creates the need of developing efficient and effective algorithms that automatically compute graph layouts. In this respect, force-directed algorithms are arguably among the most popular graph layout techniques. Aimed at leveraging the potential of modern distributed graph algorithms platforms, we present Multi-GiLA, the first multilevel force-directed graph visualization algorithm based on a vertex-centric computation paradigm. We implemented Multi-GiLA using the Apache Giraph platform. Experiments show that it can be successfully applied to compute high quality layouts of very large graphs on inexpensive cloud computing platforms."",""1558-2183"","""",""10.1109/TPDS.2018.2869805"",""Dipartimento di Ingegneria dell’Università";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462766"",""Distributed graph visualization algorithms";visual analytics;"large and complex networks"",""Visual analytics";Complex networks;Cloud computing;Data visualization;"Distributed algorithms"","""",""14"","""",""56"",""IEEE"",""12 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"A Distributed Trust Evaluation Protocol with Privacy Protection for Intercloud,""Y. Dou"; H. C. B. Chan;" M. H. Au"",""Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong"; Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong;" Department of Computing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1208"",""1221"",""Intercloud seeks to facilitate resource sharing among clouds. To support Intercloud, a trust evaluation framework among clouds and users is required. For trust evaluation, conventional protocols are typically based on a centralized architecture focusing on a one-way relationship. For Intercloud, the environment is highly dynamic and distributed, and relationships can be one-way or two-way (i.e., clouds provide services to each other). This paper presents a distributed trust evaluation protocol with privacy protection for Intercloud. The new contributions and innovative features are summarized below. First, feedback is protected by homomorphic encryption with verifiable secret sharing. Second, to cater to the dynamic nature of Intercloud, trust evaluation can be conducted in a distributed manner and is functional even when some of the parties are offline. Third, to facilitate customized trust evaluation, an innovative mechanism is used to store feedback, such that it can be processed flexibly while protecting feedback privacy. The protocol has been proved based on a formal security model. Simulations have been performed to demonstrate the effectiveness of the protocol. The results show that even when half of the clouds are malicious or offline, by choosing suitable operational parameters the protocol can still support effective trust evaluation with privacy protection."",""1558-2183"","""",""10.1109/TPDS.2018.2883080"",""Department of Computing, The Hong Kong Polytechnic University";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543602"",""Intercloud";trust evaluation;privacy;reputation;"cloud computing"",""Cloud computing";Protocols;Cryptography;Privacy;Business;Computational modeling;"Logic gates"","""",""11"","""",""39"",""IEEE"",""23 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"A Framework for Mesoscopic Traffic Simulation in GPU,""V. A. Vu";" G. Tan"",""Department of Computer Science, National University of Singapore, Singapore";" Department of Computer Science, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1691"",""1703"",""Much progress has recently been made to enhance mesoscopic traffic simulation with the focus on advanced modeling features and traffic management support capabilities. Subsequently, the computational performance needs to be improved in order to make mesoscopic traffic simulation stay effective to real-time applications. This paper presents a framework for mesoscopic traffic simulation which offloads both the demand and supply components to GPU. The simulation algorithm divides the simulation flow into different steps and designs multiple kernels to handle the steps. A high level of data parallelism is achieved by assigning the GPU threads to the appropriate components of the traffic network. Several optimization options using an innovative data structure and improved warp execution are also deployed to harness the GPU performance while preserving simulation correctness. The performance of the framework is evaluated in a real network showing the speedup of up to nearly 5 times in the demand simulation and more than 4 times in the supply simulation compared to the sequential simulation."",""1558-2183"","""",""10.1109/TPDS.2019.2896636"",""National Research Foundation Singapore(grant numbers:R-252-001-459-592)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630684"",""GPU";high-performance computing;"mesoscopic traffic simulation"",""Computational modeling";Graphics processing units;Load modeling;Data models;Vehicles;Loading;"Microscopy"","""",""4"","""",""29"",""IEEE"",""31 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"A General Analysis Framework for Soft Real-Time Tasks,""Z. Dong"; C. Liu; S. Bateni; Z. Kong; L. He; L. Zhang; R. Prakash;" Y. Zhang"",""Department of Computer Science and Engineering, Southern University of Science and Technology"; Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science and Engineering, University of Colorado Denver, Denver, CO, USA; Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, University of Texas at Dallas, Richardson, TX, USA;" Department of Computer Science and Engineering, Southern University of Science and Technology"",""IEEE Transactions on Parallel and Distributed Systems"",""15 May 2019"",""2019"",""30"",""6"",""1222"",""1237"",""Much recent work has been conducted on supporting soft real-time tasks on multiprocessors due to the multicore revolution. While most earlier works focus on the traditional sporadic task model with deterministic worst-case specification, several recent works investigate the stochastic nature of many workloads seen in practice, specifying task execution times using average-case provisioning instead of the worst case. Unfortunately, all the existing work on supporting soft real-time workloads ignores a simple practical fact that the job inter-arrival time (or task period) is also stochastic for many real-world applications. Adopting a fixed worst-case period to model all the arriving pattern is rather pessimistic and may result in significant capacity loss in practice. Based on these observations, we present a general soft real-time multiprocessor schedulability analysis framework in this paper for practical sporadic task systems specified by stochastic period and execution demand, following probability distributions. Our analysis can be generally applied to global tunable priority-based schedulers, which allow any job's priority to be changed dynamically at runtime within a priority window of constant length. We have extensively evaluated the analysis framework using a MPEG video decoding case study and simulation-based experiments. Experimental results demonstrate significant advantages of our analysis, which yields over 200 and 50 percent improvements compared to existing analysis assuming worst-case task periods in terms of schedulability and magnitude of the derived tardiness bound, respectively."",""1558-2183"","""",""10.1109/TPDS.2018.2884980"",""National Key R&D Program of China(grant numbers:2017YFC0804002)"; Shenzhen Peacock Plan(grant numbers:KQTD2016112514355531); Science and Technology Innovation Committee Foundation of Shenzhen(grant numbers:ZDSYS201703031748284,JCYJ20170817110848086); Program for University Key Laboratory of Guangdong Province(grant numbers:2017KSYS008); US NSF(grant numbers:CNS 1527727,CNS CAREER 1750263); Southern University of Science and Technology; Southern University of Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8565912"",""Real-time scheduling";stochastic tasks;schedulability test;tardiness bound;"probability distribution"",""Task analysis";Stochastic processes;Real-time systems;Schedules;Probability distribution;Multicore processing;"Runtime"","""",""6"","""",""38"",""IEEE"",""6 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;
"A Hardware Runtime for Task-Based Programming Models,""X. Tan"; J. Bosch; C. Álvarez; D. Jiménez-González; E. Ayguadé;" M. Valero"",""Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Catalonia, Spain"; Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Catalonia, Spain; Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Catalonia, Spain; Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Catalonia, Spain; Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Catalonia, Spain;" Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya (UPC), Barcelona, Catalonia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""1932"",""1946"",""Task-based programming models such as OpenMP 5.0 and OmpSs are simple to use and powerful enough to exploit task parallelism of applications over multicore, manycore and heterogeneous systems. However, their software-only runtimes introduce relevant overhead when targeting fine-grained tasks, resulting in performance losses. To overcome this drawback, we present a hardware runtime Picos++ that accelerates critical runtime functions such as task dependence analysis, nested task support, and heterogeneous task scheduling. As a proof-of-concept, the Picos++ hardware runtime has been integrated with a compiler infrastructure that supports parallel task-based programming models. A FPGA SoC running Linux OS has been used to implement the hardware accelerated part of Picos++, integrated with a heterogeneous system composed of 4 symmetric multiprocessor (SMP) cores and several hardware functional accelerators (HwAccs) for task execution. Results show significant improvements on energy and performance compared to state-of-the-art parallel software-only runtimes. With Picos++, applications can achieve up to 7.6x speedup and save up to 90 percent of energy, when using 4 threads and up to 4 HwAccs, and even reach a speedup of 16x over the software alternative when using 12 HwAccs and small tasks."",""1558-2183"","""",""10.1109/TPDS.2019.2907493"",""Spanish Government(grant numbers:SEV-2015-0493,TIN2015-65316-P)"; Generalitat de Catalunya(grant numbers:2017-SGR-1414,2017-SGR-1328); H2020 European Research Council(grant numbers:RoMoL GA 321253);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8674567"",""Fine-grained parallelism";task-dependence analysis;nested tasks;heterogeneous task scheduling;energy saving;FPGA;"task-based programming models"",""Task analysis";Hardware;Runtime;Programming;Field programmable gate arrays;Switched mode power supplies;"Multicore processing"","""",""10"","""",""30"",""IEEE"",""26 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"A Holistic Energy-Efficient Real-Time Scheduler for Mixed Stream and Batch Processing Workloads,""S. Maroulis"; N. Zacheilas;" V. Kalogeraki"",""Informatics, Athens University of Economics and Business, Athens, Greece"; Informatics, Athens University of Economics and Business, Athens, Greece;" Informatics, Athens University of Economics and Business, Athens, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2624"",""2635"",""In recent years we have experienced a wide adoption of novel distributed processing frameworks such as Apache Spark for handling batch and stream processing big data applications. An important aspect that has not been examined in these systems yet, is the energy consumption during the applications' execution. Reducing the energy consumption of modern datacenters is a necessity, as datacenters contribute over 2 percent of the total US electric usage. However, efficiently scheduling applications in distributed processing systems can be challenging as there is a trade-off between minimizing the datacenter's energy usage and satisfying the application performance requirements. In this work we propose, ExpREsS, a scheduler for orchestrating the execution of Spark applications in a way that enables us to minimize the energy consumption while ensuring that the applications' performance requirements are met. Our approach exploits time-series segmentation for capturing the applications' energy usage and execution times, and then applies a novel DVFS technique to minimize the energy consumption. In order to tackle the limited number of application's profiling runs, we exploit regression techniques to predict the applications' execution times and power consumption. Our detailed experimental evaluation using realistic workloads on our local cluster illustrates the working and benefits of our approach."",""1558-2183"","""",""10.1109/TPDS.2019.2922606"",""European Union(grant numbers:FP7 ERC IDEAS 308019 NGHCS)"; Horizon2020 688380 VaVeL;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735782"",""Distributed systems";scheduling;green computing;apache spark;"cluster management"",""Power demand";Energy consumption;Cluster computing;Central Processing Unit;Time-frequency analysis;"Distributed processing"","""",""4"","""",""34"",""IEEE"",""12 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"A Hybrid Approach for Optimizing Parallel Clustering Throughput using the GPU,""M. Gowanlock"; C. M. Rude; D. M. Blair; J. D. Li;" V. Pankratius"",""School of Informatics, Computing & Cyber Systems, Northern Arizona University, Flagstaff, AZ, USA"; Massachusetts Institute of Technology, Cambridge, MA, USA; Department of Chemistry and the Department of Earth, Environmental and Planetary Sciences, Brown University, Providence, RI, USA; AAAS Science & Technology Policy Fellow, Washington, DC, USA;" Massachusetts Institute of Technology, Cambridge, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 Mar 2019"",""2019"",""30"",""4"",""766"",""777"",""We introduce Hybrid-Dbscan, that uses the GPU and CPUs for optimizing clustering throughput. The main idea is to exploit the memory bandwidth on the GPU for fast index searches, and optimize data transfers between host and GPU, to alleviate the potential negative performance impact of the PCIe interconnect. We propose and compare two GPU kernels that exploit grid-based indexing schemes to improve neighborhood search performance. We employ a batching scheme for host-GPU data transfers to obviate limited GPU memory, and exploit concurrent operations on the host and GPU. This scheme is robust with respect to both sparse and dense data distributions and avoids buffer overflows that would otherwise degrade performance. We evaluate our approaches on ionospheric total electron content datasets as well as intermediate-redshift galaxies from the Sloan Digital Sky Survey. Hybrid-Dbscan outperforms the reference implementation across a range of application scenarios, including small workloads, which typically are the domain of CPU-only algorithms. We advance an empirical response time performance model of Hybrid-Dbscan by utilizing the underlying properties of the datasets. With only a single execution of Hybrid-Dbscan on a dataset, we are able to accurately predict the response time for a range of $\epsilon$  ε    ε   search distances."",""1558-2183"","""",""10.1109/TPDS.2018.2869777"",""National Science Foundation(grant numbers:ACI-1442997)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462789"",""DBSCAN";GPGPU;in-memory database;parallel clustering;query optimization;"spatial databases"",""Graphics processing units";Clustering algorithms;Kernel;Indexing;Throughput;"Time factors"","""",""11"","""",""34"",""IEEE"",""12 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"A Novel Task-Duplication Based Clustering Algorithm for Heterogeneous Computing Environments,""K. He"; X. Meng; Z. Pan; L. Yuan;" P. Zhou"",""School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" School of Electronic Information and Communications, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""2"",""14"",""As a crucial task in heterogeneous distributed systems, DAG-scheduling models a scheduling application with a set of distributed tasks by a Direct Acyclic Graph (DAG). The goal is to assign tasks to different processors so that the whole application can finish as soon as possible. Task Duplication-Based (TDB) scheme is an important technique addressing this problem. The main idea is to duplicate tasks on multiple machines so that the results of the duplicated tasks are available on multiple machines to trade computation time for communication time. Existing TDB algorithms enumerate and test all possible duplication candidates, and only keep the candidates that can improve the overall scheduling. We observe that while a duplication candidate is ineffective at the moment, after other duplications have been applied, this ineffective duplication candidate can become effective, which in turn can cause other ineffective duplications to become effective. We call this phenomenon the chain reaction of task duplication. We propose a novel Task Duplication based Clustering Algorithm (TDCA) to improve the schedule performance by utilizing duplication task more thoroughly. TDCA improves parameter calculation, task duplication, and task merging. The analysis and experiments are based on randomly generated graphs with various characteristics, including DAG depth and width, communication-computing cost ration, and variant computation power of processors. Our results demonstrate that the TDCA algorithm is very competitive. It improves the schedule makespan of task duplication-based algorithms for heterogeneous systems for various communication-computing cost ratios."",""1558-2183"","""",""10.1109/TPDS.2018.2851221"",""National Natural Science Foundation of China(grant numbers:61472147,61602196,61401169)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8399533"",""Task duplication";clustering and merging;optimal scheduling;"heterogeneous environment"",""Task analysis";Program processors;Scheduling;Clustering algorithms;Scheduling algorithms;"Optimal scheduling"","""",""37"","""",""35"",""IEEE"",""28 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"A Parallel Route Assignment Algorithm for Fault-Tolerant Clos Networks in OTN Switches,""L. Wang"; T. Ye;" T. T. Lee"",""State Key Laboratory of Advanced Optical Communication Systems and Networks, Shanghai Jiao Tong University, Shanghai, China"; State Key Laboratory of Advanced Optical Communication Systems and Networks, Shanghai Jiao Tong University, Shanghai, China;" Chinese University of Hong Kong (Shenzhen), Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""977"",""989"",""The three-stage fault-tolerant Clos network, where extra switch modules exist in the middle stage in case of switch failures, is widely used in the design of OTN switches. This paper proposes a route assignment algorithm for such Clos networks by solving its counterpart in edge-coloring problem. Based on complex coloring, a novel edge coloring method, the proposed algorithm possesses two properties. First, our algorithm can make full use of extra switch modules in the middle stage of Clos network. The extra switch modules provide additional colors for edge coloring, which help to reduce the running time of the coloring process remarkably. Second, our algorithm can be implemented in a parallel manner to further shorten the running time. The proposed routing algorithm achieves a low complexity of O(√N(m-1)/m-1+(m-√N)logN logN), where N is the network size and m is the number of switch modules in the middle stage. The performance of our algorithm has been verified by extensive simulation experiments."",""1558-2183"","""",""10.1109/TPDS.2018.2880782"",""Glaucoma Research Foundation(grant numbers:24207815)"; NSFC/RGC Joint Research Scheme(grant numbers:N_PolyU519/12); Natural Science Foundation of China(grant numbers:61332004); Natural Science Foundation of China(grant numbers:61572415 and 61562005); HK GRF(grant numbers:24207815); NSFC/RGC Joint Research Scheme(grant numbers:N_PolyU519/12); NSFC Key Project(grant numbers:61332004); NSFC(grant numbers:61572415,61562005);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8531701"",""Route assignment";fault tolerance;Clos network;optical switching;edge coloring;"complex coloring"",""Optical switches";Routing;Fault tolerance;Fault tolerant systems;Time complexity;"Color"","""",""6"","""",""36"",""IEEE"",""11 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"A Performance Model for GPU Architectures that Considers On-Chip Resources: Application to Medical Image Registration,""J. wu"; X. Yang; Z. Zhang; G. Chen;" R. Mao"",""College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China"; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Information Engineering, Shenzhen University, Shenzhen, Guangdong, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China;" College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""1947"",""1961"",""Graphics processing units (GPUs) have become extremely important devices for accelerating computing performance in many applications. However, there have been few accurate models to estimate the performance of such applications running on modern GPUs. In this paper, we propose a performance model to estimate the execution times for massively parallel programs running on NVIDIA GPUs, one that takes on-chip resources and cost of data transfer between CPU and GPU into consideration. Four different GPUs with different architectures were used to evaluate our model. We demonstrated the effectiveness of the proposed model by applying it to various tasks in medical image registration. Experiments have demonstrated that by capturing on-chip GPU resources and data transfer time with our model, we were able to obtain a more accurate prediction of the actual running time, compared to the traditional model. Moreover, by using the optimal value of the block size parameter, estimated by our model, to accelerate the landmark tracking task on GPU devices, speedups of approximately 80×, 100×, 200× and 800×, on the C2050, K20c, M5000 and P100 can be achieved, making it possible to track massive numbers of landmarks and thereby improving the registration accuracy."",""1558-2183"","""",""10.1109/TPDS.2019.2905213"",""National Natural Science Foundation of China(grant numbers:61871269)"; Shenzhen Fundamental Research Program(grant numbers:JCYJ20170818100006280); Science and Technology Project of Jiangxi Provincial Education Department(grant numbers:GJJ170515);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8667645"",""Performance model";graphics processing unit;on-chip resources;"medical image registration"",""Graphics processing units";Computational modeling;Predictive models;System-on-chip;Computer architecture;Image registration;"Data transfer"","""",""4"","""",""53"",""OAPA"",""15 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"A Quality-Aware Multi-Level Data Aggregation Approach to Manage Smart Grid AMI Traffic,""U. Das";" V. Namboodiri"",""Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, KS, USA";" Department of Electrical Engineering and Computer Science, Wichita State University, Wichita, KS, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""245"",""256"",""The inclusion of various intelligent electronic devices such as smart meters for AMI is expected to result in intermittent or frequent communications network congestion if additional network infrastructure investments are not made. One approach to deal with such a data volume challenge in smart grids without additional investments to increase network capacity is to aggregate data streams within the network whenever network congestion happens, but this needs to be done carefully so as not to significantly impact the applications that need the data. This paper proposes a novel approach to manage AMI data traffic volume in multi-level data collection trees through data aggregation that estimates the expected network delays messages would suffer and dynamically determines an aggregation policy to be applied at forwarding nodes of the tree to reduce the network delays with which the electric utility can get the necessary information. The proposed algorithm is evaluated for different network congestion scenarios using the NS-3 simulator. The results illustrate that the algorithm is immensely effective in controlling the increase in network latencies as congestion levels increase in AMI networks. In addition, it does well in satisfying quality-of-service (QoS) requirements in terms of the data granularity required by smart grid applications."",""1558-2183"","""",""10.1109/TPDS.2018.2865937"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8439012"",""Advanced metering infrastructures";data granularity;data management;network congestion;"smart grid"",""Data aggregation";Smart grids;Delays;Quality of service;Smart meters;"Power industry"","""",""13"","""",""41"",""IEEE"",""17 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"A Renewable Energy Driven Approach for Computational Sprinting,""H. Cai"; Q. Cao;" H. Jiang"",""Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System of Ministry of Education, Huazhong University of Science and Technology, Wuhan, China"; Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System of Ministry of Education, Huazhong University of Science and Technology, Wuhan, China;" Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1449"",""1463"",""Computational Sprinting, which allows a chip to exceed its power and thermal limits temporarily by turning on all processor cores and absorbing the extra heat dissipation with certain phase-changing materials, has proven to be an effective way to boost the computing performance for bursty workloads. However, extra power available for sprinting is constrained by existing power distribution infrastructures. Using batteries alone to provide the additional power to achieve performance target not only limits the effectiveness of sprinting, but also negatively impacts the lifetime of the batteries. Leveraging renewable power supply in a green data center provides an opportunity to make full use of Computational Sprinting. However, the intermittent nature of renewable energy, along with limited cooling capacity, makes it very challenging. In this paper, we propose GreenSprint, a renewable energy driven approach that enables a data center to boost its computing performance efficiently by conducting computational sprinting under the intermittent and time-varying nature of renewable energy supply. Three basic strategies are designed to determine the core count and frequency level for sprinting based on current power supply. Furthermore, we propose a Hybrid strategy that combines reinforcement learning to dynamically determine the optimal server setting, targeting at both the power provision safety and the quality of service. In consideration of practical cooling conditions, we also present a thermal-aware sprinting strategy Hybrid-T. Finally, we build an experimental prototype to evaluate GreenSprint on a cluster of 10 servers with a simulated solar power generator. The results show that renewable energy by itself can sustain different duration lengths of sprinting when its supply is sufficient and can improve performance by up to 4.8x for representative interactive applications. We also show the effectiveness of core-count and frequency scaling in the presence of varied renewable power and limited battery energy."",""1558-2183"","""",""10.1109/TPDS.2018.2890230"",""Nature Science Foundation of China(grant numbers:61872156,61821003)"; Fundamental Research Funds(grant numbers:2018KFYXKJC037); US NSF(grant numbers:CCF-1704504,CCF-1629625); Alibaba Innovative Research;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8594595"",""Computational sprinting";green data center;renewable energy;"energy efficiency"",""Green products";Data centers;Batteries;Renewable energy sources;Servers;"Power demand"","""","""","""",""45"",""IEEE"",""28 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"A Survey of Prediction and Classification Techniques in Multicore Processor Systems,""C. Ababei";" M. G. Moghaddam"",""Department of Electrical and Computer Engineering, Marquette University, Milwaukee, WI, USA";" Department of Electrical and Computer Engineering, Marquette University, Milwaukee, WI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1184"",""1200"",""In multicore processor systems, being able to accurately predict the future provides new optimization opportunities, which otherwise could not be exploited. For example, an oracle able to predict a certain application's behavior running on a smart phone could direct the power manager to switch to appropriate dynamic voltage and frequency scaling modes that would guarantee minimum levels of desired performance while saving energy consumption and thereby prolonging battery life. Using predictions enables systems to become proactive rather than continue to operate in a reactive manner. This prediction-based proactive approach has become increasingly popular in the design and optimization of integrated circuits and of multicore processor systems. Prediction transforms from simple forecasting to sophisticated machine learning based prediction and classification that learns from existing data, employs data mining, and predicts future behavior. This can be exploited by novel optimization techniques that can span across all layers of the computing stack. In this survey paper, we present a discussion of the most popular techniques on prediction and classification in the general context of computing systems with emphasis on multicore processors. The paper is far from comprehensive, but, it will help the reader interested in employing prediction in optimization of multicore processor systems."",""1558-2183"","""",""10.1109/TPDS.2018.2878699"",""Dept. of Electrical and Computer Engineering";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8514815"",""Multicore processor system";prediction;classification;exponential averaging;history predictor;autoregressive moving average (ARMA);Kalman filter;linear regression (LR);linear discriminant analysis (LDA);multinomial logistic regression;K-nearest neighbor (KNN);Bayes classifier;support vector machines (SVM);reinforcement learning (RL);online machine learning;neural network (NN);deep neural network (DNN);"model predictive control"",""Autoregressive processes";History;Mathematical model;Computational modeling;Multicore processing;Optimization;"Predictive models"","""",""26"","""",""111"",""IEEE"",""30 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"A Thorough Trust and Reputation Based RBAC Model for Secure Data Storage in the Cloud,""M. Ghafoorian"; D. Abbasinezhad-Mood;" H. Shakeri"",""Department of Computer Engineering and Information Technology, Imam Reza International University, Mashhad, Iran"; Department of Computer Engineering and Information Technology, Imam Reza International University, Mashhad, Iran;" Department of Computer Engineering, Mashhad Branch, Islamic Azad University, Mashhad, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Mar 2019"",""2019"",""30"",""4"",""778"",""788"",""Cloud computing is a widespread technology, which has attracted much attention nowadays. Among the many criteria that must be considered for data storage in the cloud, access control plays a vital role. Role-based access control (RBAC) is a well-known technique for secure data storage in the cloud. Since the traditional RBAC models are improper for open and decentralized environments, recently, some works have integrated the trust concept into the RBAC model. Nevertheless, they have not fully addressed the required security metrics of a trust-based system. Therefore, in this paper, we first introduce the security goals that should be considered in an efficient trust-based system. Second, we propose a novel trust and reputation based RBAC model that not only can properly withstand the security threats of trust-based RBAC models, but also is scalable as it has reasonable execution time. Third, we evaluate the proposed model using the famous trust network of advogato dataset. Eventually, we compare the proposed model with recently-published ones in terms of mean absolute error, execution time of indirect trust computation, and provided features. The achieved results are indicative of the priority of the proposed model to be employed in real cloud environments."",""1558-2183"","""",""10.1109/TPDS.2018.2870652"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466653"",""Cloud computing";role-based access control (RBAC);reputation;"trust management system (TMS)"",""Cloud computing";Computational modeling;Access control;Memory;"Measurement"","""",""48"","""",""29"",""IEEE"",""16 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"A Utility-Based Optimization Framework for Edge Service Entity Caching,""Y. Liang"; J. Ge; S. Zhang; J. Wu; Z. Tang;" B. Luo"",""Software Institute, Nanjing University, Nanjing, China"; Software Institute, Nanjing University, Nanjing, China; Department of Compute Science and Technology, Nanjing University, Nanjing, China; Center for Networked Computing, Temple University, Philadelphia, PA, USA; Software Institute, Nanjing University, Nanjing, China;" Software Institute, Nanjing University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2384"",""2395"",""Edge computing is one of the emerging technologies aiming to enable timely computation at the network edge. With virtualization technologies, the role of the traditional edge providers is separated into two: edge infrastructure providers (EIPs), who manage the physical edge infrastructure, and edge service providers (ESPs), who purchase slices of physical resources (e.g., CPU, bandwidth, memory space, disk storage) from EIPs and then cache service entities to offer their own value-added services to end users. When an ESP caches a service entity in an edge server, the ESP has to pay some fees (i.e, the cache cost) to the EIP that owns the edge server. One of the fundamental problems in edge virtualization is the so-called service entity caching problem, i.e., where to place service entities for an ESP to minimize the cache cost. In this paper, we study the service entity caching problem from the utility perspective. We use `utility' to denote the positive impact on a client from caching a service entity in an edge server, and the exact meaning of utility can vary depending on specific scenarios. We formulate the Utility-based Service Entity Caching (UtilitySEC) problem, which can be generalized to many existing problems by modifying the `utility'. We prove that the UtilitySEC problem is NP-complete and design an approximation algorithm for it. Extensive simulations are conducted to evaluate the performance of the proposed framework."",""1558-2183"","""",""10.1109/TPDS.2019.2915218"",""National Key R&D Program of China(grant numbers:2017YFB1001801,2016YFC0800803)"; NSFC(grant numbers:61872175); Natural Science Foundation of Jiangsu Province(grant numbers:BK20181252); CCF-Tencent Open Fund; Collaborative Innovation Center of Novel Software Technology and Industrialization;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708961"",""Edge computing";service entity caching;set cover;"utility"",""Servers";Internet of Things;Edge computing;Bandwidth;Virtualization;Approximation algorithms;"Probabilistic logic"","""",""22"","""",""37"",""IEEE"",""7 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"A Virtual Multi-Channel GPU Fair Scheduling Method for Virtual Machines,""H. Tan"; Y. Tan; X. He; K. Li;" K. Li"",""College of Computer Science and Electronic Engineering, Hunan University, Changsha, China"; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China; College of Computer Science and Electronic Engineering, Hunan University, Changsha, China;" Department of Computer Science, State University of New York, New Paltz, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""257"",""270"",""In modern virtual computing environment, the 2D/3D rendering performance and parallel computing potential of GPU (graphics processing unit) must be fully exploited for multiple virtual machines (VMs). Existing GPU virtualization techniques are unable to take full advantage of a GPU's powerful 2D/3D hardware-accelerated graphics rendering performance or parallel computing potential, or it has not been considered that the internal resources of a GPU domain are fairly allocated between VMs with different performance requirements. Therefore, we propose a multi-channel GPU virtualization architecture (VMCG), model the corresponding credit allocating and transferring mechanisms, and redesign the virtual multi-channel GPU fair-scheduling algorithm. VMCG provides a separate V-Channel for each guest VM (DomU) that competes with other VMs for the same physical GPU resources, and each DomU submits command request blocks to its respective V-Channel according to the corresponding DomU ID. Through the virtual multi-channel GPU fair-scheduling algorithm, not only do multiple DomUs make full use of native GPU hardware acceleration, but the fairness of GPU resource allocation is significantly improved during GPU-intensive workloads from multiple DomUs running on the same host. Experimental results show that, for 2D/3D graphics applications, performance is close to 96 percent of that of the native GPU, performance is improved by approximately 500 percent for parallel computing applications, and GPU resource-allocation fairness is improved by approximately 60-80 percent."",""1558-2183"","""",""10.1109/TPDS.2018.2865341"",""National Natural Science Foundation of China(grant numbers:61672218)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434359"",""Credit modeling";fair scheduling;GPU virtualization;hybrid application workloads;"virtual multi-channel"",""Graphics processing units";Virtualization;Resource management;Rendering (computer graphics);Computer architecture;Parallel processing;"Scheduling"","""",""13"","""",""37"",""IEEE"",""13 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Accelerating Atmospheric Chemical Kinetics for Climate Simulations,""M. Alvanos";" T. Christoudias"",""The Cyprus Institute, Aglantzia, Cyprus";" The Cyprus Institute, Aglantzia, Cyprus"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2396"",""2407"",""The study of atmospheric chemistry-climate interactions is one of today's great computational challenges. Advances in the architecture of Graphics Processing Units (GPUs) in both raw computational power and memory bandwidth sparked the interest for General-Purpose computing on graphics accelerators in scientific applications. However, the introduction of GPUs in the High Performance Computing (HPC) landscape increased the complexity of software development, due to the inherent heterogeneity requirements of programming models and design approaches, creating a gap in uptake and attainable performance in the presently available scientific community codes. This paper provides an overview of the challenges encountered when using GPU accelerators to achieve optimal performance to calculate the kinetics of chemical tracers in climate models, the techniques used to address them and the insights gained from the process. The paper presents the development of a chemical kinetics code-to-code parser to automatically generate chemical kinetics calculations on three different generations of GPU accelerators (M2070, K80, and P100). The accelerated portion of the application achieves a speedup of up to 22×, equivalent to performance gains of +19 percent up to +90 percent compared with the processor-only version, when using a cluster of 8 Nodes with dual Intel E5-2680 v3 processor and a Kepler architecture (K80), allowing faster completion of the simulations. The paper also provides practical insights and relevant considerations for the development and acceleration of complex applications."",""1558-2183"","""",""10.1109/TPDS.2019.2918798"",""European Community's Seventh Framework Programme (FP7/2007-2013)(grant numbers:287530)"; European Union's Horizon 2020 research and innovation programme(grant numbers:675121,676629); European Regional Development Fund; Research Promotion Foundation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723098"",""High performance computing";heterogeneous systems;GPU acceleration;"climate simulation"",""Chemicals";Atmospheric modeling;Meteorology;Kinetic theory;Computational modeling;Graphics processing units;"Acceleration"","""",""6"","""",""43"",""IEEE"",""27 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"An Analysis Workflow-Aware Storage System for Multi-Core Active Flash Arrays,""H. Sim"; G. Vallée; Y. Kim; S. S. Vazhkudai; D. Tiwari;" A. R. Butt"",""Oak Ridge National Laboratory, Oak Ridge, TN, USA"; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Sogang University, Seoul, South Korea; Oak Ridge National Laboratory, Oak Ridge, TN, USA; Northeastern University, Boston, MA, USA;" Virginia Tech, Blacksburg, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""271"",""285"",""The need for novel data analysis is urgent in the face of a data deluge from modern applications. Traditional approaches to data analysis incur significant data movement costs, moving data back and forth between the storage system and the processor. Emerging Active Flash devices enable processing on the flash, where the data already resides. An array of such Active Flash devices allows us to revisit how analysis workflows interact with storage systems. By seamlessly blending together the flash storage and data analysis, we create an analysis workflow-aware storage system, AnalyzeThis. Our guiding principle is that analysis-awareness be deeply ingrained in each and every layer of the storage, elevating data analyses as first-class citizens, and transforming AnalyzeThis into a potent analytics-aware appliance. To evaluate the AnalyzeThis system, we have adopted both emulation and simulation approaches. In particular, we have evaluated AnalyzeThis by implementing the AnalyzeThis storage system on top of the Active Flash Array's emulation platform. We have also implemented an event-driven AnalyzeThis simulator, called AnalyzeThisSim, which allows us to address the limitations of the emulation platform, e.g., performance impact of using multi-core SSDs. The results from our emulation and simulation platforms indicate that AnalyzeThis is a viable approach for expediting workflow execution and minimizing data movement."",""1558-2183"","""",""10.1109/TPDS.2018.2865471"",""NSF(grant numbers:CNS-1405697,CNS-1422788)"; National Research Foundation of Korea(grant numbers:2017M3C4A7080243);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8436425"",""Distributed systems";storage management;"scientific data management"",""Data analysis";Arrays;Performance evaluation;Data models;Bandwidth;Emulation;"Analytical models"","""",""1"","""",""63"",""IEEE"",""14 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"An Efficient Application Partitioning Algorithm in Mobile Environments,""H. Wu"; W. J. Knottenbelt;" K. Wolter"",""Center for Applied Mathematics, Tianjin University, Tianjin, China"; Department of Computing, Imperial College London, London, United Kingdom;" Institut für Informatik, Freie Universität Berlin, Berlin, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2019"",""2019"",""30"",""7"",""1464"",""1480"",""Application partitioning that splits the executions into local and remote parts, plays a critical role in high-performance mobile offloading systems. Optimal partitioning will allow mobile devices to obtain the highest benefit from Mobile Cloud Computing (MCC) or Mobile Edge Computing (MEC). Due to unstable resources in the wireless network (network disconnection, bandwidth fluctuation, network latency, etc.) and at the service nodes (different speeds of mobile devices and cloud/edge servers, memory, etc.), static partitioning solutions with fixed bandwidth and speed assumptions are unsuitable for offloading systems. In this paper, we study how to dynamically partition a given application effectively into local and remote parts while reducing the total cost to the degree possible. For general tasks (represented in arbitrary topological consumption graphs), we propose a Min-Cost Offloading Partitioning (MCOP) algorithm that aims at finding the optimal partitioning plan (i.e., to determine which portions of the application must run on the mobile device and which portions on cloud/edge servers) under different cost models and mobile environments. Simulation results show that the MCOP algorithm provides a stable method with low time complexity which significantly reduces execution time and energy consumption by optimally distributing tasks between mobile devices and servers, besides it adapts well to mobile environmental changes."",""1558-2183"","""",""10.1109/TPDS.2019.2891695"",""National Natural Science Foundation of China(grant numbers:61801325)"; Natural Science Foundation of Tianjin City(grant numbers:18JCQNJC00600); Huawei Innovation Research Program(grant numbers:HIRPO2017050307);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606130"",""Mobile cloud computing";mobile edge computing;communication networks;offloading;"application partitioning"",""Mobile handsets";Servers;Task analysis;Partitioning algorithms;Cloud computing;Topology;"Bandwidth"","""",""79"","""",""55"",""IEEE"",""9 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"An Efficient Hybrid I/O Caching Architecture Using Heterogeneous SSDs,""R. Salkhordeh"; M. Hadizadeh;" H. Asadi"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1238"",""1250"",""Storage subsystem is considered as the performance bottleneck of computer systems in data-intensive applications. Solid-State Drives (SSDs) are emerging storage devices which unlike Hard Disk Drives (HDDs), do not have mechanical parts and therefore, have superior performance compared to HDDs. Due to the high cost of SSDs, entirely replacing HDDs with SSDs is not economically justified. Additionally, SSDs can endure a limited number of writes before failing. To mitigate the shortcomings of SSDs while taking advantage of their high performance, SSD caching is practiced in both academia and industry. Previously proposed caching architectures have only focused on either performance or endurance and neglected to address both parameters in suggested architectures. Moreover, the cost, reliability, and power consumption of such architectures is not evaluated. This paper proposes a hybrid I/O caching architecture that while offers higher performance than previous studies, it also improves power consumption with a similar budget. The proposed architecture uses DRAM, Read-Optimized SSD (RO-SSD), and Write-Optimized SSD (WO-SSD) in a three-level cache hierarchy and tries to efficiently redirect read requests to either DRAM or RO-SSD while sending writes to WO-SSD. To provide high reliability, dirty pages are written to at least two devices which removes any single point of failure. The power consumption is also managed by reducing the number of accesses issued to SSDs. The proposed architecture reconfigures itself between performance- and endurance-optimized policies based on the workload characteristics to maintain an effective tradeoff between performance and endurance. We have implemented the proposed architecture on a server equipped with industrial SSDs and HDDs. The experimental results show that as compared to state-of-the-art studies, the proposed architecture improves performance and power consumption by an average of 8 and 28 percent, respectively, and reduces the cost by 5 percent while increasing the endurance cost by 4.7 percent and negligible reliability penalty."",""1558-2183"","""",""10.1109/TPDS.2018.2883745"",""Iran National Science Foundation(grant numbers:96006071)"; HPDS Corp;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550676"",""Solid-state drives";I/O caching;performance;"data storage systems"",""Performance evaluation";Random access memory;Computer architecture;Reliability;Power demand;Servers;"Hard disks"","""",""9"","""",""57"",""IEEE"",""28 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"An Efficient Ring-Based Metadata Management Policy for Large-Scale Distributed File Systems,""Y. Gao"; X. Gao; X. Yang; J. Liu;" G. Chen"",""Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, P.R. China"; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, P.R. China; School of Computer Science and Engineering, Northeastern University, Shenyang, P. R. China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, P.R. China;" Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Shanghai, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""1962"",""1974"",""The growing size of modern file system is expected to reach EB-scale. Therefore, an efficient and scalable metadata service is critical to system performance. Distributed metadata management schemes, which use multiple metadata servers (MDS's) to store metadata, provide a highly effective approach to alleviate the workload of a single server. However, it is difficult to maintain good metadata locality and load balancing among MDS's at the same time. In this paper, we propose a novel hashing scheme called AngleCut to partition metadata namespace tree and serve large-scale distributed storage systems. AngleCut first uses a locality preserving hashing (LPH) function to project the namespace tree into linear keyspace, i.e., multiple Chord-like rings. Then we design a history-based allocation strategy to adjust the workload of MDS's dynamically. Besides, we propose a two-layer metadata cache mechanism, including server-side cache and client-side cache to provide the two stage access acceleration. Last but not least, we introduce a distributed metadata processing 2PC Protocol Based on Message Queue (2PC-MQ) to ensure data consistency. In general, our scheme preserves good metadata locality as well as maintains a high load balancing between MDS's. The theoretical proof and extensive experiments on Amazon EC2 demonstrate the superiority of AngleCut over previous literature."",""1558-2183"","""",""10.1109/TPDS.2019.2901883"",""National Key R&D Program of China(grant numbers:2018YFB1004703)"; National Natural Science Foundation of China(grant numbers:61872238,61672353,61532021); Shanghai Science and Technology Development Foundation(grant numbers:17510740200); Huawei Innovation Research Program(grant numbers:HO2018085286); State Key Laboratory of Air Traffic Management System and Technology(grant numbers:SKLATM20180X); Tencent Social Ads Rhino-Bird Focused Research Program; Fundamental Research Funds for the Central Universities(grant numbers:N171602003);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653345"",""Metadata management";locality preserving hashing;distributed storage system;"namespace tree"",""Metadata";Servers;Load management;Distributed databases;Resource management;Protocols;"File systems"","""",""11"","""",""35"",""IEEE"",""26 Feb 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"An Empirical Study on Distributed Bayesian Approximation Inference of Piecewise Sparse Linear Models,""M. Asahara";" R. Fujimaki"",""NEC System Platform Research Laboratories, Cupertino, CA, USA";" NEC Data Science Research Laboratories, Cupertino, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2019"",""2019"",""30"",""7"",""1481"",""1493"",""The importance of interpretability of machine learning models has been increasing due to emerging enterprise predictive analytics. Piecewise linear models have been actively studied to achieve both accuracy and interpretability. They often produce competitive accuracy against state-of-the-art non-linear methods. In addition, their representations (i.e., rule-based segmentation plus sparse linear formula) are often preferred by domain experts. A disadvantage of such models, however, is high computational cost for simultaneous determinations of the number of “pieces” and cardinality of each linear predictor, which has restricted their applicability to middle-scale data sets. This paper discusses an empirical study on the derivation of a distributed factorized asymptotic Bayesian (FAB) inference of learning piece-wise sparse linear models on distributed memory architectures from the original FAB inference algorithm. The distributed FAB inference solves the simultaneous model selection issue without communicating O(N) data where N is the number of training samples and achieves linear scale-out against the number of CPU cores. Experimental results demonstrate that the distributed FAB inference achieves high prediction accuracy and performance scalability with both synthetic and public benchmark data."",""1558-2183"","""",""10.1109/TPDS.2019.2892972"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611362"",""Machine learning";distributed applications/systems;interpretable models;"sparse models"",""Sparks";Machine learning;Inference algorithms;Computational modeling;Bayes methods;Data models;"Distributed databases"","""",""3"","""",""52"",""IEEE"",""13 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"An I/O Efficient Distributed Approximation Framework Using Cluster Sampling,""X. Zhang"; J. Wang; S. Ji; J. Yin; R. Wang; X. Zhou;" C. Jiang"",""Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA"; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA; Institute of Cyberspace Research, College of Computer Science and Technology, Zhejiang University, Hangzhou, Zhejiang, China; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA;" Tongji University, Shanghai Shi, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1494"",""1511"",""In this paper, we present an I/O efficient distributed approximation framework to support approximations on arbitrary sub-datasets of a large dataset. Due to the prohibitive storage overhead of caching offline samples for each sub-dataset, existing offline sample-based systems provide high accuracy results for only a limited number of sub-datasets, such as the popular ones. On the other hand, current online sample-based approximation systems, which generate samples at runtime, do not take into account the uneven storage distribution of a sub-dataset. They work well for uniform distribution of a sub-dataset while suffer low I/O efficiency and poor estimation accuracy on unevenly distributed sub-datasets. To address the problem, we develop a distribution aware method called CLAP (cluster sampling based approximation). Our idea is to collect the occurrences of a sub-dataset at each logical partition of a dataset (storage distribution) in the distributed system, and make good use of such information to enable I/O efficient online sampling. There are three thrusts in CLAP. First, we develop a probabilistic map to reduce the exponential number of recorded sub-datasets to a linear one. Second, we apply the cluster sampling with unequal probability theory to implement a distribution-aware method for efficient online sampling for a single or multiple sub-datasets. Third, we enrich CLAP support with more complex approximations such as ratio and regression using bootstrap based estimation beyond the simple aggragation approxiamtions. Forth, we add an option in CLAP to allow users specifying a target error bound when submitting an approximation job. Fifth, we quantitatively derive the optimal sampling unit size in a distributed file system by associating it with approximation costs and accuracy. We have implemented CLAP into Hadoop as an example system and open sourced it on GitHub. Our comprehensive experimental results show that CLAP can achieve a speedup by up to 20× over the precise execution."",""1558-2183"","""",""10.1109/TPDS.2019.2892765"",""National Science Foundation(grant numbers:CCF-1337244,1527249,1717388)"; US Army/DURIP(grant numbers:W911NF-17-1-0208); National Natural Science Foundation of China(grant numbers:61772466); Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars(grant numbers:LR19F020003); Provincial Key Research and Development Program of Zhejiang, China(grant numbers:2017C01055); Alibaba-ZJU Joint Research Institute of Frontier Technologies;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611214"",""Approximation";cluster sampling;sub-dataset;storage distribution;"Hadoop"",""Linux";Smart phones;Urban areas;Estimation;Sampling methods;Probabilistic logic;"Computers"","""","""","""",""34"",""IEEE"",""13 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"An Overflow Problem in Network Coding for Secure Cloud Storage,""Y. -J. Chen";" L. -C. Wang"",""National Chiao Tung University, Hsinchu, Taiwan, R.O.C";" National Chiao Tung University, Hsinchu, Taiwan, R.O.C"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""789"",""799"",""In this paper, we present the overflow problem of a network coding storage system (NCSS) when the encoding parameters and the storage parameters are mismatched. The overflow problem of the NCSS occurs because the network-coded encryption yields extended coded data, resulting in high storage and processing overhead. To avoid the overflow problem, we propose an overflow-avoidance NCSS scheme that takes account of security and storage requirements in both encoding and storage procedures. We provide the analytical results of the maximum allowable stored encoded data under the perfect secrecy criterion. The design guidelines to achieve high coding efficiency with the lowest storage cost are also presented."",""1558-2183"","""",""10.1109/TPDS.2018.2870890"",""Ministry of Science and Technology, Taiwan(grant numbers:102-2221-E-009-0 12-MY3)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466924"",""Cloud storage";network coding;"data security"",""Network coding";Cloud computing;Encoding;Secure storage;Databases;Security;"Eavesdropping"","""",""2"","""",""38"",""IEEE"",""16 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Analytical Derivation of Concurrent Reuse Distance Profile for Multi-Threaded Application Running on Chip Multi-Processor,""J. M. Sabarimuthu";" T. G. Venkatesh"",""Department of Electrical Engineering, Indian Institute of Technology, Madras, Chennai, India";" Department of Electrical Engineering, Indian Institute of Technology, Madras, Chennai, India"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1704"",""1721"",""Reuse distance has been shown to be a useful metric for performance analysis of caches and programs, locality analysis and compiler optimization. Concurrent reuse distance profile defined as the reuse distance profile of a thread sharing the cache with many other threads varies from its standalone reuse distance profile due to interference from other threads. Measurement of reuse distance profile through simulation, especially for multi-threaded applications, consumes lot of time. Analytical model based reuse distance prediction can reduce drastically the time taken for exploring the cache memory design space. The objective of this work is to propose an analytical model to find the concurrent reuse distance profile of a thread belonging to multi-threaded applications in a shared memory environment. Using the standalone reuse distance profile of each thread as input, we derive three other reuse distance profiles: 1) The concurrent reuse distance profile of a thread sharing the cache with other threads 2) The combined reuse distance profile of all threads sharing the cache and 3) The coherent reuse distance profile of each thread, considering the coherency effect when each thread runs with private cache. We use Markov chain besides combinatorics and basic probability theory as a main analytical tool for the model. We validate our analytical model against simulations, using the multi-core simulator Sniper for the benchmarks of the PARSEC and the SPLASH benchmark suites."",""1558-2183"","""",""10.1109/TPDS.2019.2896633"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630685"",""Reuse distance profile";concurrent reuse distance;cache performance;analytical model;multi-threaded applications;multi-core processors;"simulation"",""Instruction sets";Analytical models;Measurement;Histograms;Complexity theory;"Performance analysis"","""",""4"","""",""49"",""IEEE"",""31 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Anonymous Readers Counting: A Wait-Free Multi-Word Atomic Register Algorithm for Scalable Data Sharing on Multi-Core Machines,""M. Ianni"; A. Pellegrini;" F. Quaglia"",""Department of Computer, Control, and Management Engineering, Sapienza University of Rome, Roma, RM, Italy"; Department of Computer, Control, and Management Engineering, Sapienza University of Rome, Roma, RM, Italy;" Dipartimento di Ingegneria Civile e Ingegneria Informatica, Università di Roma “Tor Vergata”, Roma, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""286"",""299"",""In this article we present Anonymous Readers Counting (ARC), a multi-word atomic (1,N) register algorithm for multi-core machines. ARC exploits Read-Modify-Write (RMW) instructions to coordinate the writer and reader threads in a wait-free manner and enables large-scale data sharing by admitting up to $(2^{32}-2)$ concurrent readers on off-the-shelf 64-bit machines, as opposed to the most advanced RMW-based approach which is limited to 58 readers on the same kind of machines. Further, ARC avoids multiple copies of the register content when accessing it—this is a problem that affects classical register algorithms based on atomic read/write operations on single words. Thus it allows for higher scalability with respect to the register size. Moreover, ARC explicitly reduces the overall power consumption, via a proper limitation of RMW instructions in case of read operations re-accessing a still-valid snapshot of the register content, and by showing constant time for read operations and amortized constant time for write operations. Our proposal has therefore a strong focus on real-world off-the-shelf architectures, allowing us to capture properties which benefit both performance and power consumption. A proof of correctness of our register algorithm is also provided, together with experimental data for a comparison with literature proposals. Beyond assessing ARC on physical platforms, we carry out as well an experimentation on virtualized infrastructures, which shows the resilience of wait-free synchronization as provided by ARC with respect to CPU-steal times, proper of modern paradigms such as cloud computing. Finally, we discuss how to extend ARC for scenarios with multiple writers and multiple readers—the so called (M,N) register. This is achieved not by changing the operations (and their wait-free nature) executed along the critical path of the threads, rather only changing the ratio between the number of buffers keeping the register snapshots and the number of threads to coordinate, as well as the number of bits used for counting readers within a 64-bit mask accessed via RMW instructions—just depending on the target balance between the number of readers and the number of writers to be supported."",""1558-2183"","""",""10.1109/TPDS.2018.2865932"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8438921"",""Atomic registers";shared-memory;multi-core computing;wait-free synchronization;"instruction-set-architecture"",""Registers";Synchronization;Proposals;Message systems;Scalability;Instruction sets;"Memory management"","""",""4"","""",""30"",""IEEE"",""17 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Applying Transactional Memory for Concurrency-Bug Failure Recovery in Production Runs,""Y. Chen"; S. Wang; S. Lu;" K. Sankaralingam"",""Department of Computer Science, University of Chicago, Chicago, IL, USA"; Department of Computer Science, University of Chicago, Chicago, IL, USA; Department of Computer Science, University of Chicago, Chicago, IL, USA;" Department of Computer Science, University of Wisconsin–Madison, Madison, WI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""990"",""1006"",""Concurrency bugs widely exist and severely threaten system availability. Techniques that help recover from concurrency-bug failures during production runs are highly desired. This paper proposes BugTM, an approach that applies transactional memory techniques for concurrency-bug recovery in production runs. Requiring no knowledge about where are concurrency bugs, BugTM uses static analysis and code transformation to enable BugTM-transformed software to recover from a concurrency-bug failure by rolling back and re-executing the recent history of a failure thread. BugTM is instantiated as three schemes that have different trade-offs in performance and recovery capability: BugTM$_H$H uses existing hardware transactional memory (HTM) support, BugTM$_S$S leverages software transactional memory techniques, and BugTM$_{\mathrm{HS}}$ HS is a software-hardware hybrid design. BugTM greatly improves the recovery capability of state-of-the-art techniques with low run-time overhead and no changes to OS or hardware, while guarantees not to introduce new bugs."",""1558-2183"","""",""10.1109/TPDS.2018.2877656"",""National Science Foundation(grant numbers:CNS-1563956,IIS-1546543,CNS-1514256,CNS-1764039,CCF-1514189,CCF-1439091)"; CERES Center for Unstoppable Computing;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502855"",""Concurrency bugs";transactional memory;failure recovery;"software availability"",""Computer bugs";Software;Concurrent computing;Message systems;Production;Hardware;"Checkpointing"","""","""","""",""73"",""IEEE"",""23 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Architecture-Based Reliability-Sensitive Criticality Measure for Fault-Tolerance Cloud Applications,""L. Wang"",""Department of Management Science and Engineering, Nanjing Forestry University, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""30 Oct 2019"",""2019"",""30"",""11"",""2408"",""2421"",""The widespread adoption of service computing allows software to be developed by outsourcing open cloud services (i.e., SOAP-based or RESTful Web APIs) through mashup or service composition techniques. Fault tolerance for the purpose of assuring the stable execution for cloud-based software (or CBS) application has attracted great attention in coping with a loosely coupled CBS operating under dynamic and uncertain running environments. It is too expensive to rent massively redundant cloud services for CBS fault tolerance application. To reduce budget but guarantee the effectiveness of CBS fault tolerance, identifying critical components within a CBS composite system is of significant importance. We integrate CBS composite system architecture analysis and reliability sensitivity analysis approaches and propose an Architecture-based Reliability-sensitive Criticality Measure (or ARCMeas) method in this paper. We verify ARCMeas application through a cost-effective fault tolerance CBS by presenting a particle swarm optimization (PSO)-based cost-effective fault tolerance strategy determination (or PSO-CFTD) algorithm. Experimental results suggest the effectiveness of the approach."",""1558-2183"","""",""10.1109/TPDS.2019.2917900"",""National Natural Science Foundation of China(grant numbers:61672152)"; Humanity and Social Science Youth Fund of Ministry of Education of China(grant numbers:18YJCZH170); Youth Innovation Fund of NJFU(grant numbers:CX2016031); Jiangsu Overseas Visiting Scholar Program for University Prominent Young & Middle-aged Teachers and Presidents;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718404"",""Cloud application";criticality measure;fault tolerance;reliability;sensitivity analysis;"system architecture"",""Cloud computing";Fault tolerance;Fault tolerant systems;Quality of service;Cloud computing;"Software reliability"","""",""26"","""",""47"",""IEEE"",""30 Oct 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Asymptotically Optimal Size-Interval Task Assignments,""J. Anselmi";" J. Doncel"",""INRIA Bordeaux Sud Ouest, Team: CQFD, Talence, France";" UPV/EHU, Barrio sarriena s/n, University of the Basque Country, Leioa, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2422"",""2433"",""Size-based routing provides robust strategies to improve the performance of computer and communication systems with highly variable workloads because it is able to isolate small jobs from large ones in a static manner. The basic idea is that each server is assigned all jobs whose sizes belong to a distinct and continuous interval. In the literature, dispatching rules of this type are referred to as SITA (Size Interval Task Assignment) policies. Though their evident benefits, the problem of finding a SITA policy that minimizes the overall mean (steady-state) waiting time is known to be intractable. In particular it is not clear when it is preferable to balance or unbalance server loads and, in the latter case, how. In this paper, we provide an answer to these questions in the celebrated limiting regime where the system capacity grows linearly with the system demand to infinity. Within this framework, we prove that the minimum mean waiting time achievable by a SITA policy necessarily converges to the mean waiting time achieved by SITA-E, the SITA policy that equalizes server loads, provided that servers are homogeneous. However, within the set of SITA policies we also show that SITA-E can perform arbitrarily bad if servers are heterogeneous. In this case we prove that there exist exactly C! asymptotically optimal policies, where C denotes the number of server types, and all of them are linked to the solution of a single strictly convex optimization problem. It turns out that the mean waiting time achieved by any of such asymptotically optimal policies does not depend on how job-size intervals are mapped to servers. Our theoretical results are validated by numerical simulations with respect to realistic parameters and suggest that the above insights are also accurate in small systems composed of a few servers, i.e., ten."",""1558-2183"","""",""10.1109/TPDS.2019.2920121"",""Marie Sklodowska-Curie(grant numbers:777778)"; Basque Government, Spain, Consolidated Research Group(grant numbers:IT649-13); Spanish Ministry of Economy and Competitiveness project(grant numbers:MTM2016-76329-R);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8726167"",""Dispatching policies";size-based routing;performance;"asymptotic optimality"",""Servers";Routing;Dispatching;Task analysis;Heuristic algorithms;Process control;"Random variables"","""",""8"","""",""31"",""IEEE"",""30 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Auction Mechanisms in Cloud/Fog Computing Resource Allocation for Public Blockchain Networks,""Y. Jiao"; P. Wang; D. Niyato;" K. Suankaewmanee"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore"; Lassonde School of Engineering, York University, Toronto, ON, Canada; School of Computer Science and Engineering, Nanyang Technological University, Singapore;" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""1975"",""1989"",""As an emerging decentralized secure data management platform, blockchain has gained much popularity recently. To maintain a canonical state of blockchain data record, proof-of-work based consensus protocols provide the nodes, referred to as miners, in the network with incentives for confirming new block of transactions through a process of “block mining” by solving a cryptographic puzzle. Under the circumstance of limited local computing resources, e.g., mobile devices, it is natural for rational miners, i.e., consensus nodes, to offload computational tasks for proof of work to the cloud/fog computing servers. Therefore, we focus on the trading between the cloud/fog computing service provider and miners, and propose an auction-based market model for efficient computing resource allocation. In particular, we consider a proof-of-work based blockchain network, which is constrained by the computing resource and deployed as an infrastructure for decentralized data management applications. Due to the competition among miners in the blockchain network, the allocative externalities are particularly taken into account when designing the auction mechanisms. Specifically, we consider two bidding schemes: the constant-demand scheme where each miner bids for a fixed quantity of resources, and the multi-demand scheme where the miners can submit their preferable demands and bids. For the constant-demand bidding scheme, we propose an auction mechanism that achieves optimal social welfare. In the multi-demand bidding scheme, the social welfare maximization problem is NP-hard. Therefore, we design an approximate algorithm which guarantees the truthfulness, individual rationality and computational efficiency. Through extensive simulations, we show that our proposed auction mechanisms with the two bidding schemes can efficiently maximize the social welfare of the blockchain network and provide effective strategies for the cloud/fog computing service provider."",""1558-2183"","""",""10.1109/TPDS.2019.2900238"",""WASP/NTU M4082187(grant numbers:4080)"; Singapore MOE Tier 1(grant numbers:2017-T1-002-007 RG122/17); MOE Tier 2(grant numbers:MOE2014-T2-2-015 ARC4/15); Singapore(grant numbers:NRF2015-NRF-ISF001-2277); Singapore EMA Energy Resilience(grant numbers:NRF2017EWT-EP003-041);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666727"",""Blockchain";auction;cloud/fog computing;social welfare;pricing;proof of work;"game theory"",""Blockchain";Resource management;Protocols;Cryptography;Task analysis;Peer-to-peer computing;"Approximation algorithms"","""",""164"","""",""43"",""IEEE"",""13 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Aurora: Seamless Optimization of OpenMP Applications,""A. F. Lorenzon"; C. C. de Oliveira; J. D. Souza;" A. C. S. Beck"",""Federal University of Pampa, Alegrete"; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil; Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil;" Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brazil"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1007"",""1021"",""Efficiently exploiting thread-level parallelism has been challenging for software developers. As many parallel applications do not scale with the number of cores, the task of rightly choosing the ideal amount of threads to produce the best results in performance or energy is not straightforward. Moreover, many variables may change according to the system at hand (e.g., application, input set, microarchitecture, number of cores) and even during execution. Existing solutions lack transparency (demand changes in the original code) or adaptability (do not automatically adjust to applications at run-time). In this scenario, we propose Aurora, an OpenMP framework that is completely transparent to both the designer and end-user. Without any code transformation or recompilation, it is capable of automatically finding, at run-time and with minimum overhead, the optimal number of threads for each parallel loop region and re-adapt in cases the behavior of a region changes during execution. When executing fifteen well-known benchmarks on four multi-core processors, Aurora improves the Energy-Delay Product by up to 98, 86 and 91 percent over the standard OpenMP execution, the OpenMP feature that dynamically adjusts the number of threads, and the Feedback-Driven Threading, respectively."",""1558-2183"","""",""10.1109/TPDS.2018.2872992"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8477128"",""Thread-level parallelism exploitation";OpenMP;optimization;"runtime environments"",""Magnetosphere";Ion radiation effects;Instruction sets;Message systems;Hardware;Runtime;"Microarchitecture"","""",""23"","""",""47"",""IEEE"",""30 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"AutoBoT: Resilient and Cost-Effective Scheduling of a Bag of Tasks on Spot VMs,""P. Varshney";" Y. Simmhan"",""Advanced Technology Group, NetApp Inc, Bangalore, India";" Department of Computational and Data Sciences, Indian Institute of Science (IISc), Bangalore, India"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1512"",""1527"",""Many data and task parallel applications can be modeled as a Bag of Tasks (BoT), and scheduled on distributed systems such as Grids, Clusters, and Clouds. We propose AutoBoT, a collection of scheduling strategies for BoTs with hard deadlines on Cloud Virtual Machines (VMs), to lower the overall monetary cost - a distinctive factor for Clouds. Besides reliable fixed-price VMs, AutoBoT uniquely reduces costs by including preemptible spot-priced VMs that are much cheaper, but are unreliable and have time-variant pricing. It guarantees timely completion by making active runtime decisions on pricing, number of VMs to acquire/release, and on task placement, checkpointing and migration. Our rigorous simulations of 7 Million BoT runs sampled from the Google cluster workload uses a realistic Cloud model and 6 months of Amazon EC2 pricing data to compare AutoBoT against two baseline algorithms. We analyze the impact of BoT size, data centers, time periods, deadline duration, loss budget and checkpointing strategies. AutoBoT often gives X80% profit and rare but bounded losses, compared to using only fixed-price VMs. Further, its 100 percent completion guarantee is 23-42 percent better than using only spot-priced VMs which offer a similar profit."",""1558-2183"","""",""10.1109/TPDS.2018.2889851"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8590769"",""Scheduling";cloud computing;bag of tasks;heuristics;checkpointing;reliability;"spot pricing"",""Task analysis";Bot (Internet);Cloud computing;Google;Reliability;Pricing;"Checkpointing"","""",""21"","""",""50"",""IEEE"",""27 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Bandwidth and Energy Efficient Image Sharing for Situation Awareness in Disasters,""P. Zuo"; Y. Hua; Y. Sun; X. Liu; J. Wu; Y. Guo; W. Xia; S. Cao;" D. Feng"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; School of Computer Science, McGill University, Montreal, Quebec, Canada; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China;" Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""15"",""28"",""In order to save human lives and reduce injury and property loss, Situation Awareness (SA) information is essential and important for rescue workers to perform the effective and timely disaster relief. The information is generally derived from the shared images via widely used smartphones. However, conventional smartphone-based image sharing schemes fail to efficiently meet the needs of SA applications due to two main reasons, i.e., real-time transmission requirement and application-level image redundancy, which is exacerbated by limited bandwidth and energy availability. In order to provide efficient image sharing in disasters, we propose a bandwidth- and energy- efficient image sharing system, called BEES. The salient feature behind BEES is to propose the concept of Approximate Image Sharing (AIS), which explores and exploits approximate feature extraction, redundancy detection, and image uploading to trade the slightly low quality of computation results in content-based redundancy elimination for higher bandwidth and energy efficiency. Nevertheless, the boundaries of the tradeoffs between the quality of computation results and efficiency are generally subjective and qualitative. We hence propose the energy-aware adaptive schemes in AIS to leverage the physical energy availability to objectively and quantitatively determine the tradeoffs between the quality of computation results and efficiency. Moreover, unlike existing work only for cross-batch similar images, BEES further eliminates in-batch ones via a similarity-aware submodular maximization model. The response time of querying similar images is reduced via leveraging a geographic coordinate based index partitioning scheme. We have implemented the BEES prototype which is evaluated via three real-world image datasets. Extensive experimental results demonstrate the efficacy and efficiency of BEES. We have released the source codes of BEES at GitHub."",""1558-2183"","""",""10.1109/TPDS.2018.2859930"",""National Key Research and Development Program of China(grant numbers:2016YFB1000202)"; National Natural Science Foundation of China(grant numbers:61772212,61502190);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419321"",""Image sharing system";content-based redundancy elimination;situation awareness;"disaster environments"",""Feature extraction";Bandwidth;Smart phones;Redundancy;Indexes;Real-time systems;"Batteries"","""",""9"","""",""57"",""IEEE"",""25 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Batch Allocation for Tasks with Overlapping Skill Requirements in Crowdsourcing,""J. Jiang"; B. An; Y. Jiang; P. Shi; Z. Bu;" J. Cao"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore"; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; Co-innovation Center of Shandong Universities for Future Intelligent Computing, Shandong Technology and Business University, Yantai, China; Co-innovation Center of Shandong Universities for Future Intelligent Computing, Shandong Technology and Business University, Yantai, China; College of Information Engineering, Nanjing University of Finance and Economics, Nanjing, China;" College of Information Engineering, Nanjing University of Finance and Economics, Nanjing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1722"",""1737"",""Existing studies on crowdsourcing often adopt the retail-style allocation approach, in which tasks are allocated individually and independently. However, such retail-style task allocation has the following problems: 1) each task is executed independently from scratch, thus the execution of one task seldom utilize the results of other tasks and the requester must pay in full for the task"; 2) many workers only undertake a very small number of tasks contemporaneously, thus the workers' skills and time may not be fully utilized. We observe that many complex tasks in real-world crowdsourcing platforms have similar skill requirements and long deadlines. Based on these real-world observations, this paper presents a novel batch allocation approach for tasks with overlapping skill requirements. Requesters' real payment can be discounted because the real execution cost of tasks can be reduced due to batch allocation and execution, and each worker's real earnings may increase because he/she can undertake more tasks contemporaneously. This batch allocation optimization problem is proved to be NP-hard. Then, two types of heuristic approaches are designed: layered batch allocation and core-based batch allocation. The former approach mainly utilizes the hierarchy pattern to form all possible batches, which can achieve better performance but may require higher computational cost since all possible batches are formed and observed;" the latter approach selects core tasks to form batches, which can achieve suboptimal performance with lower complexity and significantly reduce computational cost. With the theoretical analyses and experiments on a real-world Upwork dataset in which the proposed approaches are compared with the previous benchmark retail-style allocation approach, we find that our approaches have better performances in terms of total payment by requesters and average income of workers, as well as maintaining close successful task completion probability and consuming less task allocation time."",""1558-2183"","""",""10.1109/TPDS.2019.2894146"",""National Natural Science Foundation of China(grant numbers:61472079,61806053,61807008)"; Natural Science Foundation of Jiangsu Province(grant numbers:BK20171363,BK20180356,BK20180369);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8620370"",""Task allocation";crowdsourcing;batch formation;discounting;"skill overlapping"",""Task analysis";Resource management;Crowdsourcing;Computational efficiency;Social network services;"Benchmark testing"","""",""12"","""",""30"",""IEEE"",""20 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"BiloKey : A Scalable Bi-Index Locality-Aware In-Memory Key-Value Store,""W. Ma"; Y. Zhu; C. Li; M. Guo;" Y. Bao"",""State Key Laboratory of Computer Architecture, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China"; State Key Laboratory of Computer Architecture, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; State Key Laboratory of Computer Architecture, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China;" State Key Laboratory of Computer Architecture, Institute of Computing Technology Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1528"",""1540"",""Fast in-memory key value stores are the keys to building large-scale Internet services. The state-of-the-art solutions mainly focus on optimizing the performance for read-intensive workloads. Nevertheless, a wide range of applications demonstrate a significant amount of updates and range queries, which scale poorly with the current implementations. In this paper, we present BiloKey, a highly scalable in-memory key value store on multi-core machines, significantly outperforming Redis and Memcached for a variety of mixed read and write workloads. To achieve this, BiloKey leverages a fast bi-index comprised by a Hash Table index and a SkipList index, where the former supports feature rich operations including GET, UPDATE and DELETE with O(1) complexity, while the latter supports SCAN with O(log N) complexity. Furthermore, to make the bi-index design scale well, BiloKey adopts three techniques: lazy synchronization for reducing the overhead of maintaining index consistency, lock-free data structure for supporting multi-writers, and locality-aware data parallel processing for preserving the data locality of requests. Compared with two popular in-memory KV stores (i.e., Redis and Memcached), experimental results show that: (1) for write-intensive workloads, BiloKey outperforms Redis and Memcached by 7.8x and 3.7x on average (up to 11.5x and 4.8x), respectively"; (2) for scan-intensive workloads, BiloKey achieves an average speedup of 2.3x against Redis;" (3) for read-intensive workloads, BiloKey also outperforms Redis and Memcached by 1.2x and 1.8x on average."",""1558-2183"","""",""10.1109/TPDS.2019.2891599"",""State Key Development Program for Basic Research of China(grant numbers:2014CB340402)"; National Key R&D Program of China(grant numbers:2016YFB1000201); National Natural Science Foundation of China(grant numbers:61303054,61420106013,61432006);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8606250"",""In-memory key-value store";multi-core scalability;locality-aware networking;hybrid index;"lock-free access"",""Data structures";Concurrency control;Scalability;Indexing;Instruction sets;"Complexity theory"","""",""4"","""",""44"",""IEEE"",""9 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"BLOT: Bandit Learning-Based Offloading of Tasks in Fog-Enabled Networks,""Z. Zhu"; T. Liu; Y. Yang;" X. Luo"",""School of Information Science and Technology, ShanghaiTech University, Shanghai, China"; School of Information Science and Technology, ShanghaiTech University, Shanghai, China; School of Information Science and Technology, ShanghaiTech University, Shanghai, China;" School of Information Science and Technology, ShanghaiTech University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2636"",""2649"",""Task offloading is a promising technology to exploit the available computational resources in spatially distributed fog nodes efficiently in the era of fog computing. In this paper, we look for an online task offloading strategy to minimize the long-term cost, which factors in the latency, the energy consumption, and the switching cost. To this end, we formulate a stochastic programming problem and the expectations of the system parameters are allowed to change abruptly at unknown time instants. Meanwhile, we consider the fact that the queried nodes can only feed back the processing results after finishing the tasks. Then we put forth an effective bandit learning algorithm, i.e., the BLOT, to solve this challenging stochastic programming under the non-stationary bandit model. We also demonstrate that our proposed BLOT algorithm is asymptotically optimal in a non-stationary fog-enabled network. Numerical experiments further verify the superb performance of BLOT."",""1558-2183"","""",""10.1109/TPDS.2019.2927978"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8762108"",""Online learning";fog computing;task offloading;stochastic programming;"multi-armed bandit (MAB)"",""Task analysis";Delays;Energy consumption;Edge computing;Stochastic processes;Programming;"Computational modeling"","""",""38"","""",""32"",""IEEE"",""15 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Budget and Deadline Aware e-Science Workflow Scheduling in Clouds,""V. Arabnejad"; K. Bubendorfer;" B. Ng"",""School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand"; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand;" School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""29"",""44"",""Basic science is becoming ever more computationally intensive, increasing the need for large-scale compute and storage resources, be they within a High Performance Computer cluster, or more recently within the cloud. In most cases, large scale scientific computation is represented as a workflow for scheduling and runtime provisioning. Such scheduling become an even more challenging problem on cloud systems due to the dynamic nature of the cloud, in particular, the elasticity, the pricing models (both static and dynamic), the non-homogeneous resource types, the vast array of services, and virtualization. This mapping of workflow tasks on to a set of provisioned instances is an example of the general scheduling problem and is NP-complete. In addition, we also need to ensure that certain runtime constraints are met - the most typical being the cost of the computation and the time which that computation requires to complete. In this article, we introduce a new heuristic scheduling algorithm, Budget Deadline Aware Scheduling (BDAS), that addresses eScience workflow scheduling under budget and deadline constraints in Infrastructure as a Service (IaaS) clouds. The novelty of our work is satisfying both budget and deadline constraints while introducing a tunable cost-time trade off over heterogeneous instances. In addition, we study the stability and robustness of our algorithm by performing sensitivity analysis. The results demonstrate that overall BDAS finds a viable schedule for more than 40000 test cases accomplishing both defined constraints: budget and deadline. Moreover, our algorithm achieves a 17.0-23.8 percent higher success rate when compared to state of the art algorithms."",""1558-2183"","""",""10.1109/TPDS.2018.2849396"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8400402"",""Scientific workflow";scheduling;budget;deadline;"cloud"",""Task analysis";Cloud computing;Dynamic scheduling;Scheduling algorithms;"Schedules"","""",""99"","""",""42"",""IEEE"",""29 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Chameleon: A Hybrid, Proactive Auto-Scaling Mechanism on a Level-Playing Field,""A. Bauer"; N. Herbst; S. Spinner; A. Ali-Eldin;" S. Kounev"",""University of Würzburg, Würzburg, Germany"; University of Würzburg, Würzburg, Germany; University of Würzburg, Würzburg, Germany; UMass, Amherst, MA, USA;" University of Würzburg, Würzburg, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""800"",""813"",""Auto-scalers for clouds promise stable service quality at low costs when facing changing workload intensity. The major public cloud providers provide trigger-based auto-scalers based on thresholds. However, trigger-based auto-scaling has reaction times in the order of minutes. Novel auto-scalers from literature try to overcome the limitations of reactive mechanisms by employing proactive prediction methods. However, the adoption of proactive auto-scalers in production is still very low due to the high risk of relying on a single proactive method. This paper tackles the challenge of reducing this risk by proposing a new hybrid auto-scaling mechanism, called Chameleon, combining multiple different proactive methods coupled with a reactive fallback mechanism. Chameleon employs on-demand, automated time series-based forecasting methods to predict the arriving load intensity in combination with run-time service demand estimation to calculate the required resource consumption per work unit without the need for application instrumentation. We benchmark Chameleon against five different state-of-the-art proactive and reactive auto-scalers one in three different private and public cloud environments. We generate five different representative workloads each taken from different real-world system traces. Overall, Chameleon achieves the best scaling behavior based on user and elasticity performance metrics, analyzing the results from 400 hours aggregated experiment time."",""1558-2183"","""",""10.1109/TPDS.2018.2870389"",""Deutsche Forschungsgemeinschaft(grant numbers:3445/11-1)"; Vetenskapsrådet; Standard Performance Evaluation Corporation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8465991"",""Auto-scaling";elasticity;workload forecasting;service demand estimation;IaaS cloud;benchmarking;"metrics"",""Estimation";Measurement;Cloud computing;Time series analysis;Elasticity;Predictive models;"Benchmark testing"","""",""36"","""",""38"",""IEEE"",""14 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Clip: A Disk I/O Focused Parallel Out-of-Core Graph Processing System,""Z. Ai"; M. Zhang; Y. Wu; X. Qian; K. Chen;" W. Zheng"",""Graduate School at Shenzhen, Tsinghua University, Shenzhen, China"; Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; University of Southern California, Los Angeles, CA, USA; Graduate School at Shenzhen, Tsinghua University, Shenzhen, China;" Graduate School at Shenzhen, Tsinghua University, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""45"",""62"",""Existing parallel out-of-core graph processing systems focus on improving disk I/O locality, which leads to restrictions on their programming models. Although improving the locality, these constraints also restrict the expressiveness and hence only sub-optimal algorithms are supported. These sub-optimal algorithms typically incur sequential, but much larger, amount of disk I/O. In this paper, we explore a fundamentally different tradeoff: less total amount of I/O rather than better locality. We show that out-of-core graph processing systems uniquely provide the opportunities to lift the restrictions of the programming model in a feasible manner. To demonstrate the ideas, we build Clip, which enables more efficient algorithms that require much less amount of total disk I/O. Our experiments show that the algorithms that can be only implemented in Clip are much faster than the original disk-locality-optimized algorithms. We also further extend our technique's scope of application by providing a semi-external mode. Our analysis and evaluation demonstrate that semi-external is not only feasible for many cases, but also be able to deliver a significant speedup for important graph applications. Moreover, we further improve the performance of originally supported applications by designing more optimizations and evaluate our system on NVMe SSD."",""1558-2183"","""",""10.1109/TPDS.2018.2858250"",""National Key Research & Development Program of China(grant numbers:2016YFB1000504)"; Natural Science Foundation of China(grant numbers:61433008,61373145,61572280,61133004,61502019,U1435216); National Basic Research (973) Program of China(grant numbers:2014CB340402); Intel Labs China(grant numbers:20160520); National Science Foundation(grant numbers:CRII-1657333,SHF-1717754,CSR-1717984); Spanish Gov. & European ERDF(grant numbers:TIN2010-21291-C02-01); Consolider(grant numbers:CSD2007-00050);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416754"",""Graph processing";parallelization;big data;"disk I/O"",""Input-output programs";Graph theory;Parallel processing;"Optimization"","""",""3"","""",""48"",""IEEE"",""20 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Cloud Scheduling with Discrete Charging Units,""M. M. Tan"; R. Ren;" X. Tang"",""School of Computer Science and Engineering, Nanyang Technological University, Singapore"; School of Computer Science and Engineering, Nanyang Technological University, Singapore;" School of Computer Science and Engineering, Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1541"",""1551"",""We consider a scheduling problem for running jobs on machines rented from the cloud. Cloud service providers such as Amazon EC2 and Google Cloud offer machines to rent on demand, and charge the rental usage by a specific interval of time, say at an hourly rate. This pricing model creates an interesting optimization problem called Interval Scheduling with Discrete Charging Units (ISDCU) which assigns jobs to run on the machines with the objective of minimizing the rental cost. In this paper, we study the problem of ISDCU where each machine can process a maximum of g jobs simultaneously. We focus on interval jobs where each job must be assigned to a machine upon its arrival and run for a required processing length. We show that ISDCU is NP-hard even for the case of g = 1. We also show that no deterministic online algorithm can achieve a competitive ratio better than max{2, g} in the non-clairvoyant setting, and better than max{3/2, g} in the clairvoyant setting. Lastly, we develop and analyze several online algorithms, most of which achieve a competitive ratio of O(g)."",""1558-2183"","""",""10.1109/TPDS.2018.2889712"",""Singapore Ministry of Education Academic Research Fund Tier 1(grant numbers:2018-T1-002-063)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588304"",""Cloud scheduling";interval scheduling;"online algorithm"",""Schedules";Calibration;Color;Greedy algorithms;Minimization;Scheduling;"Parallel processing"","""",""4"","""",""17"",""IEEE"",""25 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Collaborative Optimization of Service Composition for Data-Intensive Applications in a Hybrid Cloud,""H. Ma"; H. Zhu; K. Li;" W. Tang"",""College of Information Science and Engineering, Hunan Normal University, Changsha, China"; Collaborative Systems Laboratory, Nipissing University, North Bay, ON, Canada; Department of Computer Science, State University of New York, New Paltz, NY, USA;" College of Information Science and Engineering, Hunan Normal University, Changsha, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1022"",""1035"",""The multi-valued evaluations of quality of service (QoS), the complicated constraints between cloud services (CSs) and the collaborative resource assignments add many difficulties to the problem of CS composition for data-intensive applications (DiA) in a hybrid cloud (CSCD-HC). Solving the CSCD-HC problem has become a challenging task due to the uncertain QoS, the diverse hardware configurations and the flexible pricing about CSs. This paper proposes a collaborative optimization approach for CSCD-HC. This approach models a DiA as a role-based collaboration (RBC) system and employs the environments-classes, agents, roles, groups, and objects (E-CARGO) model to formalize the CSCD-HC problem with complicated constraints. To deal with the multi-valued QoS evaluations, this paper exploits the cloud model theory to analyze the performance of CSs, and presents a new method utilizing the Mahalanobis distance to improve the similarity calculation of QoS cloud models. Based on it, the qualification of candidate CSs can be precisely measured for supporting CS composition. A solution via the IBM ILOG CPLEX optimization package is put forward to solve the CSCD-HC problem. The experimental results demonstrate that the proposed approach is effective and feasible for optimizing CSCD-HC."",""1558-2183"","""",""10.1109/TPDS.2018.2879603"",""Natural Sciences and Engineering Research Council of Canada(grant numbers:RGPIN2018-04818)"; Hunan Provincial Natural Science Foundation of China(grant numbers:2017JJ2186); Ministry of Education; Humanities and Social Science Research Youth Fund(grant numbers:18YJCZH124);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8523635"",""Collaboration optimization";data-intensive;hybrid cloud;multi-valued QoS evaluations;"service composition"",""Cascading style sheets";Quality of service;Cloud computing;Collaboration;Optimization;Task analysis;"Qualifications"","""",""19"","""",""58"",""IEEE"",""4 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Comparative Analysis of Intra-Algorithm Parallel Multiobjective Evolutionary Algorithms: Taxonomy Implications on Bioinformatics Scenarios,""S. Santander-Jiménez";" M. A. Vega-Rodríguez"",""INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisboa, Portugal";" Department of Computer and Communications Technologies, University of Extremadura, Caceres, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""63"",""78"",""Parallelism has become a recurrent tool to support computational intelligence and, particularly, evolutionary algorithms in the solution of very complex optimization problems, especially in the multiobjective case. However, the selection of parallel evolutionary designs often represents a difficult question due to the multiple variables that must be considered to attain an accurate exploitation of hardware resources, along with their influence in solution quality. This work looks into this issue by conducting a comparative performance analysis of intra-algorithm parallel multiobjective evolutionary algorithms running on shared-memory configurations. We consider different design trends including A) generational approaches based on measurements of solution quality plus diversity, B) generational approaches based on measurements of solution quality exclusively, and C) non-generational approaches. Following these trends, a total of six representative algorithms are applied to tackle a challenging bioinformatics problem as a case study, phylogenetic reconstruction. Experimentation on real-world scenarios point out the main advantages and weaknesses of each design, outlining guidelines for the selection of methods according to the characteristics of the employed hardware, evolutionary properties, and the parallelism exploitation capabilities of the evaluated approaches."",""1558-2183"","""",""10.1109/TPDS.2018.2854788"",""Albert Ellis Institute"; ERDF (European Regional Development Fund, EU)(grant numbers:TIN2016-76259-P); Post-Doctoral Fellowship from FCT (Fundação para a Ciência e a Tecnologia, Portugal)(grant numbers:SFRH/BPD/119220/2016);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8409336"",""Comparative analysis";taxonomy;parallelism;multiobjective optimization;evolutionary computation;"NP-hard problems"",""Evolutionary computation";Parallel processing;Genetic algorithms;Optimization;Hardware;Linear programming;"Sociology"","""",""4"","""",""43"",""IEEE"",""10 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"CongraPlus: Towards Efficient Processing of Concurrent Graph Queries on NUMA Machines,""P. Pan"; C. Li;" M. Guo"",""Department of Electrical and Computer Engineering, Cornell University, Ithaca, NY, USA"; Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, Minhang, China;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, Minhang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""1990"",""2002"",""Graph analytics has been routinely used to solve problems in a wide range of real-life applications. Efficiently processing concurrent graph analytics queries in a multiuser environment is highly desirable as we enter a world of edge device oriented services. Existing research, however, primarily focuses on analyzing a single, large graph dataset and leaves the efficient processing of multiple mid-sized graph analytics queries an intriguing yet challenging open problem. In this work, we investigate the scheduling of concurrent graph analytics queries on NUMA machines. We analyze the performance of several graph analytics algorithms and observe that they have diminishing performance returns as the number of processor cores increases. With concurrent graph analytics, such diminishing returns translate to no or even negative performance gains because of increasing contention on shared hardware resources. We also demonstrate the unpredictability of memory bandwidth usage for numerous graph analytics algorithms, which can lead to sub-optimal performance due to its potential to cause severe memory bandwidth contention. Motivated by the above observations, we propose CongraPlus, a NUMA-aware scheduler that intelligently manages concurrent graph analytics queries for better system throughput and memory bandwidth efficiency. CongraPlus collects the memory bandwidth consumption characteristics of graph analytics queries via offline profiling and eliminates memory bandwidth contention by computing the optimal sequence to launch queries. It also avoids computation resource contention by assigning a certain number of processor cores to the individual queries. We implement CongraPlus in C++ on top of the Ligra graph processing framework and test it with judiciously selected graph processing query combinations. Our results reveal that CongraPlus-based schemes improve query throughput by 30 percent compared to the conventional approach. It also exhibits a much better quality of service and scalability."",""1558-2183"","""",""10.1109/TPDS.2019.2899595"",""National Basic Research Program of China (973 Program)(grant numbers:2018YFB1003503)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642367"",""Concurrent graph analytics";query scheduling;memory bandwidth;efficiency;"throughput"",""Bandwidth";Instruction sets;Servers;Processor scheduling;Memory management;Scheduling;"Prediction algorithms"","""",""3"","""",""30"",""IEEE"",""14 Feb 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Constant-Time Sliding Window Framework with Reduced Memory Footprint and Efficient Bulk Evictions,""Á. Villalba"; J. L. Berral;" D. Carrera"",""Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya - BarcelonaTech (UPC), Barcelona, Spain"; Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya - BarcelonaTech (UPC), Barcelona, Spain;" Barcelona Supercomputing Center (BSC), Universitat Politècnica de Catalunya - BarcelonaTech (UPC), Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""486"",""500"",""The fast evolution of data analytics platforms has resulted in an increasing demand for real-time data stream processing. From Internet of Things applications to the monitoring of telemetry generated in large data centers, a common demand for currently emerging scenarios is the need to process vast amounts of data with low latencies, generally performing the analysis process as close to the data source as possible. Stream processing platforms are required to be malleable and absorb spikes generated by fluctuations of data generation rates. Data is usually produced as time series that have to be aggregated using multiple operators, being sliding windows one of the most common abstractions used to process data in real-time. To satisfy the above-mentioned demands, efficient stream processing techniques that aggregate data with minimal computational cost need to be developed. In this paper we present the Monoid Tree Aggregator general sliding window aggregation framework, which seamlessly combines the following features: amortized $O(1)$ time complexity and a worst-case of $O(\log {n})$ between insertions"; it provides both a window aggregation mechanism and a window slide policy that are user programmable; the enforcement of the window sliding policy exhibits amortized $O(1)$ computational cost for single evictions and supports bulk evictions with cost $O(\log {n})$;" and it requires a local memory space of $O(\log {n})$. The framework can compute aggregations over multiple data dimensions, and has been designed to support decoupling computation and data storage through the use of distributed Key-Value Stores to keep window elements and partial aggregations."",""1558-2183"","""",""10.1109/TPDS.2018.2868960"",""H2020 European Research Council"; European Unions Horizon 2020(grant numbers:639595); Ministry of Economy of Spain(grant numbers:TIN2015-65316-P); Generalitat de Catalunya(grant numbers:2014SGR1051); BSC-CNS Severo Ochoa(grant numbers:SEV-2015-0493);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8456588"",""Internet-of-things";data analytics;big data;stream processing;"real-time"",""Microsoft Windows";Data structures;Data analysis;Computational efficiency;Time complexity;"Real-time systems"","""",""14"","""",""26"",""OAPA"",""6 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Continuously Distinct Sampling over Centralized and Distributed High Speed Data Streams,""P. Wang"; X. Wang; J. Tao; P. Zhang;" X. Guan"",""Xi'an Jiaotong University, Shaanxi, China"; Xi'an Jiaotong University, Shaanxi, China; Shenzhen Research School, Xi'an Jiaotong University, Shenzhen, China; Xi'an Jiaotong University, Shaanxi, China;" Tsinghua National Lab for Information Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""300"",""314"",""Distinct sampling is fundamental for computing statistics (e.g., the age and gender distribution of distinct users accessing a particular website) depending on the set of distinct keys (e.g., user IDs) in a large and high speed data stream such as a sequence of key-update pairs. However, the major shortcoming of existing methods is their high computational cost incurred by determining whether each incoming key in the data stream is currently in the set of sampled keys and keeping track of sampled keys’ update aggregations. To solve this challenge, we develop a new method random projection and eviction (RPE) that uses a list of buckets to continuously sample distinct keys and their update aggregations. RPE processes each key-update pair with small and nearly constant time complexity $O(1)$. Besides centralized data streams, we also develop a novel method DRPE to deal with distributed data streams consisting of key-update pairs observed at multiple distributed sites. We conduct extensive experiments on real-world datasets, and the results demonstrate that RPE and DRPE reduce the memory, computational, and message costs of state-of-the-art methods by several times."",""1558-2183"","""",""10.1109/TPDS.2018.2865452"",""National Key R&D Program of China(grant numbers:2018YFC0830500)"; National Natural Science Foundation of China(grant numbers:U1301254,61603290,61602371,61772412); Ministry of Education&China Mobile Research Fund(grant numbers:MCM20160311); Natural Science Foundation of Jiangsu Province(grant numbers:SBK2014021758); 111 International Collaboration Program of China; Prospective Joint Research of Industry-Academia-Research Joint Innovation Funding of Jiangsu Province(grant numbers:BY2014074); Shenzhen Basic Research(grant numbers:JCYJ20160229195940462,JCYJ20170816100819428); China Postdoctoral Science Foundation(grant numbers:2015M582663); Natural Science Basic Research Plan in Shaanxi Province of China(grant numbers:2016JQ6034); K. C. Wong Education Foundation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8436443"",""Data stream";distinct sampling;"sketch"",""Distributed databases";IP networks;Random access memory;Monitoring;Measurement;Sampling methods;"Data models"","""",""2"","""",""73"",""IEEE"",""14 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Coordinated DMA: Improving the DRAM Access Efficiency for Matrix Multiplication,""S. Ma"; Z. Liu; S. Chen; L. Huang; Y. Guo; Z. Wang;" M. Zhang"",""State Key Laboratory of High Performance Computing, National University of Defense Technology, Changsha, Hunan, China"; College of Computer, National University of Defense Technology, Changsha, Hunan, China; College of Computer, National University of Defense Technology, Changsha, Hunan, China; College of Computer, National University of Defense Technology, Changsha, Hunan, China; College of Computer, National University of Defense Technology, Changsha, Hunan, China; College of Computer, National University of Defense Technology, Changsha, Hunan, China;" College of Computer, National University of Defense Technology, Changsha, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2148"",""2164"",""High performance implementation of matrix multiplication is essential for scientific computing. The memory access procedure is quite possible to be the bottleneck of matrix multiplication. The widely used GotoBLAS GEMM implementation divides the integral matrix into several partitions to be assigned to different cores for parallelization. Traditionally, each core deploys a DMA transfer to access its own partition in the DRAM memory. However, deploying an independent DMA transfer for each core cannot efficiently exploit the inter-core locality. Also, multiple concurrent DMA transfers interfere with each other, further reducing the DRAM access efficiency. We observe that the same row of neighboring partitions is in the same DRAM page, which means that there is significant locality inherent in the address layout. We propose the coordinated DMA to efficiently exploit the locality. It invokes one transfer to serve all cores and moves data in a row-major manner to improve the DRAM access efficiency. Compared with a baseline design, the coordinated DMA improves the bandwidth by 84.8 percent and reduces DRAM energy consumption by 43.1 percent for micro-benchmarks. It achieves higher performance for the GEMM and Linpack benchmark. With much less hardware costs, the coordinated DMA significantly outperforms an out-of-order memory controller."",""1558-2183"","""",""10.1109/TPDS.2019.2906891"",""National Natural Science Foundation of China(grant numbers:61672526,61572508,61872374,61572025)"; Research Project of NUDT(grant numbers:ZK17-03-06); Science and Technology Innovation project of Hunan Province(grant numbers:2018RS3083);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8673652"",""Coordinated DMA";DRAM access efficiency;"matrix multiplication"",""Random access memory";Bandwidth;Out of order;Computer architecture;Layout;Graphics processing units;"Hardware"","""",""5"","""",""76"",""IEEE"",""25 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Correlation-Aware Stripe Organization for Efficient Writes in Erasure-Coded Storage: Algorithms and Evaluation,""Z. Shen"; P. P. C. Lee; J. Shu;" W. Guo"",""Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong"; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" College of Mathematics and Computer Science, Fuzhou University, Fuzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2019"",""2019"",""30"",""7"",""1552"",""1564"",""Erasure coding has been extensively employed for data availability protection in production storage systems by maintaining a low degree of data redundancy. However, how to mitigate the parity update overhead of partial stripe writes in erasure-coded storage systems is still a critical concern. In this paper, we study this problem from two new perspectives: data correlation and stripe organization. We propose CASO, a correlation-aware stripe organization algorithm, which captures data correlation of a data access stream and uses the data correlation characteristics for stripe organization. It packs correlated data into a small number of stripes to reduce the incurred I/Os in partial stripe writes, and further organizes uncorrelated data into stripes to leverage the spatial locality in later access. We implement CASO over Reed-Solomon codes and Azure's Local Reconstruction Codes, and show via extensive tracedriven evaluation that CASO reduces up to 29.8 percent of parity updates and reduces the write time by up to 46.7 percent."",""1558-2183"","""",""10.1109/TPDS.2018.2890635"",""Research Grants Council of Hong Kong(grant numbers:GRF 14216316,CRF C7036-15G)"; National Natural Science Foundation of China(grant numbers:61602120,61672159,U1705262,61832011); Fujian Provincial Natural Science Foundation(grant numbers:2017J05102);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598850"",""Correlation";stripe organization;partial stripe writes;"erasure code"",""Encoding";Correlation;Organizations;Distributed databases;Redundancy;"Maintenance engineering"","""",""6"","""",""41"",""IEEE"",""1 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Cost-Effective Cloud Server Provisioning for Predictable Performance of Big Data Analytics,""F. Xu"; H. Zheng; H. Jiang; W. Shao; H. Liu;" Z. Zhou"",""Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, 3663 N. Zhongshan Road, Shanghai, China"; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, 3663 N. Zhongshan Road, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, 3663 N. Zhongshan Road, Shanghai, China; Shanghai Key Laboratory of Multidimensional Information Processing, East China Normal University, 3663 N. Zhongshan Road, Shanghai, China; Services Computing Technology and System Lab, Huazhong University of Science and Technology, 1037 Luoyu Road, Wuhan, China;" Guangdong Key Laboratory of Big Data Analysis and Processing, Sun Yat-sen University, 132 E. Waihuan Road, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1036"",""1051"",""Cloud datacenters are underutilized due to server over-provisioning. To increase datacenter utilization, cloud providers offer users an option to run workloads such as big data analytics on the underutilized resources, in the form of cheap yet revocable transient servers (e.g., EC2 spot instances, GCE preemptible instances). Though at highly reduced prices, deploying big data analytics on the unstable cloud transient servers can severely degrade the job performance due to instance revocations. To tackle this issue, this paper proposes iSpot, a cost-effective transient server provisioning framework for achieving predictable performance in the cloud, by focusing on Spark as a representative Directed Acyclic Graph (DAG)-style big data analytics workload. It first identifies the stable cloud transient servers during the job execution by devising an accurate Long Short-Term Memory (LSTM)-based price prediction method. Leveraging automatic job profiling and the acquired DAG information of stages, we further build an analytical performance model and present a lightweight critical data checkpointing mechanism for Spark, to enable our design of iSpot provisioning strategy for guaranteeing the job performance on stable transient servers. Extensive prototype experiments on both EC2 spot instances and GCE preemptible instances demonstrate that, iSpot is able to guarantee the performance of big data analytics running on cloud transient servers while reducing the job budget by up to 83.8 percent in comparison to the state-of-the-art server provisioning strategies, yet with acceptable runtime overhead."",""1558-2183"","""",""10.1109/TPDS.2018.2873397"",""National Natural Science Foundation of China(grant numbers:61502172)"; China Postdoctoral Science Foundation(grant numbers:2016T90353); Science and Technology Commission of Shanghai Municipality(grant numbers:17511102602,14DZ2260800); NSFC(grant numbers:61672251); NSFC(grant numbers:61802449); Guangdong Natural Science Funds(grant numbers:2018A030313032);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478347"",""Predictable performance";big data analytics;cloud computing;transient server provisioning;"data checkpointing"",""Servers";Transient analysis;Big Data;Cloud computing;Sparks;Checkpointing;"Data models"","""",""35"","""",""56"",""IEEE"",""30 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"CRAFT: A Library for Easier Application-Level Checkpoint/Restart and Automatic Fault Tolerance,""F. Shahzad"; J. Thies; M. Kreutzer; T. Zeiser; G. Hager;" G. Wellein"",""Erlangen Regional Computing Center (RRZE), University of Erlangen-Nuremberg, Martensstrasse 1, Erlangen, Germany"; German Aerospace Center (DLR), Simulation and Software Technology, Linder Höhe, Cologne, Germany; Erlangen Regional Computing Center (RRZE), University of Erlangen-Nuremberg, Martensstrasse 1, Erlangen, Germany; Erlangen Regional Computing Center (RRZE), University of Erlangen-Nuremberg, Martensstrasse 1, Erlangen, Germany; Erlangen Regional Computing Center (RRZE), University of Erlangen-Nuremberg, Martensstrasse 1, Erlangen, Germany;" Erlangen Regional Computing Center (RRZE), University of Erlangen-Nuremberg, Martensstrasse 1, Erlangen, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""501"",""514"",""In order to efficiently use the future generations of supercomputers, fault tolerance and power consumption are two of the prime challenges anticipated by the High Performance Computing (HPC) community. Checkpoint/Restart (CR) has been and still is the most widely used technique to deal with hard failures. Application-level CR is the most effective CR technique in terms of overhead efficiency but it takes a lot of implementation effort. This work presents the implementation of our C++ based library CRAFT (Checkpoint-Restart and Automatic Fault Tolerance), which serves two purposes. First, it provides an extendable library that significantly eases the implementation of application-level checkpointing. The most basic and frequently used checkpoint data-types are already part of CRAFT and can be directly used out of the box. The library can be easily extended to add more data-types. As means of overhead reduction, the library offers a built-in asynchronous checkpointing mechanism and also supports the Scalable Checkpoint/Restart (SCR) library for node level checkpointing. Second, CRAFT provides an easier interface for User-Level Failure Mitigation (ULFM) based dynamic process recovery, which significantly reduces the complexity and effort of failure detection and communication recovery mechanism. By utilizing both functionalities together, applications can write application-level checkpoints and recover dynamically from process failures with very limited programming effort. This work presents the challenges addressed by the library, its design, and its use. The associated overheads are analyzed using benchmarks."",""1558-2183"","""",""10.1109/TPDS.2018.2866794"",""German Research Foundation (DFG)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444763"",""Application-level checkpoint/restart";automatic fault tolerance;"user-level failure mitigation(ULFM)"",""Fault tolerance";Fault tolerant systems;Libraries;Checkpointing;Hardware;"Kernel"","""",""25"","""",""48"",""IEEE"",""23 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"CrowdBC: A Blockchain-Based Decentralized Framework for Crowdsourcing,""M. Li"; J. Weng; A. Yang; W. Lu; Y. Zhang; L. Hou; J. -N. Liu; Y. Xiang;" R. H. Deng"",""College of Cyber Security, Jinan University, Guangzhou, China"; College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; College of Cyber Security, Jinan University, Guangzhou, China; School of Software and Electrical Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia;" School of Information Systems, Singapore Management University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1251"",""1266"",""Crowdsourcing systems which utilize the human intelligence to solve complex tasks have gained considerable interest and adoption in recent years. However, the majority of existing crowdsourcing systems rely on central servers, which are subject to the weaknesses of traditional trust-based model, such as single point of failure. They are also vulnerable to distributed denial of service (DDoS) and Sybil attacks due to malicious users involvement. In addition, high service fees from the crowdsourcing platform may hinder the development of crowdsourcing. How to address these potential issues has both research and substantial value. In this paper, we conceptualize a blockchain-based decentralized framework for crowdsourcing named CrowdBC, in which a requester's task can be solved by a crowd of workers without relying on any third trusted institution, users' privacy can be guaranteed and only low transaction fees are required. In particular, we introduce the architecture of our proposed framework, based on which we give a concrete scheme. We further implement a software prototype on Ethereum public test network with real-world dataset. Experiment results show the feasibility, usability, and scalability of our proposed crowdsourcing system."",""1558-2183"","""",""10.1109/TPDS.2018.2881735"",""National Key R&D Plan of China(grant numbers:2017YFB0802203,2018YFB1003701)"; National Natural Science Foundation of China(grant numbers:61825203,U1736203,61702222,61472165,61732021,61877029,61872153,61802145,U1636209); National Joint Engineering Research Center of Network Security Detection and Protection Technology; Guangdong Provincial Special Funds for Applied Technology Research and Development and Transformation of Important Scientific and Technological Achieve(grant numbers:2016B010124009,2017B010124002); Guangdong Key Laboratory of Data Security and Privacy Preserving(grant numbers:2017B030301004); Guangzhou Key Laboratory of Data Security and Privacy Preserving(grant numbers:201705030004); China Postdoctoral Science Foundation(grant numbers:2017M612842);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8540048"",""Decentralized framework";crowdsourcing;blockchain;"smart contract"",""Crowdsourcing";Task analysis;Computer crime;"Data privacy"","""",""298"","""",""57"",""IEEE"",""18 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"Data Prefetching and Eviction Mechanisms of In-Memory Storage Systems Based on Scheduling for Big Data Processing,""C. -H. Chen"; T. -Y. Hsia; Y. Huang;" S. -Y. Kuo"",""Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan"; Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan; Research Center for Information Technology Innovation, Taipei, Taiwan;" Department of Electrical Engineering, National Taiwan University, Taipei, Taiwan"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1738"",""1752"",""In-memory techniques keep data into faster and more expensive storage media for improving performance of big data processing. However, existing mechanisms do not consider how to expedite the data processing applications that access the input datasets only once. Another problem is how to reclaim memory without affecting other running applications. In this paper, we provide scheduling-aware data prefetching and eviction mechanisms based on Spark, Alluxio, and Hadoop. The mechanisms prefetch data and release memory resources based on the scheduling information. A mathematical method is proposed for maximizing the reduction of data access time. To make the mechanisms applicable in large-scale environments, we propose a heuristic algorithm to reduce the computational time. Furthermore, an enhanced version of the heuristic algorithm is also proposed to increase the amount of prefetched data. Finally, we perform real-testbed and simulation experiments to show the effectiveness of the proposed mechanisms."",""1558-2183"","""",""10.1109/TPDS.2019.2892957"",""Ministry of Science and Technology, Taiwan(grant numbers:MOST 103-2221-E-001-028-MY3,MOST 105-2221-E-002-119-MY3)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611384"",""Big data processing";in-memory systems;scheduling information;data prefetching;"data eviction"",""Prefetching";Data processing;Task analysis;Distributed databases;Data centers;Sparks;"Processor scheduling"","""",""7"","""",""40"",""IEEE"",""13 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Deadline-Aware MapReduce Job Scheduling with Dynamic Resource Availability,""D. Cheng"; X. Zhou; Y. Xu; L. Liu;" C. Jiang"",""Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA"; Department of Computer Science, University of Colorado, Colorado Springs, Boulder, CO, USA; Department of Computer Science & Technology, Tongji University, Shanghai, China; Department of Computer Science & Technology, Tongji University, Shanghai, China;" Department of Computer Science & Technology, Tongji University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""814"",""826"",""As MapReduce is becoming ubiquitous in large-scale data analysis, many recent studies have shown that the performance of MapReduce could be improved by different job scheduling approaches, e.g., Fair Scheduler and Capacity Scheduler. However, most exiting MapReduce job schedulers focus on the scenario that MapReduce cluster is stable and pay little attention to the MapReduce cluster with dynamic resource availability. In fact, MapReduce cluster resources may fluctuate as there is a growing number of Hadoop clusters deployed on hybrid systems, e.g., infrastructure powered by mix of traditional and renewable energy, and cloud platforms hosting heterogeneous workloads. Thus, there is a growing need for providing predictable services to users who have strict requirements on job completion times in such dynamic environments. In this paper, we propose, RDS, a Resource and Deadline-aware Hadoop job Scheduler that takes future resource availability into consideration when minimizing job deadline misses. We formulate the job scheduling problem as an online optimization problem and solve it using an efficient receding horizon control algorithm. To aid the control, we design a self-learning model to estimate job completion times. We further extend the design of RDS scheduler to support flexible performance goals in various dynamic clusters. In particular, we use flexible deadline time bounds instead of the single fixed job completion deadline. We have implemented RDS in the open-source Hadoop implementation and performed evaluations with various benchmark workloads. Experimental results show that RDS substantially reduces the penalty of deadline misses by at least 36 and 10 percent compared with Fair Scheduler and Earliest Deadline First (EDF) scheduler, respectively. In a Hadoop cluster running partially on renewable energy, the experimental result shows the green power based resource prediction approach can further reduce the penalty of deadline misses by 16 percent compared to Auto-Regressive Integrated Moving Average (ARIMA) prediction approach."",""1558-2183"","""",""10.1109/TPDS.2018.2873373"",""University of North Carolina at Charlotte";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478331"",""MapReduce";job scheduling;deadline-aware;dynamic resource availability;horizon control;"job completion times"",""Dynamic scheduling";Green products;Resource management;Task analysis;Optimal scheduling;"Renewable energy sources"","""",""24"","""",""37"",""IEEE"",""30 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Decision-Making Approaches for Performance QoS in Distributed Storage Systems: A Survey,""F. Karniavoura";" K. Magoutis"",""TomTom Germany GmbH & Co. KG, Harsum, Germany";" Institute of Computer Science (ICS), Foundation for Research and Technology - Hellas (FORTH), Heraklion, Crete, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1906"",""1919"",""Distributed storage systems designed to offer explicit performance quality-of-service (QoS) guarantees must regulate the allocation and use of resources to achieve a user-specified level of service. QoS-driven systems employ decision-making techniques to decide on appropriate actions to take during initial deployment or under variations in workload and/or system configuration. In this survey we cover both traditional approaches to decision-making for explicit performance QoS (control theory, multi-dimensional constrained optimization, policy-based techniques) as well as more recent approaches based on machine-learning, offering a broad perspective to the state-of-the-art in the field. As performance prediction is a central concept in decision-making, we also summarize research on performance prediction techniques used in this context."",""1558-2183"","""",""10.1109/TPDS.2019.2893940"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8618414"",""Quality of service";performance;"distributed storage"",""Quality of service";Decision making;Performance evaluation;Resource management;Servers;Predictive models;"Analytical models"","""",""6"","""",""113"",""IEEE"",""18 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;;
"Dependency-Aware Network Adaptive Scheduling of Data-Intensive Parallel Jobs,""S. Wang"; W. Chen; X. Zhou; L. Zhang;" Y. Wang"",""Department of Computer Science, University of Colorado, Colorado Springs, CO, USA"; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Computer & Information Sciences, Indiana University South Bend, South Bend, IN, USA;" Department of Compute Science, Tongji University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""515"",""529"",""Datacenter clusters often run data-intensive jobs in parallel for improving resource utilization and cost efficiency. The performance of parallel jobs is often constrained by the cluster's hard-to-scale network bisection bandwidth. Various solutions have been proposed to address the issue, however, most of them do not consider inter-job data dependencies and schedule jobs independently from one another. In this work, we find that aggregating and co-locating the data and tasks of dependent jobs offer an extra opportunity for data locality improvement that can help to greatly enhance the performance of jobs. We propose and design Dawn, a dependency-aware network-adaptive scheduler that includes an online plan and an adaptive task scheduler. The online plan, taking job dependencies into consideration, determines where (i.e., preferred racks) to place tasks in order to proactively aggregate dependent data. The task scheduler, based on the output of online plan and dynamic network status, adaptively schedules tasks to co-locate with the dependent data in order to take advantage of data locality. We implement Dawn on Apache Yarn and evaluate it on physical and virtual clusters using various machine learning and query workloads. Results show that Dawn effectively improves cluster throughput by up to 73 and 38 percent compared to Fair Scheduler and ShuffleWatcher, respectively. Dawn not only significantly enhances the performance of jobs with dependency, but also works well for jobs without dependency."",""1558-2183"","""",""10.1109/TPDS.2018.2866993"",""US National Science Foundation(grant numbers:CNS-1422119)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8447511"",""Adaptive task scheduler";network adaptive;job dependency;"data-parallel clusters"",""Task analysis";Bandwidth;Schedules;Data aggregation;Adaptive systems;Dynamic scheduling;"Yarn"","""",""10"","""",""45"",""IEEE"",""26 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Designing High-Performance Interconnection Networks with Host-Switch Graphs,""R. Yasudo"; M. Koibuchi; K. Nakano; H. Matsutani;" H. Amano"",""Keio University, Kohoku-ku, Yokohama, Japan"; National Institute of Informatics, Chiyoda-ku, Tokyo, Japan; Hiroshima University, Higashihiroshima-shi, Hiroshima, Japan; Keio University, Kohoku-ku, Yokohama, Japan;" Keio University, Kohoku-ku, Yokohama, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""315"",""330"",""This paper aims at establishing a method for designing high-performance network topologies to bridge a gap between theoretical and practical studies. To this end, we present a novel graph called a host-switch graph, which consists of host vertices and switch vertices with maximum degree 1 and $r$, respectively. This graph represents a network topology of a practical parallel/distributed computer system with host computers connected by $r$-port switches. We discuss important metrics for designing high-performance interconnection networks: the host-to-host average shortest path length (h-ASPL) and the bisection width (BiW). In particular, we explore a method for constructing host-switch graphs with low h-ASPL and high BiW that connect the fixed number of hosts via any number of $r$-port switches. We demonstrate that the number of switches that provides the minimum h-ASPL can mathematically be approximated, and the minimum number of switches that provides a certain BiW can experimentally be approximated. On the basis of the approximations, we propose a randomized algorithm for searching host-switch graphs. We then apply the graphs to interconnection networks and compare them with typical network topologies. As compared with the torus, the dragonfly, and the fat-tree, our networks attain higher performance and smaller power and costs."",""1558-2183"","""",""10.1109/TPDS.2018.2864286"",""KAKENHI(grant numbers:#16H02816)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8428449"",""Network topology";interconnection network;average shortest path length;bisection width;"optimization"",""Switches";Network topology;Multiprocessor interconnection;Topology;Upper bound;Graph theory;"Computational modeling"","""",""5"","""",""53"",""IEEE"",""7 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Diamond Sketch: Accurate Per-flow Measurement for Big Streaming Data,""T. Yang"; S. Gao; Z. Sun; Y. Wang; Y. Shen;" X. Li"",""School of EECS, Peking University, Beijing, P.R. China"; School of EECS, Peking University, Beijing, P.R. China; School of EECS, Peking University, Beijing, P.R. China; School of EECS, Peking University, Beijing, P.R. China; School of Computer Science and Technology, Xidian University, Xian, China;" School of EECS, Peking University, Beijing, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2650"",""2662"",""Per-flow measurement is a critical issue in computer networks, and one of its key tasks is to count the number of packets in each flow (for big streaming data). The literature has demonstrated that sketch is the most memory-efficient data structure for the counting task, and is widely used in distributed systems. Existing sketches often use many counters that are of the same size to record the number of packets in a flow, thus the counters are forced to be large enough to accommodate the size of the largest flow. Unfortunately, as most flows are small (i.e., mice flows) and only a very few flows are large (i.e., elephant flows), many counters represent very small values, which is a waste of memory. Sketches are often stored in fast but expensive memory (e.g., SRAM), thus it is critical to achieve high memory efficiency. To address this issue, we propose a novel sketch, namely the Diamond sketch. The Diamond sketch is composed of atom sketches, and each atom sketch uses small counters. The key idea of Diamond is to dynamically assign an appropriate number of atom sketches to each flow on demand, thus optimizing memory efficiency. Experimental results show that the Diamond sketch outperforms the best of the five typical sketches by up to 508.3 times in terms of relative error while keeping comparable speed. We made the source code of all the six sketches available on GitHub [1] ."",""1558-2183"","""",""10.1109/TPDS.2019.2923772"",""Primary Research & Development Plan of China(grant numbers:2018YFB1004403,2016YFB1000304)"; National Natural Science Foundation of China(grant numbers:61672061);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8742627"",""Sketch";data streams;accuracy;"distributed monitoring"",""Streaming media";Random access memory;Hash functions;Data structures;Size measurement;"Atomic measurements"","""",""22"","""",""35"",""IEEE"",""20 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Distributed Bottleneck-Aware Coflow Scheduling in Data Centers,""T. Zhang"; R. Shu; Z. Shan;" F. Ren"",""Department of Computer Science and Techonolgy, Tsinghua University, Beijing, China"; Microsoft Research, Beijing, China; Informatization and Industry Development Department, State Information Center, Beijing, China;" Department of Computer Science and Techonolgy, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1565"",""1579"",""With the booming development of data parallel frameworks, the coflow abstraction has been greatly favored by data center transport designs, for its prominent ability in capturing application-level semantics. To accelerate job completion, coflow completion time (CCT) is a most important metric, and coflow scheduling is the most effective and widely-adopted means of optimizing CCT. However, most existing coflow scheduling mechanisms neglect the ubiquitous in-network bottlenecks and schedule coflows based on non-blocking giant switch hyperthesis. Such a practice is likely to result in undesired link contention inside the fabric, finally impairing CCT performance. To address this problem, we propose the Distributed Bottleneck-Aware coflow scheduling algorithm called DBA, which approximates the minimum remaining time first (MRTF) heuristic on all fabric-wide links. In this way, core link bandwidths are allocated to coflows as expected and the CCT performance will not be violated. As an evolutionary algorithm, DBA enhances the traditional dual decomposition method thus converges to the optimal bandwidth allocation very fast. Extensive simulations verify DBA's outstanding CCT performance as well as high link utilization. Furthermore, DBA introduces very little overhead and is robust to routing strategies, parameter variations and computation delays."",""1558-2183"","""",""10.1109/TPDS.2018.2889685"",""National Natural Science Foundation of China(grant numbers:61872208)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588374"",""Coflow scheduling";Coflow completion time;in-network bottleneck;"data center"",""Data centers";Bandwidth;Fabrics;Routing;Switches;"Job shop scheduling"","""",""8"","""",""42"",""IEEE"",""25 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Distributed Personal Cloud Storage without Third Parties,""M. Konstantopoulos"; P. Diamantopoulos; N. Chondros;" M. Roussopoulos"",""University of Athens, Athens, Greece"; University of Athens, Athens, Greece; University of Athens, Athens, Greece;" University of Athens, Athens, Greece"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2434"",""2448"",""We present the “It's My Data file system” (IMDfs), a personal, version-tracking, distributed file system that allows users to take custody of their cross-device data management needs, without depending on cloud storage providers. Such a system may be desirable to privacy-conscious users, who worry about the increasing number of cloud storage incidents involving data leakage and corruption, or simply want to synchronize their devices without depending on third parties. IMDfs provides a global, unified view of a user's distributed data collection from any device, user-controlled replication of data across devices, transparent remote access, and support for non-restrictive conflict management. IMDfs achieves all this without sacrificing fault-tolerance or performance and with modest storage overhead. We present the design, implementation, and detailed performance evaluation of IMDfs using benchmarks and traces from real workloads, and we demonstrate that: 1) IMDfs, as a local file system, has reasonable overhead compared with a FUSE loopback file system and Wayback, a local versioning file system, and 2) IMDfs, as a network file system, outperforms NFS and Ori, a state-of-the-art file system for cross-device data management."",""1558-2183"","""",""10.1109/TPDS.2019.2915073"",""ERC Starting(grant numbers:#279237)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8706605"",""Distributed file systems"",""Cloud computing";Synchronization;Data collection;Metadata;Distributed databases;Portable computers;"Performance evaluation"","""",""8"","""",""50"",""IEEE"",""6 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Dynamic Autoselection and Autotuning of Machine Learning Models for Cloud Network Analytics,""R. R. Karn"; P. Kudva;" I. A. M. Elfadel"",""Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE"; IBM Research, Yorktown Heights, NY, USA;" Center for Cyber-Physical Systems, Khalifa University, Abu Dhabi, UAE"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1052"",""1064"",""Cloud network monitoring data is dynamic and distributed. Signals to monitor the cloud can appear, disappear or change their importance and clarity over time. Machine learning (ML) models tuned to a given data set can therefore quickly become inadequate. A model might be highly accurate at one point in time but may lose its accuracy at a later time due to changes in input data and their features. Distributed learning with dynamic model selection is therefore often required. Under such selection, poorly performing models (although aggressively tuned for the prior data) are retired or put on standby while new or standby models are brought in. The well-known method of Ensemble ML (EML) may potentially be applied to improve the overall accuracy of a family of ML models. Unfortunately, EML has several disadvantages, including the need for continuous training, excessive computational resources, requirement for large training datasets, high risks of overfitting, and a time-consuming model-building process. In this paper, we propose a novel cloud methodology for automatic ML model selection and tuning that automates model building and selection and is competitive with existing methods. We use unsupervised learning to better explore the data space before the generation of targeted supervised learning models in an automated fashion. In particular, we create a Cloud DevOps architecture for autotuning and selection based on container orchestration and messaging between containers, and take advantage of a new autoscaling method to dynamically create and evaluate instantiations of ML algorithms. The proposed methodology and tool are demonstrated on cloud network security datasets."",""1558-2183"","""",""10.1109/TPDS.2018.2876844"",""IBM T. J. Watson Research Center, Yorktown Heights, NY"; Joint Study Agreement(grant numbers:W1463335); IBM Research; Khalifa University, Abu Dhabi, UAE;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8500348"",""Cloud analytics";machine learning;ensemble learning;distributed learning;clustering;classification;autoselection;autotuning;decision feedback;cloud DevOps;containers;docker;"Kafka"",""Machine learning";Cloud computing;Containers;Data models;Machine learning algorithms;Tuning;"Computational modeling"","""",""15"","""",""34"",""IEEE"",""19 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Dynamic Network Function Instance Scaling Based on Traffic Forecasting and VNF Placement in Operator Data Centers,""H. Tang"; D. Zhou;" D. Chen"",""School of Data and Computer Science, Sun Yat-sen University, Guangzhou, China"; Huawei Technologies Corp., Shenzhen, China;" Nokia Shanghai Bell Co., Ltd., Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""530"",""543"",""Traffic in operator networks is time varying. Conventional network functions implemented by black-boxes should satisfy the peak traffic requirement, and hence result in low resource utilization. Thanks to the emergence of Virtual Network Function (VNF), which is realized by running networking software on Virtual Machines (VMs), the operator can dynamically scale in or scale out the VNF instances and hence save the required resources. In this paper, we introduce how the dynamic VNF scaling is implemented in practical operator Data Center Networks (DCNs). First, we analyze the traffic characteristics in our operator networks, and introduce how the VNFs are organized in a common operator DCN. Based on these backgrounds, we not only propose a traffic forecasting method, but also design two VNF placement algorithms to guide the dynamic VNF instance scaling. Through both the implementation in a real operator network and extensive real trace driven simulations, we demonstrate that our dynamic VNF instance scaling system can achieve higher service availability and save the VNF resources (e.g., CPU and memory) by up to 30 percent."",""1558-2183"","""",""10.1109/TPDS.2018.2867587"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449985"",""Data center networks (DCNs)";virtual network functions (VNFs);"dynamic service function chaining (SFC)"",""Forecasting";Software;Middleboxes;Heuristic algorithms;Data centers;Servers;"Predictive models"","""",""69"","""",""29"",""IEEE"",""29 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Easy Dataflow Programming in Clusters with UPC++ DepSpawn,""B. B. Fraguela";" D. Andrade"",""Universidade da Coruña, Grupo de Arquitectura de Computadores, A Coruña, Spain";" Universidade da Coruña, Grupo de Arquitectura de Computadores, A Coruña, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1267"",""1282"",""The Partitioned Global Address Space (PGAS) programming model is one of the most relevant proposals to improve the ability of developers to exploit distributed memory systems. However, despite its important advantages with respect to the traditional message-passing paradigm, PGAS has not been yet widely adopted. We think that PGAS libraries are more promising than languages because they avoid the requirement to (re)write the applications using them, with the implied uncertainties related to portability and interoperability with the vast amount of APIs and libraries that exist for widespread languages. Nevertheless, the need to embed these libraries within a host language can limit their expressiveness and very useful features can be missing. This paper contributes to the advance of PGAS by enabling the simple development of arbitrarily complex task-parallel codes following a dataflow approach on top of the PGAS UPC++ library, implemented in C++. In addition, our proposal, called UPC++ DepSpawn, relies on an optimized multithreaded runtime that provides very competitive performance, as our experimental evaluation shows."",""1558-2183"","""",""10.1109/TPDS.2018.2884716"",""Ministerio de Economía, Industria y Competitividad of Spain and FEDER funds of the EU(grant numbers:TIN2016-75845-P)"; European Regional Development Fund; Consolidation Programme of Competitive Reference Groups(grant numbers:ED431C 2017/04); Centro Singular de Investigación de Galicia accreditation(grant numbers:2016-2019,ED431G/01);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8565930"",""Libraries";parallel programming models;distributed memory;multithreading;programmability;"dataflow"",""Electronics packaging";Libraries;C++ languages;Programming;Arrays;Task analysis;"Proposals"","""",""3"","""",""40"",""IEEE"",""6 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Effective Extensible Programming: Unleashing Julia on GPUs,""T. Besard"; C. Foket;" B. De Sutter"",""Department of Electronics and Information Systems, Ghent University, Gent, Belgium"; Department of Electronics and Information Systems, Ghent University, Gent, Belgium;" Department of Electronics and Information Systems, Ghent University, Gent, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""827"",""841"",""GPUs and other accelerators are popular devices for accelerating compute-intensive, parallelizable applications. However, programming these devices is a difficult task. Writing efficient device code is challenging, and is typically done in a low-level programming language. High-level languages are rarely supported, or do not integrate with the rest of the high-level language ecosystem. To overcome this, we propose compiler infrastructure to efficiently add support for new hardware or environments to an existing programming language. We evaluate our approach by adding support for NVIDIA GPUs to the Julia programming language. By integrating with the existing compiler, we significantly lower the cost to implement and maintain the new compiler, and facilitate reuse of existing application code. Moreover, use of the high-level Julia programming language enables new and dynamic approaches for GPU programming. This greatly improves programmer productivity, while maintaining application performance similar to that of the official NVIDIA CUDA toolkit."",""1558-2183"","""",""10.1109/TPDS.2018.2872064"",""Science and Technology in Flanders"; Ghent University;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8471188"",""Graphics processors";very high-level languages;code generation;"retargetable compilers"",""Graphics processing units";Programming;Hardware;High level languages;"Libraries"","""",""81"","""",""44"",""IEEE"",""23 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Efficient and Scalable Functional Dependency Discovery on Distributed Data-Parallel Platforms,""G. Zhu"; Q. Wang; Q. Tang; R. Gu; C. Yuan;" Y. Huang"",""National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China"; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China;" National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing Shi, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2663"",""2676"",""Functional dependencies (FDs) play a very important role in many data management tasks such as schema normalization, data cleaning, and query optimization. Meanwhile, there are ever-increasing application demands for efficient FD discovery on large-scale datasets. Unfortunately, due to huge runtime and memory overhead, the existing single-machine FD discovery algorithms are inefficient for large-scale datasets. Recently, distributed data-parallel computing has become the de facto standard for large-scale data processing. However, it is challenging to design an efficient distributed FD discovery algorithm. In this paper, we present SmartFD, which is an efficient and scalable algorithm for distributed FD discovery. First, we propose a novel attribute sorting-based algorithm framework. Next, to discover all the FDs grouped by a given attribute, we propose an efficient distributed algorithm Attribute-centric Functional Dependency Discovery (AFDD). In AFDD, we design an Fast Sampling and Early Aggregation (FSEA) mechanism to improve the efficiency of distributed sampling and propose a memory-efficient index-based method for distributed FD validation. Moreover, AFDD employs an attribute-parallel method to accelerate the pruning-and-generation of candidate FDs. Furthermore, we propose an adaptive switching strategy between distributed sampling and distributed validation based on the unified time-based efficiency metric. Also, we employ a distributed probing based method to make the switching strategy more accurate. Experimental results on Apache Spark reveal that SmartFD outperforms the state-of-the-art single-machine algorithm HyFD and the existing distributed algorithm HFDD with 3.2×-44.9× and 2.5×-455.7× speedup respectively. Moreover, SmartFD achieves good row scalability and column scalability. Additionally, SmartFD has sub-linear node scalability."",""1558-2183"","""",""10.1109/TPDS.2019.2925014"",""National Natural Science Foundation of China(grant numbers:61572250,U1811461,61702254)"; Jiangsu Province Science and Technology Program(grant numbers:BE2017155); Natural Science Foundation of Jiangsu Province(grant numbers:BK20170651); Program B for Outstanding PhD candidate of Nanjing University; Collaborative Innovation Center of Novel Software Technology and Industrialization; China Postdoctoral Science Foundation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747416"",""Functional dependency discovery";distributed computing;"data-parallel algorithms"",""Distributed databases";Scalability;Remuneration;Data processing;Distributed algorithms;Switches;"Query processing"","""",""5"","""",""30"",""IEEE"",""27 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Efficient Concurrent Search Trees Using Portable Fine-Grained Locality,""P. H. Ha"; O. J. Anshus;" I. Umar"",""Department of Computer Science, UiT The Arctic University of Norway, Tromsø, Norway"; Department of Computer Science, UiT The Arctic University of Norway, Tromsø, Norway;" Institute of Marine Research, Bergen, Norway"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1580"",""1595"",""Concurrent search trees are crucial data abstractions widely used in many important systems such as databases, file systems and data storage. Like other fundamental abstractions for energy-efficient computing, concurrent search trees should support both high concurrency and fine-grained data locality in a platform-independent manner. However, existing portable fine-grained locality-aware search trees such as ones based on the van Emde Boas layout (vEB-based trees) poorly support concurrent update operations while existing highly-concurrent search trees such as non-blocking search trees do not consider fine-grained data locality. In this paper, we first present a novel methodology to achieve both portable fine-grained data locality and high concurrency for search trees. Based on the methodology, we devise a novel locality-aware concurrent search tree called GreenBST. To the best of our knowledge, GreenBST is the first practical search tree that achieves both portable fine-grained data locality and high concurrency. We analyze and compare GreenBST energy efficiency (in operations/Joule) and performance (in operations/second) with seven prominent concurrent search trees on a high performance computing (HPC) platform (Intel Xeon), an embedded platform (ARM), and an accelerator platform (Intel Xeon Phi) using parallel micro- benchmarks (Synchrobench). Our experimental results show that GreenBST achieves the best energy efficiency and performance on all the different platforms. GreenBST achieves up to 50 percent more energy efficiency and 60 percent higher throughput than the best competitor in the parallel benchmarks. These results confirm the viability of our new methodology to achieve both portable fine-grained data locality and high concurrency for search trees."",""1558-2183"","""",""10.1109/TPDS.2019.2892968"",""Norges Forskningsråd(grant numbers:231746/F20,270053)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611364"",""Concurrent data abstractions";trees;energy efficiency;performance optimization;data locality;"portability"",""Data models";Concurrent computing;Layout;Data structures;Random access memory;Upper bound;"System-on-chip"","""","""","""",""47"",""IEEE"",""13 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Efficient Data Placement and Replication for QoS-Aware Approximate Query Evaluation of Big Data Analytics,""Q. Xia"; Z. Xu; W. Liang; S. Yu; S. Guo;" A. Y. Zomaya"",""Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, International School of Information Science and Engineering, Dalian University of Technology, Dalian, China"; School of Software, Dalian University of Technology, Dalian, China; Research School of Computer Science, Australian National University, Canberra, Australia; School of Software, University of Technology Sydney, Ultimo, Australia; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong;" School of Computer Science, University of Sydney, Camperdown, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2677"",""2691"",""Enterprise users at different geographic locations generate large-volume data that is stored at different geographic datacenters. These users may also perform big data analytics on the stored data to identify valuable information in order to make strategic decisions. However, it is well known that performing big data analytics on data in geographical-located datacenters usually is time-consuming and costly. In some delay-sensitive applications, the query result may become useless if answering a query takes too long time. Instead, sometimes users may only be interested in timely approximate rather than exact query results. When such approximate query evaluation is the case, applications must sacrifice timeliness to get more accurate evaluation results or tolerate evaluation result with a guaranteed error bound obtained from analyzing the samples of the data to meet their stringent timeline. In this paper, we study quality-of-service (QoS)-aware data replication and placement for approximate query evaluation of big data analytics in a distributed cloud, where the original (source) data of a query is distributed at different geo-distributed datacenters. We focus on the problems of placing data samples of the source data at some strategic datacenters to meet stringent query delay requirements of users, by exploring a non-trivial trade-off between the cost of query evaluation and the error bound of the evaluation result. We first propose an approximation algorithm with a provable approximation ratio for a single approximate query. We then develop an efficient heuristic algorithm for evaluating a set of approximate queries with the aim to minimize the evaluation cost while meeting the delay requirements of these queries. We finally demonstrate the effectiveness and efficiency of the proposed algorithms through both experimental simulations and implementations in a real test-bed, real datasets are employed. Experimental results show that the proposed algorithms are promising."",""1558-2183"","""",""10.1109/TPDS.2019.2921337"",""National Natural Science Foundation of China(grant numbers:61802047,61802048,61772113,61872053)"; fundamental research funds for the central universities in China(grant numbers:DUT19RC(4)035,DUT19RC(5)001,DUT19GJ204); Dalian University of Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8732398"",""Data replication and placement";big data analytics;approximate query evaluation;approximation algorithms;"algorithm analysis"",""Big Data";Query processing;Delays;Approximation algorithms;Quality of service;Distributed databases;"Data analysis"","""",""17"","""",""38"",""IEEE"",""6 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Efficient Lossy Compression for Scientific Data Based on Pointwise Relative Error Bound,""S. Di"; D. Tao; X. Liang;" F. Cappello"",""Mathematics and Computer Science (MCS) Division, Argonne National Laboratory, Lemont, IL, USA"; Department of Computer Science, University of Alabama, Tuscaloosa, AL, USA; Computer Science Department, University of California, Riverside, CA, USA;" Mathematics and Computer Science (MCS) Division, Argonne National Laboratory, Lemont, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""331"",""345"",""An effective data compressor is becoming increasingly critical to today's scientific research, and many lossy compressors are developed in the context of absolute error bounds. Based on physical/chemical definitions of simulation fields or multiresolution demand, however, many scientific applications need to compress the data with a pointwise relative error bound (i.e., the smaller the data value, the smaller the compression error to tolerate). To this end, we propose two optimized lossy compression strategies under a state-of-the-art three-staged compression framework (prediction + quantization + entropy-encoding). The first strategy (called block-based strategy) splits the data set into many small blocks and computes an absolute error bound for each block, so it is particularly suitable for the data with relatively high consecutiveness in space. The second strategy (called multi-threshold-based strategy) splits the whole value range into multiple groups with exponentially increasing thresholds and performs the compression in each group separately, which is particularly suitable for the data with a relatively large value range and spiky value changes. We implement the two strategies rigorously and evaluate them comprehensively by using two scientific applications which both require lossy compression with point-wise relative error bound. Experiments show that the two strategies exhibit the best compression qualities on different types of data sets respectively. The compression ratio of our lossy compressor is higher than that of other state-of-the-art compressors by 17.2-618 percent on the climate simulation data and 30-210 percent on the N-body simulation data, with the same relative error bound and without degradation of the overall visualization effect of the entire data."",""1558-2183"","""",""10.1109/TPDS.2018.2859932"",""Exascale Computing Project (ECP)(grant numbers:17-SC-20-SC)"; U.S. Department of Energy, Office of Science(grant numbers:DE-AC02-06CH11357); US National Science Foundation(grant numbers:1619253);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8421751"",""Lossy compression";science data;high performance computing;"relative error bound"",""Data models";Quantization (signal);Meteorology;Computational modeling;Error correction;"Wavelet transforms"","""",""12"","""",""31"",""IEEE"",""27 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Efficient Operator Placement for Distributed Data Stream Processing Applications,""M. Nardelli"; V. Cardellini; V. Grassi;" F. L. Presti"",""University of Rome Tor Vergata, Roma, Italy"; University of Rome Tor Vergata, Roma, Italy; University of Rome Tor Vergata, Roma, Italy;" University of Rome Tor Vergata, Roma, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1753"",""1767"",""In the last few years, a large number of real-time analytics applications rely on the Data Stream Processing (DSP) so to extract, in a timely manner, valuable information from distributed sources. Moreover, to efficiently handle the increasing amount of data, recent trends exploit the emerging presence of edge/Fog computing resources so to decentralize the execution of DSP applications. Since determining the Optimal DSP Placement (for short, ODP) is an NP-hard problem, we need efficient heuristics that can identify a good application placement on the computing infrastructure in a feasible amount of time, even for large problem instances. In this paper, we present several DSP placement heuristics that consider the heterogeneity of computing and network resources";" we divide them in two main groups: model-based and model-free. The former employ different strategies for efficiently solving the ODP model. The latter implement, for the problem at hand, some of the well-known meta-heuristics, namely greedy first-fit, local search, and tabu search. By leveraging on ODP, we conduct a thorough experimental evaluation, aimed to assess the heuristics' efficiency and efficacy under different configurations of infrastructure size, application topology, and optimization objectives."",""1558-2183"","""",""10.1109/TPDS.2019.2896115"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630099"",""Distributed data stream processing";geo-distributed systems;heuristics;operator placement;"quality of service"",""Computational modeling";Quality of service;Search problems;Delays;Optimization;"Storms"","""",""50"","""",""47"",""IEEE"",""30 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Efficient Scheduling of Weighted Coflows in Data Centers,""Z. Wang"; H. Zhang; X. Shi; X. Yin; Y. Li; H. Geng; Q. Wu;" J. Liu"",""Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China"; School of Cyber Science and Technology, Beihang University, Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; Beijing National Research Center for Information Science and Technology (BNRist), Beijing, China; School of Software Engineering, Shanxi University, Taiyuan, China; School of Cyber Science and Technology, Beihang University, Beijing, China;" School of Cyber Science and Technology, Beihang University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""2003"",""2017"",""Traditional network resource management mechanisms are mainly flow or packet based. Recently, coflow has been proposed as a new abstraction to capture the communication patterns in a rich set of data parallel applications in data centers. Coflows effectively model the application-level semantics of network resource usage, so high-level optimization goals, such as reducing the transfer latency of applications, can be better achieved by taking coflows as the basic elements in network resource allocation or scheduling. Although efficient coflow scheduling methods have been studied, in this paper, we advocate to schedule weighted coflows as a further step in this direction, where weights are used to express the importances or priorities of different coflows or their corresponding applications. We propose the Weighted Coflow Completion Time (WCCT) minimization problem and a (2-2/n+1)-approximate optimal offline algorithm, where n is the concurrent number of coflows. We then design an information-agnostic online algorithm named IAOA to dynamically schedule coflows according to their weights and the instantaneous network condition. We also design and implement a coflow scheduling system named FlyTransfer, which can use the online algorithm as its scheduling method. We test the performance of FlyTransfer by trace-driven simulations as well as real deployment in openstack. Our evaluation results show that, compared to the latest information-agnostic coflow scheduling algorithms, FlyTransfer can reduce more than 40 percent of the WCCT, and more than 30 percent of the completion time for coflows with above-the-average level of importance. It even outperforms the most efficient clairvoyant coflow scheduling method by reducing around 30 percent WCCT, and 25- 30 percent of the completion time for coflows with above-the-average importance, respectively."",""1558-2183"","""",""10.1109/TPDS.2019.2905560"",""National Natural Science Foundation of China(grant numbers:61702315)"; National Key R&D Program of China(grant numbers:2017YFB0802500); National Cryptography Development Fund(grant numbers:MMJJ20170106); foundation of Science and Technology on Information Assurance Laboratory(grant numbers:61421120305162112006); National Natural Science Foundation of China(grant numbers:61772538,61672083,61532021,61472429,61402029);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668552"",""DataCenter";coflow;weight;"CCT, WCCT"",""Scheduling";Data centers;Schedules;Scheduling algorithms;Minimization;"Approximation algorithms"","""",""21"","""",""51"",""OAPA"",""17 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Efficient Time-Evolving Stream Processing at Scale,""X. Liao"; Y. Huang; L. Zheng;" H. Jin"",""National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China"; National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China; National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China;" National Engineering Research Center for Big Data Technology and System, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2165"",""2178"",""Time-evolving stream datasets exist ubiquitously in many real-world applications where their inherent hot keys often evolve over times. Nevertheless, few existing solutions can provide efficient load balancing on these time-evolving datasets while preserving low memory overhead. In this paper, we present a novel load balancing mechanism (named FISH), which can provide the efficient time-evolving stream processing at scale through recent hot keys identification and worker assignment. The key insight of this work is that the keys of time-evolving stream data can have a skewed distribution within the bounded distance of time interval. This enables to accurately identify the recent hot keys for the real-time load balancing within a bounded scope. We therefore propose an epoch-based recent hot key identification with specialized intra-epoch frequency counting (for maintaining low memory overhead) and inter-epoch hotness decaying (for suppressing superfluous computation). We also propose to heuristically infer the accurate information of remote workers through computation rather than communication for cost-efficient worker assignment. We have integrated our approach into Apache Storm. Our results on a cluster of 128 nodes for both synthetic and real-world stream datasets show that FISH significantly outperforms state-of-the-arts with the average and the 99th percentile latency reduction by 87.12 and 76.34 percent (versus W-Choices), and memory overhead reduction by 96.66 percent (versus Shuffle Grouping)."",""1558-2183"","""",""10.1109/TPDS.2019.2911495"",""National Basic Research Program of China (973 Program)(grant numbers:2018YFB1003502)"; National Natural Science Foundation of China(grant numbers:61825202,61832006,61702201);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691781"",""Stream processing";streaming partition;load balancing;efficiency;"scalability"",""Load management";Scalability;Real-time systems;Fish;Time-frequency analysis;Micromechanical devices;"Motion pictures"","""",""5"","""",""45"",""IEEE"",""14 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Efficiently Embedding Service Function Chains with Dynamic Virtual Network Function Placement in Geo-Distributed Cloud System,""J. Pei"; P. Hong; K. Xue;" D. Li"",""Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China, Hefei, China"; Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China, Hefei, China; Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China, Hefei, China;" Key Laboratory of Wireless-Optical Communications, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2179"",""2192"",""Network Function Virtualization (NFV) and Software-Defined Networks (SDN) enable Internet Service Providers (ISPs) to place Virtual Network Functions (VNFs) to achieve the performance and security benefit without incurring high Operating Expenses (OPEX) and Capital Expenses (CAPEX). In NFV environment, Service Function Chains (SFCs) always need to steer the traffic through a series of VNF instances in predefined orders. Moreover, the required number and placement of VNF instances should be optimized to adapt to dynamic network load. Therefore, it is considerable for ISPs to conduct an optimal SFC embedding strategy to improve the network performance and revenue. In the paper, we study the SFC Embedding Problem (SFC-EP) with dynamic VNF placement in geo-distributed cloud system. We formulate this problem as a Binary Integer Programming (BIP) model aiming to embed SFC requests with the minimum embedding cost. Furthermore, the novel SFC eMbedding APproach (SFC-MAP) and VNF Dynamic Release Algorithm (VNF-DRA) have been proposed to efficiently embed SFC requests and optimize the number of placed VNF instances. Performance evaluation results show that the proposed algorithms can provide higher performance in terms of SFC request acceptance rate, network throughput, and mean VNF utilization rate and efficiently reduce the total VNF running time compared with the algorithms in existing literatures."",""1558-2183"","""",""10.1109/TPDS.2018.2880992"",""National Natural Science Foundation of China(grant numbers:61671420,61672484)"; Youth Innovation Promotion Association CAS(grant numbers:2016394); Fundamental Research Funds for the Central Universities;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8532318"",""Service function chain";virtual network function;"dynamic VNF placement"",""Heuristic algorithms";Bandwidth;Switches;Cloud computing;Linear programming;Load modeling;"Middleboxes"","""",""125"","""",""35"",""IEEE"",""11 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Emulating a Shared Register in a System That Never Stops Changing,""H. Attiya"; H. C. Chung; F. Ellen; S. Kumar;" J. L. Welch"",""Computer Science Department, Technion, Haifa, Israel"; Epoch Labs, Inc., Austin, TX, USA; Department of Computer Science, University of Toronto, Toronto, ON, Canada; Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA;" Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""544"",""559"",""Emulating a shared register can mask the intricacies of designing algorithms for asynchronous message-passing systems subject to crash failures, since it allows them to run algorithms designed for the simpler shared-memory model. Typically such emulations replicate the value of the register in multiple servers and require readers and writers to communicate with a majority of servers. The success of this approach for static systems, where the set of nodes (readers, writers, and servers) is fixed, has motivated several similar emulations for dynamic systems, where nodes may enter and leave. However, existing emulations need to assume that the system eventually stops changing for a long enough period or that the system size is bounded. This paper presents the first emulation of a register supporting any number of readers and writers in a crash-prone system that can withstand nodes continually entering and leaving and imposes no upper bound on the system size. The algorithm works as long as the number of nodes entering and leaving during a fixed time interval is at most a constant fraction of the system size at the beginning of the interval, and as long as the number of crashed nodes in the system is at most a constant fraction of the current system size. The paper includes a lower bound on the fraction of correct nodes that is strictly larger than the fraction sufficient to solve the problem in the static case."",""1558-2183"","""",""10.1109/TPDS.2018.2867479"",""Israel Science Foundation(grant numbers:1227/10,1749/14)"; US National Science Foundation(grant numbers:0964696,1526725); Natural Science and Engineering Research Council of Canada;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8449124"",""Read / write register emulation";message passing systems;churn;"crash resilience"",""Computer crashes";Registers;Servers;Emulation;Upper bound;Resilience;"Computer science"","""",""5"","""",""28"",""IEEE"",""28 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Enabling Encrypted Rich Queries in Distributed Key-Value Stores,""y. GUO"; X. Yuan; X. Wang; C. Wang; B. Li;" X. Jia"",""Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong"; Faculty of Information Technology, Monash University, Clayton, VIC, Australia; Shenzhen Research Institute, City University of Hong Kong, Shenzhen, Nanshan, China; Shenzhen Research Institute, City University of Hong Kong, Shenzhen, Nanshan, China; Department of Electrical and Computer Engineering, University of Toronto, Toronto, Ontario, Canada;" Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1283"",""1297"",""To accommodate massive digital data, distributed data stores have become the main solution for cloud services. Among others, key-value stores are widely adopted due to their superior performance. But with the rapid growth of cloud storage, there are growing concerns about data privacy. In this paper, we design and build EncKV, an encrypted and distributed key-value store with rich query support. First, EncKV partitions data records with secondary attributes into a set of encrypted key-value pairs to hide relations between data values. Second, EncKV uses the latest cryptographic techniques for searching on encrypted data, i.e., searchable symmetric encryption (SSE) and order-revealing encryption (ORE) to support secure exact-match and range-match queries, respectively. It further employs a framework for encrypted and distributed indexes supporting query processing in parallel. To address inference attacks on ORE, EncKV is equipped with an enhanced ORE scheme with reduced leakage. For practical considerations, EncKV also enables secure system scaling in a minimally intrusive way. We complete the prototype implementation and deploy it on Amazon Cloud. Experimental results confirm that EncKV preserves the efficiency and scalability of distributed key-value stores."",""1558-2183"","""",""10.1109/TPDS.2018.2885519"",""China National Science and Technology(grant numbers:2016YFB0800804)"; National Natural Science Foundation of China(grant numbers:61732022,61572412,61672195); Research Grants Council of Hong Kong(grant numbers:CityU 11204215,CityU 11276816,CityU 11212717,CityU C1008-16G); Oceania Cyber Security Centre POC scheme, and an AWS in Education Research Grant award;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8567979"",""Encrypted key-value store";searchable encryption;"order-revealing encryption"",""Indexes";Encryption;Distributed databases;Protocols;"Servers"","""",""21"","""",""45"",""IEEE"",""7 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Enabling Flexible Resource Allocation in Mobile Deep Learning Systems,""C. Wu"; l. Zhang; Q. Li; Z. Fu; W. Zhu;" Y. Zhang"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; School of Computer Science and Technology, University of Science and Technology China, Hefei, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""346"",""360"",""Deep learning provides new opportunities for mobile applications to achieve higher performance than before. Rather, the deep learning implementation on mobile device today is largely demanding on expensive resource overheads, imposes a significant burden on the battery life and limited memory space. Existing methods either utilize cloud or edge infrastructure that require to upload user data, however, resulting in a risk of privacy leakage and large data transfers";" or adopt compressed deep models, nevertheless, downgrading the algorithm accuracy. This paper provides DeepShark, a platform to enable mobile devices with the ability of flexible resource allocation in using commercial-off-the-shelf (COTS) deep learning systems. Compared to existing approaches, DeepShark seeks a balanced point between time and memory efficiency by user requirements, breaks down sophisticated deep model into code block stream and incrementally executes such blocks on system-on-chip (SoC). Thus, DeepShark requires significantly less memory space on mobile device and achieves the default accuracy. In addition, all referred user data of model processing is handled locally, thus to avoid unnecessary data transfer and network latency. DeepShark is now developed on two COTS deep learning systems, i.e., Caffe and TensorFlow. The experimental evaluations demonstrate its effectiveness in the aspects of memory space and energy cost."",""1558-2183"","""",""10.1109/TPDS.2018.2865359"",""National Key R&D Program of China(grant numbers:2017YFB1003003)"; NSF China(grant numbers:61572281); Tsinghua University Initiative Scientific Research Program(grant numbers:20161080066);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8434315"",""Deep learning system";flexible resource allocation;computation optimization;"mobile device"",""Machine learning";Mobile handsets;Computational modeling;Tools;Memory management;Resource management;"Performance evaluation"","""",""9"","""",""41"",""IEEE"",""13 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Enabling Heterogeneous Network Function Chaining,""L. Cui"; F. P. Tso; S. Guo; W. Jia; K. Wei;" W. Zhao"",""Jinan University, Guangzhou, Guangdong, CN"; Loughborough University, Loughborough, Leicestershire, GB; Hong Kong Polytechnic University, Kowloon, HK; University of Macau, Taipa, Macau, MO; Jinan University, Guangzhou, Guangdong, CN;" American University of Sharjah, Sharjah, Sharjah, AE"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""842"",""854"",""Today's data center operators deploy network policies in both physical (e.g., middleboxes, switches) and virtualized (e.g., virtual machines on general purpose servers) network function boxes (NFBs), which reside in different points of the network, to exploit their efficiency and agility respectively. Nevertheless, such heterogeneity has resulted in a great number of independent network nodes that can dynamically generate and implement inconsistent and conflicting network policies, making correct policy implementation a difficult problem to solve. Since these nodes have varying capabilities, services running atop are also faced with profound performance unpredictability. In this paper, we propose a Heterogeneous netwOrk Policy Enforcement (HOPE) scheme to overcome these challenges. HOPE guarantees that network functions (NFs) that implement a policy chain are optimally placed onto heterogeneous NFBs such that the network cost of the policy is minimized. We first experimentally demonstrate that the processing capacity of NFBs is the dominant performance factor. This observation is then used to formulate the Heterogeneous Network Policy Placement problem, which is shown to be NP-Hard. To solve the problem efficiently, an online algorithm is proposed. Our experimental results demonstrate that HOPE achieves the same optimality as Branch-and-bound optimization but is 3 orders of magnitude more efficient."",""1558-2183"","""",""10.1109/TPDS.2018.2871845"",""National Natural Science Foundation of China(grant numbers:61772235,61502202,61872310,61872239)"; Fundamental Research Funds for the Central Universities(grant numbers:21617409); Engineering and Physical Sciences Research Council(grant numbers:EP/P004407/2,EP/P004024/1,FDCT 0007/2018/A1,DCT-MoST 025/2015/AMJ); University of Macau(grant numbers:CPG2018-00032-FST,SRG2018-00111-FST); National Natural Science Foundation of China(grant numbers:61532013); National China 973(grant numbers:2015CB352401); Shanghai Jiao Tong University(grant numbers:WF220103001); Natural Science Foundation of Guangdong Province(grant numbers:2017A030313334);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8470961"",""Network policy";service chain;middleboxes;network functions;heterogeneous;"datacenters"",""Feedback amplifiers";Servers;Random access memory;Hardware;Middleboxes;Noise measurement;"Heterogeneous networks"","""",""14"","""",""50"",""IEEE"",""23 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"Energy-Efficient Multiple Producer-Consumer,""R. Medhat"; B. Bonakdarpour;" S. Fischmeister"",""Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada"; Department of Computer Science, Iowa State University, Ames, IA, USA;" Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""560"",""574"",""Hardware energy efficiency has been one of the prominent objectives of system design in the last two decades. However, with the recent explosion in mobile computing and the increasing demand for green data centers, software energy efficiency has also risen to be an equally important factor. The majority of classic concurrency control algorithms were designed in an era when energy efficiency was not an important dimension in algorithm design. Concurrency control algorithms are applied to solve a wide range of problems from kernel-level primitives in operating systems to networking devices and web services. These primitives and services are constantly and heavily invoked in any computing system and by a larger scale in networking devices and data centers. Thus, even a small change in their energy spectrum can make a huge impact on overall energy consumption for long periods of time. This paper focuses on the classic producer-consumer problem. First, we study the energy profile of a set of existing producer-consumer algorithms. In particular, we present evidence that although these algorithms share the same functional goals, their behavior with respect to energy consumption are drastically different. Then, we present a dynamic algorithm for the multiple producer-consumer problem, where consumers in a multicore system use learning mechanisms to predict the rate of production, and effectively utilize this prediction to attempt to latch onto previously scheduled CPU wake-ups. Such group latching increases the idle time between consumer activations resulting in more CPU idle time and, hence, lower average CPU frequency. This in turn reduces energy consumption. We enable consumers to dynamically reserve more pre-allocated memory in cases where the production rate is too high. Consumers may compete for the extra space and dynamically release it when it is no longer needed. Our experiments show that our algorithm provides a 38 percent decrease in energy consumption compared to a mainstream semaphore-based producer-consumer implementation when running 10 parallel consumers. We validate the effectiveness of our algorithm with a set of thorough experiments on varying parameters of scalability. Finally, we present our recommendations on when our algorithm is most beneficial."",""1558-2183"","""",""10.1109/TPDS.2018.2867853"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451894"",""Concurrency control";green computing;power;energy;"synchronization"",""Energy consumption";Batch production systems;Heuristic algorithms;Hardware;Time factors;Voltage control;"Data centers"","""",""2"","""",""32"",""IEEE"",""30 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Energy-Efficient Task Scheduling for CPU-Intensive Streaming Jobs on Hadoop,""P. Jin"; X. Hao; X. Wang;" L. Yue"",""Key Laboratory of Electromagnetic Space Information, Chinese Academy of Sciences, Hefei, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China;" Key Laboratory of Electromagnetic Space Information, Chinese Academy of Sciences, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1298"",""1311"",""Hadoop, especially Hadoop 2.0, has been a dominant framework for real-time big data processing. However, Hadoop is not optimized for energy efficiency. Aiming to solve this problem, in this paper, we propose a new framework to improve the energy efficiency of Hadoop 2.0. We focus on the resource manager in Hadoop 2.0, namely YARN, and propose energy-efficient task scheduling mechanisms on YARN. Particularly, we focus on CPU-intensive streaming jobs and classify streaming jobs into two types, namely batch streaming jobs (i.e., a set of jobs are submitted simultaneously) and online streaming jobs (i.e., jobs are continuously submitted one by one). We devise different energy-efficient task scheduling algorithms for each kind of streaming jobs. Specially, we first propose to abstractly model performance and energy consumption by considering the characteristics of tasks as well as the computational resources in YARN. Based on this model, we study the energy efficiency of streaming tasks which consist of the performance model and energy consumption model of task. We propose two key principles for improving energy efficiency: 1) CPU usage aware task allocation, partitions tasks to NMs based on the task characteristic in term of CPU usage";" and 2) resource efficient task allocation, reduce idle resource. Then, we propose a D-based binning algorithm for the batch task scheduling and K-based binning algorithm for the online task scheduling that can adapt to continuously arriving tasks. We conduct extensive experiments on a real Hadoop 2.0 cluster and use two kinds of workloads to evaluate the performance and energy efficiency of our proposal. Compared with Storm (the streaming data processing tool in Hadoop 2.0) and other approaches including TAPA and DVFS-MR, our proposal is more energy efficient. The batch task scheduling algorithm reduces up to 10 percent of energy consumption and keeps comparable performance. In addition, the online task scheduling algorithm reduces up to 7 percent over the existing algorithms."",""1558-2183"","""",""10.1109/TPDS.2018.2881176"",""National Natural Science Foundation of China(grant numbers:61672479,61472376)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8533365"",""Energy efficiency";scheduling algorithms;Hadoop;"YARN"",""Task analysis";Energy efficiency;Yarn;Energy consumption;Processor scheduling;Servers;"Big Data"","""",""18"","""",""43"",""IEEE"",""13 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Engineering Algorithms for Scalability through Continuous Validation of Performance Expectations,""S. Shudler"; Y. Berens; A. Calotoiu; T. Hoefler; A. Strube;" F. Wolf"",""Argonne National Laboratory, Lemont, IL, USA"; Technische Universität Darmstadt, Darmstadt, Germany; Technische Universität Darmstadt, Darmstadt, Germany; ETH Zürich, Zürich, Switzerland; Jülich Supercomputing Centre, Jülich, Germany;" Technische Universität Darmstadt, Darmstadt, Germany"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1768"",""1785"",""Many libraries in the HPC field use sophisticated algorithms with clear theoretical scalability expectations. However, hardware constraints or programming bugs may sometimes render these expectations inaccurate or even plainly wrong. While algorithm and performance engineers have already been advocating the systematic combination of analytical performance models with practical measurements for a very long time, we go one step further and show how this comparison can become part of automated testing procedures. The most important applications of our method include initial validation, regression testing, and benchmarking to compare implementation and platform alternatives. Advancing the concept of performance assertions, we verify asymptotic scaling trends rather than precise analytical expressions, relieving the developer from the burden of having to specify and maintain very fine-grained and potentially non-portable expectations. In this way, scalability validation can be continuously applied throughout the whole development cycle with very little effort. Using MPI and parallel sorting algorithms as examples, we show how our method can help uncover non-obvious limitations of both libraries and underlying platforms."",""1558-2183"","""",""10.1109/TPDS.2019.2896993"",""Deutsche Forschungsgemeinschaft"; Swiss US National Science Foundation; German Federal Ministry of Education and Research(grant numbers:01IH16008D); U.S. Department of Energy(grant numbers:DE-SC0015524);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632716"",""Software engineering";high performance computing;parallel programming;performance analysis;"performance modeling"",""Libraries";Scalability;Measurement;Analytical models;Benchmark testing;"Adaptation models"","""",""4"","""",""58"",""IEEE"",""1 Feb 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"ERPOT: A Quad-Criteria Scheduling Heuristic to Optimize Execution Time, Reliability, Power Consumption and Temperature in Multicores,""a. abdi"; A. Girault;" H. R. Zarandi"",""Department of Computer Engineering and Information Technology, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran"; Universit Grenoble Alpes, LIG, Grenoble, France;" Department of Computer Engineering and Information Technology, Amirkabir University of Technology (Tehran Polytechnic), Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2193"",""2210"",""We investigate multi-criteria optimization and Pareto front generation. Given an application modeled as a Directed Acyclic Graph (DAG) of tasks and a multicore architecture, we produce a set of non-dominated (in the Pareto sense) static schedules of this DAG onto this multicore. The criteria we address are the execution time, reliability, power consumption, and peak temperature. These criteria exhibit complex antagonistic relations, which make the problem challenging. For instance, improving the reliability requires adding some redundancy in the schedule, which penalizes the execution time. To produce Pareto fronts in this 4-dimension space, we transform three of the four criteria into constraints (the reliability, the power consumption, and the peak temperature), and we minimize the fourth one (the execution time of the schedule) under these three constraints. By varying the thresholds used for the three constraints, we are able to produce a Pareto front of non-dominated solutions. We propose two algorithms to compute static schedules. The first is a ready list scheduling heuristic called Execution time, Reliability, POwer consumption and Temperature (ERPOT). ERPOT actively replicates the tasks to increase the reliability, uses Dynamic Voltage and Frequency Scaling to decrease the power consumption, and inserts cooling times to control the peak temperature. The second algorithm uses an Integer Linear Programming (ILP) program to compute an optimal schedule. However, because our multi-criteria scheduling problem is NP-complete, the ILP algorithm is limited to very small problem instances. Comparisons showed that the schedules produced by ERPOT are on average only 10 percent worse than the optimal schedules computed by the ILP program, and that ERPOT outperforms the PowerPerf-PET heuristic from the literature on average by 33 percent."",""1558-2183"","""",""10.1109/TPDS.2019.2906172"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669960"",""Multicore static scheduling";reliability;temperature;power consumption;multi-objective optimization;"pareto front"",""Power demand";Reliability;Schedules;Task analysis;Multicore processing;Optimization;"Scheduling"","""",""12"","""",""45"",""IEEE"",""19 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Explicit Data Correlations-Directed Metadata Prefetching Method in Distributed File Systems,""Y. Chen"; C. Li; M. Lv; X. Shao; Y. Li;" Y. Xu"",""Department of Computer Science and Technology, University of Science and Technology of China, Hefei, China"; Department of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Department of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Department of Computer Science and Technology, University of Science and Technology of China, Hefei, China; Department of Computer Science and Technology, University of Science and Technology of China, Hefei, China;" Department of Computer Science and Technology, University of Science and Technology of China, Hefei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2692"",""2705"",""Metadata performance in distributed file systems (DFS) is critical, due to the following trends: (a) the growing size of modern storage systems is expected to exceed billions of files and most files are small";" (b) over half of the file accesses are metadata operations. In this work, we present SMeta, a metadata prefetching method that is seamlessly integrated into DFS for easy-of-use and significantly scales the metadata performance. Previous prefetching proposals primarily focus on mining groups of files that tend to be accessed together from the access history. Nevertheless, our study discovered that these solutions likely miss a huge number of correlated files whose co-occurrence frequency is not high enough. Unlike access correlations, we take a novel and completely different approach to explore explicit data correlations by understanding the reference relationships between files encoded in some forms of hyperlinks, which naturally exist in many applications. To embrace this new concept, SMeta explores correlations upon files are written via a light-weight pattern matching algorithm, stores correlations in the reserved extended attributes of file metadata to avoid changes in DFS APIs, and collapses multiple I/O rounds for accessing metadata of the target file and its data-correlated files into one round. A cost-efficient adaptive feedback mechanism is introduced to improve prefetching accuracy. We implemented SMeta atop of Ceph and evaluated it using synthetic and real system workloads. Compared to baselines, SMeta provides better metadata performance in terms of latency, throughput and scalability."",""1558-2183"","""",""10.1109/TPDS.2019.2921760"",""National Key R&D Program of China(grant numbers:2018YFB1003204)"; National Natural Science Foundation of China(grant numbers:61772486,61832011,61802358,61772484);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733006"",""Distributed file system";metadata performance;prefetching;"data correlations"",""Data processing";Metadata;Prefetching;Distributed management;File systems;Data mining;"Servers"","""",""15"","""",""54"",""IEEE"",""10 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Exploiting GPUs for Efficient Gradient Boosting Decision Tree Training,""Z. Wen"; J. Shi; B. He; J. Chen; K. Ramamohanarao;" Q. Li"",""SoC, National University of Singapore, Singapore"; South China University of Technology, Guangzhou, China; SoC, National University of Singapore, Singapore; South China University of Technology, Guangzhou, China; The University of Melbourne, Parkville, Australia;" SoC, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2706"",""2717"",""In this paper, we present a novel parallel implementation for training Gradient Boosting Decision Trees (GBDTs) on Graphics Processing Units (GPUs). Thanks to the excellent results on classification/regression and the open sourced libraries such as XGBoost, GBDTs have become very popular in recent years and won many awards in machine learning and data mining competitions. Although GPUs have demonstrated their success in accelerating many machine learning applications, it is challenging to develop an efficient GPU-based GBDT algorithm. The key challenges include irregular memory accesses, many sorting operations with small inputs and varying data parallel granularities in tree construction. To tackle these challenges on GPUs, we propose various novel techniques including (i) Run-length Encoding compression and thread/block workload dynamic allocation, (ii) data partitioning based on stable sort, and fast and memory efficient attribute ID lookup in node splitting, (iii) finding approximate split points using two-stage histogram building, (iv) building histograms with the aware of sparsity and exploiting histogram subtraction to reduce histogram building workload, (v) reusing intermediate training results for efficient gradient computation, and (vi) exploiting multiple GPUs to handle larger data sets efficiently. Our experimental results show that our algorithm named ThunderGBM can be 10x times faster than the state-of-the-art libraries (i.e., XGBoost, LightGBM and CatBoost) running on a relatively high-end workstation of 20 CPU cores. In comparison with the libraries on GPUs, ThunderGBM can handle higher dimensional problems which the libraries become extremely slow or simply fail. For the data sets the existing libraries on GPUs can handle, ThunderGBM achieves up to 10 times speedup on the same hardware, which demonstrates the significance of our GPU optimizations. Moreover, the models trained by ThunderGBM are identical to those trained by XGBoost, and have similar quality as those trained by LightGBM and CatBoost."",""1558-2183"","""",""10.1109/TPDS.2019.2920131"",""MoE AcRF Tier 1(grant numbers:T1 251RES1824)"; Tier 2(grant numbers:MOE2017-T2-1-122); Guangdong special branch plans young talent with scientific and technological innovation(grant numbers:2016TQ03X445); Guangzhou science and technology planning project(grant numbers:201904010197);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8727750"",""Graphics processing units";gradient boosting decision trees;"machine learning"",""Graphics processing units";Decision trees;Training;Histograms;Machine learning;"Instruction sets"","""",""28"","""",""34"",""IEEE"",""31 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Exploiting Hardware Multicast and GPUDirect RDMA for Efficient Broadcast,""C. -H. Chu"; X. Lu; A. A. Awan; H. Subramoni; B. Elton;" D. K. Panda"",""Department of Computer Science and Engineering, Ohio State University, Columbus, OH, USA"; Department of Computer Science and Engineering, Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, Ohio State University, Columbus, OH, USA; Department of Computer Science and Engineering, Ohio State University, Columbus, OH, USA; Engility Corporation, WPAFB, Dayton, OH, USA;" Department of Computer Science and Engineering, Ohio State University, Columbus, OH, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""575"",""588"",""Broadcast is a widely used operation in many streaming and deep learning applications to disseminate large amounts of data on emerging heterogeneous High-Performance Computing (HPC) systems. However, traditional broadcast schemes do not fully utilize hardware features for Graphics Processing Unit (GPU)-based applications. In this paper, a model-oriented analysis is presented to identify performance bottlenecks of existing broadcast schemes on GPU clusters. Next, streaming-based broadcast schemes are proposed to exploit InfiniBand hardware multicast (IB-MCAST) and NVIDIA GPUDirect technology for efficient message transmission. The proposed designs are evaluated in the context of using Message Passing Interface (MPI) based benchmarks and applications. The experimental results indicate improved scalability and up to 82 percent reduction of latency compared to the state-of-the-art solutions in the benchmark-level evaluation. Furthermore, compared to the state-of-the-art, the proposed design yields stable higher throughput for a synthetic streaming workload, and 1.3x faster training time for a deep learning framework."",""1558-2183"","""",""10.1109/TPDS.2018.2867222"",""U.S. Department of Defense(grant numbers:#GS04T09DBC0017)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8447246"",""Broadcast";deep learning;hardware multicast;GPU;GPUDirect RDMA;heterogeneous broadcast;"streaming"",""Graphics processing units";Hardware;Analytical models;Machine learning;Clustering algorithms;Scalability;"Bandwidth"","""",""9"","""",""43"",""IEEE"",""26 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Exploiting Parallelism for CNN Applications on 3D Stacked Processing-In-Memory Architecture,""Y. Wang"; W. Chen; J. Yang;" T. Li"",""College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China"; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; Experimental and Innovation Practice Center, Harbin Institute of Technology, Shenzhen, China;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""589"",""600"",""Deep convolutional neural networks (CNNs) are widely adopted in intelligent systems with unprecedented accuracy but at the cost of a substantial amount of data movement. Although the emerging processing-in-memory (PIM) architecture seeks to minimize data movement by placing memory near processing elements, memory is still the major bottleneck in the entire system. The selection of hyper-parameters in the training of CNN applications requires over hundreds of kilobytes cache capacity for concurrent processing of convolutions. How to jointly explore the computation capability of the PIM architecture and the highly parallel property of neural networks remains a critical issue. This paper presents Para-Net, that exploits Parallelism for deterministic convolutional neural Networks on the PIM architecture. Para- Net achieves data-level parallelism for convolutions by fully utilizing the on-chip processing engine (PE) in PIM. The objective is to capture the characteristics of neural networks and present a hardware-independent design to jointly optimize the scheduling of both intermediate results and computation tasks. We formulate this data allocation problem as a dynamic programming model and obtain an optimal solution. To demonstrate the viability of the proposed Para-Net, we conduct a set of experiments using a variety of realistic CNN applications. The graph abstractions are obtained from deep learning framework Caffe. Experimental results show that Para-Net can significantly reduce processing time and improve cache efficiency compared to representative schemes."",""1558-2183"","""",""10.1109/TPDS.2018.2868062"",""National Natural Science Foundation of China(grant numbers:61502309,61702357)"; Guangdong Natural Science Foundation(grant numbers:2016A030313045,2017B030314073); Shenzhen Science and Technology Foundation(grant numbers:JCYJ20150529164656096,JCYJ20170302153955969,JCYJ20170817100300603,JCYJ20170818100006280,JCYJ20170816093943197); Guangdong Provincial General University National Development Program(grant numbers:2014GKXM054); Natural Science Foundation of SZU(grant numbers:803/000026080154,827-000073); Natural Science Foundation of Tianjin City(grant numbers:18JCQNJC00300); Institute of Computing Technology, Chinese Academy of Sciences(grant numbers:CARCH201608);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8452983"",""Near-data processing";neuromorphic computing;scheduling;memory management;"parallel computing"",""Computer architecture";Task analysis;Parallel processing;Neural networks;Random access memory;Schedules;"Three-dimensional displays"","""",""16"","""",""43"",""IEEE"",""31 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Exploring Fault-Tolerant Erasure Codes for Scalable All-Flash Array Clusters,""S. Koh"; J. Zhang; M. Kwon; J. Yoon; D. Donofrio; N. S. Kim;" M. Jung"",""Computer Architecture and Memory Systems Laboratory, Yonsei University, Seoul, Korea"; Computer Architecture and Memory Systems Laboratory, Yonsei University, Seoul, Korea; Computer Architecture and Memory Systems Laboratory, Yonsei University, Seoul, Korea; SK Telecom, Seoul, Korea; Lawrence Berkeley National Laboratory, Berkeley, CA, USA; University of Illinois Urbana-Champaign, Champaign, IL, USA;" Computer Architecture and Memory Systems Laboratory, Yonsei University, Seoul, Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""15 May 2019"",""2019"",""30"",""6"",""1312"",""1330"",""Large-scale systems with all-flash arrays have become increasingly common in many computing segments. To make such systems resilient, we can adopt erasure coding such as Reed-Solomon (RS) code as an alternative to replication because erasure coding incurs a significantly lower storage overhead than replication. To understand the impact of using erasure coding on the system performance and other system aspects such as CPU utilization and network traffic, we build a storage cluster that consists of approximately 100 processor cores with more than 50 high-performance solid-state drives (SSDs), and evaluate the cluster with a popular open-source distributed parallel file system, called Ceph. Specifically, we analyze the behaviors of a system adopting erasure coding from the following five viewpoints, and compare with those of another system using replication: (1) storage system I/O performance"; (2) computing and software overheads; (3) I/O amplification;" (4) network traffic among storage nodes, and (5) impact of physical data layout on performance of RS-coded SSD arrays. For all these analyses, we examine two representative RS configurations, used by Google file systems, and compare them with triple replication employed by a typical parallel file system as a default fault tolerance mechanism. Lastly, we collect 96 block-level traces from the cluster and release them to the public domain for the use of other researchers."",""1558-2183"","""",""10.1109/TPDS.2018.2884722"",""NRF(grant numbers:2016R1C1B2015312)"; Yonsei Future Research(grant numbers:2017-22-0105,IITP-2017-2017-0-01015,NRF-2015M3C4A7065645); DOE(grant numbers:DE-AC02-05CH 11231); MemRay(grant numbers:2015-11-1731); NSF(grant numbers:1640196,SRC/NRC NERC 2016-NE-2697-A);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8565994"",""Distributed system";SSD array system;fault tolerance mechanism;erasure coding;"replication"",""Encoding";Arrays;Fault tolerance;Fault tolerant systems;Distributed databases;"Monitoring"","""",""3"","""",""40"",""IEEE"",""6 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"Exploring GPU-Accelerated Routing for FPGAs,""M. Shen"; G. Luo;" N. Xiao"",""School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China"; Center for Energy-Efficient Computing and Applications, Peking University, Beijing, China;" School of Data and Computer Science, Sun Yat-Sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1331"",""1345"",""Field Programmable Gate Arrays (FPGAs) are reconfigurable architectures able to provide a good balance between energy efficiency and flexibility with respect to CPUs and ASICs. The main drawback in using FPGAs, however, is their timing-consuming routing process, significantly hindering the designer productivity. An emerging solution to this problem is to accelerate the routing by parallelization. Existing attempts of parallelizing the FPGA routing either do not fully exploit the parallelism or suffer from an excessive quality loss. Massive parallelism using GPUs has the potential to solve this issue but faces non-trivial challenges. To cope with these challenges, this paper explores GPU-accelerated routing approach for FPGAs. We leverage the idea of problem size reduction by limiting the single-net routing in a small subgraph rather than in an entire graph, further enabling the GPU-friendly shortest path algorithm to be used in FPGA routing. We maintain the convergence after problem size reduction by using the dynamic expansion of the routing resource subgraph, where the routing region of subgraph will be progressively expanded to find a feasible solution to each net. In addition, we are based on a GPU platform to explore the fine-grained single-net parallel routing in three ways and propose a hybrid approach to combine the static and dynamic parallelization for better speedup in FPGA routing. To explore the coarse-grained multi-net parallelization, We propose a dynamic programming-based partitioning algorithm to parallelize the routing of multiple nets while generating the equivalent routing results as the original single-net routing. Experimental results show that our proposed approach can provide an average of about 21.53× speedup on a single GPU with a tolerable loss in the routing quality and maintain a scalable speedup on large-scale routing resource graphs. To our knowledge, this is the first work to demonstrate the effectiveness of GPU-accelerated routing for FPGAs."",""1558-2183"","""",""10.1109/TPDS.2018.2885745"",""National Natural Science Foundation of China(grant numbers:61433019,61802446)"; Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2016ZT06D211);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8567949"",""Hardware";reconfigurable architectures;FPGAs;routing;"GPU parallelization"",""Routing";Field programmable gate arrays;Heuristic algorithms;Parallel processing;Graphics processing units;Acceleration;"Nickel"","""",""5"","""",""43"",""IEEE"",""7 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Exploring Properties and Correlations of Fatal Events in a Large-Scale HPC System,""S. Di"; H. Guo; R. Gupta; E. R. Pershey; M. Snir;" F. Cappello"",""Argonne National Laboratory, Mathematics and Computer Science (MCS), Lemont, IL, USA"; Argonne National Laboratory, Mathematics and Computer Science (MCS), Lemont, IL, USA; Argonne National Laboratory, Mathematics and Computer Science (MCS), Lemont, IL, USA; Argonne National Laboratory, Mathematics and Computer Science (MCS), Lemont, IL, USA; Department of Computer Science, University of Illinois Urbana-Champaign, Champaign, IL, USA;" Argonne National Laboratory, Mathematics and Computer Science (MCS), Lemont, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""361"",""374"",""In this paper, we explore potential correlations of fatal system events for one of the most powerful supercomputers-IBM Blue Gene/Q Mira, which is deployed at Argonne National Laboratory, based on its 5-year reliability, availability, and serviceability (RAS) log. Our contribution is two-fold. (1) We design an efficient log analysis tool, namely LogAider, with a novel filtering method to effectively extract fatal events from masses of system messages that are heavily duplicated in the log. LogAider exhibits a very precise detection of temporal-correlation with a high similarity (up to 95 percent) to the ground-truth (i.e., compared to the failure records reported by the administrators). The total number of fatal events can be reduced to about 1,255 compared with originally 2.6 million duplicated fatal messages. (2) We analyze the 5-year RAS log of the MIRA system using LogAider, and summarize six important “takeaways” which can help system vendors and administrators better understand an extreme-scale system's fatal events. Specifically, we find that the distribution or proportion of the fatal system events follow a Pareto-like principle in general. The temporal correlation among fatal events is much stronger than that of warn messages and info messages, and the correlated events tend to constitute a few clusters. The mean time between fatal events (MTBFE) of the Mira system is about 1.3 days from the perspective of the system, and the MTTI is 2-4 days from the perspective of users. The most error-prone item value with respect to any key attribute appears likely in the log every 2-10 days. Weibull, Gamma, and Pearson6 are the three best-fit distributions for the fatal event intervals. The overall correlation of fatal events on the 5D torus network is not prominent, whereas the small-region locality correlation (e.g., the fatal events inside racks) is relatively strong. We believe our work will be interesting to large-scale HPC system administrators and vendors and to fault tolerance researchers, enabling them to better understand fatal events and mitigate such events accordingly."",""1558-2183"","""",""10.1109/TPDS.2018.2864184"",""U.S. Department of Energy(grant numbers:DE-AC02-06CH11357)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8436427"",""Peta-scale supercomputer";mining correlations;fatal event analysis;"reliability-availability-serviceability (RAS)"",""Correlation";Supercomputers;Tools;Reliability;Random access memory;Brain modeling;"Computer science"","""",""13"","""",""38"",""IEEE"",""14 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Fast AES Implementation: A High-Throughput Bitsliced Approach,""O. Hajihassani"; S. K. Monfared; S. H. Khasteh;" S. Gorgin"",""Research in Fundamental Sciences (IPM)"; K. N. Toosi University of Technology, Tehran, Iran; Faculty of Computer Engineering, K. N. Toosi University of Technology, Tehran, Iran;" Electrical Engineering and Information Technology Department of Iranian Research Organization for Science and Technology (IROST), Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2211"",""2222"",""In this work, a high-throughput bitsliced AES implementation is proposed, which builds upon a new data representation scheme that exploits the parallelization capability of modern multi/many-core platforms. This representation scheme is employed as a building block to redesign all of the AES stages to tailor them for multi/many-core AES implementation. With the proposed bitsliced approach, each parallelization unit processes an unprecedented number of thirty-two 128-bit input data. Hence, a high order of prallelization is achieved by the proposed implementation technique. Based on the characteristics of this new implementation model, the ShiftRows stage can be implicitly handled through input rearrangement and is simplified to the point where its computing process can be neglected. In this implementation, costly Byte-wise operations are performed through register shift and swapping. In addition, the need for look-up table based I/O operations, which are used by the Substitute Bytes stage is eliminated through using S-box logic circuit. The S-box logic circuit is optimized to simultaneously process 32 chunks of 128-bit input data. We develop high-throughput CTR and ECB AES encryption/decryption on 6 CUDA-enabled GPUs, which achieve 1.47 and 1.38 Tbps of encryption throughput on Tesla V100 GPU, respectively."",""1558-2183"","""",""10.1109/TPDS.2019.2911278"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691582"",""AES";CTR;ECB;GPU;data representation;CUDA;"high-performance"",""Encryption";Table lookup;Graphics processing units;Throughput;Standards;"Data models"","""",""30"","""",""39"",""IEEE"",""14 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Fast and Communication-Efficient Algorithm for Distributed Support Vector Machine Training,""J. Dass"; V. Sarin;" R. N. Mahapatra"",""Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA"; Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA;" Department of Computer Science and Engineering, Texas A&M University, College Station, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1065"",""1076"",""Support Vector Machines (SVM) are widely used as supervised learning models to solve the classification problem in machine learning. Training SVMs for large datasets is an extremely challenging task due to excessive storage and computational requirements. To tackle so-called big data problems, one needs to design scalable distributed algorithms to parallelize the model training and to develop efficient implementations of these algorithms. In this paper, we propose a distributed algorithm for SVM training that is scalable and communication-efficient. The algorithm uses a compact representation of the kernel matrix, which is based on the QR decomposition of low-rank approximations, to reduce both computation and storage requirements for the training stage. This is accompanied by considerable reduction in communication required for a distributed implementation of the algorithm. Experiments on benchmark data sets with up to five million samples demonstrate negligible communication overhead and scalability on up to 64 cores. Execution times are vast improvements over other widely used packages. Furthermore, the proposed algorithm has linear time complexity with respect to the number of samples making it ideal for SVM training on decentralized environments such as smart embedded systems and edge-based internet of things, IoT."",""1558-2183"","""",""10.1109/TPDS.2018.2879950"",""Texas A&M High Performance Research Computing.";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8526323"",""Machine learning";support vector machines;classification algorithms;parallel programming;distributed computing;message passing;quadratic programming;iterative algorithms;optimization;"multicore processing"",""Support vector machines";Kernel;Training;Matrix decomposition;Optimization;Benchmark testing;"Convergence"","""",""18"","""",""24"",""IEEE"",""7 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Fast Deep Neural Network Training on Distributed Systems and Cloud TPUs,""Y. You"; Z. Zhang; C. -J. Hsieh; J. Demmel;" K. Keutzer"",""Computer Science Division, University of California at Berkeley, Berkeley, CA, USA"; Texas Advanced Computing Center, University of Texas at Austin, Austin, TX, USA; Department of Computer Science, University of California at Los Angeles, Los Angeles, CA, USA; Computer Science Division, University of California at Berkeley, Berkeley, CA, USA;" Computer Science Division, University of California at Berkeley, Berkeley, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2449"",""2462"",""Since its creation, the ImageNet-1k benchmark set has played a significant role as a benchmark for ascertaining the accuracy of different deep neural net (DNN) models on the image classification problem. Moreover, in recent years it has also served as the principal benchmark for assessing different approaches to DNN training. Finishing a 90-epoch ImageNet-1k training with ResNet-50 on a NVIDIA M40 GPU takes 14 days. This training requires 1018 single precision operations in total. On the other hand, the world's current fastest supercomputer can finish 3 x 1017 single precision operations per second (according to the Nov 2018 Top 500 results). If we can make full use of the computing capability of the fastest supercomputer, we should be able to finish the training in several seconds. Over the last two years, researchers have focused on closing this significant performance gap through scaling DNN training to larger numbers of processors. Most successful approaches to scaling ImageNet training have used the synchronous minibatch stochastic gradient descent (SGD). However, to scale synchronous SGD one must also increase the batch size used in each iteration. Thus, for many researchers, the focus on scaling DNN training has translated into a focus on developing training algorithms that enable increasing the batch size in data-parallel synchronous SGD without losing accuracy over a fixed number of epochs. In this paper, we investigate supercomputers' capability of speeding up DNN training. Our approach is to use a large batch size, powered by the Layer-wise Adaptive Rate Scaling (LARS) algorithm, for efficient usage of massive computing resources. Our approach is generic, as we empirically evaluate the effectiveness on five neural networks: AlexNet, AlexNet-BN, GNMT, ResNet-50, and ResNet-50-v2 trained with large datasets while preserving the state-of-the-art test accuracy. Compared to the baseline of a previous study from Goyal et al. [1], our approach shows higher test accuracy on batch sizes that are larger than 16K.When we use the same baseline, our results are better than Goyal et al. for all the batch sizes (Fig. 20). Using 2,048 Intel Xeon Platinum 8160 processors, we reduce the 100-epoch AlexNet training time from hours to 11 minutes. With 2,048 Intel Xeon Phi 7250 Processors, we reduce the 90-epoch ResNet-50 training time from hours to 20 minutes. Our implementation is open source and has been released in the Intel distribution of Caffe, Facebook's PyTorch, and Google's TensorFlow. The difference between this paper and the conference-version of our work [2] includes: (1) we implement our approach on Google's cloud Tensor Processing Unit (TPU) platform, which verifies our previous success on CPUs and GPUs. (2) we scale the batch size of ResNet-50-v2 to 32K and achieve 76.3 percent accuracy, which is better than the 75.3 percent accuracy achieved in our conference paper. (3) we apply our approach to Google's Neural Machine Translation (GNMT) application, which helps us to achieves 4x speedup on the cloud TPUs."",""1558-2183"","""",""10.1109/TPDS.2019.2913833"",""National Science Foundation(grant numbers:OAC-1540931)"; U.S. DOE Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program(grant numbers:DE-SC0010200); DARPA(grant numbers:HR0011-12- 2-0016); ASPIRE Lab industrial sponsors and affiliates Intel; Google; HP; Huawei Technologies; LG Display; Nokia; Nvidia; Oracle; Samsung;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8703162"",""Fast algorithm";deep learning;"parallel & distributed processing"",""Training";Program processors;Google;Parallel processing;Synchronization;Neural networks;"Supercomputers"","""",""36"","""",""36"",""IEEE"",""30 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"Fast Wait-Free Construction for Pool-Like Objects with Weakened Internal Order: Stacks as an Example,""Y. Peng"; X. Yun;" Z. Hao"",""Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"; Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China;" Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2019"",""2019"",""30"",""7"",""1596"",""1612"",""This paper focuses on a large class of concurrent data structures that we call pool-like objects (e.g., stack, double-ended queue, and queue). Performance and progress guarantee are two important characteristics for concurrent data structures. In the aspect of performance, weakening the internal order in a pool-like object is an effective technique to reduce the synchronization cost among threads accessing the object, but no objects with weakened internal order provide a progress guarantee as strong as wait-freedom. Meanwhile, wait-free algorithms tend to be inefficient, which is mainly attributed to the helping mechanisms. Based on the philosophy of existing helping mechanisms, a wait-free pool-like object with weakened internal order would suffer from unnecessary process of getting the latest object state and synchronization. This paper takes a state-of-the-art implementation of stacks with weakened internal order as an example, and transforms it into a highly-efficient wait-free stack named WF-TS-Stack. The transformation method includes a helping mechanism with state reuse and a relaxed removal scheme. In addition, we use a simple and effective scheme to further improve the performance of WF-TS-Stack in Non-Uniform Memory Access (NUMA) architectures. Our evaluation with representative benchmarks shows that WF-TS-Stack outperforms its original building blocks by up to 1.45× at maximum concurrency. We also discuss how to yield an efficient double-ended queue (deque) variant of WF-TS-Stack, because deque is a more generalized pool-like object."",""1558-2183"","""",""10.1109/TPDS.2018.2889048"",""National Natural Science Foundation of China(grant numbers:61702499)"; National Key Research and Development Program of China(grant numbers:2016QY04W0804); Beijing Municipal Natural Science Foundation(grant numbers:4172069);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8585072"",""Concurrent data structure";stack;double-ended queue;performance;"wait-freedom"",""Data structures";Message systems;Synchronization;Multicore processing;Transforms;Benchmark testing;"Concurrent computing"","""",""2"","""",""22"",""IEEE"",""21 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Functional and Contextual Attention-Based LSTM for Service Recommendation in Mashup Creation,""M. Shi"; y. tang;" J. Liu"",""Department of Computer & Electrical Engineering and Computer Science, Florida Atlantic University, FL, USA"; Department of Computer & Electrical Engineering and Computer Science and the Institute for Sensing and Embedded Network Systems Engineering, Florida Atlantic University, Boca Raton, FL, USA;" School of Computer Science and Engineering, Hunan University of Science and Technology, Xiangtan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1077"",""1090"",""Service recommendation is a fundamental task in many application environments (e.g., Mashup creation and cloud computing). In the past, various methods have been proposed to facilitate the service selection process based on the original functional descriptions. However, the mined features from the descriptions are usually too sparse for training a well-performed model. In addition, most methods neglect to differentiate the weights of various features, while words included in descriptions usually exhibit different intentions (e.g., functional or non-functional). To address these challenges, in this paper we propose a text expansion and deep model-based approach for service recommendation. Specifically, we first expand the description of services at sentence level based on a novel probabilistic topic model that learns topics of words, sentences and descriptions in a stratified fashion. The expansion process can bridge the vocabulary gap between services and user queries with the collective semantic similarity of sentences and descriptions. Then, we propose a Long Short-Term Memory-based model to recommend services with two attention mechanisms - a functional attention mechanism that takes tags as functional prior to mine the function-related features of services and Mashups, and a contextual attention mechanism that considers Mashup requirements as application scenario to help select the most appropriate services. We evaluate the proposed approach on a real-world dataset and the results show it has an improvement of 34 percent in F-measure over the basic LSTM model."",""1558-2183"","""",""10.1109/TPDS.2018.2877363"",""Florida Center for Cybersecurity(grant numbers:FC2)"; Walter and Lalita Janke Innovations in Sustainability Science Research Fund(grant numbers:N_PolyU519/12); National Natural Science Foundation of China(grant numbers:61872139,61572187);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8502148"",""Web service";mashup;recommendation;description expansion;"attention mechanism"",""Mashups";Quality of service;Feature extraction;Vocabulary;Bicycles;"Semantics"","""",""52"","""",""48"",""IEEE"",""21 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"GCache: Neighborhood-Guided Graph Caching in a Distributed Environment,""Y. Yuan"; X. Lian; L. Chen; G. Wang; J. X. Yu; Y. Wang;" Y. M. Ma"",""School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China"; Department of Computer Science, Kent State University, Kent, OH, USA; CSE, HKUST, Hong Kong, China; School of Computer Science and Technology, Beijing Institute of Technology, Beijing, China; Systems Engineering and Engineering Management, Chinese University of Hong Kong, Hong Kong, China; School of Computer Science and Engineering, Northeastern University, Shenyang, China;" School of Computer Science and Engineering, Northeastern University, Shenyang, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2463"",""2477"",""Distributed graph systems are becoming extremely popular due to their flexibility, scalability, and robustness in big graph processing. In order to improve the performance of the distributed graph systems, caching is a very effective technique to achieve fast response and reduce the communication cost. Existing works include online and offline caching algorithms. Online caching algorithms (such as least recently used (LRU) and most recently used (MRU)) are lightweight and flexible, however, neglect the topological properties of big graphs. Offline caching algorithms (such as node pre-ordered) consider the graph topology, but are very expensive and heavy. In this paper, we propose a novel caching mechanism, GraphCache (GCache), for big distributed graphs. GCache consists of an offline phase and an online phase, which inherits the advantages of online and offline caching algorithms. Specifically, the offline phase provides a caching model based on the bipartite graph clustering and give efficient algorithms to solve it. The online phase caches and schedules the graph clusters output from the offline phase, based on the LRU and MRU strategies. GCache can be seamlessly integrated into the state-of-the-art graph processing systems, e.g., Giraph. Finally, our experimental results demonstrate the feasibility of our proposed caching techniques in speeding up graph algorithms over distributed big graphs."",""1558-2183"","""",""10.1109/TPDS.2019.2915300"",""National Natural Science Foundation of China(grant numbers:61572119,61622202)"; Fundamental Research Funds for the Central Universities(grant numbers:N181605012); NSF(grant numbers:1739491); Lian Start Up(grant numbers:220981); Kent State University; NSFC(grant numbers:61672145,61732003); Hong Kong RGC GRF(grant numbers:16214716); NSFC(grant numbers:61729201); Hong Kong ITC ITF(grant numbers:ITS/391/15FX); Research Grants Council of Hong Kong, China(grant numbers:14203618,14221716);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8708952"",""Distributed caching";"large graph"",""Clustering algorithms";Computational modeling;Bipartite graph;Schedules;Partitioning algorithms;Scalability;"Robustness"","""",""4"","""",""50"",""IEEE"",""7 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"Geometric Mapping of Tasks to Processors on Parallel Computers with Mesh or Torus Networks,""M. Deveci"; K. D. Devine; K. Pedretti; M. A. Taylor; S. Rajamanickam;" Ü. V. Çatalyürek"",""Google, Mountain View, CA"; Sandia National Laboratories, Center for Computing Research, Albuquerque, NM, USA; Sandia National Laboratories, Center for Computing Research, Albuquerque, NM, USA; Sandia National Laboratories, Center for Computing Research, Albuquerque, NM, USA; Sandia National Laboratories, Center for Computing Research, Albuquerque, NM, USA;" School of Computational Science and Engineering, Georgia Institute of Technology, Atlanta, GA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""2018"",""2032"",""We present a new method for reducing parallel applications' communication time by mapping their MPI tasks to processors in a way that lowers the distance messages travel and the amount of congestion in the network. Assuming geometric proximity among the tasks is a good approximation of their communication interdependence, we use a geometric partitioning algorithm to order both the tasks and the processors, assigning task parts to the corresponding processor parts. In this way, interdependent tasks are assigned to “nearby” cores in the network. We also present a number of algorithmic optimizations that exploit specific features of the network or application to further improve the quality of the mapping. We specifically address the case of sparse node allocation, where the nodes assigned to a job are not necessarily located in a contiguous block nor within close proximity to each other in the network. However, our methods generalize to contiguous allocations as well, and results are shown for both contiguous and non-contiguous allocations. We show that, for the structured finite difference mini-application MiniGhost, our mapping methods reduced communication time up to 75 percent relative to MiniGhost's default mapping on 128K cores of a Cray XK7 with sparse allocation. For the atmospheric modeling code E3SM/HOMME, our methods reduced communication time up to 31% on 16K cores of an IBM BlueGene/Q with contiguous allocation."",""1558-2183"","""",""10.1109/TPDS.2019.2900043"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8666156"",""Task mapping";geometric partitioning;spatial partitioning;recursive bisection;jagged partitioning;"load balancing"",""Task analysis";Resource management;Program processors;Network topology;Partitioning algorithms;Measurement;"Bandwidth"","""",""4"","""",""43"",""IEEE"",""12 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"GPU-Job Migration: The rCUDA Case,""J. Prades";" F. Silla"",""Departament d'Informàtica de Sistemes i Computadors, Universitat Politècnica de València, Camino de Vera s/n, Valencia, Spain";" Departament d'Informàtica de Sistemes i Computadors, Universitat Politècnica de València, Camino de Vera s/n, Valencia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2718"",""2729"",""Virtualization techniques have been shown to report benefits to data centers and other computing facilities. In this regard, not only virtual machines allow to reduce the size of the computing infrastructure while increasing overall resource utilization, but also virtualizing individual components of computers may provide significant benefits. This is the case, for instance, for the remote GPU virtualization technique, implemented in several frameworks during the recent years. The large degree of flexibility provided by the remote GPU virtualization technique can be further increased by applying the migration mechanism to it, so that the GPU part of applications can be live-migrated to another GPU elsewhere in the cluster during execution time in a transparent way. In this paper we present the implementation of the migration mechanism within the rCUDA remote GPU virtualization middleware. Furthermore, we present a thorough performance analysis of the implementation of the migration mechanism within rCUDA. To that end, we leverage both synthetic and real production applications as well as three different generations of NVIDIA GPUs. Additionally, two different versions of the InfiniBand interconnect are used in this study. Several use cases are provided in order to show the extraordinary benefits that the GPU-job migration mechanism can report to data centers."",""1558-2183"","""",""10.1109/TPDS.2019.2924433"",""Generalitat Valenciana(grant numbers:PROMETEO/2017/77)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744256"",""CUDA";GPU;virtualization;migration;"rCUDA"",""Graphics processing units";Virtualization;Virtualization;Middleware;Proposals;Virtual machining;"Resource management"","""",""8"","""",""24"",""IEEE"",""24 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Graph Filter: Enabling Efficient Topology Calibration,""L. Luo"; D. Guo; J. Xu;" X. Luo"",""Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Hunan, China"; Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha, P. R. China; School of Computer, Electronics and Information, Guangxi University, Guilin, China;" Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Hunan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2730"",""2742"",""The topology of a network may change inevitably, due to dynamic behaviors of nodes and links, and failures of hardware and software. Many protocols and applications must be aware of the up-to-date topology of the underlying network. This triggers the topology calibration problem, which means to deduce those different nodes and links between two topologies. The Bloom filter and its variants are efficient to represent and calibrate two general sets. They, however, fail to represent all links and nodes in a topology simultaneously, and thus remain inapplicable to the topology calibration problem. In this paper, we design the graph filter, a novel space-efficient data structure to record not only the node set but also the link set of any given topology. Accordingly, given two topologies we aim to represent them via two respective graph filters, and thereafter deduce those different links in an invertible manner. To this end, we design three essential operations for graph filter, i.e., encoding, subtracting and decoding. Although such operations are sufficient to solve the topology calibration problem, two challenging issues still remain open. First, the XOR traps which occur with low probability at the encoding stage may result in a few miscalculations at the decoding stage. Thus, we propose another augmented decoding algorithm to lessen the impact of XOR traps via terminating illegal decodings. Second, several different links may form cycles in the worst case";" hence, we further design a cycle destruction algorithm to make such different links decodable. We implement the graph filter and the associated topology calibration method. Comprehensive evaluations indicate that our method finishes the topology calibration task efficiently with high probability, incurs the least space overhead, and supports invertible decoding reasonably."",""1558-2183"","""",""10.1109/TPDS.2019.2925001"",""Tianjin Science and Technology Foundation(grant numbers:18ZXJMTG00290)"; National Natural Science Foundation of China(grant numbers:61772544);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8747454"",""topology calibration";graph filter;false positive;"false negative"",""Network topology";Calibration;Decoding;Data structures;Encoding;"Computer networks"","""",""1"","""",""31"",""IEEE"",""27 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"GreenDB: Energy-Efficient Prefetching and Caching in Database Clusters,""Y. Zhou"; S. Taneja; C. Zhang;" X. Qin"",""TSYS School of Computer Science, Columbus State University, 4225 University Avenue, Columbus, GA, USA"; Department of Computer Science, Sonoma State University, CA; Department of Computer Science and Software Engineering, Auburn University, AL, USA;" Department of Computer Science and Software Engineering, Auburn University, AL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1091"",""1104"",""In this study, we propose an energy-efficient database system called GreenDB running on clusters. GreenDB applies a workload-skewness strategy by managing hot nodes coupled with a set of cold nodes in a database cluster. GreenDB fetches popular data tables to hot nodes, aiming to keep cold nodes in the low-power mode in increased time periods. GreenDB is conducive to reducing the number of power-state transitions, thereby lowering energy-saving overhead. A prefetching model and an energy saving model are seamlessly integrated into GreenDB to facilitate the power management in database clusters. We quantitatively evaluate GreenDB's energy efficiency in terms of managing, fetching, and storing data. We compare GreenDB's prefetching strategy with the one implemented in Postgresql. Experimental results indicate that GreenDB conserves the energy consumption of the existing solution by up to 98.4 percent. The findings show that the energy efficiency of GreenDB can be optimized by tuning system parameters, including table size, hit rates, number of nodes, number of disks, and inter-arrival delays."",""1558-2183"","""",""10.1109/TPDS.2018.2874014"",""National Science Foundation(grant numbers:IIS-1618669,CNS-0917137,CCF-0845257)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8481569"",""Energy efficiency";prefetching;"energy conservation"",""Database systems";Prefetching;Energy consumption;Data centers;Distributed databases;"Mathematical model"","""",""7"","""",""34"",""IEEE"",""4 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"GVTS: Global Virtual Time Fair Scheduling to Support Strict Fairness on Many Cores,""C. Kim"; S. Choi;" J. Huh"",""Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea"; Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea;" Department of Computer Science, Korea Advanced Institute of Science and Technology, Daejeon, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""79"",""92"",""Proportional fairness in CPU scheduling has been widely adopted to fairly distribute CPU shares corresponding to their weights. With the emergence of cloud environments, the proportionally fair scheduling has been extended to groups of threads or nested groups to support virtual machines or containers. Such proportional fairness has been supported by popular schedulers, such as Linux Completely Fair Scheduler (CFS) through virtual time scheduling. However, CFS, with a distributed runqueue per CPU, implements the virtual time scheduling locally. Across different queues, the virtual times of threads are not strictly maintained to avoid potential scalability bottlenecks. The uneven fluctuation of CPU shares caused by the limitations of CFS not only violates the fairness support for CPU assignments, but also significantly increases the tail latencies of latency-sensitive applications. To mitigate the limitations of CFS, this paper proposes a global virtual-time fair scheduler (GVTS), which enforces global virtual time fairness for threads and thread groups, even if they run across many physical cores. The new scheduler employs the hierarchical enforcement of target virtual time to enhance the scalability of schedulers, which is aware of the topology of CPU organization. We implemented GVTS in Linux kernel 4.6.4 with several optimizations to provide global virtual time efficiently. Our experimental results show that GVTS can almost eliminate the fairness violation of CFS for both non-grouped and grouped executions. Furthermore, GVTS can curtail the tail latency when latency-sensitive applications are co-running with batch tasks."",""1558-2183"","""",""10.1109/TPDS.2018.2851515"",""National Research Foundation of Korea(grant numbers:NRF-2016R1A2B4013352)"; Institute for Information & communications Technology Promotion(grant numbers:IITP-2017-0-00466); Ministry of Science and ICT, Korea;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8400397"",""Proportional Fairness";CPU Scheduling;Group Fairness;"Tail Latency"",""Scheduling";Message systems;Load management;Containers;Cloud computing;Scalability;"Linux"","""","""","""",""25"",""OAPA"",""29 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Hardware Accelerated Semantic Declarative Memory Systems through CUDA and MapReduce,""M. Edmonds"; T. Atahary; S. Douglass;" T. Taha"",""Department of Electrical and Computer Engineering, University of Dayton, Dayton, OH, USA"; Department of Electrical and Computer Engineering, University of Dayton, Dayton, OH, USA; Air Force Research Lab, United States Air Force, Wright-Patterson AFB, Dayton, OH, USA;" Department of Electrical and Computer Engineering, University of Dayton, Dayton, OH, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""601"",""614"",""Declarative memory enables cognitive agents to effectively store and retrieve factual memory in real-time. Increasing the capacity of a real-time agent's declarative memory increases an agent's ability to interact intelligently with its environment but requires a scalable retrieval system. This work represents an extension of the Accelerated Declarative Memory (ADM) system, referred to as Hardware Accelerated Declarative Memory (HADM), to execute retrievals on a GPU. HADM also presents improvements over ADM's CPU execution and considers critical behavior for indefinitely running declarative memories. The negative effects of a constant maximum associative strength are considered, and mitigating solutions are proposed. HADM utilizes a GPU to process the entire semantic network in parallel during retrievals, yielding significantly faster declarative retrievals. The resulting GPU-accelerated retrievals show an average speedup of approximately 70 times over the previous Service Oriented Architecture Declarative Memory (soaDM) implementation and an average speedup of approximately 5 times over ADM. HADM is the first GPU-accelerated declarative memory system in existence."",""1558-2183"","""",""10.1109/TPDS.2018.2866848"",""Air Force Office of Sponsored Research (AFOSR)"; Department of Energy Oak Ridge Institute for Science & Education;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444694"",""Declarative memory";ACT-R;semantic networks;"parallel activation calculation"",""Computer architecture";Graphics processing units;Acceleration;Hardware;Semantics;Real-time systems;"Toy manufacturing industry"","""",""4"","""",""29"",""IEEE"",""23 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Harnessing Data Movement in Virtual Clusters for In-Situ Execution,""D. Huang"; Q. Liu; S. Klasky; J. Wang; J. Y. Choi; J. Logan;" N. Podhorszki"",""Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA"; Department of Electrical and Computer Engineering, New Jersey Institute of Technology, Newark, NJ, USA; Computer Science and Mathematics Division, Oak Ridge National Lab, Oak Ridge, TN, USA; Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA; Computer Science and Mathematics Division, Oak Ridge National Lab, Oak Ridge, TN, USA; Computer Science and Mathematics Division, Oak Ridge National Lab, Oak Ridge, TN, USA;" Computer Science and Mathematics Division, Oak Ridge National Lab, Oak Ridge, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""615"",""629"",""As a result of increasing data volume and velocity, Big Data science at exascale has shifted towards the in-situ paradigm, where large scale simulations run concurrently alongside data analytics. With in-situ, data generated from simulations can be processed while still in memory, thereby avoiding the slow storage bottleneck. However, running simulations and analytics together on shared resources will likely result in substantial contention if left unmanaged, as demonstrated in this work, leading to much reduced efficiency of simulations and analytics. Recently, virtualization technologies such as Linux containers have been widely applied to data centers and physical clusters to provide highly efficient and elastic resource provisioning for consolidated workloads including scientific simulations and data analytics. In this paper, we investigate to facilitate network traffic manipulation and reduce mutual interference on the network for in-situ applications in virtual clusters. In order to dynamically allocate the network bandwidth when it is needed, we adopt SARIMA-based techniques to analyze and predict MPI traffic issued from simulations. Although this can be an effective technique, the naïve usage of network virtualization can lead to performance degradation for bursty asynchronous transmissions within an MPI job. We analyze and resolve this performance degradation in virtual clusters."",""1558-2183"","""",""10.1109/TPDS.2018.2867879"",""US National Science Foundation(grant numbers:CCF-1718297,CCF-1527249,CCF-1337244,CCF-1717338)"; US Army/DURIP(grant numbers:W911NF-17-1-0208); Department of Energy Advanced Scientific Computing Research;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451897"",""In-situ applications";virtual network;virtual switch;ARIMA;collective communication;"MPI"",""Analytical models";Data models;Predictive models;Computational modeling;Virtualization;Linux;"Containers"","""",""8"","""",""79"",""IEEE"",""30 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"HeteroCore GPU to Exploit TLP-Resource Diversity,""X. Zhao"; Z. Wang;" L. Eeckhout"",""Department of Electronics and Information Systems, Ghent University, Gent, Belgium"; State Key Laboratory of High Performance Computing, National University of Defense Technology, Changsha, China;" Department of Electronics and Information Systems, Ghent University, Gent, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""93"",""106"",""Graphics processing units (GPUs) are widely adopted as compute accelerators in cloud computing environments and supercomputers. Sharing GPU resources in such environments requires effective multitasking support. Unfortunately, conventional GPUs lack the ability to adapt to diverse thread-level parallelism (TLP) resource demands among co-executing kernels. Previous work such as SM partitioning and simultaneously multitasking (SMK) increase system throughput, however, they degrade per-application performance significantly. This paper proposes the HeteroCore GPU to significantly improve multitasking performance with a similar area cost as a conventional GPU. After rebalancing TLP-related SM resources, a HeteroCore GPU consists of two types of SMs to support diverse TLP-resource demands. Dynamic scheduling performs low-overhead spatial profiling during runtime across the different SM types and steers scheduling decisions based on the TLP-resource demands of the co-executing kernels. Compared to a conventional GPU, HeteroCore GPU improves system throughput by 20.1 percent on average (up to 80.9 percent) and per-application performance by 29.8 percent on average (up to 50.3 percent), for workload mixes composed of kernels with different TLP-resource demands."",""1558-2183"","""",""10.1109/TPDS.2018.2854764"",""European Research Council (ERC)(grant numbers:741097)"; Fonds Wetenschappelijk Onderzoek(grant numbers:G.0434.16N,G.0144.17N); National Natural Science Foundation of China(grant numbers:61572508,61672526); National University of Defense Technology(grant numbers:ZK17-03-06);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8409306"",""Heterogeneous";graphics processing units (GPUs);thread level parallelism (TLP);"scheduling"",""Kernel";Graphics processing units;Multitasking;Hardware;Throughput;Benchmark testing;"Dynamic scheduling"","""",""6"","""",""52"",""IEEE"",""10 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Heterogeneity Aware Workload Management in Distributed Sustainable Datacenters,""D. Cheng"; X. Zhou; Z. Ding; Y. Wang;" M. Ji"",""Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA"; Department of Computer Science, University of Colorado, Colorado Springs, CO, USA; Department of Computer Science & Technology, Tongji University, Shanghai, China; Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA;" chief scientist in Valley Technologies Ltd., Saint Louis, MO, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""375"",""387"",""The tremendous growth of cloud computing and large-scale data analytics highlight the importance of reducing datacenter power consumption and environmental impact of brown energy. While many Internet service operators have at least partially powered their datacenters by green energy, it is challenging to effectively utilize green energy due to the intermittency of renewable sources, such as solar or wind. We find that the geographical diversity of internet-scale services can be carefully scheduled to improve the efficiency of applying green energy in datacenters. In this paper, we propose a holistic heterogeneity-aware cloud workload management approach, sCloud, that aims to maximize the system goodput in distributed self-sustainable datacenters. sCloud adaptively places the transactional workload to distributed datacenters, allocates the available resource to heterogeneous workloads in each datacenter, and migrates batch jobs across datacenters, while taking into account the green power availability and QoS requirements. We formulate the transactional workload placement as a constrained optimization problem that can be solved by nonlinear programming. Then, we propose a batch job migration algorithm to further improve the system goodput when the green power supply varies widely at different locations. Finally, we extend sCloud by integrating a flexible batch job manager to dynamically control the job execution progress without violating the deadlines. We have implemented sCloud in a university cloud testbed with real-world weather conditions and workload traces. Experimental results demonstrate sCloud can achieve near-to-optimal system performance while being resilient to dynamic power availability. sCloud with the flexible batch job management approach outperforms a heterogeneity-oblivious approach by 37 percent in improving system goodput and 33 percent in reducing QoS violations."",""1558-2183"","""",""10.1109/TPDS.2018.2865927"",""University of North Carolina at Charlotte";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8439018"",""Sustainable datacenter";heterogeneity;job migration;optimization;system goodput;"workload placement"",""Power supplies";Green products;Task analysis;Cloud computing;System performance;Quality of service;"Clouds"","""",""6"","""",""33"",""IEEE"",""17 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Hierarchical Hybrid Memory Management in OS for Tiered Memory Systems,""L. Liu"; S. Yang; L. Peng;" X. Li"",""SKLCA, Sys-Inventor Lab, Haidian District, Beijing, China"; SKLCA, Sys-Inventor Lab, Haidian District, Beijing, China; Division of Electrical & Computer Engineering, Louisiana State University, Baton Rouge, LA, USA;" SKLCA, Sys-Inventor Lab, Haidian District, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2223"",""2236"",""The emerging hybrid DRAM-NVM architecture is challenging the existing memory management mechanism at the level of the architecture and operating system. In this paper, we introduce Memos, a memory management framework which can hierarchically schedule memory resources over the entire memory hierarchy including cache, channels, and main memory comprising DRAM and NVM simultaneously. Powered by our newly designed kernel-level monitoring module that samples the memory patterns by combining TLB monitoring with page walks, and page migration engine, Memos can dynamically optimize the data placement in the memory hierarchy in response to the memory access pattern, current resource utilization, and memory medium features. Our experimental results show that Memos can achieve high memory utilization, improving system throughput by around 20.0 percent"; reduce the memory energy consumption by up to 82.5 percent;" and improve the NVM lifetime by up to 34X."",""1558-2183"","""",""10.1109/TPDS.2019.2908175"",""National Key Research and Development Plan of China(grant numbers:2017YFB1001602)"; NSF(grant numbers:61502452);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8676386"",""Memory";DRAM;NVM;operating system;"scheduling"",""Random access memory";Memory management;Nonvolatile memory;Bandwidth;Monitoring;Operating systems;"Throughput"","""",""30"","""",""83"",""IEEE"",""29 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"High-Performance Tucker Factorization on Heterogeneous Platforms,""S. Oh"; N. Park; J. -G. Jang; L. Sael;" U. Kang"",""Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea"; Computer Science Department, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea;" Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2237"",""2248"",""Given large-scale multi-dimensional data (e.g., (user, movie, time";" rating) for movie recommendations), how can we extract latent concepts/relations of such data? Tensor factorization has been widely used to solve such problems with multi-dimensional data, which are modeled as tensors. However, most tensor factorization algorithms exhibit limited scalability and speed since they require huge memory and heavy computational costs while updating factor matrices. In this paper, we propose GTA, a general framework for Tucker factorization on heterogeneous platforms. GTA performs alternating least squares with a row-wise update rule in a fully parallel way, which significantly reduces memory requirements for updating factor matrices. Furthermore, GTA provides two algorithms: GTA-PART for partially observable tensors and GTA-FULL for fully observable tensors, both of which accelerate the update process using GPUs and CPUs. Experimental results show that GTA exhibits 5.6~44.6× speed-up for large-scale tensors compared to the state-of-the-art. In addition, GTA scales near linearly with the number of GPUs and computing nodes used for experiments."",""1558-2183"","""",""10.1109/TPDS.2019.2908639"",""National Research Foundation of Korea"; Ministry of Science, ICT and Future Planning(grant numbers:NRF-2016M3C4A7952587); ICT; Seoul National University provides research facilities; The Institute of Engineering Research; Seoul National University provided research facilities;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8678477"",""Tensor analysis";tucker factorization;heterogeneous computing;GPGPU;"OpenCL"",""Memory management";Computer science;Graphics processing units;Motion pictures;Scalability;"Heterogeneous networks"","""",""18"","""",""42"",""IEEE"",""31 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"HitGraph: High-throughput Graph Processing Framework on FPGA,""S. Zhou"; R. Kannan; V. K. Prasanna; G. Seetharaman;" Q. Wu"",""Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA"; US Army Research Lab, Los Angeles, CA, USA; Department of Electrical and Computer Engineering, University of Southern California, Los Angeles, CA, USA; US Naval Research Laboratory, Washington, DC, USA;" Air Force Research Laboratory, Rome, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2249"",""2264"",""This paper presents, HitGraph, an FPGA framework to accelerate graph processing based on the edge-centric paradigm. HitGraph takes in an edge-centric graph algorithm and hardware resource constraints, determines design parameters and then generates a Register Transfer Level (RTL) FPGA design. This makes accelerator design for various graph analytics transparent and user-friendly by masking internal details of the accelerator design process. HitGraph enables increased data reuse and parallelism through novel algorithmic optimizations, including (1) an optimized data layout that reduces non-sequential external memory accesses, (2) an efficient update merging and filtering scheme to reduce the data communication between the FPGA and external memory, and (3) a partition skipping scheme to reduce redundant edge traversals for non-stationary graph algorithms. Based on our design methodology, we accelerate Sparse Matrix Vector Multiplication (SpMV), PageRank (PR), Single Source Shortest Path (SSSP), and Weakly Connected Component (WCC). Experimental results show that HitGraph sustains a high throughput of 2076 Million Traversed Edges Per Second (MTEPS) for SpMV, 2225 MTEPS for PR, 2916 MTEPS for SSSP, and 3493 MTEPS for WCC, respectively. Compared with highly-optimized multi-core implementations, HitGraph achieves up to 37.9× speedup. Compared with state-of-the-art FPGA frameworks, HitGraph achieves up to 50.7× throughput improvement."",""1558-2183"","""",""10.1109/TPDS.2019.2910068"",""Intel Strategic Research Alliance funding"; U.S. National Science Foundation(grant numbers:ACI-1339756,CNS-1643351); Air Force Research Laboratory(grant numbers:FA8750-18-2-0034);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8685122"",""FPGA framework";graph processing;"energy-efficient acceleration"",""Field programmable gate arrays";Acceleration;Optimization;Sparse matrices;Throughput;Program processors;"Memory management"","""",""56"","""",""51"",""IEEE"",""11 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"HSDC: A Highly Scalable Data Center Network Architecture for Greater Incremental Scalability,""Z. Zhang"; Y. Deng; G. Min; J. Xie; L. T. Yang;" Y. Zhou"",""Department of Computer Science, Jinan University, Guangzhou, China"; the State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Beijing, China; College of Engineering, Mathematics and Physical Sciences, University of Exeter, Exeter, United Kingdom; Department of Computer Science, Jinan University, Guangzhou, China; Department of Computer Science, St. Francis Xavier University, Antigonish, NS, Canada;" Department of Computer Science, Jinan University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1105"",""1119"",""As the volume of data keeps growing rapidly, more and more storage devices, servers and network devices are continuously added into data centers to store, manage and analyze the data. The industry experience indicates that, instead of a huge number of servers added at a time, the data center network also expands gradually by adding a small number of servers from time to time. As a result, how to achieve an incremental scalability is becoming a very important challenge in designing modern data center network architectures in order to maintain the topological properties unchanged when the size of data centers grows. In this paper, we propose a new type of data center network architecture named HSDC (High Scalability Data Center Network Architecture) based on the hypercube network. The HSDC is constructed by using $m$m-port switches and 2-port servers. The fault-tolerant routing algorithm designed in this paper for HSDC can be executed on any vertex and is able to construct a path between any pair of vertices. In order to achieve an incremental scalability, we further propose three types of incomplete HSDC structures that allow gradually adding servers into the structures, while maintaining all the topological properties. The simulation experiments and performance results demonstrate that the throughput of HSDC is comparable to that of Fat-Tree, BCube and DCell. Furthermore, the analysis results indicate that HSDC strikes a good balance among diameter, bisection width, incremental scalability, cost and energy consumption in contrast to the state-of-the-art data center network architectures."",""1558-2183"","""",""10.1109/TPDS.2018.2874659"",""National Natural Science Foundation of China(grant numbers:61872165,61572232)"; Key Laboratory of Computer System and Architecture; Institute of Computing Technology, Chinese Academy of Sciences(grant numbers:CARCH201705);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8485759"",""Data center";interconnection network;network topology;hypercube;incremental scalability;"routing algorithm"",""Servers";Data centers;Hypercubes;Scalability;Network architecture;"Switches"","""",""31"","""",""42"",""IEEE"",""7 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"HyConv: Accelerating Multi-Phase CNN Computation by Fine-Grained Policy Selection,""X. Li"; G. Zhang; Z. Wang;" W. Zheng"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""388"",""399"",""Existing GPU-based approaches cannot yet meet the performance requirement for training very large convolutional neural networks (CNNs), where convolutional layers (Conv-layers) dominate the training time. In this paper, we find that no single convolution policy can always perform the fastest across all the computing phases. Then, we propose an approach called HyConv to accelerating multi-phase CNN computation by fine-grained policy selection. HyConv encapsulates existing convolution policies into a set of modules, and selects the fastest policy (a.k.a., winner policy) via one-round runtime measurement for computing each phase. Furthermore, HyConv uses a winner database to record the current winner policies, avoiding duplicate measurement later for the same parameter configuration. Our experimental results indicate that over all the used real-world CNN networks, HyConv consistently outperforms existing approaches on either a single GPU or four GPUs, with speedups of up to 3.3× and up to 1.6× over cuDNN-MM respectively. Such improvement can be explained by our result that HyConv delivers obviously better performance for most of single Conv-layers. Furthermore, HyConv has the ability to work with any parameter configuration and thus keeps better usability."",""1558-2183"","""",""10.1109/TPDS.2018.2864299"",""National Grand Fundamental Research 973 Program of China(grant numbers:2014CB340402)"; National Natural Science Foundation of China(grant numbers:61672315);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8428450"",""Convolution policy";convolutional neural network;deep learning;general-purpose GPU;"parallel computing"",""Convolution";Usability;Kernel;Training;Graphics processing units;Acceleration;"Convolutional neural networks"","""",""5"","""",""40"",""IEEE"",""7 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Hytrace: A Hybrid Approach to Performance Bug Diagnosis in Production Cloud Infrastructures,""T. Dai"; D. Dean; P. Wang; X. Gu;" S. Lu"",""North Carolina State University, Raleigh, NC, USA"; InsightFinder Inc, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA;" University of Chicago, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""107"",""118"",""Server applications running inside production cloud infrastructures are prone to various performance problems (e.g., software hang, performance slowdown). When those problems occur, developers often have little clue to diagnose those problems. In this paper, we present Hytrace, a novel hybrid approach to diagnosing performance problems in production cloud infrastructures. Hytrace combines rule-based static analysis and runtime inference techniques to achieve higher bug localization accuracy than pure-static and pure-dynamic approaches for performance bugs. Hytrace does not require source code and can be applied to both compiled and interpreted programs such as C/C++ and Java. We conduct experiments using real performance bugs from seven commonly used server applications in production cloud infrastructures. The results show that our approach can significantly improve the performance bug diagnosis accuracy compared to existing diagnosis techniques."",""1558-2183"","""",""10.1109/TPDS.2018.2858800"",""National Science Foundation(grant numbers:CNS1513942,CNS1149445)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417446"",""Static analysis";dynamic analysis;reliability, availability, and serviceability;debugging aids;"performance"",""Computer bugs";Production;Static analysis;Runtime;Sockets;Servers;"Tools"","""",""5"","""",""47"",""IEEE"",""23 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Improving Cache Performance for Large-Scale Photo Stores via Heuristic Prefetching Scheme,""K. Zhou"; S. Sun; H. Wang; P. Huang; X. He; R. Lan; W. Li; W. Liu;" T. Yang"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China"; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; Temple University, Philadelphia, PA, USA; Tencent Inc, Shenzhen, China; Tencent Inc, Shenzhen, China; Temple University, Philadelphia, PA, USA;" Huanghuai University, Zhumadian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""2033"",""2045"",""Photo service providers are facing critical challenges of dealing with the huge amount of photo storage, typically in a magnitude of billions of photos, while ensuring national-wide or world-wide satisfactory user experiences. Distributed photo caching architecture is widely deployed to meet high performance expectations, where efficient still mysterious caching policies play essential roles. In this work, we present a comprehensive study on internet-scale photo caching algorithms in the case of QQPhoto from Tencent Inc., the largest social network service company in China. We unveil that even advanced cache algorithms can only perform at a similar level as simple baseline algorithms and there still exists a large performance gap between these cache algorithms and the theoretically optimal algorithm due to the complicated access behaviors in such a large multi-tenant environment. We then expound the reasons behind this phenomenon via extensively investigating the characteristics of QQPhoto workloads. Finally, in order to realistically further improve QQPhoto cache efficiency, we propose to incorporate a prefetcher in the cache stack based on the observed immediacy feature that is unique to the QQPhoto workload. The prefetcher proactively prefetches selected photos into cache before they are requested for the first time to eliminate compulsory misses and promote hit ratios. Our extensive evaluation results show that with appropriate prefetching we improve the cache hit ratio by up to 7.4 percent, while reducing the average access latency by 6.9 percent at a marginal cost of 4.14 percent backend network traffic compared to the original system that performs no prefetching."",""1558-2183"","""",""10.1109/TPDS.2019.2902392"",""National Natural Science Foundation of China(grant numbers:61821003)"; National Key Research and Development Program of China(grant numbers:2016YFB0800402); National Science Foundation(grant numbers:CCF-1717660,CNS-1702474,CCF1813081);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8658007"",""Caching algorithm";distributed storage;photo storage;"cloud computing"",""Prefetching";Servers;Data centers;Social networking (online);Cloud computing;Sun;"Prediction algorithms"","""",""8"","""",""45"",""IEEE"",""4 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"Improving Efficiency of Parallel Vertex-Centric Algorithms for Irregular Graphs,""M. M. Ozdal"",""Computer Engineering Department, Bilkent University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2265"",""2282"",""Memory access is known to be the main bottleneck for shared-memory parallel graph applications especially for large and irregular graphs. Propagation blocking (PB) idea was proposed recently to improve the parallel performance of PageRank and sparse matrix and vector multiplication operations. The idea is based on separating parallel computation into two phases, binning and accumulation, such that random memory accesses are replaced with contiguous accesses. In this paper, we propose an algorithm that allows execution of these two phases concurrently. We propose several improvements to increase parallel throughput, reduce memory overhead, and improve work efficiency. Our experimental results show that our proposed algorithms improve shared-memory parallel throughput by a factor of up to 2× compared to the original PB algorithms. We also show that the memory overhead can be reduced significantly (from 170 percent down to less than 5 percent) without significant degradation of performance. Finally, we demonstrate that our concurrent execution model allows asynchronous parallel execution, leading to significant work efficiency in addition to throughput improvements."",""1558-2183"","""",""10.1109/TPDS.2019.2906166"",""TUBITAK 2232 Program(grant numbers:116C079)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8669966"",""Parallel algorithms";graph algorithms;sparse matrix vector multiplication (SpMV);sparse matrix sparse vector multiplication (SpMSpV);"high performance computing"",""Throughput";Memory management;Random access memory;Instruction sets;Sparse matrices;Computational modeling;"Indexes"","""",""3"","""",""46"",""IEEE"",""19 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"Improving Restore Performance in Deduplication Systems via a Cost-Efficient Rewriting Scheme,""J. Wu"; Y. Hua; P. Zuo;" Y. Sun"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China;" Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""119"",""132"",""In chunk-based deduplication systems, logically consecutive chunks are physically scattered in different containers after deduplication, which results in the serious fragmentation problem. The fragmentation significantly reduces the restore performance due to reading the scattered chunks from different containers. Existing work aims to rewrite the fragmented duplicate chunks into new containers to improve the restore performance, which however produces the redundancy among containers, decreasing the deduplication ratio and resulting in redundant chunks in containers retrieved to restore the backup, which wastes limited disk bandwidth and decreases restore speed. To improve the restore performance while ensuring the high deduplication ratio, this paper proposes a cost-efficient submodular maximization rewriting scheme (SMR). SMR first formulates the defragmentation as an optimization problem of selecting suitable containers, and then builds a submodular maximization model to address this problem by selecting containers with more distinct referenced chunks. Moveover, this paper further leverages the grouped form, i.e., GSMR, to reduce the fragmented chunks caused by the accumulated differences among backup versions. We implement SMR in the deduplication system, which is evaluated via three real-world datasets. Experimental results demonstrate that SMR is superior to the state-of-the-art work in terms of the restore performance as well as deduplication ratio, and GSMR further improves the restore performance. We have released the source code of SMR in Github for public use."",""1558-2183"","""",""10.1109/TPDS.2018.2852642"",""National Key Research and Development Program of China(grant numbers:2016YFB1000202)"; National Natural Science Foundation of China(grant numbers:61772212);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8402122"",""Data deduplication";restore performance;"rewriting scheme"",""Containers";Redundancy;Bandwidth;Indexes;Linux;Layout;"Sun"","""",""10"","""",""32"",""IEEE"",""3 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Improving the Performance and Energy Efficiency of GPGPU Computing through Integrated Adaptive Cache Management,""K. Y. Kim"; J. Park;" W. Baek"",""School of Electrical and Computer Engineering, UNIST, Ulsan, Republic of Korea"; School of Electrical and Computer Engineering, UNIST, Ulsan, Republic of Korea;" School of Electrical and Computer Engineering, UNIST, Ulsan, Republic of Korea"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""630"",""645"",""Hardware caches are widely employed in GPGPUs to achieve higher performance and energy efficiency. Incorporating hardware caches in GPGPUs, however, does not immediately guarantee enhanced performance and energy efficiency due to high cache contention and thrashing. To address the inefficiency of GPGPU caches, various adaptive techniques (e.g., warp limiting) have been proposed. However, relatively little work has been done in the context of creating an architectural framework that tightly integrates adaptive cache management techniques and investigating their effectiveness and interaction. To bridge this gap, we propose IACM, integrated adaptive cache management for high-performance and energy-efficient GPGPU computing. IACM integrates the state-of-the-art adaptive cache management techniques (i.e., cache indexing, bypassing, and warp limiting) in a unified architectural framework. Our quantitative evaluation demonstrates that IACM significantly improves the performance and energy efficiency of various GPGPU workloads over the baseline architecture (i.e., 98.1 and 61.9 percent on average, respectively), achieves considerably higher performance than the state-of-the-art technique (i.e., 361.4 percent at maximum and 7.7 percent on average), and delivers significant performance and energy-efficiency gains over the baseline GPGPU architecture enhanced with advanced architectural technologies."",""1558-2183"","""",""10.1109/TPDS.2018.2868658"",""National Research Foundation of Korea(grant numbers:NRF-2016M3C4A7952587)"; National Research Foundation of Korea(grant numbers:NRF-2018R1C1B6005961); Korea government (MSIP)(grant numbers:R0190-16-2012);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454288"",""Integrated adaptive cache management";GPGPU computing;high performance;"energy efficiency"",""Computer architecture";Indexing;Limiting;Instruction sets;Hardware;Pathology;"Interference"","""","""","""",""31"",""IEEE"",""2 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Integrating Coflow and Circuit Scheduling for Optical Networks,""H. Wang"; X. Yu; H. Xu; J. Fan; C. Qiao;" L. Huang"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; Department of Computer Science and Engineering, State University of New York, Buffalo, NY, USA; Department of Computer Science and Engineering, State University of New York, Buffalo, NY, USA;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1346"",""1358"",""There are more and more structured traffic flows (a.k.a coflow) in today's data center networks. Completing a coflow is extremely important for various applications, e.g., MapReduce. To reduce the coflow completion time or CCT, one may increase the link capacity by applying advanced optical circuit switches in data center networks. Due to special features of optical circuit switches, both traffic scheduling and circuit scheduling will influence the CCT. However, previous solutions have some significant limitations: they consider either coflow scheduling, or circuit scheduling for only one optical circuit switch, which are both insufficient. In this paper, we study the integrated coflow and circuit scheduling (GCCS) problem with the objective to minimize the CCT, and prove its NP-hardness. We present an integrated algorithm which includes two steps, coflow scheduling and circuit scheduling, respectively. We also analyze that the proposed algorithm can achieve the approximation ratio O(h) in most practical situations, where h is the maximum number of ports among all lightpaths. Through large-scale simulations, we demonstrate that the integrated solution can significantly reduce the CCT by about 43-70 percent compared with the state-of-the-art coflow scheduler for optical networks."",""1558-2183"","""",""10.1109/TPDS.2018.2889251"",""National Natural Science Foundation of China(grant numbers:61822210,U1709217,61472383,61728207,61472385)"; Anhui Initiative in Quantum Information Technologies(grant numbers:AHY150300); US National Science Foundation(grant numbers:CNS-1626374);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8587213"",""Optical networks";coflow scheduling;circuit scheduling;"approximation"",""Optical switches";Optical fiber networks;Optical buffering;Scheduling;"Optical packet switching"","""",""12"","""",""36"",""IEEE"",""23 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Integrating Concurrency Control in n-Tier Application Scaling Management in the Cloud,""Q. Wang"; H. Chen; S. Zhang; L. Hu;" B. Palanisamy"",""Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, LA, USA"; Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, LA, USA; Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, LA, USA; Computing and Information Sciences, Florida International University, Miami, FL, USA;" School of Information Sciences, University of Pittsburgh, Pittsburgh, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""855"",""869"",""Scaling complex distributed systems such as e-commerce is an importance practice to simultaneously achieve high performance and high resource efficiency in the cloud. Most previous research focuses on hardware resource scaling to handle runtime workload variation. Through extensive experiments using a representative n-tier web application benchmark (RUBBoS), we demonstrate that scaling an n-tier system by adding or removing VMs without appropriately re-allocating soft resources (e.g., server threads and connections) may lead to significant performance degradation resulting from implicit change of request processing concurrency in the system, causing either over- or under-utilization of the critical hardware resource in the system. We build a concurrency-aware model that determines a near optimal soft resource allocation of each tier by combining some operational queuing laws and the fine-grained online measurement data of the system. We then develop a dynamic concurrency management (DCM) framework that integrates the concurrency-aware model to intelligently reallocate soft resources in the system during the system scaling process. We compare DCM with Amazon EC2-AutoScale, the state-of-the-art hardware only scaling management solution using six real-world bursty workload traces. The experimental results show that DCM achieves significantly shorter tail latency and higher throughput compared to Amazon EC2-AutoScale under all the workload traces."",""1558-2183"","""",""10.1109/TPDS.2018.2871086"",""US National Science Foundation(grant numbers:*****)"; Division of Computer and Network Systems(grant numbers:1566443); Louisiana Board of Regents(grant numbers:LEQSF(2015-18)-RD-A-11);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8468089"",""Scalability";soft resources;configuration;web application;parallel processing;"cloud computing"",""Servers";Concurrent computing;Throughput;Hardware;Resource management;Databases;"Benchmark testing"","""",""12"","""",""42"",""IEEE"",""19 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Intra-Task Priority Assignment in Real-Time Scheduling of DAG Tasks on Multi-Cores,""Q. He"; x. jiang; N. Guan;" Z. Guo"",""Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong"; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong; Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong;" Department of Electrical and Computer Engineering, University of Central Florida, Orlando, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2283"",""2295"",""Real-time scheduling and analysis of parallel tasks modeled as directed acyclic graphs (DAG) have been intensively studied in recent years. However, no existing work has explored the execution order of eligible vertices within a DAG task. In this paper, we show that this intra-task vertex execution order has a large impact on system schedulability and propose to control the execution order by vertex-level priority assignment. We develop analysis techniques to bound the worst-case response time for the proposed scheduling strategy and design heuristics for proper priority assignment to improve system schedulability as much as possible. We further extend the proposed approach to the general setting of multiple recurrent DAG tasks. Experiments with both realistic parallel benchmark applications and randomly generated workload show that our method consistently outperforms state-of-the-art methods with different task graph structures and parameter configurations."",""1558-2183"","""",""10.1109/TPDS.2019.2910525"",""Council of Hong Kong(grant numbers:GRF 15204917,GRF 15213818)"; NSF(grant numbers:CNS-1850851); NSFC(grant numbers:61532007,61672140); Ministry of Education Joint Foundation for Equipment Pre-Research(grant numbers:6141A020333); Fundamental Research Funds for the Central Universities(grant numbers:N172304025);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8686234"",""Intra-task priority assignment";response time analysis;parallel real-time tasks;"multi-cores"",""Task analysis";Time factors;Real-time systems;Scheduling algorithms;Runtime;Heuristic algorithms;"Dynamic scheduling"","""",""39"","""",""33"",""IEEE"",""11 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"ISEE: An Intelligent Scene Exploration and Evaluation Platform for Large-Scale Visual Surveillance,""D. Li"; Z. Zhang; K. Yu; K. Huang;" T. Tan"",""School of Artificial Intelligence, University of Chinese Academy of Sciences (UCAS), Beijing, China"; CRIPAC and NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China; Carnegie Mellon University, Pittsburgh, USA; CRISE, Institute of Automation, Chinese Academy of Sciences, Beijing, China;" CRIPAC and NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2743"",""2758"",""Intelligent video surveillance (IVS) is always an interesting research topic to utilize visual analysis algorithms for exploring richly structured information from big surveillance data. However, existing IVS systems either struggle to utilize computing resources adequately to improve the efficiency of large-scale video analysis, or present a customized system for specific video analytic functions. It still lacks of a comprehensive computing architecture to enhance efficiency, extensibility and flexibility of IVS system. Moreover, it is also an open problem to study the effect of the combinations of multiple vision modules on the final performance of end applications of IVS system. Motivated by these challenges, we develop an Intelligent Scene Exploration and Evaluation (ISEE) platform based on a heterogeneous CPU-GPU cluster and some distributed computing tools, where Spark Streaming serves as the computing engine for efficient large-scale video processing and Kafka is adopted as a middle-ware message center to decouple different analysis modules flexibly. To validate the efficiency of the ISEE and study the evaluation problem on composable systems, we instantiate the ISEE for an end application on person retrieval with three visual analysis modules, including pedestrian detection with tracking, attribute recognition and re-identification. Extensive experiments are performed on a large-scale surveillance video dataset involving 25 camera scenes, totally 587 hours 720p synchronous videos, where a two-stage question-answering procedure is proposed to measure the performance of execution pipelines composed of multiple visual analysis algorithms based on millions of attribute-based and relationship-based queries. The case study of system-level evaluations may inspire researchers to improve visual analysis algorithms and combining strategies from the view of a scalable and composable system in the future."",""1558-2183"","""",""10.1109/TPDS.2019.2921956"",""National Basic Research Program of China (973 Program)(grant numbers:2016YFB1001005)"; National Natural Science Foundation of China(grant numbers:61473290);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8734005"",""Intelligent surveillance system";big visual data;"distributed system and parallel computing"",""Visualization";Surveillance;Task analysis;Streaming media;Computer architecture;"Visual analytics"","""",""10"","""",""60"",""IEEE"",""10 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"JSensor: A Parallel Simulator for Huge Wireless Sensor Networks Applications,""M. L. Silva"; L. N. S. Júnior; A. L. L. Aquino;" J. d. C. Lima"",""Department of Computer Science, Federal University of Ouro Preto, Ouro Preto, MG, Brazil"; Department of Computer Science, Federal University of Ouro Preto, Ouro Preto, MG, Brazil; Computer Institute, Federal University of Alagoas, Macei, AL, Brazil;" Department of Computer Science, Federal University of Ouro Preto, Ouro Preto, MG, Brazil"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2296"",""2308"",""This paper presents JSensor, a parallel general purpose simulator which enables huge simulations of Wireless Sensor Networks applications. Its main advantages are: i) to have a simple API with few classes to be extended, allowing easy prototyping and validation of WSNs applications and protocols"; ii) to enable transparent and reproducible simulations, regardless of the number of threads of the parallel kernel;" and iii) to scale over multi-core computer architectures, allowing simulations of more realistic applications. JSensor is a parallel event-driven simulator which executes according to event timers. The simulation elements, nodes, application, and events, can send messages, process task or move around the simulated environment. The mentioned environment follows a grid structure of extensible spatial cells. The results demonstrated that JSensor scales well, precisely it achieved a speedup of 7.45 with 16 threads in a machine with 16 cores (eight physical and eight virtual cores), and comparative evaluations versus OMNeT++ showed that the presented solution could be 43 times faster."",""1558-2183"","""",""10.1109/TPDS.2019.2908845"",""Brazilian research agency CNPq"; Research Foundation of the State of Alagoas (FAPEAL); Research Foundation of the State of São Paulo; UFOP and TerraLab(grant numbers:CNPq 481285/2012-1);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8680714"",""Parallel simulation";wireless sensor network;"reproducible simulation"",""Wireless sensor networks";Protocols;Object oriented modeling;Computational modeling;Kernel;Message systems;"Computer architecture"","""",""5"","""",""28"",""IEEE"",""2 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Learn-as-you-go with Megh: Efficient Live Migration of Virtual Machines,""D. Basu"; X. Wang; Y. Hong; H. Chen;" S. Bressan"",""Department of Computer Science, National University of Singapore, Singapore"; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China; Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University, Shanghai, China;" Department of Computer Science, National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1786"",""1801"",""Cloud providers leverage live migration of virtual machines to reduce energy consumption and allocate resources efficiently in data centers. Each migration decision depends on three questions: when to move a virtual machine, which virtual machine to move and where to move it? Dynamic, uncertain, and heterogeneous workloads running on virtual machines make such decisions difficult. Knowledge-based and heuristics-based algorithms are commonly used to tackle this problem. Knowledge-based algorithms, such as MaxWeight scheduling algorithms, are dependent on the specifics and the dynamics of the targeted Cloud architectures and applications. Heuristics-based algorithms, such as MMT algorithms, suffer from high variance and poor convergence because of their greedy approach. We propose an online reinforcement learning algorithm called Megh. Megh does not require prior knowledge of the workload rather learns the dynamics of workloads as-it-goes. Megh models the problem of energy- and performance-efficient resource management during live migration as a Markov decision process and solves it using a functional approximation scheme. While several reinforcement learning algorithms are proposed to solve this problem, these algorithms remain confined to the academic realm as they face the curse of dimensionality. They are either not scalable in real-time, as it is the case of MadVM, or need an elaborate offline training, as it is the case of Q-learning. These algorithms often incur execution overheads which are comparable with the migration time of a VM. Megh overcomes these deficiencies. Megh uses a novel dimensionality reduction scheme to project the combinatorially explosive state-action space to a polynomial dimensional space with a sparse basis. Megh has the capacity to learn uncertain dynamics and the ability to work in real-time without incurring significant execution overhead. Megh is both scalable and robust. We implement Megh using the CloudSim toolkit and empirically evaluate its performance with the PlanetLab and the Google Cluster workloads. Experiments validate that Megh is more cost-effective, converges faster, incurs smaller execution overhead and is more scalable than MadVM and MMT. An empirical sensitivity analysis explicates the choice of parameters in experiments."",""1558-2183"","""",""10.1109/TPDS.2019.2893648"",""National Research Foundation Singapore"; National University of Singapore;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8618307"",""Cloud computing";reinforcement learning;virtual machine;live migration;Markov decision process;energy efficiency;"performance efficiency"",""Heuristic algorithms";Cloud computing;Resource management;Data centers;Clustering algorithms;Virtual machining;"Knowledge based systems"","""",""43"","""",""50"",""IEEE"",""18 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Learning Driven Computation Offloading for Asymmetrically Informed Edge Computing,""M. Hu"; L. Zhuang; D. Wu; Y. Zhou; X. Chen;" L. Xiao"",""School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China"; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China; Department of Computing, Macquarie University, Sydney, NSW, Australia; School of Data and Computer Science, Sun Yat-sen University, Guangzhou, Guangdong, China;" Department of Communication Engineering, Xiamen University, Xiamen, Fujian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1802"",""1815"",""Edge computing emerges as a promising paradigm to decentralize computation power to the edge of the network and thus improve user experience by task offloading. A user can perfectly schedule his tasks to be executed on edge servers if the execution time of all tasks can be known beforehand. However, it is difficult to know the task execution time (TET) before performing actual offloading, which normally varies on edge servers with different software and hardware configurations. Moreover, such configuration information is not always available to end users due to security concerns. In this paper, we first propose a learning-driven algorithm to accurately predict TETs of all tasks in such an asymmetrically informed edge computing environment. The basic idea is to predict unknown TETs using only a small sampled set of TETs by exploiting the underlying correlation between TETs and edge server configurations. Next, we formulate the problem of task offloading into a constrained optimization problem, which is unfortunately proved to be NP-hard. To address the above challenge, we design a task offloading algorithm, called Maximum Efficiency First Ordered (MEFO), to achieve near-optimal efficiency. Field measurements and experiments have been conducted to demonstrate that our proposed learning-driven algorithm can predict TETs more accurately than other algorithms as long as the fraction of sampled TETs is larger than a small predefined threshold, and our proposed MEFO algorithm achieves a much higher success rate of task offloading and a shorter processing delay with very limited information of edge servers."",""1558-2183"","""",""10.1109/TPDS.2019.2893925"",""National Key R&D Program of China(grant numbers:2018YFB0204100)"; National Natural Science Foundation of China(grant numbers:61802452,61572538,61671396); Natural Science Foundation of Guangdong Province(grant numbers:2018A030310079); Guangdong Special Support Program(grant numbers:2017TX04X148); Program for Guangdong Introducing Innovative and Enterpreneurial Teams(grant numbers:2017ZT07X355); China Postdoctoral Science Foundation(grant numbers:2018M631025); Australian Research Council(grant numbers:DE180100950);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8618389"",""Asymmetric information";edge computing;low rank learning;task execution time (TET);"task offloading"",""Task analysis";Servers;Prediction algorithms;Edge computing;Computational modeling;Frequency measurement;"Schedules"","""",""48"","""",""45"",""IEEE"",""18 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Local Differential Private Data Aggregation for Discrete Distribution Estimation,""S. Wang"; L. Huang; Y. Nie; X. Zhang; P. Wang; H. Xu;" W. Yang"",""School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China;" School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""2046"",""2059"",""For the purpose of improving the quality of services, softwares or online services are collecting various of user data, such as personal information and locations. Such data facilitates mining statistical knowledge of users, but threatens users’ privacy as it may reveal sensitive information (e.g., identities and activities) about individuals. This work considers distribution estimation over user-contributed data meanwhile providing rigid protection of their data with local $\epsilon$ε-differential privacy ($\epsilon$ε-LDP), which sanitizes each user's data on the client's side (e.g, on the user's mobile device). Our privacy protection covers both qualitative data (e.g., categorical data) and discrete quantitative data (e.g., location data). Specifically, for categorical data, we derive an optimal $\epsilon$ε-LDP mechanism (termed as $k$k-subset mechanism) from mutual information perspective, and further show its optimality over existing approaches within the context of discrete distribution estimation";" for discrete quantitative data that have arbitrary distance metric, we provide an efficient extension of $k$k-subset mechanism by proposing a variant of the popular Exponential Mechanism (EM) to tackle the asymmetry issue on the data domain. Experiments on real-world datasets and simulated scenarios show that our mechanism is highly efficient and reduces nearly a fraction of $\exp (-\frac{\epsilon }{2})$exp(-ε2) error for distribution estimation when compared to existing approaches."",""1558-2183"","""",""10.1109/TPDS.2019.2899097"",""National Natural Science Foundation of China(grant numbers:U1709217,61822210,61472383,61728207,61472385)"; Anhui Initiative in Quantum Information Technologies(grant numbers:AHY150300);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8640266"",""Data privacy";data aggregation;distribution estimation;crowdsourcing;"mutual information"",""Estimation";Data models;Mutual information;Servers;Privacy;"Differential privacy"","""",""42"","""",""47"",""IEEE"",""12 Feb 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Loosely-Stabilizing Leader Election for Arbitrary Graphs in Population Protocol Model,""Y. Sudo"; F. Ooshita; H. Kakugawa; T. Masuzawa; A. K. Datta;" L. L. Larmore"",""Osaka University, Osaka, Japan"; Nara Institute of Science and Technology, Nara, Japan; Osaka University, Osaka, Japan; Osaka University, Osaka, Japan; University of Nevada, Las Vegas, NV, USA;" University of Nevada, Las Vegas, NV, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1359"",""1373"",""In the population protocol model [Angluin et al. 2006], it is impossible to design a self-stabilizing leader election protocol without any knowledge of the exact number of nodes in the system. The notion of loose-stabilization, which relaxes the closure requirement of self -stabilization, was introduced in 2009 to circumvent this impossibility. The notion can be described as follows: a loosely-stabilizing protocol guarantees that, starting from any initial configuration, a system reaches a safe configuration eventually, and after that, the system maintains its specification (e.g., the unique leader) not forever, but for a sufficiently long time. The previous work of the authors presented a loosely-stabilizing protocol that solves the leader election on complete graphs using only a given upper bound N on the number of nodes n in the system, instead of the exact value of n. In this paper, we propose two loosely-stabilizing protocols that solve leader election for arbitrary graphs. One is a deterministic protocol that uses the unique identifiers of nodes while the other is a probabilistic protocol that works on anonymous networks. Given an upper bound N on the number of nodes, both protocols maintain a unique leader for Ω(Ne2N) expected steps (holding time) after entering a safe configuration. The first algorithm enters a safe configuration within O(mN log n) expected steps (convergence time) while the second one does this within O(mN2 log N) expected steps, where m is the number of edges in the graph. Both protocols require only O(log N) bits for each node's memory. A novel concept, called the same speed timer is introduced, by which all nodes of the system can count down their timers at the same speed. This concept allows to achieve fast convergence time of both algorithms. To design the second protocol, we design a self-stabilizing two-hop coloring protocol, which is interesting in its own right. This protocol uses only O(log N) memory space per node. We establish a lower bound. Any loosely-stabilizing leader election protocol with expected exponential holding time requires Ω(mN) expected convergence time. This lower bound shows a near-optimality of the first algorithm."",""1558-2183"","""",""10.1109/TPDS.2018.2881125"",""JSPS KAKENHI(grant numbers:17K19977,16K00018,18K11167,18K18000)"; Japan Science and Technology Agency;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8533437"",""Population protocols";loose-stabilization;leader election;"the same speed timer"",""Protocols";Voting;Sociology;Statistics;Convergence;Upper bound;"Measurement"","""",""11"","""",""24"",""IEEE"",""13 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"LPM: A Systematic Methodology for Concurrent Data Access Pattern Optimization from a Matching Perspective,""Y. Liu";" X. -H. Sun"",""State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Beijing, China";" Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2478"",""2493"",""As applications become increasingly data intensive, conventional computing systems become increasingly inefficient due to data access performance bottlenecks. While intensive efforts have been made in developing new memory technologies and in designing special purpose machines, there is a lack of solutions for evaluating and utilizing recent hardware advancements to address the memory-wall problem in a systematic way. In this study, we present the memory Layered Performance Matching (LPM) methodology to provide a systematic approach for data access performance optimization. LPM uniquely presents and utilizes the data access concurrency, in addition to data access locality, in a memory hierarchical system. The LPM methodology consists of models and algorithms, and is supported with a series of analytic results for its correctness. The rationale of LPM is to reduce the overall data access delay through the matching of data request rate and data supply rate at each layer of a memory hierarchy, with a balanced consideration of data locality, data concurrency, and latency hiding of data flow. Extensive experimentations on both physical platforms and software simulators confirm our theoretical findings, and they show that the LPM approach can be applied in diverse computing platforms and can effectively guide performance optimization of memory systems."",""1558-2183"","""",""10.1109/TPDS.2019.2912573"",""National Natural Science Foundation of China(grant numbers:61772497,61521092)"; State Key Laboratory of Computer Architecture Foundation(grant numbers:CARCH2601); National Key R&D Program of China(grant numbers:2016YFB1000201); National Science Foundation(grant numbers:CCF-1536079,CNS-1162540,CCF-0937877);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8695868"",""Memory wall";memory stall time;efficiency;performance optimization;layered performance matching (LPM);"memory concurrency"",""Concurrent computing";Optimization;Delays;Program processors;Hardware;Systematics;"Analytical models"","""",""9"","""",""49"",""IEEE"",""23 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"LSM-Tree Managed Storage for Large-Scale Key-Value Store,""F. Mei"; Q. Cao; H. Jiang;" L. Tian"",""Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China"; Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China; University of Texas at Arlington, Arlington, TX, USA;" Tintri, Inc., Mountain View, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""400"",""414"",""Key-value stores are increasingly adopting LSM-trees as their enabling data structure in the backend block storage, and persisting their clustered data through a block manager, usually a file system. In general, a file system is expected to not only provide file/directory abstraction to organize data but also retain the key benefits of LSM-trees, namely, sequential and aggregated I/O patterns on the physical device. Unfortunately, our in-depth experimental analysis reveals that some of these benefits of LSM-trees can be completely negated by the underlying file level indexes from the perspectives of both data layout and I/O processing. As a result, the write performance of LSM-trees is kept at a level far below that promised by the sequential bandwidth offered by the storage devices. In this paper, we address this problem and propose LDS, an LSM-tree Direct Storage system that manages the storage space based on the LSM-tree objects and provides simplified consistency control by leveraging the copy-on-write nature of the LSM-tree structure, to fully reap the benefits of LSM-trees. Running LevelDB, a popular LSM-tree based key-value store, on LDS as a baseline, comparing that to LevelDB running on three representative file systems (ext4, f2fs, btrfs) with HDDs and SSDs, respectively, we evaluate and study the performance potentials of LSM-trees. Evaluation results show that the write throughputs of LevelDB can be improved by from 1.8× to 3× on HDDs, and from 1.3× to 2.5× on SSDs, by employing the LSM-tree friendly data layout of LDS."",""1558-2183"","""",""10.1109/TPDS.2018.2864209"",""Fundamental Research Funds(grant numbers:2018KFYXKJC037)"; US NSF(grant numbers:CCF-1704504,CCF-1629625);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8428463"",""LSM-tree";key-value store;file system performance;"application managed storage"",""Indexes";Compaction;Resource management;Metadata;Layout;Throughput;"File systems"","""",""13"","""",""73"",""IEEE"",""7 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Luopan: Sampling-Based Load Balancing in Data Center Networks,""P. Wang"; G. Trimponias; H. Xu;" Y. Geng"",""Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong"; Huawei Noah's Ark Lab, Hong Kong; Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong;" Huawei Montreal Research Centre, Markham, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""133"",""145"",""Data center networks demand high-performance, robust, and practical data plane load balancing protocols. Despite progress, existing work falls short of meeting these requirements. We design, analyze, and evaluate Luopan, a novel sampling based load balancing protocol that overcomes these challenges. Luopan operates at flowcell granularity similar to Presto. It periodically samples a few paths for each destination switch and directs flowcells to the least congested one. By being congestion-aware, Luopan improves flow completion time (FCT), and is more robust to topological asymmetries compared to Presto. The sampling approach simplifies the protocol and makes it much more scalable for implementation in large-scale networks compared to existing congestion-aware schemes. We provide analysis to show that Luopan's periodic sampling has the same asymptotic behavior as instantaneous sampling: taking 2 random samples provides exponential improvements over 1 sample. We conduct comprehensive packet-level simulations with production workloads. The results show that Luopan consistently outperforms state-of-the-art schemes in large-scale topologies. Compared to Presto, Luopan with 2 samples improves the 99.9%ile FCT of mice flows by up to 35 percent, and average FCT of medium and elephant flows by up to 30 percent. Luopan also performs significantly better than Local Sampling with large asymmetry."",""1558-2183"","""",""10.1109/TPDS.2018.2858815"",""Huawei Technologies(grant numbers:9231208)"; Research Grants Council, University Grants Committee of Hong Kong(grant numbers:GRF-11202315);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8417438"",""Data center networks";load balancing;network congestion;"distributed"",""Switches";Load management;Topology;Mice;Data centers;Protocols;"Production"","""",""34"","""",""38"",""IEEE"",""23 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Making Big Data Open in Edges: A Resource-Efficient Blockchain-Based Approach,""C. Xu"; K. Wang; P. Li; S. Guo; J. Luo; B. Ye;" M. Guo"",""Key Laboratory of Broadband Wireless Communication and Sensor Network Technology, Nanjing University of Posts and Telecommunications, Nanjing, China"; Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing University of Posts and Telecommunications, Nanjing, China; School of Computer Science and Engineering, The University of Aizu, Aizu-Wakamatsu City, Fukushima, Japan; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Electronic Information and Networking Research Institute, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Computer and Infomation, Hohai University, Nanjing, China;" Department of Computer Science and Engineering, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""870"",""882"",""The emergence of edge computing has witnessed a fast-growing volume of data on edge devices belonging to different stakeholders which, however, cannot be shared among them due to the lack of the trust. By exploiting blockchain's non-repudiation and non-tampering properties that enable trust, we develop a blockchain-based big data sharing framework to support various applications across resource-limited edges. In particular, we devise a number of novel resource-efficient techniques for the framework: (1) the PoC (Proof-of-Collaboration) based consensus mechanism with low computation complexity which is especially beneficial to the edge devices with low computation capacity, (2) the blockchain transaction filtering and offloading scheme that can significantly reduce the storage overhead, and (3) new types of blockchain transaction (i.e., Express Transaction) and block (i.e., Hollow Block) to enhance the communication efficiency. Extensive experiments are conducted and the results demonstrate the superior performance of our proposal."",""1558-2183"","""",""10.1109/TPDS.2018.2871449"",""National Basic Research Program of China (973 Program)(grant numbers:2018YFB1004700)"; National Natural Science Foundation of China(grant numbers:61872195,61872310,61832005,61572262,61872240);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8469010"",""Big data";blockchain;collaborative edges;transaction offloading;"consensus mechanism"",""Cryptocurrency";Big Data;Distributed databases;Distributed processing;"Edge computing"","""",""173"","""",""55"",""IEEE"",""20 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Managing Recurrent Virtual Network Updates in Multi-Tenant Datacenters: A System Perspective,""Z. Liu"; Y. Cao; X. Zhang; C. Zhu;" F. Zhang"",""Google Inc., Mountain View, CA, USA"; College of Internet of Things Engineering, Hohai University, Changzhou, China; College of Internet of Things Engineering, Hohai University, Changzhou, China; Key Laboratory of Unmanned Aerial Vehicle Development and Data Application of Anhui Higher Education Institutes, Wanjiang University of Technology, Maanshan, China;" College of Information Science and Electronic Engineering, Zhejiang University, Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Institute of Cyberspace Research, and Zhejiang Lab, Hangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1816"",""1825"",""With the advent of software-defined networking, network configuration through programmable interfaces becomes practical, leading to various on-demand opportunities for network routing update in multi-tenant datacenters, where tenants have diverse requirements on network routings such as short latency, low path inflation, large bandwidth, high reliability, etc. Conventional solutions that rely on topology search coupled with an objective function to find desired routings have at least two shortcomings: ${\sf (i)}$(i) they run into scalability issues when handling consistent and frequent routing updates and ${\sf (ii)}$(ii) they restrict the flexibility and capability to satisfy various routing requirements. To address these issues, this paper proposes a novel search and optimization decoupled design, which not only saves considerable topology search costs via search result reuse, but also avoids possible sub-optimality in greedy routing search algorithms by making decisions based on the global view of all possible routings. We implement a prototype of our proposed system, OpReduce, and perform extensive evaluations to validate its design goals."",""1558-2183"","""",""10.1109/TPDS.2019.2893239"",""National Natural Science Foundation of China(grant numbers:61601168,61671202)"; National Key R&D Program of China(grant numbers:2016YFC0401606); Fundamental Research Funds for the Central Universities(grant numbers:2019B22714); Ministry of Education of the People's Republic of China; Fundamental Research Foundation of Shenzhen(grant numbers:JCYJ20170302151209762); Major Scientific Research Project of Zhejiang Lab(grant numbers:2018FD0ZX01); Alibaba-Zhejiang University Joint Institute of Frontier Technologies;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8613794"",""Recurrent virtual network";multi-tenant datacenters;"routing management"",""Routing";Topology;Network topology;Search problems;Linear programming;Optimization;"Cloud computing"","""",""3"","""",""45"",""IEEE"",""16 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Managing Rich Metadata in High-Performance Computing Systems Using a Graph Model,""D. Dai"; Y. Chen; P. Carns; J. Jenkins; W. Zhang;" R. Ross"",""Department of Computer Science, University of North Carolina at Charlotte, Charlotte, NC, USA"; Department of Computer Science, Texas Tech University, Lubbock, TX, USA; Argonne National Laboratory, Lemont, IL, USA; Argonne National Laboratory, Lemont, IL, USA; Department of Computer Science, Texas Tech University, Lubbock, TX, USA;" Argonne National Laboratory, Lemont, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1613"",""1627"",""High-performance computing (HPC) systems generate huge amounts of metadata about different entities such as jobs, users, and files. Existing systems can efficiently record and manage part of these metadata, mainly the POSIX metadata of data files (e.g., file size, name, and permissions mode). But another important set of metadata, referred to as “rich” metadata in this study, which record not only wider range of entities (e.g., running processes and jobs) but also more complex relationships between them, are mostly missing in current HPC systems. Yet such rich metadata are critical for supporting many advanced data management functions such as identifying data sources and parameters behind a given result"; auditing data usage;" or understanding details about how inputs are transformed into outputs. To uniformly and efficiently manage the rich metadata generated in HPC systems, We propose to utilize a graph model in this study. We identify the key challenges of implementing such a graph-based HPC rich metadata management system and present GraphMeta, a graph-based rich metadata management system designed and optimized for HPC platforms, to tackle these challenges. Extensive evaluations on both synthetic and real HPC metadata workloads show its advantages in both performance and scalability compared with existing solutions."",""1558-2183"","""",""10.1109/TPDS.2018.2887380"",""U.S. Department of Energy, Office of Science(grant numbers:DE-AC02-06CH11357)"; National Science Foundation(grant numbers:CNS-1852815,CNS-1162488,CNS-1338078,IIP-1362134,CCF-1409946);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8580412"",""Data models";metadata;high performance computing;"graph partitioning"",""Metadata";Computational modeling;Runtime;History;Task analysis;Computer science;"Engines"","""",""3"","""",""48"",""IEEE"",""18 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Massively Parallel Tree Search for High-Dimensional Sphere Decoders,""K. Nikitopoulos"; G. Georgis; C. Jayawardena; D. Chatzipanagiotis;" R. Tafazolli"",""5G Innovation Centre, University of Surrey, Guildford, United Kingdom"; 5G Innovation Centre, University of Surrey, Guildford, United Kingdom; 5G Innovation Centre, University of Surrey, Guildford, United Kingdom; 5G Innovation Centre, University of Surrey, Guildford, United Kingdom;" 5G Innovation Centre, University of Surrey, Guildford, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2309"",""2325"",""The recent paradigm shift towards the transmission of large numbers of mutually interfering information streams, as in the case of aggressive spatial multiplexing, combined with requirements towards very low processing latency despite the frequency plateauing of traditional processors, initiates a need to revisit the fundamental maximum-likelihood (ML) and, consequently, the sphere-decoding (SD) detection problem. This work presents the design and VLSI architecture of MultiSphere";" the first method to massively parallelize the tree search of large sphere decoders in a nearly-concurrent manner, without compromising their maximum-likelihood performance, and by keeping the overall processing complexity comparable to that of highly-optimized sequential sphere decoders. For a 10 × 10 MIMO spatially multiplexed system with 16-QAM modulation and 32 processing elements, our MultiSphere architecture can reduce latency by 29× against well-known sequential SDs, approaching the processing latency of linear detection methods, without compromising ML optimality. In MIMO multicarrier systems targeting exact ML decoding, MultiSphere achieves processing latency and hardware efficiency that are orders of magnitude improved compared to approaches employing one SD per subcarrier. In addition, for 16×16 both “hard”and “soft”-output MIMO systems, approximate MultiSphere versions are shown to achieve similar error rate performance with state-of-the art approximate SDs having akin parallelization properties, by using only one tenth of the processing elements, and to achieve up to approximately 9× increased energy efficiency."",""1558-2183"","""",""10.1109/TPDS.2018.2874002"",""Engineering and Physical Sciences Research Council(grant numbers:EP/M029441/1)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8489997"",""Sphere decoding";parallel processing;large multiple-input–multiple-output (MIMO);"lattice search"",""MIMO communication";Complexity theory;Multiplexing;Maximum likelihood decoding;Parallel processing;"Very large scale integration"","""",""19"","""",""50"",""CCBY"",""11 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Maximum Data-Resolution Efficiency for Fog-Computing Supported Spatial Big Data Processing in Disaster Scenarios,""J. Wang"; M. C. Meyer; Y. Wu;" Y. Wang"",""School of Computer Science and Engineering, University of Aizu, Aizu-wakamatsu, Fukushima, Japan"; School of Computer Science and Engineering, University of Aizu, Aizu-wakamatsu, Fukushima, Japan; School of Computer Science and Engineering, University of Aizu, Aizu-wakamatsu, Fukushima, Japan;" School of Computer Science and Engineering, University of Aizu, Aizu-wakamatsu, Fukushima, Japan"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1826"",""1842"",""Spatial big data analysis is very important in disaster scenarios to understand distribution patterns of situations, e.g., people's movements, people's requirements, resource shortage situations, and so on. In a general case, spatial big data is generated from distributed sensing devices and analyzed in a centralized way, e.g., a cloud center with high-performance computing resources. However, data transmission from sensing devices to cloud centers always takes a long time, especially in disaster scenarios with an unstable network. Fog computing is a promising technique to solve the above problem by offloading data processing tasks from the cloud to nearby computation devices. But data resolution also decreases after local processing in the fog nodes. It is necessary to investigate the optimal task distribution solutions to efficiently use computation resources in the fog layer. In this paper, we take the above research problem, and study fog-computing supported spatial big data processing. We analyze the process for spatial clustering, which is a typical category for spatial data analysis, and propose an architecture to integrate data processing into fog computing. We formalize a problem to maximize the data-resolution efficiency by considering data resolution and delay. We further propose core algorithms to enable spatial clustering in a fog-computing environment and implement the above algorithms in a real system. We have performed both simulations and experiments on a real Twitter dataset collected when Kumamoto-city suffered an earthquake. Through the simulations and the experiments, we have determined that the proposed solution significantly outperforms the other solutions."",""1558-2183"","""",""10.1109/TPDS.2019.2896143"",""Japan Science and Technology Agency"; Strategic International Collaborative Research Program (SICORP);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630038"",""Spatial big data analytics";spatial clustering;data resolution;fog computing;"disaster"",""Spatial databases";Big Data;Spatial resolution;Edge computing;Clustering algorithms;"Distributed databases"","""",""17"","""",""33"",""IEEE"",""30 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"MOANA: Modeling and Analyzing I/O Variability in Parallel System Experimental Design,""K. W. Cameron"; A. Anwar; Y. Cheng; L. Xu; B. Li; U. Ananth; J. Bernard; C. Jearls; T. Lux; Y. Hong; L. T. Watson;" A. R. Butt"",""Virginia Tech, Blacksburg, VA, USA"; Virginia Tech, Blacksburg, VA, USA; George Mason University, Fairfax, VA, USA; Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA; Virginia Tech, Blacksburg, VA, USA;" Virginia Tech, Blacksburg, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1843"",""1856"",""Exponential increases in complexity and scale make variability a growing threat to sustaining HPC performance at exascale. Performance variability in HPC I/O is common, acute, and formidable. We take the first step towards comprehensively studying linear and nonlinear approaches to modeling HPC I/O system variability in an effort to demonstrate that variability is often a predictable artifact of system design. Using over 8 months of data collection on 6 identical systems, we propose and validate a modeling and analysis approach (MOANA) that predicts HPC I/O variability for thousands of software and hardware configurations on highly parallel shared-memory systems. Our findings indicate nonlinear approaches to I/O variability prediction are an order of magnitude more accurate than linear regression techniques. We demonstrate the use of MOANA to accurately predict the confidence intervals of unmeasured I/O system configurations for a given number of repeat runs - enabling users to quantitatively balance experiment duration with statistical confidence."",""1558-2183"","""",""10.1109/TPDS.2019.2892129"",""National Science Foundation(grant numbers:#1838271,#1565314)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8631172"",""Variability";performance modeling;"machine learning"",""Throughput";Reactive power;Analytical models;Hardware;Jitter;Benchmark testing;"Force"","""",""11"","""",""46"",""IEEE"",""31 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;
"Modeling and Decoupling the GPU Power Consumption for Cross-Domain DVFS,""J. Guerreiro"; A. Ilic; N. Roma;" P. Tomás"",""INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal"; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal; INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal;" INESC-ID, Instituto Superior Técnico, Universidade de Lisboa, Lisbon, Portugal"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2494"",""2506"",""Dynamic voltage and frequency scaling (DVFS) is a popular technique to improve the energy-efficiency of high-performance computing systems. It allows placing the devices into lower performance states when the computational demands are lower, opening the possibility for significant power/energy savings. This work presents a GPU power consumption model, used to predict the GPU power consumption of any application at different frequency levels. To obtain this model, an estimation algorithm is proposed, relying on careful benchmarking of the GPU architecture. The model can estimate the contribution of twelve different GPU components (FP32-ADD/MUL/FMA, FP64-ADD/MUL/FMA, INT, SF, CF units, shared memory, L2-cache, and DRAM) to the GPU power consumption. Different model use cases are evaluated (fixed-frequency, DVFS, and scaling-factors), which can obtain both the total or the per-component GPU power consumption. A technique to export models to a distinct GPU from the one it was estimated on is also proposed. These approaches were extensively validated on five different GPUs from the three most recent microarchitectures with a set of 42 standard benchmarks, achieving very accurate predictions. In particular, the scaling-factor power model achieves an average prediction error of 3.5 percent (Titan Xp), 4.6 percent (GTX Titan X), 3.1 percent (GTX 980) and 2.4 percent (Tesla K40c)."",""1558-2183"","""",""10.1109/TPDS.2019.2917181"",""national funds through Fundação para a Ciência e a Tecnologia (FCT)(grant numbers:SFRH/BD/101457/2014,UID/CEC/50021/2019,PTDC/EEI-HAC/30485/2017,PTDC/CCI-COM/31901/2017)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716300"",""GPGPU";DVFS;power modeling;"scaling-factors"",""Graphics processing units";Power demand;Predictive models;Benchmark testing;Memory management;Frequency-domain analysis;"Computational modeling"","""",""14"","""",""38"",""IEEE"",""16 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Modeling Non-Uniform Memory Access on Large Compute Nodes with the Cache-Aware Roofline Model,""N. Denoyelle"; B. Goglin; A. Ilic; E. Jeannot;" L. Sousa"",""Atos Bull Technologies, Echirolles, France"; Inria – Bordeaux - Sud-Ouest, University Bordeaux, Bordeaux, France; INESC-ID, Instituto Superior Técnico, Lisbon, Portugal; LaBRI - Laboratoire Bordelais de Recherche en Informatique, Inria Bordeaux - Sud-Ouest, Talence, France;" INESC-ID, Instituto Superior Técnico, Lisbon, Portugal"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1374"",""1389"",""NUMA platforms, emerging memory architectures with on-package high bandwidth memories bring new opportunities and challenges to bridge the gap between computing power and memory performance. Heterogeneous memory machines feature several performance trade-offs, depending on the kind of memory used, when writing or reading it. Finding memory performance upper-bounds subject to such trade-offs aligns with the numerous interests of measuring computing system performance. In particular, representing applications performance with respect to the platform performance bounds has been addressed in the state-of-the-art Cache-Aware Roofline Model (CARM) to troubleshoot performance issues. In this paper, we present a Locality-Aware extension (LARM) of the CARM to model NUMA platforms bottlenecks, such as contention and remote access. On top of this, the new contribution of this paper is the design and validation of a novel hybrid memory bandwidth model. This new hybrid model quantifies the achievable bandwidth upper-bound under above-described trade-offs with less than 3 percent error. Hence, when comparing applications performance with the maximum attainable performance, software designers can now rely on more accurate information."",""1558-2183"","""",""10.1109/TPDS.2018.2883056"",""COST Action(grant numbers:IC1305)"; Fundação para a Ciência e a Tecnologia(grant numbers:UID/CEC/50021/2013,LISBOA-01-0145-FEDER-031901,PTDC/CCI-COM/31901/2017); COST Action IC1305 (NESUS); Intel Corporation and Atos; Fundação para a Ciência e a Tecnologia(grant numbers:UID/CEC/50021/2013,LISBOA-01-0145-FEDER-031901,PTDC/CCI-COM/31901/2017);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8543640"",""Roofline model";cache-aware roofline model;heterogeneous memory;knights landing;skylake;platform modeling;benchmarking;NUMA;multi-core/single-chip multiprocessors;shared memory;"memory bandwidth"",""Bandwidth";Computational modeling;Throughput;Artificial intelligence;Data models;"Memory management"","""",""9"","""",""22"",""IEEE"",""23 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;
"moDNN: Memory Optimal Deep Neural Network Training on Graphics Processing Units,""X. Chen"; D. Z. Chen; Y. Han;" X. S. Hu"",""State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Beijing, China"; Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; State Key Laboratory of Computer Architecture, Chinese Academy of Sciences, Beijing, China;" Department of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""646"",""661"",""Graphics processing units (GPUs) have been widely adopted to accelerate the training of deep neural networks (DNNs). Although the computational performance of GPUs has been improving steadily, the memory size of modern GPUs is still quite limited, which restricts the sizes of DNNs that can be trained on GPUs, and hence raises serious challenges. This paper introduces a framework, referred to as moDNN (memory optimal DNN training on GPUs), to optimize the memory usage in DNN training. moDNN supports automatic tuning of DNN training code to match any given memory budget (not smaller than the theoretical lower bound). By taking full advantage of overlapping computations and data transfers, we develop new heuristics to judiciously schedule data offloading and prefetching transfers, together with convolution algorithm selection, to optimize memory usage. We further devise a new sub-batch size selection method which also greatly reduces memory usage. moDNN can save memory usage up to 59×, compared with an ideal case which assumes that the GPU memory is sufficient to hold all data. When executing moDNN on a GPU with 12 GB memory, the training time is increased by only 3 percent, which is much shorter than that incurred by the best known approach, vDNN. Furthermore, we propose an optimization strategy for moDNN on multiple GPUs again by utilizing the idea of overlapping data transfers and GPU computations. The results show that 3.7× speedup is attained on four GPUs."",""1558-2183"","""",""10.1109/TPDS.2018.2866582"",""National Science Foundation(grant numbers:CCF-1217906,CNS-1629914,CCF-1617735,CCF-1640081)"; Nanoelectronics Research Corporation; Semiconductor Research Corporation; Extremely Energy Efficient Collective Electronics; SRC-NRI Nanoelectronics Research Initiative(grant numbers:2698.004,2698.005);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444714"",""Deep neural networks";graphics processing units;"memory usage"",""Training";Graphics processing units;Memory management;Resource management;Neural networks;Data transfer;"Prefetching"","""",""7"","""",""46"",""IEEE"",""23 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Multi-Hop Cooperative Computation Offloading for Industrial IoT–Edge–Cloud Computing Environments,""Z. Hong"; W. Chen; H. Huang; S. Guo;" Z. Zheng"",""School of Data and Computer Science, and the National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China"; School of Data and Computer Science, and the National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China; School of Data and Computer Science, and the National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China; Department of Computing, Hong Kong Polytechnic University, Hong Kong;" School of Data and Computer Science, and the National Engineering Research Center of Digital Life, Sun Yat-sen University, Guangzhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2759"",""2774"",""The concept of the industrial Internet of things (IIoT) is being widely applied to service provisioning in many domains, including smart healthcare, intelligent transportation, autopilot, and the smart grid. However, because of the IIoT devices' limited onboard resources, supporting resource-intensive applications, such as 3D sensing, navigation, AI processing, and big-data analytics, remains a challenging task. In this paper, we study the multi-hop computation-offloading problem for the IIoT-edge-cloud computing model and adopt a game-theoretic approach to achieving Quality of service (QoS)-aware computation offloading in a distributed manner. First, we study the computation-offloading and communication-routing problems with the goal of minimizing each task's computation time and energy consumption, formulating the joint problem as a potential game in which the IIoT devices determine their computation-offloading strategies. Second, we apply a free-bound mechanism that can ensure a finite improvement path to a Nash equilibrium. Third, we propose a multi-hop cooperative-messaging mechanism and develop two QoS-aware distributed algorithms that can achieve the Nash equilibrium. Our simulation results show that our algorithms offer a stable performance gain for IIoT in various scenarios and scale well as the device size increases."",""1558-2183"","""",""10.1109/TPDS.2019.2926979"",""National Key Research and Development Plan(grant numbers:2018YFB1003803)"; National Natural Science Foundation of China(grant numbers:61802450,61722214); Natural Science Foundation of Guangdong Province(grant numbers:2018A030313005); Program for Guangdong Introducing Innovative and Entrepreneurial Teams(grant numbers:2017ZT07X355);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8756083"",""Computation offloading";edge computing;cloud computing;game theory;"industrial IoT"",""Cloud computing";Task analysis;Quality of service;Computational modeling;Servers;Distributed algorithms;"Edge computing"","""",""142"","""",""48"",""IEEE"",""5 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Multi-Level Elasticity for Data Stream Processing,""V. Marangozova-Martin"; N. de Palma;" A. El Rheddane"",""CNRS, University Grenoble Alpes, Grenoble, France"; CNRS, University Grenoble Alpes, Grenoble, France;" CNRS, University Grenoble Alpes, Grenoble, France"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2326"",""2337"",""This paper investigates reactive elasticity in stream processing environments where the performance goal is to analyze large amounts of data with low latency and minimum resources. Working in the context of Apache Storm, we propose an elastic management strategy which modulates the parallelism degree of applications’ components while explicitly addressing the hierarchy of execution containers (virtual machines, processes and threads). We show that provisioning the wrong kind of container may lead to performance degradation and propose a solution that provisions the least expensive container (with minimum resources) to increase performance. We describe our monitoring metrics and show how we take into account the specifics of an execution environment. We provide an experimental evaluation with real-world applications which validates the applicability of our approach."",""1558-2183"","""",""10.1109/TPDS.2019.2907950"",""BPI and FEDER(grant numbers:16.010402.01.)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675450"",""Stream processing";multi-level elasticity;"apache storm"",""Containers";Storms;Measurement;Elasticity;Data models;Proposals;"Computational modeling"","""",""14"","""",""45"",""IEEE"",""27 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;
"Numerically Stable Recurrence Relations for the Communication Hiding Pipelined Conjugate Gradient Method,""S. Cools"; J. Cornelis;" W. Vanroose"",""Applied Mathematics Group, University of Antwerp, Antwerp, Belgium"; Applied Mathematics Group, University of Antwerp, Antwerp, Belgium;" Applied Mathematics Group, University of Antwerp, Antwerp, Belgium"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2507"",""2522"",""Pipelined Krylov subspace methods (also referred to as communication-hiding methods) have been proposed in the literature as a scalable alternative to classic Krylov subspace algorithms for iteratively computing the solution to a large linear system in parallel. For symmetric and positive definite system matrices the pipelined Conjugate Gradient method, p(ll)-CG, outperforms its classic Conjugate Gradient counterpart on large scale distributed memory hardware by overlapping global communication with essential computations like the matrix-vector product, thus “hiding” global communication. A well-known drawback of the pipelining technique is the (possibly significant) loss of numerical stability. In this work a numerically stable variant of the pipelined Conjugate Gradient algorithm is presented that avoids the propagation of local rounding errors in the finite precision recurrence relations that construct the Krylov subspace basis. The multi-term recurrence relation for the basis vector is replaced by ℓ three-term recurrences, improving stability without increasing the overall computational cost of the algorithm. The proposed modification ensures that the pipelined Conjugate Gradient method is able to attain a highly accurate solution independently of the pipeline length. Numerical experiments demonstrate a combination of excellent parallel performance and improved maximal attainable accuracy for the new pipelined Conjugate Gradient algorithm. This work thus resolves one of the major practical restrictions for the useability of pipelined Krylov subspace methods."",""1558-2183"","""",""10.1109/TPDS.2019.2917663"",""University of Antwerp Research Council"; University Research Fund; Flemish Research Foundation (FWO Flanders)(grant numbers:12H4617N);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8718313"",""Krylov subspace methods";pipelining;parallel performance;global communication;latency hiding;conjugate gradients;numerical stability;inexact computations;"attainable accuracy"",""Gradient methods";Symmetric matrices;Global communication;Pipeline processing;Numerical stability;"Hardware"","""",""11"","""",""47"",""IEEE"",""20 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"On Cost-Driven Collaborative Data Caching: A New Model Approach,""Y. Wang"; S. He; X. Fan; C. Xu;" X. -H. Sun"",""Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China"; School of Computer Science, Wuhan University, Luojiashan, Wuhan, Hubei, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China;" Department of Computer Science, Illinois Institute of Technology, Chicago, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""662"",""676"",""In this paper we consider a new caching model that enables data sharing for network services in a cost-effective way. The proposed caching algorithms are characterized by using monetary cost and access information to control the cache replacements, instead of exploiting capacity-oriented strategies as in traditional approaches. In particular, given a stream of requests to a shared data item with respect to a homogeneous cost model, we first propose a fast off-line algorithm using dynamic programming techniques, which can generate an optimal schedule within $O(mn)$ time-space complexity by using cache, migration as well as replication to serve a $n$-length request sequence in a $m$-node network, substantially improving the previous results. Furthermore, we also study the online form of this problem, and present an 3-competitive online algorithm by leveraging an idea of anticipatory caching. The algorithm can serve an online request in constant time and is space efficient in $O(m)$ as well, rendering it more practical in reality. We evaluate our algorithms, together with some variants, by conducting extensive simulation studies. Our results show that the optimal cost of the off-line algorithm is changed in a parabolic form as the ratio of caching cost to transfer cost is increased, and the online algorithm is less than 2 times worse in most cases than its optimal off-line counterpart."",""1558-2183"","""",""10.1109/TPDS.2018.2868642"",""National Key R&D Program of China(grant numbers:2018YFB1004804)"; National Science Foundation of China(grant numbers:61672513); Shenzhen Oversea High-Caliber Personnel Innovation Funds(grant numbers:KQCX20170331161854); National Science Foundation of China(grant numbers:61572377); Shenzhen Basic Research Program(grant numbers:JCYJ20170818163026031,JCYJ20170818153016513);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8454824"",""Data caching and migration";collaborative caching;anticipatory caching;dynamic programming;"competitive analysis"",""Heuristic algorithms";Data models;Servers;Trajectory;Streaming media;"Dynamic programming"","""",""14"","""",""30"",""IEEE"",""5 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"On Optimal Trees for Irregular Gather and Scatter Collectives,""J. L. Träff"",""Faculty of Informatics, TU Wien (Vienna University of Technology), Vienna, Austria"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""2060"",""2074"",""We study the complexity of finding communication trees with the lowest possible completion time for rooted, irregular gather and scatter collective communication operations in fully connected, k-ported communication networks under a linear-time transmission cost model. Consecutively numbered processors specify data blocks of possibly different sizes to be collected at (gather) or distributed from (scatter) some (given) root processor where they are stored in processor order. We distinguish between ordered and non-ordered communication trees depending on whether segments of blocks are maintained in processor order. We show that lowest completion time, ordered communication trees under one-ported communication can be found in polynomial time by giving simple, but costly dynamic programming algorithms. In contrast, we show that it is an NP-hard problem to construct completion-time optimal, non-ordered communication trees. We have implemented the dynamic programming algorithms for homogeneous networks to evaluate the quality of different types of communication trees, in particular to analyze a recent, distributed, problem-adaptive tree construction algorithm. Model experiments show that this algorithm is close to optimum for a selection of block size and root processor distributions. A concrete implementation for specially structured problems shows that optimal, non-binomial trees can possibly have even further practical advantage."",""1558-2183"","""",""10.1109/TPDS.2019.2899843"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8642840"",""Gather and scatter collective operations";irregular collective operations;communication trees; $k$   k    -ported communication networks;dynamic programming;"NP-hardness"",""Program processors";Communication networks;Heuristic algorithms;Dynamic programming;Data models;Computational modeling;"Complexity theory"","""",""1"","""",""23"",""IEEE"",""15 Feb 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;;
"On Runtime Communication and Thermal-Aware Application Mapping and Defragmentation in 3D NoC Systems,""B. Li"; X. Wang; A. K. Singh;" T. Mak"",""School of Software Engineering, South China University of Technology, Guangzhou, China"; School of Software Engineering, South China University of Technology, Guangzhou, China; School of Computer Science and Electronic Engineering, University of Essex, Colchester, United Kingdom;" School of Electronics and Computer Science, University of Southampton, Southampton, United Kingdom"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2775"",""2789"",""Many-core systems connected by 3D Networks-on-Chip (NoC) are emerging as a promising computation engine for systems like cloud computing servers, big data systems, etc. Mapping applications at runtime to 3D NoCs is the key to maintain high throughput of the overall chip under a thermal/power constraint. However, the goals of optimizing both the communication latency and chip peak temperature are contradicting due to several reasons. First, exploiting the vertical TSV links can accelerate communications, while low peak temperature prefers that the tasks to be mapped closer to the heat sink, instead of using the vertical links. Second, mapping tasks in close proximity can reduce communication latency, but at the cost of poor heat dissipation. To address these issues, in this paper, we propose an efficient runtime mapping algorithm to reduce both communication latency and overall application running time under thermal constraint. In essence, this algorithm first selects a 3D cuboid core region of a specific shape for each incoming application by setting the region's number of occupied vertical layers and its distance to the heat sink, in order to optimize its communication performance and peak temperature. Next, the exact locations of the core regions in the chip are determined, followed by a task-to-core mapping. A defragmentation algorithm is also proposed to keep free core regions contiguous. The experimental results have confirmed that, compared to two recently proposed runtime mapping algorithms, our proposed approach can reduce the total running time by up to 48% and communication cost by up to 44%, with a low runtime overhead."",""1558-2183"","""",""10.1109/TPDS.2019.2921542"",""Natural Science Foundation of Guangdong Province(grant numbers:2018A030313166)"; Research Grant of Guangdong Province(grant numbers:2017A050501003); Pearl River S and T Nova Program of Guangzhou(grant numbers:201806010038); Fundamental Research Funds for the Central Universities(grant numbers:2019MS087); National Natural Science Foundation of China(grant numbers:61971200);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733089"",""3D NoC";application mapping;thermal management;"defragmentation"",""Task analysis";Three-dimensional displays;Runtime;Heat sinks;Through-silicon vias;Heating systems;Thermal management;"Multicore processing"","""",""17"","""",""31"",""IEEE"",""10 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"On the Design of a Time, Resource and Energy Efficient Multi-Installment Large-Scale Workload Scheduling Strategy for Network-Based Compute Platforms,""X. Wang"; B. Veeravalli;" H. Ma"",""School of Computer Science and Technology, Xidian University, Xi’an, Shaanxi, China"; Department of Electrical and Computer Engineering, National University of Singapore, 4 Engineering Drive 3, Singapore;" School of Computer Science and Technology, Xidian University, Xi’an, Shaanxi, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1120"",""1133"",""Multi-installment scheduling (MIS) has been deemed as a promising paradigm that can sharply reduce the processing time of large-scale divisible workloads on various network-based compute platforms. Unfortunately, the practicality of MIS was crippled due to its overwhelming complexity for deriving optimal values for (n × m) + 2 related variables, i.e., we have to obtain an optimal number n of required computing resources, optimal number m of installments, and optimal load partition matrix A = (αij)n×m which determines the sizes of load fractions assigned to each computing unit in every installment. To circumvent this complexity, in this paper, we first derive explicit analytical expressions for optimal load partition matrix A of size n × m based on a given number of n and m. Then we propose a heuristic algorithm referred to as Time, Resource, and Energy Efficient MIS (TREE-MIS) to determine optimal values of n and m. The efficiency of our approach is shown to significantly improve since it can produce globally optimal solutions directly for (n × m) variables among (n × m) + 2 in total for MIS problems based on the derived analytical expressions within a short runtime. We conduct extensive simulations to demonstrate the effectiveness of the proposed algorithm. Simulation results show that our TREE-MIS can not only minimize the processing time of workloads as well as improve resource utilization of the compute platform but also drastically reduce the runtime compared to other state-of-art MIS strategies. Furthermore, while handling large-scale workloads in any large network infrastructures would inexorably result in significant amounts of energy wastage if the strategy is not prudently designed. As an offshoot of our analysis and design, we clearly demonstrate that the energy wastage in adopting our TREE-MIS is kept minimum when compared to other currently available strategies in practice."",""1558-2183"","""",""10.1109/TPDS.2018.2877668"",""National Natural Science Foundation of China(grant numbers:61472297,61572391)"; Natural Science Foundation of Shaanxi Province(grant numbers:2018JQ6038,2017JM6036);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8515109"",""Multi-installment scheduling";load partition;processing time;divisible load;"resource utilization"",""Servers";Computational modeling;Processor scheduling;Heuristic algorithms;Runtime;Load modeling;"Complexity theory"","""",""2"","""",""20"",""IEEE"",""30 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"On the Feasibility of Inter-Domain Routing via a Small Broker Set,""T. Liu"; J. C. S. Lui; D. Lin;" D. Hui"",""Chinese University of Hong Kong, Shatin, Hong Kong"; Chinese University of Hong Kong, Shatin, Hong Kong; Huawei Technology Co., Shenzhen, China;" Huawei Technology Co., Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""415"",""427"",""The Internet is a gigantic distributed system where the end-to-end (E2E) quality-of-service (QoS) plays an important role. Yet the current inter-domain routing protocol, namely, the Border Gateway Protocol (BGP), cannot provide E2E QoS guarantees. The main reason is that an autonomous system (AS) can only receive guarantees from its first-hop ASes via service level agreements (SLAs). But beyond the first-hop, QoS along the path from a source AS to a destination AS is not within the source AS's control regime. This makes it difficult to provide high quality-of-experience services to many Internet users even when many content providers are willing to pay for such high quality E2E guarantees. In this paper, we investigate the feasibility of providing high QoS-guaranteed E2E transit services by utilizing a (small) set of ASes/IXPs to serve as “brokers” to provide supervision, control and resource negotiation. Finding an optimal set of ASes as brokers can be formulated as a Maximum Coverage with $B-$dominating path Guarantee (MCBG) problem, and we show that it is in fact NP-hard. To address this problem, we design a $(\frac{1{-}e^{-\!1}}{2}){-}$approximation algorithm and also an efficient heuristic algorithm when additional constraints (e.g., the path length) are considered. We further analyze the APX-hardness of the MCBG problem to reveal the existence of the best approximation ratio. Based on the current Internet topology, we demonstrate that it is indeed feasible to provide high QoS guarantees for most E2E connections with only a small broker set: with only 0.19, 1.9 or 6.8 percent ASes/IXPs serving as brokers, 53.13, 85.41 or 99.29 percent of all global E2E connections can receive high QoS-guaranteed services. Finally, we provide an economic model to study the behaviours of ASes when cooperating our brokerage scheme with the BGP protocol, and show that there are incentives to form and maintain such a brokerage coalition."",""1558-2183"","""",""10.1109/TPDS.2018.2865572"",""GRF"; Huawei Research Fund;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8437158"",""Inter-domain routing";E2E QoS;"dominating-path guarantee"",""Quality of service";Routing;Internet;Economics;Approximation algorithms;Heuristic algorithms;"Routing protocols"","""",""1"","""",""37"",""IEEE"",""15 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"On the Scheduling of Energy-Aware Fault-Tolerant Mixed-Criticality Multicore Systems with Service Guarantee Exploration,""S. Safari"; M. Ansari; G. Ershadi;" S. Hessabi"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2338"",""2354"",""Advancement of Cyber-Physical Systems has attracted attention to Mixed-Criticality Systems (MCSs), both in research and in industrial designs. As multicore platforms are becoming the dominant trend in MCSs, joint energy and reliability management is a crucial issue. In addition, providing guaranteed service level for low-criticality tasks in critical mode is of great importance. To address these problems, we propose “LETR-MC” scheme that simultaneously supports certification, energy management, fault-tolerance, and guaranteed service level in mixed-criticality multicore systems. In this paper, we exploit task-replication to not only satisfy reliability requirements, but also to improve the QoS of low-criticality tasks in overrun situation. Our proposed LETR-MC scheme determines the number of replicas, and reduces the execution time overlap between the primary tasks and replicas. Moreover, instead of ignoring low-criticality tasks or selectively executing them without any guaranteed service level in overrun mode, it mathematically explores the minimum achievable service guarantee for each low-criticality task in different execution modes, i.e., normal, fault-occurrence, overrun and critical operation modes. We develop novel unified demand bound functions (DBF), along with a DVFS method based on the proposed DBF analysis. Our experimental results show that LETR-MC provides up to 59 percent (24 percent on average) energy saving, and significantly improves the service levels of low-criticality tasks compared to the state-of-the-art schemes."",""1558-2183"","""",""10.1109/TPDS.2019.2907846"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8675526"",""Task replication";Energy management;Guaranteed service level;DBF;Multicores;"Mixed-criticality systems"",""Task analysis";Multicore processing;Fault tolerance;Fault tolerant systems;Quality of service;"Certification"","""",""41"","""",""67"",""IEEE"",""27 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Online Diagnosis of Performance Variation in HPC Systems Using Machine Learning,""O. Tuncer"; E. Ates; Y. Zhang; A. Turk; J. Brandt; V. J. Leung; M. Egele;" A. K. Coskun"",""Electrical and Computer Engineering Department, Boston University, Boston, MA, USA"; Electrical and Computer Engineering Department, Boston University, Boston, MA, USA; Electrical and Computer Engineering Department, Boston University, Boston, MA, USA; Electrical and Computer Engineering Department, Boston University, Boston, MA, USA; Sandia National Laboratories, Albuquerque, NM, USA; Sandia National Laboratories, Albuquerque, NM, USA; Electrical and Computer Engineering Department, Boston University, Boston, MA, USA;" Electrical and Computer Engineering Department, Boston University, Boston, MA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""883"",""896"",""As the size and complexity of high performance computing (HPC) systems grow in line with advancements in hardware and software technology, HPC systems increasingly suffer from performance variations due to shared resource contention as well as software- and hardware-related problems. Such performance variations can lead to failures and inefficiencies, which impact the cost and resilience of HPC systems. To minimize the impact of performance variations, one must quickly and accurately detect and diagnose the anomalies that cause the variations and take mitigating actions. However, it is difficult to identify anomalies based on the voluminous, high-dimensional, and noisy data collected by system monitoring infrastructures. This paper presents a novel machine learning based framework to automatically diagnose performance anomalies at runtime. Our framework leverages historical resource usage data to extract signatures of previously-observed anomalies. We first convert collected time series data into easy-to-compute statistical features. We then identify the features that are required to detect anomalies, and extract the signatures of these anomalies. At runtime, we use these signatures to diagnose anomalies with negligible overhead. We evaluate our framework using experiments on a real-world HPC supercomputer and demonstrate that our approach successfully identifies 98 percent of injected anomalies and consistently outperforms existing anomaly diagnosis techniques."",""1558-2183"","""",""10.1109/TPDS.2018.2870403"",""Sandia National Laboratories(grant numbers:*****)"; National Technology and Engineering Solutions of Sandia, LLC.; U.S. Department of Energy(grant numbers:DE-NA0003525);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466019"",""High performance computing";anomaly detection;machine learning;"performance variation"",""Feature extraction";Measurement;Runtime;Anomaly detection;Machine learning;Monitoring;"Time series analysis"","""",""31"","""",""47"",""IEEE"",""14 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"Online Job Scheduling with Redundancy and Opportunistic Checkpointing: A Speedup-Function-Based Analysis,""H. Xu"; G. De Veciana; W. C. Lau;" K. Zhou"",""School of Computer Science and Network Security, Dongguan University of Technology, Dongguan, Guangdong, China"; Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA; Department of Information Engineering, The Chinese University of Hong Kong, Shatin, N.T., Hong Kong;" School of Computer Science and Network Security, Dongguan University of Technology, Dongguan, Guangdong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""897"",""909"",""In a large-scale computing cluster, the job completions can be substantially delayed due to two sources of variability, namely, variability in the job size and that in the machine service capacity. To tackle this issue, existing works have proposed various scheduling algorithms which exploit redundancy wherein a job runs on multiple servers until the first completes. In this paper, we explore the impact of variability in the machine service capacity and adopt a rigorous analytical approach to design scheduling algorithms using redundancy and checkpointing. We design several online algorithms which can dynamically vary the number of redundant copies for jobs. We also provide new theoretical performance bounds for these algorithms in terms of the overall job flowtime by introducing the notion of a speedup function, based on which a novel potential function can be defined to enable the corresponding competitive ratio analysis. In particular, by adopting the online primal-dual fitting approach, we prove that our SRPT+R Algorithm in a non-multitasking cluster is $(1+\epsilon)$(1+ε)(1+ε)-speed, $\ O(\frac{1}{\epsilon })$O(1ε)O(1ε)-competitive. We also show that our proposed Fair+R and LAPS+R($\beta$ββ) Algorithms for a multitasking cluster are $(4+\epsilon)$(4+ε)(4+ε)-speed, $\ O(\frac{1}{\epsilon })$O(1ε)O(1ε)-competitive and ($2 + 2\beta + 2\epsilon)$2+2β+2ε)2+2β+2ε)-speed $O(\frac{1}{\beta \epsilon })$O(1βε)O(1βε)-competitive respectively. We demonstrate via extensive simulations that our proposed algorithms can significantly reduce job flowtime under both the non-multitasking and multitasking modes."",""1558-2183"","""",""10.1109/TPDS.2018.2871135"",""IEEE Infocom";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8468118"",""Online scheduling";redundancy;optimization;competitive analysis;dual-fitting;"potential function"",""Redundancy";Multitasking;Checkpointing;Clustering algorithms;Task analysis;Scheduling algorithms;"Servers"","""",""2"","""",""55"",""IEEE"",""19 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Optimistic Modeling and Simulation of Complex Hardware Platforms and Embedded Systems on Many-Core HPC Clusters,""A. Poshtkohi"; M. B. Ghaznavi-Ghoushchi;" K. Saghafi"",""Department of Electrical Engineering, Shahed University, Tehran, Iran"; Department of Electrical Engineering, Shahed University, Tehran, Iran;" Department of Electrical Engineering, Shahed University, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""428"",""444"",""The large size and complexity of the modern digital hardware impose great challenges to design and validation. Hardware Description Languages (HDLs) and System-Level Description Languages (SLDLs) rely on sequential discrete event semantics. Parallel Discrete Event Simulation (PDES) has recently gained extensive attention for parallelizing these languages due to the ever-increasing complexity of embedded and cyber physical systems. However, PDES application has not yet reached acceptable maturity and pervasiveness for accelerating computer architecture problems. This is due to inherent complexity of hardware components that require using different advanced PDES techniques. In this paper, we look at the main problem from a radically different angle. First, we suppose there is only a single universal discrete event model of computation for simulation purely defined by distributed optimistic PDES, i.e., logical-process-based event scheduling worldview. Second, we construct a new parallel system-level simulation language called OSML for ESL. Third, we propose a unified Cloud-based CAD tool called Troodon to automatically parallelize existing hardware languages atop OSML. To the best of our knowledge, OSML is the first work on optimistic synchronization applied to hardware-specific SLDLs and hardware models at different levels of abstraction in ESL, which contain complex data structures by proposing a hybrid checkpointing scheme."",""1558-2183"","""",""10.1109/TPDS.2018.2860014"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419330"",""Parallel discrete event simulation (PDES)";optimistic synchronization;system-level description languages (SLDLs);electronic system level (ESL);"hybrid checkpointing"",""Solid modeling";Hardware;Computational modeling;Synchronization;Object oriented modeling;Tools;"Complexity theory"","""","""","""",""44"",""IEEE"",""25 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Optimizing Egalitarian Performance when Colocating Tasks with Types for Cloud Data Center Resource Management,""F. Pascual";" K. Rzadca"",""CNRS, Laboratoire d'Informatique de Paris 6, Sorbonne Université, Paris, France";" Institute of Informatics, University of Warsaw, Warszawa, Poland"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2523"",""2535"",""In data centers, up to dozens of tasks are colocated on a single physical machine. Machines are used more efficiently, but the performance of the tasks deteriorates, as the colocated tasks compete for shared resources. Since the tasks are heterogeneous, the resulting performance dependencies are complex. In our previous work [1], [2] we proposed a new combinatorial optimization model that uses two parameters of a task - its size and its type - to characterize how a task influences the performance of other tasks allocated to the same machine. In this paper, we study the egalitarian optimization goal: the aim is to optimize the performance of the worst-off task. This problem generalizes the classic makespan minimization on multiple processors (PIICmax). We prove that polynomially-solvable variants of PIICmax are NP-hard forthis generalization, and that the problem is hard to approximate when the number of types is not constant. For a constant number of types, we propose a PTAS, a fast approximation algorithm, and a series of heuristics. We simulate the algorithms on instances derived from a trace of one of Google clusters. Compared with baseline algorithms solving PIICmax, our proposed algorithms aware of the types of the jobs lead to significantly better tasks' performance. The notion of type enables us to extend standard combinatorial optimization methods to handle degradation of performance caused by colocation. Types add a layer of additional complexity. However, our results - approximation algorithms and good average-case performance - show that types can be handled efficiently."",""1558-2183"","""",""10.1109/TPDS.2019.2911084"",""Polish National Science Center(grant numbers:UMO-2017/25/B/ST6/00116)"; Polonium;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691592"",""Cloud computing";scheduling;complexity;approximation algorithm;heterogeneity;co-tenancy;"workload co-location"",""Task analysis";Approximation algorithms;Data centers;Resource management;Clustering algorithms;Computational modeling;"Load modeling"","""","""","""",""37"",""IEEE"",""14 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Optimizing Finite Volume Method Solvers on Nvidia GPUs,""J. Xu"; H. Fu; W. Luk; L. Gan; W. Shi; W. Xue; C. Yang; Y. Jiang; C. He;" G. Yang"",""Tsinghua University, Beijing, China"; Tsinghua University, Beijing, China; Imperial College, London, United Kingdom; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Tsinghua University, Beijing, China; Peking University, Beijing, China; Graduate School at Shenzhen, Tsinghua University, Shenzhen, China; Computer Science and Technology, Tsinghua University, Beijing, China;" Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2790"",""2805"",""As scientific applications are increasingly ported to GPUs to benefit from both the powerful computing capacity and high throughput, accelerating explicit solvers for GPU-based finite volume methods is gaining more and more attention. In this paper, based on the detailed analysis of the FVM algorithm, we present a set of novel optimization methods, including the explicit data cache mechanism, optimal global memory loading strategy, as well as the inner-thread rescheduling method, which derives a suitable mapping from the solver algorithm to the underlying GPU hardware architecture, so as to remarkably improve the solving performance of structured mesh based FVM. We demonstrate the impact of our tuning techniques on two widely-used atmospheric dynamic kernels (3-D Euler and 2-D SWE) on five kinds of mainstream GPU platforms, and make a detailed analysis of the different tuning methodologies so as to demonstrate how to select the proper tuning strategy to different applications on various GPU platforms. Specifically, 93.9x speedup is achieved for the 3D Euler solver on Nvidia V100 over one 12-core Intel E5-2697 (v2) CPU, which is a 77 percent improvement compared with the original speedup without adopting the tuning techniques presented in this work."",""1558-2183"","""",""10.1109/TPDS.2019.2926084"",""National Key Research & Development Plan of China(grant numbers:2016YFA0602200,2017YFA0604500)"; National Natural Science Foundation of China(grant numbers:51761135015); Basic Research Discipline Project of Shenzhen(grant numbers:JCYJ20180508152204044); Key Science & Technology Laboratory Project of Shenzhen(grant numbers:ZDSYS20140509172959989); Center for High Performance Computing and System Simulation; Pilot National Laboratory for Marine Science and Technology;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8753678"",""Finite volume method";GPU;performance optimization;"scientific applications"",""Graphics processing units";Tuning;Hardware;Computer architecture;Optimization methods;"Instruction sets"","""",""10"","""",""49"",""IEEE"",""2 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;
"Optimizing Lossy Compression Rate-Distortion from Automatic Online Selection between SZ and ZFP,""D. Tao"; S. Di; X. Liang; Z. Chen;" F. Cappello"",""Department of Computer Science, The University of Alabama, Tuscaloosa, AL, USA"; Mathematics and Computer Science division, Argonne National Laboratory, Lemont, IL, USA; Department of Computer Science and Engineering, University of California, Riverside, CA, USA; Department of Computer Science and Engineering, University of California, Riverside, CA, USA;" Mathematics and Computer Science division, Argonne National Laboratory, Lemont, IL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1857"",""1871"",""With ever-increasing volumes of scientific data produced by high-performance computing applications, significantly reducing data size is critical because of limited capacity of storage space and potential bottlenecks on I/O or networks in writing/reading or transferring data. SZ and ZFP are two leading BSD licensed open source C/C++ libraries for compressed floating-point arrays that support high throughput read and write random access. However, their performance is not consistent across different data sets and across different fields of some data sets, which raises the need for an automatic online (during compression) selection between SZ and ZFP, with minimal overhead. In this paper, the automatic selection optimizes the rate-distortion, an important statistical quality metric based on the signal-to-noise ratio. To optimize for rate-distortion, we investigate the principles of SZ and ZFP. We then propose an efficient online, low-overhead selection algorithm that predicts the compression quality accurately for two compressors in early processing stages and selects the best-fit compressor for each data field. We implement the selection algorithm into an open-source library, and we evaluate the effectiveness of our proposed solution against plain SZ and ZFP in a parallel environment with 1,024 cores. Evaluation results on three data sets representing about 100 fields show that our selection algorithm improves the compression ratio up to 70 percent with the same level of data distortion because of very accurate selection (around 99 percent) of the bestfit compressor, with little overhead (less than 7 percent in the experiments)."",""1558-2183"","""",""10.1109/TPDS.2019.2894404"",""Exascale Computing Project(grant numbers:17-SC-20-SC)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621017"",""Lossy compression";scientific data;rate-distortion;compression ratio;"high-performance computing"",""Compressors";Distortion;Rate-distortion;Data models;Measurement;Quantization (signal);"Transforms"","""",""46"","""",""56"",""IEEE"",""22 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Parallel Refined Isogeometric Analysis in 3D,""L. Siwik"; M. Woźniak; V. Trujillo; D. Pardo; V. M. Calo;" M. Paszyński"",""Faculty of Computer Science, AGH University of Science and Technology, aleja Adama Mickiewicza 30, Kraków, Poland"; Faculty of Computer Science, AGH University of Science and Technology, aleja Adama Mickiewicza 30, Kraków, Poland; Department of Mathematics, Pontifical Catholic University of Valparaiso, Valparaiso, Chile; University of the Basque Country (UPV/EHU), Bilbao, Bizkaia, Spain; Mineral Resources, Commonwealth Scientific and Industrial Research Organization (CSIRO), Kensington, WA, Australia;" Faculty of Computer Science, AGH University of Science and Technology, aleja Adama Mickiewicza 30, Kraków, Poland"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1134"",""1142"",""We study three-dimensional isogeometric analysis (IGA) and the solution of the resulting system of linear equations via a direct solver. IGA uses highly continuous $C^{p-1}$Cp-1 basis functions, which provide multiple benefits in terms of stability and convergence properties. However, smooth basis significantly deteriorate the direct solver performance and its parallel scalability. As a partial remedy for this, refined Isogeometric Analysis (rIGA) method improves the sequential execution of direct solvers. The refinement strategy enriches traditional highly-continuous $C^{p-1}$Cp-1 IGA spaces by introducing low-continuity $C^0$C0-hyperplanes along the boundaries of certain pre-defined macro-elements. In this work, we propose a solution strategy for rIGA for parallel distributed memory machines and compare the computational costs of solving rIGA versus IGA discretizations. We verify our estimates with parallel numerical experiments. Results show that the weak parallel scalability of the direct solver improves approximately by a factor of $p^2$p2 when considering rIGA discretizations rather than highly-continuous IGA spaces."",""1558-2183"","""",""10.1109/TPDS.2018.2879664"",""Polish National Science Centre(grant numbers:DEC-2016/21/B/ST6/01539)"; PL-Grid Infrastructure; Chilean CONICYT project(grant numbers:PAI80160025); European Union’s Horizon 2020 research and innovation programme(grant numbers:777778); Projects of the Spanish Ministry of Economy and Competitiveness(grant numbers:MTM2016-76329-R (AEI/FEDER, EU),MTM2016-81697-ERC/AEI); BCAM ‘’Severo Ochoa” accreditation of excellence(grant numbers:SEV-2018-0718); Basque Government through the BERC 2014-2017 program; Consolidated Research Group(grant numbers:IT649-13); CSIRO Professorial Chair in Computational Geoscience at Curtin University; Deep Earth Imaging Enterprise Future Science Platforms of the Commonwealth Scientific Industrial Research Organisation, CSIRO, of Australia; Russian Federation Government(grant numbers:N 14.Y26.31.0013); Curtin Institute for Computation;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8523633"",""Isogeometric analysis";direct solvers;"parallel computing"",""Particle separators";Splines (mathematics);Scalability;Skeleton;Computational efficiency;Mathematical model;"Tensile stress"","""",""2"","""",""39"",""IEEE"",""4 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;
"Parallel Traversal of Large Ensembles of Decision Trees,""F. Lettich"; C. Lucchese; F. M. Nardini; S. Orlando; R. Perego; N. Tonellotto;" R. Venturini"",""Federal University of Ceará, Fortaleza, CE, Brazil"; Ca’ Foscari University of Venice, Venezia, Italy; ISTI–CNR, Pisa, Italy; Ca’ Foscari University of Venice, Venezia, Italy; ISTI–CNR, Pisa, Italy; ISTI–CNR, Pisa, Italy;" University of Pisa, Pisa, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""2075"",""2089"",""Machine-learnt models based on additive ensembles of regression trees are currently deemed the best solution to address complex classification, regression, and ranking tasks. The deployment of such models is computationally demanding: to compute the final prediction, the whole ensemble must be traversed by accumulating the contributions of all its trees. In particular, traversal cost impacts applications where the number of candidate items is large, the time budget available to apply the learnt model to them is limited, and the users' expectations in terms of quality-of-service is high. Document ranking in web search, where sub-optimal ranking models are deployed to find a proper trade-off between efficiency and effectiveness of query answering, is probably the most typical example of this challenging issue. This paper investigates multi/many-core parallelization strategies for speeding up the traversal of large ensembles of regression trees thus obtaining machine-learnt models that are, at the same time, effective, fast, and scalable. Our best results are obtained by the GPU-based parallelization of the state-of-the-art algorithm, with speedups of up to 102.6x."",""1558-2183"","""",""10.1109/TPDS.2018.2860982"",""BIGDATAGRAPES(grant numbers:780751)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8430583"",""Efficient machine learning";learning-to-rank;decision tree ensembles;parallel algorithms;SIMD;NUMA multiprocessors;"GPUs"",""Regression tree analysis";Data structures;Data models;Parallel processing;Pattern classification;Query processing;"Machine learning"","""",""8"","""",""28"",""IEEE"",""9 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Parallelizing Word2Vec in Shared and Distributed Memory,""S. Ji"; N. Satish; S. Li;" P. K. Dubey"",""Department of Computer Science, Georgia State University, Atlanta, GA, USA"; Intel Labs, Santa Clara, CA, USA; Intel Labs, Santa Clara, CA, USA;" Intel Labs, Santa Clara, CA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""2090"",""2100"",""Word2vec is a widely used algorithm for extracting low-dimensional vector representations of words. State-of-the-art algorithms including those by Mikolov et al. [1] , [2] have been parallelized for multi-core CPU architectures, but are based on vector-vector operations with “Hogwild” updates that are memory-bandwidth intensive and do not efficiently use computational resources. In this paper, we propose “HogBatch” by improving reuse of various data structures in the algorithm through the use of minibatching and negative sample sharing, hence allowing us to express the problem using matrix multiply operations. We also explore different techniques to distribute word2vec computation across nodes in a computer cluster, and demonstrate good strong scalability up to 32 nodes. The new algorithm is particularly suitable for modern multi-core/many-core architectures, especially Intel's latest Knights Landing processors, and allows us to scale up the computation near linearly across cores and nodes, and process hundreds of millions of words per second, which is the fastest word2vec implementation to the best of our knowledge. We released the source code for reproducible research and general usage."",""1558-2183"","""",""10.1109/TPDS.2019.2904058"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8663393"",""Word2Vec";parallel algorithms;distributed computing;"multi-core and many-core systems"",""Computational modeling";Computer architecture;Task analysis;Vocabulary;Context modeling;Training;"Neural networks"","""",""31"","""",""28"",""IEEE"",""8 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Parana: A Parallel Neural Architecture Considering Thermal Problem of 3D Stacked Memory,""S. Yin"; S. Tang; X. Lin; P. Ouyang; F. Tu; L. liu; J. Zhao; C. Xu; S. Li; Y. Xie;" S. Wei"",""Institute of Microelectronics, Tsinghua University, Beijing, China"; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; Institute of Microelectronics, Tsinghua University, Beijing, China; University of California, San Diego, CA, USA; University of California, Santa Barbara, CA, USA; University of California, Santa Barbara, CA, USA; University of California, Santa Barbara, CA, USA;" Institute of Microelectronics, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""146"",""160"",""Recent advances in deep learning (DL) have stimulated increasing interests in neural networks (NN). From the perspective of operation type and network architecture, deep neural networks can be categorized into full convolution-based neural network (ConvNet), recurrent neural network (RNN), and fully-connected neural network (FCNet). Different types of neural networks are usually cascaded and combined as a hybrid neural network (Hybrid-NN) to complete real-life cognitive tasks. Such hybrid-NN implementation is memory-intensive with large number of memory accesses, hence the performance of hybrid-NN is often limited by the insufficient memory bandwidth. A “3D + 2.5D” integration system, which integrates a high-bandwidth 3D stacked DRAM side-by-side with a highly-parallel neural processing unit (NPU) on a silicon interposer, overcomes the bandwidth bottleneck in hybrid-NN acceleration. However, intensive concurrent 3D DRAM accesses produced by the NPU lead to a serious thermal problem in 3D DRAM. In this paper, we propose a neural processor called Parana for hybrid-NN acceleration in consideration of thermal problem of 3D DRAM. Parana solves the thermal problem of 3D memory by optimizing both the total number of memory accesses and memory accessing behaviors. For memory accessing behaviors, Parana balances the memory bandwidth by spatial division mapping hybrid-NN onto computing resources, which efficiently avoids that masses of memory accesses are issued in a short time period. To reduce the total number of memory accesses, we design a new NPU architecture and propose a memory-oriented tiling and scheduling mechanism to exploit the maximum utilization of on-chip buffer. Experimental results show that Parana reduces the peak temperature by up to 54.72  $^\circ$ C and the steady temperature by up to 32.27  $^\circ$C over state-of-the-art accelerators with 3D memory without performance degradation."",""1558-2183"","""",""10.1109/TPDS.2018.2858230"",""National Natural Science Foundation of China(grant numbers:61774094)"; China Major S&T Project(grant numbers:2018ZX01031101-002); National Science Foundation(grant numbers:1718158/1725447/1719160);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8416708"",""Neural processor";hybrid neural networks;thermal problem;"3D memory"",""Three-dimensional displays";Random access memory;Acceleration;Bandwidth;Computer architecture;"Artificial neural networks"","""",""8"","""",""64"",""IEEE"",""20 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;
"Peak Power Management to Meet Thermal Design Power in Fault-Tolerant Embedded Systems,""M. Ansari"; S. Safari; A. Yeganeh-Khaksar; M. Salehi;" A. Ejlali"",""Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; Department of Computer Engineering, Sharif University of Technology, Tehran, Iran; University of Guilan, Rasht, Iran;" Department of Computer Engineering, Sharif University of Technology, Tehran, Iran"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""161"",""173"",""Multicore platforms provide a great opportunity for implementation of fault-tolerance techniques to achieve high reliability in real-time embedded systems. Passive redundancy is well-suited for multicore platforms and a well-established technique to tolerate transient and permanent faults. However, it incurs significant power overheads, which go wasted in fault-free execution scenarios. Meanwhile, due to the Thermal Design Power (TDP) constraint, in some cases, it is not feasible to simultaneously power on all cores on a multicore platform. Since TDP is the maximum sustainable power that a chip can consume, violating TDP makes some cores automatically restart or significantly reduce their performance to prevent a permanent damage. This may affect timeliness of the system, and hence, designers face a challenge in deciding how to use multicore platforms in real-time embedded systems. In this paper, at first, we study how the use of passive redundancy (especially for Triple Modular redundancy) can violate TDP on multicore platforms. Then, we propose a scheme for scheduling real-time tasks in multicore systems to conquer the peak power problem in NMR systems. This is because in multicore embedded systems an efficient solution for meeting the TDP constraint is reducing the peak power consumption. The proposed scheme tries to remove overlaps of the peak power of concurrently executing tasks to keep the maximum power consumption below the chip TDP. In the proposed scheme, we devised a policy called PPA-LTF to manage peak power consumption. This policy prevents tasks execution that consume higher power according to the tasks’ power traces. Our experimental results show that our scheme provides up to 50 percent (on average by 39 percent) peak power reduction compared to state-of-the-art schemes."",""1558-2183"","""",""10.1109/TPDS.2018.2858816"",""Vice-Presidency of Sharif University of Technology(grant numbers:G930827)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8419269"",""Peak power consumption";fault tolerance;embedded systems;multicore platforms;"thermal design power"",""Task analysis";Multicore processing;Power demand;Redundancy;Real-time systems;"Embedded systems"","""",""33"","""",""38"",""IEEE"",""25 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Pec: Proactive Elastic Collaborative Resource Scheduling in Data Stream Processing,""X. Wei"; L. Li; X. Li; X. Wang; S. Gao;" H. Li"",""College of Computer Science & Technology, Symbol Computation and Knowledge Engineer of Ministry of Education, Jilin University, Changchun, China"; College of Computer Science & Technology, Jilin University, Changchun, China; High Performance Computing Center, Jilin University, Changchun, China; College of Computer Science & Technology, Jilin University, Changchun, China; College of Computer Science & Technology, Jilin University, Changchun, China;" College of Computer Science & Technology, Jilin University, Changchun, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1628"",""1642"",""In the Distributed Parallel Stream Processing Systems (DPSPS), elastic resource allocation allows applications to dynamically response to workload fluctuations. However, resource provisioning can be particularly challenging, due to the unpredictability of the workload. In addition, unlike CPU resources, bandwidth resources are often ignored in resource allocation. Moreover, resource allocation and resource placement are considered separately. In this paper, we investigate the proactive elastic resource scheduling problem for computation-intensive and communication-intensive applications, which aims at meeting the latency requirement with the minimal energy cost, and propose a dynamic collaborative strategy from the systemic perspective. Specifically, we first model a collaborative workload prediction pattern to accurately predict the upcoming workload, and construct a latency estimation model to estimate the latency of the application. Then, we design an energy-efficient resource pre-allocation method, in which the CPU frequency adjustment and the stability of resource reconfigurations are both considered. Finally, we present a communication-aware resource placement approach. Simulation results show that, compared with the reactive strategies, our strategy achieves an obviously better latency performance, and effectively avoids unnecessary resource adjustments. Meanwhile, the energy consumption is about saved by 50 percent on average, and the communication cost is maintained at a very low level of 4 percent."",""1558-2183"","""",""10.1109/TPDS.2019.2891587"",""National Natural Science Foundation of China(grant numbers:61772228,61602205)"; National Key Research and Development Program of China(grant numbers:2017YFC1502306,2016YFB0701101); Jilin Scientific and Technological Development Program(grant numbers:20170520066JH,20190201024JC);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8608018"",""Data stream processing";resource scheduling;workload prediction;elastic resource allocation;proactive strategy;"stable reconfiguration"",""Resource management";Collaboration;Predictive models;Bandwidth;Load modeling;Dynamic scheduling;"Estimation"","""",""16"","""",""53"",""IEEE"",""10 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Performance Analysis and Modeling of Video Transcoding Using Heterogeneous Cloud Services,""X. Li"; M. A. Salehi; Y. Joshi; M. K. Darwich; B. Landreneau;" M. Bayoumi"",""Brightcove Inc., Boston, MA, USA"; HPCC Lab., School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; HPCC Lab., School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA; School of Engineering, Math and Technology, Navajo Technical University, Crownpoint, NM, USA; HPCC Lab., School of Computing and Informatics, University of Louisiana at Lafayette, Lafayette, LA, USA;" Department of Electrical and Computer Engineering, University of Louisiana at Lafayette, Lafayette, LA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""910"",""922"",""High-quality video streaming, either in form of Video-On-Demand (VOD) or live streaming, usually requires converting (i.e., transcoding) video streams to match the characteristics of viewers’ devices (e.g., in terms of spatial resolution or supported formats). Considering the computational cost of the transcoding operation and the surge in video streaming demands, Streaming Service Providers (SSPs) are becoming reliant on cloud services to guarantee Quality of Service (QoS) of streaming for their viewers. Cloud providers offer heterogeneous computational services in form of different types of Virtual Machines (VMs) with diverse prices. Effective utilization of cloud services for video transcoding requires detailed performance analysis of different video transcoding operations on the heterogeneous cloud VMs. In this research, for the first time, we provide a thorough analysis of the performance of the video stream transcoding on heterogeneous cloud VMs. Providing such analysis is crucial for efficient prediction of transcoding time on heterogeneous VMs and for the functionality of any scheduling methods tailored for video transcoding. Based upon the findings of this analysis and by considering the cost difference of heterogeneous cloud VMs, in this research, we also provide a model to quantify the degree of suitability of each cloud VM type for various transcoding tasks. The provided model can supply resource (VM) provisioning methods with accurate performance and cost trade-offs to efficiently utilize cloud services for video streaming."",""1558-2183"","""",""10.1109/TPDS.2018.2870651"",""Louisiana Board of Regents(grant numbers:LEQSF(2016-19)-RD-A-25)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8466657"",""Heterogeneous cloud service";performance analysis;GOP suitability matrix;"video transcoding"",""Streaming media";Transcoding;Task analysis;Cloud computing;Spatial resolution;Bit rate;"Quality of service"","""",""34"","""",""55"",""IEEE"",""16 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Performance Modeling and Workflow Scheduling of Microservice-Based Applications in Clouds,""L. Bao"; C. Wu; X. Bu; N. Ren;" M. Shen"",""Xidian University, Xian, Shaanxi, CN"; New Jersey Institute of Technology, Newark, NJ, US; Xidian University, Xian, Shaanxi, CN; Xidian University, Xian, Shaanxi, CN;" Xidian University, Xian, Shaanxi, CN"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Aug 2019"",""2019"",""30"",""9"",""2114"",""2129"",""Microservice has been increasingly recognized as a promising architectural style for constructing large-scale cloud-based applications within and across organizational boundaries. This microservice-based architecture greatly increases application scalability, but meanwhile incurs an expensive performance overhead, which calls for a careful design of performance modeling and task scheduling. However, these problems have thus far remained largely unexplored. In this paper, we develop a performance modeling and prediction method for independent microservices, design a three-layer performance model for microservice-based applications, formulate a Microservice-based Application Workflow Scheduling problem for minimum end-to-end delay under a user-specified Budget Constraint (MAWS-BC), and propose a heuristic microservice scheduling algorithm. The performance modeling and prediction method are validated and justified by experimental results generated through a well-known microservice benchmark on disparate computing nodes, and the performance superiority of the proposed scheduling solution is illustrated by extensive simulation results in comparison with existing algorithms."",""1558-2183"","""",""10.1109/TPDS.2019.2901467"",""National Key R&D Program of China(grant numbers:2018YFC0831200)"; National Natural Science Foundation of China(grant numbers:61202040); National Science Foundation(grant numbers:CNS-1828123); Fundamental Research Funds for the Central Universities(grant numbers:JB171005);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651324"",""Microservice";performance modeling and prediction;task scheduling;"cloud computing"",""Computational modeling";Predictive models;Scheduling;Computer architecture;Cloud computing;Processor scheduling;"Benchmark testing"","""",""78"","""",""64"",""IEEE"",""24 Feb 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Performance-Aware Model for Sparse Matrix-Matrix Multiplication on the Sunway TaihuLight Supercomputer,""Y. Chen"; K. Li; W. Yang; G. Xiao; X. Xie;" T. Li"",""National Supercomputing Center in Changsha, Changsha, Hunan, China"; National Supercomputing Center in Changsha, Changsha, Hunan, China; National Supercomputing Center in Changsha, Changsha, Hunan, China; National Supercomputing Center in Changsha, College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; State Key Laboratory of Mathematic Engineering and Advance Computing, Jiangnan Institute of Computing Technology, Wuxi, Jiangsu, China;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""923"",""938"",""General sparse matrix-sparse matrix multiplication (SpGEMM) is one of the fundamental linear operations in a wide variety of scientific applications. To implement efficient SpGEMM for many large-scale applications, this paper proposes scalable and optimized SpGEMM kernels based on COO, CSR, ELL, and CSC formats on the Sunway TaihuLight supercomputer. First, a multi-level parallelism design for SpGEMM is proposed to exploit the parallelism of over 10 millions cores and better control memory based on the special Sunway architecture. Optimization strategies, such as load balance, coalesced DMA transmission, data reuse, vectorized computation, and parallel pipeline processing, are applied to further optimize performance of SpGEMM kernels. Second, we thoroughly analyze the performance of the proposed kernels. Third, a performance-aware model for SpGEMM is proposed to select the most appropriate compressed storage formats for the sparse matrices that can achieve the optimal performance of SpGEMM on the Sunway. The experimental results show the SpGEMM kernels have good scalability and meet the challenge of the high-speed computing of large-scale data sets on the Sunway. In addition, the performance-aware model for SpGEMM achieves an absolute value of relative error rate of 8.31 percent on average when the kernels are executed in one single process and achieves 8.59 percent on average when the kernels are executed in multiple processes. It is proved that the proposed performance-aware model can perform at high accuracy and satisfies the precision of selecting the best formats for SpGEMM on the Sunway TaihuLight supercomputer."",""1558-2183"","""",""10.1109/TPDS.2018.2871189"",""National Key R&D Program of China(grant numbers:2016YFB0200201)"; National Natural Science Foundation of China(grant numbers:61625202); National Natural Science Foundation of China(grant numbers:61661146006,61860206011); National Natural Science Foundation of China(grant numbers:61751204,61806077);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8468040"",""Heterogeneous many-core processor";parallelism;performance analysis;performance-aware;SpGEMM;"Sunway TaihuLight supercomputer"",""Sparse matrices";Kernel;Supercomputers;Parallel processing;Computer architecture;Analytical models;"Computational modeling"","""",""87"","""",""31"",""IEEE"",""19 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Persistent Octrees for Parallel Mesh Refinement through Non-Volatile Byte-Addressable Memory,""B. Nguyen"; H. Tan; K. Davis;" X. Zhang"",""School of Engineering and Computer Science, Washington State University, Vancouver, WA, USA"; School of Engineering and Computer Science, Washington State University, Vancouver, WA, USA; Los Alamos National Laboratory, Los Alamos, NM, USA;" School of Engineering and Computer Science, Washington State University, Vancouver, WA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""677"",""691"",""Adaptive mesh refinement based on octree data structures has enabled efficient simulations of complex physical phenomena. Existing meshing algorithms were proposed with the assumption that computer memory is volatile. Consequently, for failure recovery, in-core algorithms need to save memory states as snapshots with slow file I/O, while out-of-core algorithms store octants on disk for persistence. However, neither was designed to best exploit the unique characteristics of non-volatile byte-addressable memory (NVBM). We propose a novel data structure, the Distributed Persistent Merged octree (DPM-octree), for both meshing and in-memory storage of persistent octrees using NVBM. DPM-octree is a multi-version data structure that can recover from failures using an earlier persistent version stored in NVBM. In addition, we design a feature-directed sampling approach to help dynamically transform the DPM-octree layout for reducing NVBM-induced memory write latency. DPM-octree uses parity trees which are created using erasure coding and stored in NVBM to support low-latency in-memory octant recovery after data loss. DPM-octree has been successfully integrated with the Gerris software for simulation of fluid dynamics. Our experimental results with real-world scientific workloads show that DPM-octree scales up to 1.1 billion mesh elements with 1,000 processors on the Titan supercomputer."",""1558-2183"","""",""10.1109/TPDS.2018.2867867"",""US National Science Foundation(grant numbers:CRII ACI 1565338)"; US Department of Energy Exascale Computing;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8451966"",""Octree";adaptive mesh refinement;"non-volatile byte-addressable memory"",""Octrees";Random access memory;Data models;Nonvolatile memory;Computational modeling;"Three-dimensional displays"","""",""4"","""",""68"",""IEEE"",""30 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"PFP: Improving the Reliability of Deduplication-based Storage Systems with Per-File Parity,""S. Wu"; B. Mao; H. Jiang; H. Luan;" J. Zhou"",""Computer Science Department, Xiamen University, Xiamen, Fujian, China"; Software School, Xiamen University, Xiamen, Fujian, China; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, TX, USA; Computer Science Department, Xiamen University, Xiamen, Fujian, China;" Software School, Xiamen University, Xiamen, Fujian, China"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""2117"",""2129"",""Data deduplication weakens the reliability of storage systems since by design it removes duplicate data chunks common to different files and forces these files to share a single physical date chunk, or critical chunk, after deduplication. Thus, the loss of a single such critical data chunk can potentially render all referencing (sharing) files unavailable. However, the reliability issue in deduplication-based storage systems has not received adequate attention. Existing approaches introduce data redundancy after files have been deduplicated, either by replication on critical data chunks, i.e., chunks with high reference count, or RAID schemes on unique data chunks, which means that these schemes are based on individual unique data chunks rather than individual files. This can leave individual files vulnerable to losses, particularly in the presence of transient and unrecoverable data chunk errors such as latent sector errors. To address this file reliability issue, this paper proposes a Per-File Parity (short for PFP) scheme to improve the reliability of deduplication-based storage systems. PFP computes the XOR parity within parity groups of data chunks of each file after the chunking process but before the data chunks are deduplicated. Therefore, PFP can provide parity redundancy protection for all files by intra-file recovery and a higher-level protection for data chunks with high reference counts by inter-file recovery. Our reliability analysis and extensive data-driven, failure-injection based experiments conducted on a prototype implementation of PFP show that PFP significantly outperforms the existing redundancy solutions, DTR and RCR, in system reliability, tolerating multiple data chunk failures and guaranteeing file availability upon multiple data chunk failures. Moreover, a performance evaluation shows that PFP only incurs an average of 5.7 percent performance degradation to the deduplication-based storage system."",""1558-2183"","""",""10.1109/TPDS.2019.2898942"",""National Natural Science Foundation of China(grant numbers:U1705261,61872305,61772439,61472336)"; US NSF(grant numbers:CCF-1704504,CCF-1629625);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8640256"",""Data deduplication";reliability;per-file parity;intra-file recovery;"inter-file recovery"",""Redundancy";Indexes;Reliability engineering;Performance evaluation;Drives;"Distributed databases"","""",""8"","""",""41"",""IEEE"",""12 Feb 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"PHAST - A Portable High-Level Modern C++ Programming Library for GPUs and Multi-Cores,""B. Peccerillo";" S. Bartolini"",""Department of Information Engineering and Mathematical Sciences, University of Siena, Siena, Italy";" Department of Information Engineering and Mathematical Sciences, University of Siena, Siena, Italy"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""174"",""189"",""A decade after the beginning of the many-core era, multi-core CPU and GPU architectures are everywhere, from mobile devices up to high-performance workstations and servers. To this day, programmers willing to harness their power need to express their code via languages and frameworks that often lack of expressivity and high-level abstractions. These solutions, despite allowing users to reach unprecedented performance, can still be a hampering factor for productivity and portability. In this paper we propose PHAST, a modern C++, STL-like, single-source programming library and approach based on multi-dimensional dynamic containers and multi-layered functors that can be targeted on NVIDIA GPUs and multi-core CPUs. Its main purpose is to let programmers write code once for different architectures at a high level of abstraction, to reach high-performance while allowing fine parameter tuning and not shielding code from low-level target-specific optimizations. To assess the value of our proposal, we consider benchmarks from different application domains, and we evaluate their PHAST implementations against CUDA, OpenCL, Kokkos, and SYCL ones from both performance and productivity points of view. We show that PHAST can significantly reduce code complexity metrics while reaching very good performance."",""1558-2183"","""",""10.1109/TPDS.2018.2855182"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8410014"",""PHAST Library";multi-dimensional containers;heterogeneous systems;modern C++;CUDA;performance comparison;productivity comparison;complexity metrics;Kokkos;"SYCL"",""Graphics processing units";C++ languages;Libraries;Programming;Containers;Productivity;"Computer architecture"","""",""10"","""",""50"",""IEEE"",""11 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Piggyback Game: Efficient Event Stream Dissemination in Online Social Network Systems,""F. Zhang"; H. Chen;" H. Jin"",""Big Data Technology and System Lab, Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China"; Big Data Technology and System Lab, Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China;" Big Data Technology and System Lab, Cluster and Grid Computing Lab, Services Computing Technology and System Lab, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""692"",""709"",""Event stream dissemination dominates the workloads in large-scale Online Social Network (OSN) systems. Based on the de facto per-user view data storage, event stream dissemination raises a large amount of inter-server traffic due to the complex interconnection among OSN users. The state-of-the-art schemes mainly explore the structure features of social graphs to reduce the inter-server communications for event stream dissemination. Different sub-graph structures are exploited for achieving approximated optimal solutions. However, such schemes incur prohibitively high cost of either computation or communication. In this work, we follow a different design philosophy by using a game theoretic approach, which decomposes the highly complex graph computation problem into rational decision making of every individual social link. Specifically, we propose a novel social piggyback game to achieve a more efficient solution. We mathematically prove the existence of the Nash Equilibrium of the social piggyback game. Moreover, we propose an efficient best response dynamic algorithm to achieve the Nash Equilibrium, which quickly converges in a small number of iterations for large-scale OSNs. We further show that the communication cost of this design achieves a 1.5-approximation of the theoretical social optimum. We conduct comprehensive experiments using large-scale real-world traces from popular OSN systems as well as implement a prototype system to evaluate the performance of this design. Results show that the social piggyback game achieves a significant 302× improvement in system efficiency compared to existing schemes."",""1558-2183"","""",""10.1109/TPDS.2018.2866242"",""National Key Research and Development Program of China(grant numbers:2016QY02D0302)"; NSFC(grant numbers:61422202,61370233); Research Fund of Guangdong Province(grant numbers:2015B010131001);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8440751"",""Online social network";event stream dissemination;"piggyback"",""Games";Nash equilibrium;Heuristic algorithms;Servers;Prototypes;"Facebook"","""",""1"","""",""37"",""IEEE"",""19 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"Popularity-Aware Multi-Failure Resilient and Cost-Effective Replication for High Data Durability in Cloud Storage,""J. Liu"; H. Shen;" H. S. Narman"",""Department of Computer and Information Sciences at Florida A&M University, Tallahassee, FL, USA"; Computer Science Department, University of Virginia, Charlottesville, VA, USA;" Computer Science Department, Marshall University, Huntington, WV, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Sep 2019"",""2019"",""30"",""10"",""2355"",""2369"",""Large-scale data stores are an increasingly important component of cloud datacenter services. However, cloud storage system usually experiences data loss, hindering data durability. Three-way random replication is commonly used to lead better data durability in cloud storage systems. However, three-way random replication cannot effectively handle correlated machine failures to prevent data loss. Although Copyset Replication and Tiered Replication can reduce data loss in correlated and independent failures, and enhance data durability, they fail to leverage different data popularities to substantially reduce the storage cost and bandwidth cost caused by replication. To address these issues, we present a popularity-aware multi-failure resilient and cost-effective replication (PMCR) scheme for high data durability in cloud storage. PMCR splits the cloud storage system into primary tier and backup tier, and classifies data into hot data, warm data and cold data based on data popularities. To handle both correlated and independent failures, PMCR stores the three replicas of the same data into one Copyset formed by two servers in the primary tier and one server in the backup tier. For the third replicas of warm data and cold data in the backup tier, PMCR uses the compression methods to reduce storage cost and bandwidth cost. Extensive numerical results based on trace parameters and experimental results from real-world Amazon S3 show that PMCR achieves high data durability, low probability of data loss, and low storage cost and bandwidth cost compared to previous replication schemes."",""1558-2183"","""",""10.1109/TPDS.2018.2873384"",""National Science Foundation(grant numbers:OAC-1724845,CNS-1733596,ACI-1661378)"; Microsoft Research Faculty Fellowship(grant numbers:8300751); IBM Ph.D. fellowship(grant numbers:2017);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8478382"",""Cloud storage";replication;data durability;cost-effectiveness;"SLA"",""Cloud computing";Probability;Servers;Distributed databases;"Storage management"","""",""19"","""",""58"",""IEEE"",""30 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Portable Programming with RAPID,""K. Angstadt"; J. Wadden; W. Weimer;" K. Skadron"",""Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA"; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA; Department of Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI, USA;" Department of Computer Science, University of Virginia, Charlottesville, VA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""939"",""952"",""As the hardware found within data centers becomes more heterogeneous, it is important to allow for efficient execution of algorithms across architectures. We present RAPID, a high-level programming language and combined imperative and declarative model for functionally- and performance-portable execution of sequential pattern-matching applications across CPUs, GPUs, Field-Programmable Gate Arrays (FPGAs), and Micron's D480 AP. RAPID is clear, maintainable, concise, and efficient both at compile and run time. Language features, such as code abstraction and parallel control structures, map well to pattern-matching problems, providing clarity and maintainability. For generation of efficient runtime code, we present algorithms to convert RAPID programs into finite automata. Our empirical evaluation of applications in the ANMLZoo benchmark suite demonstrates that the automata processing paradigm provides an abstraction that is portable across architectures. We evaluate RAPID programs against custom, baseline implementations previously demonstrated to be significantly accelerated. We also find that RAPID programs are much shorter in length, are expressible at a higher level of abstraction than their handcrafted counterparts, and yield generated code that is often more compact."",""1558-2183"","""",""10.1109/TPDS.2018.2869736"",""National Science Foundation(grant numbers:CCF-0954024,CCF-1629450,CCF-1116289,CCF-1116673,CCF-1619123,CDI-1124931,CNS-1619098)"; Air Force(grant numbers:FA8750-15-2-0075); Virginia Commonwealth Fellowship; Jefferson Scholars Foundation; Achievement Rewards for College Scientists; Xilinx; Microelectronics Advanced Research Corporation; Defense Advanced Research Projects Agency;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462761"",""Automata processing";sequential pattern matching;heterogeneous (hybrid) systems;concurrent, distributed, and parallel languages;"concurrent programming structures, patterns"",""Automata";Field programmable gate arrays;Computer architecture;Programming;Hardware;Computer languages;"Logic gates"","""",""3"","""",""48"",""IEEE"",""12 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Privacy Regulation Aware Process Mapping in Geo-Distributed Cloud Data Centers,""A. C. Zhou"; Y. Xiao; Y. Gong; B. He; J. Zhai;" R. Mao"",""Shenzhen University, Shenzhen, China"; Shenzhen University, Shenzhen, China; TuSimple, Beijing, China; National University of Singapore, Singapore; Tsinghua University, Beijing, China;" Shenzhen University, Shenzhen, China"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1872"",""1888"",""Recently, various applications including data analytics and machine learning have been developed for geo-distributed cloud data centers. For those applications, the ways of mapping parallel processes to physical nodes (i.e., “process mapping”) could significantly impact the performance of the applications because of non-uniform communication cost in geo-distributed environments. What's more, the different data privacy requirements in geo-distributed data centers pose additional constraints on process mapping solutions. While process mapping has been widely studied in grid/cluster environments, few of the existing studies have considered the problem in geo-distributed cloud environment, which is a challenging task due to the multi-level data privacy constraints, heterogeneous network performance and process failures. In this paper, we introduce the special privacy requirements in geo-distributed data centers and formulate the geo-distributed process mapping problem as an optimization problem with multiple constraints. We develop a new method to efficiently find good process mapping solutions to the problem. Experimental results on real clouds (including Amazon EC2 and Windows Azure) and simulations demonstrate that our proposed approach can achieve significant performance improvement compared to the state-of-the-art algorithms."",""1558-2183"","""",""10.1109/TPDS.2019.2896894"",""National Natural Science Foundation of China(grant numbers:61802260,61722208)"; Natural Science Foundation of Guangdong Province(grant numbers:2018A030310440); Shenzhen University(grant numbers:2018062); MoE AcRF Tier 1(grant numbers:T1 251RES1610); Microsoft Research; National Key R&D Program of China(grant numbers:2016YFB0200100); Guangdong Key Laboratory(grant numbers:2017B030314073);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8632727"",""Process mapping";geo-distributed data centers;cloud computing;"data privacy regulation"",""Cloud computing";Data centers;Bandwidth;General Data Protection Regulation;Europe;"Optimization"","""",""17"","""",""51"",""IEEE"",""1 Feb 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;
"Profit Maximization for Admitting Requests with Network Function Services in Distributed Clouds,""Y. Ma"; W. Liang; Z. Xu;" S. Guo"",""Research School of Computer Science, Australian National University, Canberra, ACT, Australia"; Research School of Computer Science, Australian National University, Canberra, ACT, Australia; School of Software, Dalian University of Technology, Dalian, China;" Hong Kong Polytechnic University Shenzhen Research Institute and Department of Computing, Hong Kong Polytechnic University, Hung Hom, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1143"",""1157"",""Traditional networks employ expensive dedicated hardware devices as middleboxes to implement Service Function Chains of user requests by steering data traffic along middleboxes in the service function chains before reaching their destinations. Network Function Virtualization (NFV) is a promising virtualization technique that implements network functions as pieces of software in servers or data centers. The integration of NFV and Software Defined Networking (SDN) further simplifies service function chain provisioning, making its implementation simpler and cheaper. In this paper, we consider dynamic admissions of delay-aware requests with service function chain requirements in a distributed cloud with the objective to maximize the profit collected by the service provider, assuming that the distributed cloud is an SDN that consists of data centers located at different geographical locations and electricity prices at different data centers are different. We first formulate this novel optimization problem as a dynamic profit maximization problem. We then show that the offline version of the problem is NP-hard and formulate an integer linear programming solution to it. We third propose an online heuristic for the problem. We also devise an online algorithm with a provable competitive ratio for a special case of the problem where the end-to-end delay requirement of each request is negligible. We finally evaluate the performance of the proposed algorithms through experimental simulations. The simulation results demonstrate that the proposed algorithms are promising."",""1558-2183"","""",""10.1109/TPDS.2018.2874257"",""National Natural Science Foundation of China(grant numbers:61802048)"; fundamental research funds for the central universities in China(grant numbers:DUT17RC(3)061); Xinghai Scholar Program in Dalian University of Technology, China; National Natural Science Foundation of China(grant numbers:61872310);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482326"",""Network function virtualization";software defined networking;distributed data centers;online algorithms;service function chain consolidation;profit maximization;"request admission scheduling"",""Data centers";Heuristic algorithms;Delays;Servers;Software;Routing;"Virtualization"","""",""31"","""",""35"",""IEEE"",""5 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Profit Maximization for Cloud Brokers in Cloud Computing,""J. Mei"; K. Li; Z. Tong; Q. Li;" K. Li"",""ed HPCSIP, Hunan Normal University, Changsha, Hunan, China"; College of Information Science and Engineering, Hunan University, Changsha, Hunan, China; ed HPCSIP, Hunan Normal University, Changsha, Hunan, China; ed HPCSIP, Hunan Normal University, Changsha, Hunan, China;" Department of Computer Science, State University of New York, New Paltz, NY, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""190"",""203"",""Along with the development of cloud computing, more and more applications are migrated into the cloud. An important feature of cloud computing is pay-as-you-go. However, most users always should pay more than their actual usage due to the one-hour billing cycle. In addition, most cloud service providers provide a certain discount for long-term users, but short-term users with small computing demands cannot enjoy this discount. To reduce the cost of cloud users, we introduce a new role, which is cloud broker. A cloud broker is an intermediary agent between cloud providers and cloud users. It rents a number of reserved VMs from cloud providers with a good price and offers them to users on an on-demand basis at a cheaper price than that provided by cloud providers. Besides, the cloud broker adopts a shorter billing cycle compared with cloud providers. By doing this, the cloud broker can reduce a great amount of cost for user. In addition to reduce the user cost, the cloud broker also could earn the difference in prices between on-demand and reserved VMs. In this paper, we focus on how to configure a cloud broker and how to price its VMs such that its profit can be maximized on the premise of saving costs for users. Profit of a cloud broker is affected by many factors such as the user demands, the purchase price and the sales price of VMs, the scale of the cloud broker, etc. Moreover, these factors are affected mutually, which makes the analysis on profit more complicated. In this paper, we first give a synthetically analysis on all the affecting factors, and define an optimal multiserver configuration and VM pricing problem which is modeled as a profit maximization problem. Second, combining the partial derivative and bisection search method, we propose a heuristic method to solve the optimization problem. The near-optimal solutions can be used to guide the configuration and VM pricing of the cloud broker. Moreover, a series of comparisons are given which show that a cloud broker can save a considerable cost for users."",""1558-2183"","""",""10.1109/TPDS.2018.2851246"",""National Natural Science Foundation of China(grant numbers:61432005)"; National Outstanding Youth Science Program of National Natural Science Foundation of China(grant numbers:61625202); National Natural Science Foundation of China(grant numbers:61602170,61502165,61772182,61370095,61472124); China Postdoctoral Science Foundation(grant numbers:2018T110829);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8399541"",""Cloud broker";cloud computing;cost reduction;profit maximization;queue model;service demand;VM configuration;"VM pricing"",""Cloud computing";Pricing;Data centers;Optimization;Computational modeling;"Time factors"","""",""74"","""",""30"",""IEEE"",""28 Jun 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Promenade: Proportionally Fair Multipath Rate Control in Datacenter Networks with Random Network Coding,""L. Chen"; Y. Feng; B. Li;" B. Li"",""Department of Computer Science, University of Louisiana at Lafayette, Lafayette, LA, USA"; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada; Department of Electrical and Computer Engineering, University of Toronto, Toronto, ON, Canada;" Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2536"",""2546"",""In today's datacenter topologies, there exist multiple equal-cost paths between each pair of communicating virtual machines. Yet, splitting flows and routing them along multiple paths may lead to packet reordering, which may affect the performance of TCP. In this paper, we propose Promenade, a new protocol that uses random network coding to mitigate the negative effects of packet reordering, while at the same time achieving weighted proportional fairness in bandwidth allocation across different tenants. To achieve weighted proportional fairness when allocating bandwidth to tenants, the problem of rate control is formulated as a convex optimization problem, and Promenade uses its distributed solution as a theoretical foundation to design its bandwidth allocation protocol. With our real-world implementation of Promenade in the Mininet testbed, we are able to show that Promenade is able to achieve weighted proportional fairness in its rate control when individual flows are split into multiple paths."",""1558-2183"","""",""10.1109/TPDS.2019.2915638"",""BoRSF-RCS"; NSERC Discovery Research Program(grant numbers:16211715,16206417,16207818); RGC CRF(grant numbers:C7036-15G);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8709701"",""Datacenter networks";multipath rate control;network coding;"fairness"",""Network coding";Network topology;Topology;Bandwidth;Protocols;Receivers;"Load management"","""",""10"","""",""43"",""IEEE"",""8 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Proofs of Writing for Robust Storage,""D. Dobre"; G. O. Karame; W. Li; M. Majuntke; N. Suri;" M. Vukolić"",""NEC Laboratories Europe, Heidelberg, Germany"; NEC Laboratories Europe, Heidelberg, Germany; NEC Laboratories Europe, Heidelberg, Germany; Capgemini Deutschland, Berlin, Germany; TU Darmstadt, Darmstadt, Germany;" IBM Research - Zurich, Rüschlikon, Switzerland"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2547"",""2566"",""Existing Byzantine fault tolerant (BFT) storage solutions that achieve strong consistency and high availability, are costly compared to solutions that tolerate simple crashes. This cost is one of the main obstacles in deploying BFT storage in practice. In this paper, we present PoWerStore, a robust and efficient data storage protocol. PoWerStore's robustness comprises tolerating network outages, maximum number of Byzantine storage servers, any number of Byzantine readers and crash-faulty writers, and guaranteeing high availability (wait-freedom) and strong consistency (linearizability) of read/write operations. PoWerStore's efficiency stems from combining lightweight cryptography, erasure coding and metadata write-backs, where readers write-back only metadata to achieve strong consistency. Central to PoWerStore is the concept of “Proofs of Writing” (PoW), a novel data storage technique inspired by commitment schemes. PoW rely on a 2-round write procedure, in which the first round writes the actual data and the second round only serves to “prove” the occurrence of the first round. PoW enable efficient implementations of strongly consistent BFT storage through metadata write-backs and low latency reads. We implemented PoWerStore and show its improved performance when compared to state of the art robust storage protocols, including protocols that tolerate only crash faults."",""1558-2183"","""",""10.1109/TPDS.2019.2919285"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8723148"",""Byzantine fault tolerant storage";proofs of writing;"commitment schemes"",""Servers";Protocols;Computer crashes;Writing;Metadata;History;"Fault tolerance"","""",""2"","""",""50"",""IEEE"",""27 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Putting the Next 500 VM Placement Algorithms to the Acid Test: The Infrastructure Provider Viewpoint,""A. Lebre"; J. Pastor; A. Simonet;" M. Südholt"",""Inria, IMT Atlantique, LS2N, Nantes, France"; Inria, IMT Atlantique, LS2N, Nantes, France; Rutgers Discovery Informatics Institute, Rutgers University, Piscataway, NJ, USA;" Inria, IMT Atlantique, LS2N, Nantes, France"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""204"",""217"",""Most current infrastructures for cloud computing leverage static and greedy policies for the placement of virtual machines. Such policies impede the optimal allocation of resources from the infrastructure provider viewpoint. Over the last decade, more dynamic and often more efficient policies based, e.g., on consolidation and load balancing techniques, have been developed. Due to the underlying complexity of cloud infrastructures, these policies are evaluated either using limited scale testbeds/in-vivo experiments or ad-hoc simulators. These validation methodologies are unsatisfactory for two important reasons: they (i) do not model precisely enough real production platforms (size, workload variations, failure, etc.) and (ii) do not enable the fair comparison of different approaches. More generally, new placement algorithms are thus continuously being proposed without actually identifying their benefits with respect to the state of the art. In this article, we show how VMPlaceS, a dedicated simulation framework enables researchers (i) to study and compare VM placement algorithms from the infrastructure perspective, (ii) to detect possible limitations at large scale and (iii) to easily investigate different design choices. Built on top of the SimGrid simulation platform, VMPlaceS provides programming support to ease the implementation of placement algorithms and runtime support dedicated to load injection and execution trace analysis. To illustrate the relevance of VMPlaceS, we first discuss a few experiments that enabled us to study in details three well known VM placement strategies. Diving into details, we also identify several modifications that can significantly increase their performance in terms of reactivity. Second, we complete this overall presentation of VMPlaceS by focusing on the energy efficiency of the well-know FFD strategy. We believe that VMPlaceS will allow researchers to validate the benefits of new placement algorithms, thus accelerating placement research and favouring the transfer of results to IaaS production platforms."",""1558-2183"","""",""10.1109/TPDS.2018.2855158"",""Agence Nationale de la Recherche(grant numbers:11-INFRA-13)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8409978"",""Cloud computing";infrastructure-as-a-service;VM placement;simulation;"energy"",""Cloud computing";Processor scheduling;Schedules;Virtual machining;Proposals;"Scalability"","""",""12"","""",""37"",""IEEE"",""11 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;
"Quasi-Streaming Graph Partitioning: A Game Theoretical Approach,""Q. -S. Hua"; Y. Li; D. YU;" H. Jin"",""Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, P.R. China"; Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, P.R. China; School of Computer Science and Technology, Shandong University, Qingdao, P.R. China;" Services Computing Technology and System Lab, Big Data Technology and System Lab, Cluster and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1643"",""1656"",""Graph partitioning is a fundamental problem to enable scalable graph computation on large graphs. Existing partitioning models are either streaming based or offline based. In the streaming model, the current edge needs all previous edges' partition choices to make a decision. As a result, it is hard to carry out partitioning in parallel. Besides, offline based partitioning requires full knowledge about the input graph which may not suit well for large graphs. In this work, we propose a quasi-streaming partitioning model and a game theory based solution for the edge partitioning problem. Specifically, we separate the whole edge stream into a series of batches where the batch size is a constant multiple of the number of partitions. In each batch, we model the graph edge partitioning problem as a game process, where the edge's partition choice is regarded as a rational strategy choice of the player in the game. As a result, the edge partitioning problem is decomposed into finding Nash Equilibriums in a series of game processes. We mathematically prove the existence of Nash Equilibrium in such a game process, and analyze the number of rounds needed to converge into a Nash Equilibrium. We further measure the quality of these Nash Equilibriums via computing the PoA (Price of Anarchy), which is bounded by the number of partitions. Then we evaluate the performance of our strategy via comprehensive experiments on both real-world graphs and random graphs. Results show that our solution achieves significant improvements on load balance and replication factor when compared with five exsiting streaming partitioning strategies."",""1558-2183"","""",""10.1109/TPDS.2018.2890515"",""National Basic Research Program of China (973 Program)(grant numbers:2018YFB1003203)"; National Natural Science Foundation of China(grant numbers:61572216,61602195); Fundamental Research Funds for the Central Universities(grant numbers:2016YXMS075);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8598984"",""Graph edge partitioning";exact potential game;Nash Equilibrium;replication factor;"load balance"",""Games";Nash equilibrium;Mathematical model;Memory management;Parallel processing;"Instruction sets"","""",""15"","""",""25"",""IEEE"",""1 Jan 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;
"Recent Advances in Matrix Partitioning for Parallel Computing on Heterogeneous Platforms,""O. Beaumont"; B. A. Becker; A. DeFlumere; L. Eyraud-Dubois; T. Lambert;" A. Lastovetsky"",""Inria and LaBRI, University of Bordeaux, Bordeaux, France"; University College Dublin, Dublin 4, Ireland; Wellesley College, Wellesley, MA, USA; Inria and LaBRI, University of Bordeaux, Bordeaux, France; Inria and LaBRI, University of Bordeaux, Bordeaux, France;" University College Dublin, Dublin 4, Ireland"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""218"",""229"",""The problem of partitioning dense matrices into sets of sub-matrices has received increased attention recently and is crucial when considering dense linear algebra and kernels with similar communication patterns on heterogeneous platforms. The problem of load balancing and minimizing communication is traditionally reducible to an optimization problem that involves partitioning a square into rectangles. This problem has been proven to be NP-Complete for an arbitrary number of partitions. In this paper, we present recent approaches that relax the restriction that all partitions be rectangles. The first approach uses an original mathematical technique to find the exact optimal partitioning. Due to the complexity of the technique, it has been developed for a small number of partitions only. However, even at a small scale, the optimal partitions found by this approach are often non-rectangular and sometimes non-intuitive. The second approach is the study of approximate partitioning methods utilizing recursive partitioning algorithms. In particular we use the work on optimal partitioning to improve pre-existing algorithms. In this paper we discuss the different perspectives this approach opens and present two algorithms, SNRPP which is a  $\sqrt{\frac{3}{2}}$  approximation, and NRPP which is a $\frac{2}{\sqrt{3}}$  approximation. While sub-optimal, the NRRP approach works for an arbitrary number of partitions. We use the first exact approach to analyse how close to the known optimal solutions the NRRP algorithm is for small numbers of partitions."",""1558-2183"","""",""10.1109/TPDS.2018.2853151"",""Science Foundation Ireland(grant numbers:14/IA/2474)"; European Cooperation in Science and Technology(grant numbers:IC1305: Network for Sustainable Ultrascale Computi);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404128"",""Approximation algorithms";high performance computing;matrix partitioning;parallel matrix multiplication;"heterogeneous computing"",""Partitioning algorithms";Approximation algorithms;Shape;Optimization;Linear algebra;Kernel;"Load management"","""",""17"","""",""26"",""IEEE"",""5 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Region-Based Compressive Networked Storage with Lazy Encoding,""s. zhou"; Y. He; S. Xiang; K. Li;" Y. Liu"",""College of Computer Science and Electrical Engineering, Hunan University, Changsha, China"; College of Computer Science and Electrical Engineering, Hunan University, Changsha, China; College of Computer Science and Electrical Engineering, Hunan University, Changsha, China; Department of Computer Science, State University of New York, New Paltz, NY, USA;" Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1390"",""1402"",""Existing work on distributed networked storage, although extensive, has generally focused on the recovery of global data field covering the entire network. This, while demanded by a broad range of applications, has ignored cases where only a subset of the data are needed, for example, from a local region of the network. Based on this observation and the fact that the sensor readings are correlated, this paper proposes a compressive networked storage solution. Specifically, by employing the compressive sensing (CS) theory, we present a lazy-encoding algorithm with local dissemination and a region-based reconstruction algorithm. Utilizing our local dissemination strategy, sensor readings only have to be disseminated and stored in their respective regions, which makes the dissemination cost decrease significantly. With the lazy-encoding algorithm, the readings in specified local regions are capable of being encoded individually, dramatically reducing the decoding ratio. The region-based reconstruction algorithm is introduced to explore the inter-region correlation, aiming at offering improved data accuracy. We further provide the mathematical foundation that our reconstruction algorithm could ensure efficient CS recovery. Experimental results using real sensor readings show that the proposed scheme is especially beneficial to the recovery of local data. At the same time, our scheme can recover the global data field as well without increasing reconstruction error."",""1558-2183"","""",""10.1109/TPDS.2018.2883550"",""CERNET Innovation Project(grant numbers:NGII20160323)"; Natural Science Foundation of China(grant numbers:61672221);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8550698"",""Compressive sensing";data storage;sensor network;"encoding"",""Memory";Wireless sensor networks;Encoding;Decoding;Sparse matrices;Data dissemination;"Correlation"","""",""16"","""",""35"",""IEEE"",""28 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Resource-Aware Scheduling for Dependable Multicore Real-Time Systems: Utilization Bound and Partitioning Algorithm,""J. -J. Han"; Z. Wang; S. Gong; T. Miao;" L. T. Yang"",""School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China; School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China;" School of Computer Science and Technology, Huazhong University of Science and Technology, Wuhan, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2806"",""2819"",""As the computing devices and software executions are susceptible to manifold faults, fault tolerance has been an important research topic in safety-critical real-time systems. Moreover, multicore processors have recently emerged as prevailing computing engines for modern embedded systems. However, there exists rather rare work on the fault-tolerant scheduling of real-time tasks executing on multicores with shared resources, where the task synchronization originated from resource access contention may significantly degrade the schedulability of task system. With the focus on the partitioned-EDF scheduler with the MSRP (Multiprocessor Stack Resource Policy) protocol and primary/backup recovery mechanism, we first investigate a utilization bound and then identify its anomaly where the bound may decrease when more cores are deployed. Next, following the insights gained by the analysis of the bound, we propose a reliability and synchronization aware task partitioning algorithm (RSA-TPA) together with an efficient version to implement the joint management of task synchronization and system reliability, where several resource-oriented heuristics are developed to improve both the schedulability performance and workload balancing. The extensive simulation results show that the RSA-TPA schemes can obtain higher acceptance ratio (e.g., 60 percent more) and generate more balanced partitions, when compared to the existing schemes that consider either reliability management or task synchronization. Finally, with the different fault arrival rates being considered, the actual implementation in Linux kernel further demonstrates the applicability of RSA-TPA that has lower run-time overhead (e.g., 20 percent less) in comparison with other mapping algorithms."",""1558-2183"","""",""10.1109/TPDS.2019.2926455"",""National Natural Science Foundation of China(grant numbers:61872411,61472150)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8753611"",""Multicore processor";real-time systems;shared resources;reliability;primary/backup;"partitioned scheduling"",""Task analysis";Real-time systems;Multicore processing;Fault tolerance;Synchronization;Processor scheduling;"Program processors"","""",""5"","""",""38"",""IEEE"",""2 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;
"Resource-Efficient Index Shard Replication in Large Scale Search Engines,""Y. Li"; X. Tang; W. Cai; J. Tong; X. Liu;" G. Wang"",""Department of Computer Science, Nankai University, Nankai Qu, China"; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Baidu Inc., Beijing, China; Department of Computer Science, Nankai University, Nankai Qu, China;" Department of Computer Science, Nankai University, Nankai Qu, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2820"",""2835"",""With the rapid growth of the Web scale, large scale search engines have to set up a huge number of machines to place the index files of the Web contents. The index files are normally divided into smaller index shards which are often replicated so that queries can be processed in parallel. We observe from real systems that the index shard replication strategy could have a significant impact on the resource usage. In this paper, we investigate the index shard replication problem with the goal of minimizing the resource usage in search engine datacenters. We consider both the offline version and online version of the problem, and formulate the problems as non-linear integer programming problems. We propose several heuristic algorithms to approximate the optimal solution. The proposed algorithms are evaluated by extensive experiments using both synthetic data and real data from commercial search engines. The results demonstrate the effectiveness of the proposed algorithms. Our work also yields many insights about the impact of different input properties on the performance of each algorithm. We believe that this paper will provide valuable guidance to the design of the index shard replication strategy in practice."",""1558-2183"","""",""10.1109/TPDS.2019.2924423"",""National Natural Science Foundation of China(grant numbers:61602266)"; Science and Technology Development Plan of Tianjin(grant numbers:17JCYBJC15300,16JCYBJC41900); Fundamental Research Funds for the Central Universities; Thousand Youth Talents Plan in Tianjin;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8744314"",""Index shard";search engine;replication;resource usage;"heuristic algorithms"",""Indexes";Search engines;Heuristic algorithms;"Approximation algorithms"","""",""5"","""",""41"",""IEEE"",""24 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Response Time Bounds for Typed DAG Parallel Tasks on Heterogeneous Multi-Cores,""M. Han"; N. Guan; J. Sun; Q. He; Q. Deng;" W. Liu"",""Department of Computing, Hong Kong Polytechnic University, Hong Kong, China"; Hong Kong Polytechnic University, Hung Hom, Hong Kong; Northeastern University, Shenyang, China; Hong Kong Polytechnic University, Hung Hom, Hong Kong; Northeastern University, Shenyang, China;" Nanyang Technological University, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2567"",""2581"",""Heterogenerous multi-cores utilize the strength of different architectures for executing particular types of workload, and usually offer higher performance and energy efficiency. In this paper, we study the worst-case response time (WCRT) analysis of typed scheduling of parallel DAG tasks on heterogeneous multi-cores, where the workload of each vertex in the DAG is only allowed to execute on a particular type of cores. The only known WCRT bound for this problem is grossly pessimistic and suffers the non-self-sustainability problem. In this paper, we propose two new WCRT bounds. The first new bound has the same time complexity as the existing bound, but is more precise and solves its non-self-sustainability problem. The second new bound explores more detailed task graph structure information to greatly improve the precision, but is computationally more expensive. We prove that the problem of computing the second bound is strongly NP-hard if the number of types in the system is a variable, and develop an efficient algorithm which has polynomial time complexity if the number of types is a constant. Experiments with randomly generated workload show that our proposed new methods are more precise than the existing bound while having good scalability."",""1558-2183"","""",""10.1109/TPDS.2019.2916696"",""National Key R&D Program of China(grant numbers:2018YFB1702000)"; Research Grants Council of Hong Kong(grant numbers:GRF 15204917,15213818); National Natural Science Foundation of China(grant numbers:61532007,61672140); Ministry of Education Joint Foundation for Equipment Pre-Research(grant numbers:6141A020333); Fundamental Research Funds for the Central Universities(grant numbers:N172304025);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8714030"",""Heterogenerous multi-cores system";embedded real-time scheduling;response time analysis;"DAG parallel task"",""Task analysis";Time factors;Multicore processing;Scheduling algorithms;Time complexity;"Real-time systems"","""",""20"","""",""43"",""IEEE"",""14 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Retroscope: Retrospective Monitoring of Distributed Systems,""A. Charapko"; A. Ailijiang; M. Demirbas;" S. Kulkarni"",""Computer Science and Engineering, University at Buffalo - The State University of New York, Buffalo, NY, USA"; Computer Science and Engineering, University at Buffalo - The State University of New York, Buffalo, NY, USA; Computer Science and Engineering, University at Buffalo - The State University of New York, Buffalo, NY, USA;" Department of Computer Science and Engineering, Michigan State University, East Lansing, MI, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2582"",""2594"",""Retroscope is a comprehensive lightweight distributed monitoring tool that enables users to query and reconstruct past consistent global states of the system. Retroscope achieves this by augmenting the system with Hybrid Logical Clocks (HLC) and by streaming HLC-stamped event logs for storage and processing";" these HLC timestamps are then used for constructing global (or nonlocal) snapshots upon request. Retroscope provides a rich querying language (RQL) to facilitate searching for global predicates across past consistent states. The search is performed by advancing through global states in small incremental steps, greatly reducing the amount of computation needed to construct consistent states. The Retroscope search algorithm is embarrassingly-parallel and can employ many worker processes (each processing up to 150,000 consistent snapshots per second) to handle a single query. We evaluate Retroscope's monitoring capabilities in two case studies: Chord and Apache ZooKeeper."",""1558-2183"","""",""10.1109/TPDS.2019.2911944"",""National Science Foundation(grant numbers:XPS-1533870,XPS-1533802)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8693529"",""Distributed debugging";distributed applications;tracing;"query languages"",""Clocks";Monitoring;Synchronization;Database languages;History;Debugging;"Uncertainty"","""",""4"","""",""44"",""IEEE"",""17 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"RF-RPC: Remote Fetching RPC Paradigm for RDMA-Enabled Network,""Y. Wu"; T. Ma; M. Su; M. Zhang; K. Chen;" Z. Guo"",""Department of Computer Science and Technology, Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Alibaba Inc., Hangzhou, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China;" Ant Financial Inc., HangZhou, China"",""IEEE Transactions on Parallel and Distributed Systems"",""13 Jun 2019"",""2019"",""30"",""7"",""1657"",""1671"",""Remote Direct Memory Access (RDMA) devices are widely deployed in modern data centers. However, existing RDMA usages lead to a dilemma between performance and redesign cost. Server-reply mode directly replaces socket-based send/receive primitives with corresponding RDMA counterparts. It can not fully harness the power of RDMA devices. Server-bypass is distinct mode provided by RDMA for reaching the hardware limits. This mode can totally bypass the server by using one-sided RDMA operations but at the cost of redesigning software. This paper provides a different approach, called RF-RPC (Remote Fetching RPC Paradigm). Different from server-reply, RF-RPC makes client fetch the results from server using one-sided RDMA instead of waiting the results pushed by server. Different from server-bypass, RF-RPC demands server to process client's requests for supporting programming paradigm like RPC. Thus, RF-RPC can achieve high performance without abandoning traditional programming models. An RF-RPC supported in-memory key-value store shows that the performance can be improved by 1.6× comparing to server-reply paradigm and 4× comparing to server-bypass paradigm."",""1558-2183"","""",""10.1109/TPDS.2018.2889718"",""National Key Research & Development Program of China(grant numbers:2016YFB1000504)"; National Natural Science Foundation of China(grant numbers:61433008,61373145,61572280,U1435216,61802219); China Postdoctoral Science Foundation(grant numbers:2018M630162);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588372"",""RDMA";RPC;key-value database;"distributed systems"",""Servers";Programming;Data structures;Instruction sets;Ethernet;Protocols;"Data centers"","""",""8"","""",""46"",""IEEE"",""25 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"SDCon: Integrated Control Platform for Software-Defined Clouds,""J. Son";" R. Buyya"",""Cloud Computing and Distributed Systems (CLOUDS) Laboratory, The University of Melbourne, VIC, Australia";" Cloud Computing and Distributed Systems (CLOUDS) Laboratory, The University of Melbourne, VIC, Australia"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Dec 2018"",""2019"",""30"",""1"",""230"",""244"",""Cloud computing has been empowered with the introduction of Software-Defined Networking (SDN) which enabled dynamic controllability of cloud network infrastructure. Despite the increasing popularity of studies for joint resource optimization in the cloud environment with SDN technology, the realization is still limited for developing integrated management platform providing a simultaneous controllability of computing and networking infrastructures. In this paper, we propose SDCon, a practical platform developed on OpenStack and OpenDaylight to provide integrated manageability for both resources in cloud infrastructure. The platform can perform VM placement and migration, network flow scheduling and bandwidth allocation, real-time monitoring of computing and networking resources, and measuring power usage of the infrastructure with a single platform. We also propose a network topology aware VM placement algorithm for heterogeneous resource configuration (TOPO-Het) that consolidates the connected VMs into closely connected compute nodes to reduce the overall network traffic. The proposed algorithm is evaluated on SDCon and compared with the results from the state-of-the-art baselines. Results of the empirical evaluation with Wikipedia application show that the proposed algorithm can effectively improve the response time while reducing the total network traffic. It also shows the effectiveness of SDCon to manage both resources simultaneously."",""1558-2183"","""",""10.1109/TPDS.2018.2855119"",""ARC Discovery Project";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8409965"",""Cloud computing";software-defined networking;resource management;software-defined clouds;IaaS;OpenStack;"OpenDaylight"",""Cloud computing";Data centers;Network topology;Controllability;Centralized control;"Software"","""",""21"","""",""31"",""IEEE"",""11 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;
"SEALDB: An Efficient LSM-tree Based KV Store on SMR Drives with Sets and Dynamic Bands,""T. Yao"; Z. Tan; J. Wan; P. Huang; Y. Zhang; C. Xie;" X. He"",""Key Laboratory of Information Storage System, Ministry of Education of China, Wuhan, China"; Key Laboratory of Information Storage System, Ministry of Education of China, Wuhan, China; Key Laboratory of Information Storage System, Ministry of Education of China, Wuhan, China; Department of Computer and Information Sciences, Temple University, PA, USA; Key Laboratory of Information Storage System, Ministry of Education of China, Wuhan, China; Key Laboratory of Information Storage System, Ministry of Education of China, Wuhan, China;" Department of Computer and Information Sciences, Temple University, PA, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2595"",""2607"",""Key-value (KV) stores play an increasingly critical role in supporting diverse large-scale applications in modern data centers hosting terabytes of KV items which even might reside on a single server due to virtualization purposes. The combination of the ever-growing volume of KV items and storage/application consolidation is driving a trend of high storage density for KV stores. Shingled Magnetic Recording (SMR) represents a promising technology for increasing disk capacity, which however comes with the increased complexity of handling random writes. To take the best advantages of SMR drives, applications are expected to work in an SMR-friendly way. In this work, we present SEALDB, a Log-Structured Merge tree (LSM-tree) based key-value store that is specifically optimized for SMR drives via avoiding random writes and the corresponding write amplification on SMR drives. First, for LSM-trees, SEALDB collects and groups participating data of each compaction into sets. Using a set as the basic unit for compactions, SEALDB improves compaction efficiency by reducing random I/Os. Second, SEALDB creates variable sized bands on original HM-SMR drives, named dynamic bands. Dynamic bands store sets in an SMR-friendly way to eliminate the auxiliary write amplification from SMR drives. Third, SEALDB employs two light-weight garbage collection (GC) policies to further improve the space efficiency. We demonstrate the advantages of SEALDB via extensive experiments with various workloads. Overall, SEALDB delivers impressive performance compared with LevelDB, e.g., 3.42×/2.65× faster for random writes (without or with GCs), and 3.96× faster for sequential reads."",""1558-2183"","""",""10.1109/TPDS.2019.2918219"",""National Natural Science Foundation of China(grant numbers:61472152)"; NSFC(grant numbers:61821003); National Natural Science Foundation of China(grant numbers:61300047,61572209,61432007); National Science Foundation(grant numbers:CCF-1717660,CNS-1702474);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721159"",""LSM-tree";SMR;key-value store;set;"dynamic band"",""Compaction";Drives;Computer architecture;Magnetic recording;Software;Data centers;"Databases"","""",""10"","""",""48"",""IEEE"",""23 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;
"SimEDC: A Simulator for the Reliability Analysis of Erasure-Coded Data Centers,""M. Zhang"; S. Han;" P. P. C. Lee"",""Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, Hong Kong"; Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, Hong Kong;" Department of Computer Science and Engineering, Chinese University of Hong Kong, Shatin, Hong Kong"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2836"",""2848"",""Modern data centers employ erasure coding to protect data storage against failures. Given the hierarchical nature of data centers, characterizing the effects of erasure coding and redundancy placement on the reliability of erasure-coded data centers is critical yet unexplored. This paper presents a discrete-event simulator called SimEDC, which enables us to conduct a comprehensive simulation analysis of reliability on erasure-coded data centers. SimEDC reports reliability metrics of an erasure-coded data center based on the configurable inputs of the data center topology, erasure codes, redundancy placement, and failure/repair patterns of different subsystems obtained from statistical models or production traces. It can further accelerate the simulation analysis via importance sampling. Our simulation analysis based on SimEDC shows that placing erasure-coded data in fewer racks generally improves reliability by reducing cross-rack repair traffic, even though it sacrifices rack-level fault tolerance in the face of correlated failures."",""1558-2183"","""",""10.1109/TPDS.2019.2921551"",""Research Grants Council of Hong Kong(grant numbers:GRF 14216316,CRF C7036-15G)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8733052"",""Erasure coding";data centers;"reliability"",""Maintenance engineering";Data centers;Redundancy;Encoding;Data models;"Bandwidth"","""",""5"","""",""38"",""IEEE"",""10 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;;;;
"Slow Links, Fast Links, and the Cost of Gossip,""S. Sourav"; P. Robinson;" S. Gilbert"",""National University of Singapore, Singapore"; McMaster University, Hamilton, ON, Canada;" National University of Singapore, Singapore"",""IEEE Transactions on Parallel and Distributed Systems"",""7 Aug 2019"",""2019"",""30"",""9"",""2130"",""2147"",""Consider the classical problem of information dissemination: one (or more) nodes in a network have some information that they want to distribute to the remainder of the network. In this paper, we study the cost of information dissemination in networks where edges have latencies, i.e., sending a message from one node to another takes some amount of time. We first generalize the idea of conductance to weighted graphs by defining φ* to be the “critical weighted conductance” and ℓ* to be the “critical latency”. One goal of this paper is to argue that φ* characterizes the connectivity of a weighted graph with latencies in much the same way that conductance characterizes the connectivity of unweighted graphs. We give near tight lower and upper bounds on the problem of information dissemination, up to polylogarithmic factors. Specifically, we show that in a graph with (weighted) diameter D (with latencies as weights) and maximum degree Δ, any information dissemination algorithm requires at least Ω(min(D+Δ,ℓ*/φ*)) time in the worst case. We show several variants of the lower bound (e.g., for graphs with small diameter, graphs with small max-degree, etc.) by reduction to a simple combinatorial game. We then give nearly matching algorithms, showing that information dissemination can be solved in O(min((D+Δ)log3n,(ℓ*/φ*)log n) time. This is achieved by combining two cases. We show that the classical push-pull algorithm is (near) optimal when the diameter or the maximum degree is large. For the case where the diameter and the maximum degree are small, we give an alternative strategy in which we first discover the latencies and then use an algorithm for known latencies based on a weighted spanner construction. (Our algorithms are within polylogarithmic factors of being tight both for known and unknown latencies.) While it is easiest to express our bounds in terms of φ* and ℓ*, in some cases they do not provide the most convenient definition of conductance in weighted graphs. Therefore we give a second (nearly) equivalent characterization, namely the average weighted conductance φavg."",""1558-2183"","""",""10.1109/TPDS.2019.2905568"",""Singapore MOE ARC(grant numbers:MOE2018-T2-1-160)"; Natural Sciences and Engineering Research Council of Canada;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8668473"",""information dissemination";gossip model;edge latencies;conductance;weighted graph;"push-pull"",""Upper bound";Games;Image edge detection;Delays;Indexes;Delay effects;"Hardware"","""",""1"","""",""30"",""IEEE"",""17 Mar 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Smart Contract-Based Negotiation for Adaptive QoS-Aware Service Composition,""P. Wang"; J. Meng; J. Chen; T. Liu; Y. Zhan; W. -T. Tsai;" Z. Jin"",""School of Information, Renmin University of China, and Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education, Beijing, China"; School of Information, Renmin University of China, and Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education, Beijing, China; School of Information, Renmin University of China, and Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education, Beijing, China; School of Information, Renmin University of China, and Key Laboratory of Data Engineering and Knowledge Engineering of Ministry of Education, Beijing, China; School of Humanities and Law, Guizhou University of Finance and Economics, Guiyang, Guizhou, China; School of Computer Science and Engineering, Beihang University, Beijing, China;" Key Lab. of High-Confidence Software Technologies, Peking University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1403"",""1420"",""Smart contracts (SCs) run on the distributed ledger technology (DLT) platform and can implement agreements between participants without a trusted third party. This paper uses the DLT and SC techniques to build distributed applications composed of existing services. In practice, there are many functionally-equivalent services on the Internet. To beat their competitors, the service providers usually offer flexible QoS and use dynamic pricing strategies. Moreover, the service providers can change at runtime, e.g., they may encounter problems so that their QoS drops suddenly. This makes achieving the optimization goal at runtime (e.g., the maximization of the utility) more difficult. To address this problem, first, this paper proposes an SC-based negotiation framework. The SCs can ensure that the transactions are automatically and reliably performed as agreed upon between the service requesters and providers. The DLTs can provide the reliable data of the requests and responses of the service requesters and providers to the SCs. In addition, the SCs can identify the troubled service providers, and find other service providers to replace them at runtime. Second, this paper proposes a Bayesian Nash equilibrium (BNE) of the service providers. In the BNE, the cost-efficient service providers offer the high QoS the service requester asks for and report their costs truthfully. This BNE enables the selection of the cost-efficient service providers and the achievement of the (near) maximization of the service requesters' utility. This paper implements the proposed negotiation framework on a DLT platform called Hyperledger Fabric. The experiment results demonstrate that the proposed approach outperforms the existing approaches and can adapt to the changes of the service providers."",""1558-2183"","""",""10.1109/TPDS.2018.2885746"",""Ministry of Science and Technology of China"; National Key Research and Development Program(grant numbers:2016YFB1000700); National Natural Science Foundation of China(grant numbers:61003084,61232007,61472428,61620106007);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8568003"",""Service composition";negotiation;distributed ledger technology;smart contract;"Bayesian nash Equilibrium"",""Quality of service";Fabrics;Runtime;Reliability;"Bayes methods"","""",""18"","""",""43"",""IEEE"",""7 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;
"Solving Linear Diophantine Systems on Parallel Architectures,""D. Zaitsev"; S. Tomov;" J. Dongarra"",""Department of Computer Engineering and Innovative Technology, International Humanitarian University, Fontanskaya Doroga 33, Odessa, Ukraine"; Innovative Computing Laboratory, The University of Tennessee, Knoxville, TN, USA;" Innovative Computing Laboratory, The University of Tennessee, Knoxville, TN, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1158"",""1169"",""Solving linear Diophantine systems of equations is applied in discrete-event systems, model checking, formal languages and automata, logic programming, cryptography, networking, signal processing, and chemistry. For modeling discrete systems with Petri nets, a solution in non-negative integer numbers is required, which represents an intractable problem. For this reason, solving such kinds of tasks with significant speedup is highly appreciated. In this paper we design a new solver of linear Diophantine systems based on the parallel-sequential composition of the system clans. The solver is studied and implemented to run on parallel architectures using a two-level parallelization concept based on MPI and OpenMP. A decomposable system is usually represented by a sparse matrix";" a minimal clan size of the decomposition restricts the granulation of the technique. MPI is applied for solving systems for clans using a parallel-sequential composition on distributed-memory computing nodes, while OpenMP is applied in solving a single indecomposable system on a single node using multiple cores. A dynamic task-dispatching subsystem is developed for distributing systems on nodes in the process of compositional solution. Computational speedups are obtained on a series of test examples, e.g., illustrating that the best value constitutes up to 45 times speedup obtained on 5 nodes with 20 cores each."",""1558-2183"","""",""10.1109/TPDS.2018.2873354"",""Fulbright Association(grant numbers:Fulbright Schollarship,Dmitry Zaitsev,Sep-Nov,2017)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8482295"",""MPI";OpenMP;linear diophantine system;Petri net;clan;"speed-up"",""Petri nets";Matrix decomposition;Mathematical model;Task analysis;Sparse matrices;Software algorithms;"Parallel architectures"","""",""10"","""",""29"",""IEEE"",""5 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Spatiotemporal Graph and Hypergraph Partitioning Models for Sparse Matrix-Vector Multiplication on Many-Core Architectures,""N. Abubaker"; K. Akbudak;" C. Aykanat"",""Department of Computer Engineering, Bilkent University, Ankara, Turkey"; Department of Applied Mathematics and Computational Science, KSA, Thuwal, Saudi Arabia;" Department of Computer Engineering, Bilkent University, Ankara, Turkey"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""445"",""458"",""There exist graph/hypergraph partitioning-based row/column reordering methods for encoding either spatial or temporal locality for sparse matrix-vector multiplication (SpMV) operations. Spatial and temporal hypergraph models in these methods are extended to encapsulate both spatial and temporal localities based on cut/uncut net categorization obtained from vertex partitioning. These extensions of spatial and temporal hypergraph models encode the spatial locality primarily and the temporal locality secondarily, and vice-versa, respectively. However, the literature lacks models that simultaneously encode both spatial and temporal localities utilizing only vertex partitioning for further improving the performance of SpMV on shared-memory architectures. In order to fill this gap, we propose a novel spatiotemporal hypergraph model that leads to a one-phase spatiotemporal reordering method which encodes both types of locality simultaneously. We also propose a framework for spatiotemporal methods which encodes both types of locality in two dependent phases and two separate phases. The validity of the proposed spatiotemporal models and methods are tested on a wide range of sparse matrices and the experiments are performed on both a 60-core Intel Xeon Phi processor and a Xeon processor. Results show the validity of the methods via almost doubling the Gflop/s performance through enhancing data locality in parallel SpMV operations."",""1558-2183"","""",""10.1109/TPDS.2018.2864729"",""The Scientific and Technological Research Council of Turkey TUBITAK(grant numbers:EEEAG-115E212)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8432126"",""Sparse matrix";sparse matrix-vector multiplication;data locality;spatial locality;temporal locality;hypergraph model;bipartite graph model;graph model;hypergraph partitioning;graph partitioning;Intel many integrated core architecture;"Intel Xeon Phi"",""Spatiotemporal phenomena";Sparse matrices;Data models;Task analysis;Bipartite graph;Encoding;"Taxonomy"","""",""6"","""",""32"",""IEEE"",""10 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"T-Gaming: A Cost-Efficient Cloud Gaming System at Scale,""H. Chen"; X. Zhang; Y. Xu; J. Ren; J. Fan; Z. Ma;" W. Zhang"",""Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China"; School of Electronic Science and Engineering, Nanjing University, Nanjing, China; Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China; School of Information Science and Engineering, Central South University, Changsha, China; Department of Automation, Tsinghua University, Beijing, China; School of Electronic Science and Engineering, Nanjing University, Nanjing, China;" Cooperative Medianet Innovation Center, Shanghai Jiao Tong University, Shanghai, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2849"",""2865"",""Cloud gaming (CG) system could pursue both high-quality gaming experience via intensive computing, and ultimate convenience anywhere at anytime through any energy-constrained mobile devices. Despite the abundance of efforts devoted, state-of-the-art CG systems still suffer from multiple key limitations: expensive deployment cost, high bandwidth consumption and unsatisfied quality of experience (QoE). As a result, existing works are not widely adopted in reality. This paper proposes a Transparent Gaming framework called T-Gaming that allows users to play any popular high-end desktop/console games on-the-fly over the Internet. T-Gaming utilizes the off-the-shelf consumer GPUs without resorting to the expensive proprietary GPU virtualization (vGPU) technology to reduce the deployment cost. Moreover, it enables prioritized video encoding based on the human visual feature to reduce the bandwidth consumption without noticeable visual quality degradation. Last but not least, T-Gaming adopts adaptive real-time streaming based on deep reinforcement learning (RL) to improve user's QoE. To evaluate the performance of T-Gaming, we implement and test a prototype system in the real world. Compared with the existing cloud gaming systems, T-Gaming not only reduces the expense per user by 75 percent hardware cost reduction and 14.3 percent network cost reduction, but also improves the normalized average QoE by 3.6-27.9 percent."",""1558-2183"","""",""10.1109/TPDS.2019.2922205"",""National Natural Science Foundation of China(grant numbers:61650101,61571215,61702562,61671265,61902178)"; Natural Science Foundation of Jiangsu Province(grant numbers:BK20190295); Science and Technology Commission of Shanghai Municipality(grant numbers:18511105400); Shanghai Key Lab of Digital Media Processing and Transmission;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8735806"",""Cloud gaming";cost-efficient;HVS-based compression;"adaptive real-time streaming"",""Cloud gaming";Graphics processing units;Streaming media;Quality of experience;Virtualization;"Encoding"","""",""23"","""",""56"",""IEEE"",""12 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Task-Based Augmented Contour Trees with Fibonacci Heaps,""C. Gueunet"; P. Fortin; J. Jomier;" J. Tierny"",""Sorbonne Université, CNRS, LIP6, Paris, France"; Université de Lille, CNRS, Centrale Lille, CRIStAL, Lille, France; Kitware SAS, Villeurbanne, France;" Sorbonne Université, CNRS, LIP6, Paris, France"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jul 2019"",""2019"",""30"",""8"",""1889"",""1905"",""This paper presents a new algorithm for the fast, shared memory, multi-core computation of augmented contour trees on triangulations. In contrast to most existing parallel algorithms our technique computes augmented trees, enabling the full extent of contour tree based applications including data segmentation. Our approach completely revisits the traditional, sequential contour tree algorithm to re-formulate all the steps of the computation as a set of independent local tasks. This includes a new computation procedure based on Fibonacci heaps for the join and split trees, two intermediate data structures used to compute the contour tree, whose constructions are efficiently carried out concurrently thanks to the dynamic scheduling of task parallelism. We also introduce a new parallel algorithm for the combination of these two trees into the output global contour tree. Overall, this results in superior time performance in practice, both in sequential and in parallel thanks to the OpenMP task runtime. We report performance numbers that compare our approach to reference sequential and multi-threaded implementations for the computation of augmented merge and contour trees. These experiments demonstrate the run-time efficiency of our approach and its scalability on common workstations. We demonstrate the utility of our approach in data segmentation applications."",""1558-2183"","""",""10.1109/TPDS.2019.2898436"",""BPI(grant numbers:P112017-2661376/DOS0021427)"; ANRT; Kitware SAS CIFRE(grant numbers:#2015/1039);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8637719"",""Scientific visualization";topological data analysis;task parallelism;"multi-core architecture"",""Task analysis";Data visualization;Data analysis;Runtime;Parallel algorithms;"Data structures"","""",""21"","""",""69"",""IEEE"",""8 Feb 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"The Case for Phase-Based Transactional Memory,""J. P. L. de Carvalho"; G. Araujo;" A. Baldassin"",""UNICAMP – Institute of Computing, Campinas, Brazil"; UNICAMP – Institute of Computing, Campinas, Brazil;" UNESP – Univ Estadual Paulista, São Paulo, Brazil"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""459"",""472"",""In recent years, Hybrid TM (HyTM) has been proposed as a transactional memory approach that leverages on the advantages of both hardware (HTM) and software (STM) execution modes. HyTM assumes that concurrent transactions can have very different phases and thus should run under different execution modes. Although HyTM has shown to improve performance, the overall solution can be complicated to manage, both in terms of correctness and performance. On the other hand, Phased Transactional Memory (PhTM) considers that concurrent transactions have similar phases, and thus all transactions could run under the same mode. As a result, PhTM does not require coordination between transactions on distinct modes making its implementation simpler and more flexible. In this article we make the case for phase-based transactional systems using PhTM*, the first implementation of PhTM on modern HTM-ready processors. PhTM* novelty relies on avoiding unnecessary transitions to software mode by: (i) taking into account the categories of hardware aborts";" (ii) adding a new serialization mode. Experimental results using Broadwell's TSX reveal that, for the STAMP benchmark suite, PhTM* performs on average 1.68x better than PhTM, a previous phase-based TM, 2.08x better than HyTM-NOrec, a state-of-the-art HyTM, and 2.28x better than HyCO, the most recent hybrid system in the literature. In addition, PhTM* also showed to be effective when running on a Power8 machine by performing over 1.18x, 1.36x and 1.81x better than PhTM, HyTM-NOrec and HyCO, respectively. We also show that STAMP applications do not exhibit hybrid behavior to justify the use of conventional hybrid systems, thus making PhTM* a better solution to those type of programs. Finally, we show for the first time that conventional hybrid systems do not perform better than phased-based system in a scenario with hybrid-behaved transactions."",""1558-2183"","""",""10.1109/TPDS.2018.2861712"",""Fundação de Amparo à Pesquisa do Estado de São Paulo(grant numbers:2016/15337-9)"; Center for Computational Engineering and Sciences (CCES); CNPq(grant numbers:446160/2014-8);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8423653"",""Transactional memory";phase-based execution;"performance evaluation"",""Hardware";Program processors;Synchronization;Benchmark testing;Runtime;"Performance evaluation"","""",""6"","""",""35"",""IEEE"",""31 Jul 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;;;
"Towards Efficient Multi-Channel Data Broadcast for Multimedia Streams,""X. Gao"; A. Song; L. Hao; J. Zou; G. Chen;" S. Tang"",""Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Minhang, China"; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Minhang, China; Department of Computer Science, Columbia University, New York, NY, USA; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Minhang, China; Shanghai Key Laboratory of Scalable Computing and Systems, Shanghai Jiao Tong University, Minhang, China;" University of Texas at Dallas, Richardson, TX, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""10 Sep 2019"",""2019"",""30"",""10"",""2370"",""2383"",""Multi-channel data broadcast attracts increasingly focus in recent years. For the complex and large multimedia data like images, audios, and videos etc, multi-channel data broadcast is a promising approach to mitigate various limitations of data dissemination in mobile environment, such as narrow bandwidth, unreliable connections, and battery limitation. However, existing data broadcast schemes are inefficient for employing multiple channels. In this paper, we present five novel multimedia data broadcast schemes (SDAA, ISDAA, LDAA, AEA, and COA) specifically designed for wireless multichannel communications. The key idea behind those schemes is to utilize SVC (Scalable Video Coding) to generate data segments with different qualities and implement indexing and channel assignment to minimize the expected waiting time for clients. In terms of waiting time minimization, we prove theoretically that SDAA is a 4/3-approximation algorithm and ISDAA is a 6/5-approximation algorithm for global allocation, and AEA can achieve locally optimal solution. We show that integrating SDAA and AEA form a best schedule for practical applications. We provide numerical experiments by using a SVC dataset and a YouTube dataset to evaluate the system performance and prove the efficiency of our schemes. We also propose a multiple video broadcast framework and analyze its performance."",""1558-2183"","""",""10.1109/TPDS.2019.2908904"",""National Key R&D Program of China(grant numbers:2018YFB1004703)"; National Natural Science Foundation of China(grant numbers:61872238,61672353); Shanghai Science and Technology Fund(grant numbers:17510740200); Huawei Innovation Research Program(grant numbers:HO2018085286); State Key Laboratory of Air Traffic Management System and Technology(grant numbers:SKLATM20180X); Tencent Social Ads Rhino-Bird Focused Research Porgram;" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8680668"",""Multimedia data broadcast";data scheduling;"scalable video coding"",""Streaming media";Multimedia databases;Videos;Multimedia communication;Resource management;Wireless communication;"Broadcasting"","""",""4"","""",""49"",""IEEE"",""2 Apr 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Towards Fast and Lightweight Checkpointing for Mobile Virtualization Using NVRAM,""K. Zhong"; D. Liu; Y. Wu; L. Long; W. Liu; J. Ren; R. Liu; L. Liang; Z. Shao;" T. Li"",""Key Laboratory of Dependable Service Computing in Cyber Physical Society, Chongqing University, Chongqing, China"; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Chongqing University, Chongqing, China; College of Computer Science and Technology, Chongqing University of Posts and Telecommunications, Chongqing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Chongqing University, Chongqing, China; Key Laboratory of Dependable Service Computing in Cyber Physical Society, Chongqing University, Chongqing, China; College of Communication Engineering, Chongqing University, Chongqing, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong;" Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL, USA"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1421"",""1433"",""Checkpointing is a key enabler of hibernation, live migration and fault-tolerance for virtual machines (VMs) in mobile devices. However, checkpointing a VM is usually heavyweight: the VM's entire memory needs to be dumped to storage, which induces a significant amount of (slow) I/O operations, degrading system performance and user experience. In this paper, we propose FLIC, a fast and lightweight checkpointing machinery for virtualized mobile devices by taking advantages of recent byte-addressable, non-volatile memory (NVRAM). Instead of saving the VM's entire memory to storage, we store its working set pages in NVRAM, avoiding accessing slow flash memory (compared to server-grade SSDs). To further reduce the write activities to flash memory, we propose an energy-efficient data deduplication to eliminate redundant data in VM snapshot and save storage space. Experimental results based on an Exynos 5250 SoC show that our approach can effectively improve the performance of checkpointing in mobile virutalization and save energy."",""1558-2183"","""",""10.1109/TPDS.2018.2886906"",""National Natural Science Foundation of China(grant numbers:61672116,61601067)"; Chongqing High-Tech Research Program(grant numbers:cstc2016jcyjA0332); Fundamental Research Funds; Central Universities(grant numbers:0214005207005); Science and Technology Research Program of Chongqing Municipal Education Commission(grant numbers:KJ1704085); Chongqing Research Program of Basic Research and Frontier Technology(grant numbers:cstc2017jcyjAX0164); Research Grants Council of the Hong Kong Special Administrative Region(grant numbers:GRF 15213814,GRF 15222315); Direct Grant for Research; The Chinese University of Hong Kong(grant numbers:4055096);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8576677"",""Mobile virutalization";checkpointing;"non-volatile memory"",""Random access memory";Nonvolatile memory;Checkpointing;Virtualization;Mobile handsets;Performance evaluation;"Phase change materials"","""","""","""",""49"",""IEEE"",""14 Dec 2018"","""","""",""IEEE"",""IEEE Journals"""
"Towards Low-Latency Batched Stream Processing by Pre-Scheduling,""H. Jin"; F. Chen; S. Wu; Y. Yao; Z. Liu; L. Gu;" Y. Zhou"",""Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China"; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China; Services Computing Technology and System Lab, Cluster and Grid Computing Lab, Huazhong University of Science and Technology, Wuhan, Hubei, China;" Department of Computer Science, University of Copenhagen, Copenhagen, Denmark"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Feb 2019"",""2019"",""30"",""3"",""710"",""722"",""Many stream processing frameworks have been developed to meet the requirements of real-time processing. Among them, batched stream processing frameworks are widely advocated with the consideration of their fault-tolerance, high throughput and unified runtime with batch processing. In batched stream processing frameworks, straggler, happened due to the uneven task execution time, has been regarded as a major hurdle of latency-sensitive applications. Existing straggler mitigation techniques, operating in either reactive or proactive manner, are all post-scheduling methods, and therefore inevitably result in high resource overhead or long job completion time. We notice that batched stream processing jobs are usually recurring with predictable characteristics. By exploring such a heuristic, we present a pre-scheduling straggler mitigation framework called Lever. Lever first identifies potential stragglers and evaluates nodes’ capacity by analyzing execution information of historical jobs. Then, Lever carefully pre-schedules job input data to each node before task scheduling so as to mitigate potential stragglers. We implement Lever and contribute it as an extension of Apache Spark Streaming. Our experimental results show that Lever can reduce job completion time by 30.72 to 42.19 percent over Spark Streaming, a widely adopted batched stream processing system and outperforms traditional techniques significantly."",""1558-2183"","""",""10.1109/TPDS.2018.2866581"",""National Key Research and Development Program of China(grant numbers:2018YFB1004805)"; Pre-research Project of Beifang(grant numbers:FFZ-1601);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444732"",""Stream processing";recurring jobs;straggler;scheduling;"data assignment"",""Task analysis";Batch production systems;Sparks;Cloning;Data models;Fault tolerance;"Fault tolerant systems"","""",""11"","""",""35"",""IEEE"",""23 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Ultra-Fast Bloom Filters using SIMD Techniques,""J. Lu"; Y. Wan; Y. Li; C. Zhang; H. Dai; Y. Wang; G. Zhang;" B. Liu"",""Tsinghua University, Beijing, China"; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Department of Computer Science and Technology, Tsinghua University, Beijing, China; Huawei Future Network Theory Lab, Hong Kong, China; Huawei Future Network Theory Lab, Hong Kong, China;" Department of Computer Science and Technology, Tsinghua University, Beijing, China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Mar 2019"",""2019"",""30"",""4"",""953"",""964"",""The network link speed is growing at an ever-increasing rate, which requires all network functions on routers/switches to keep pace. Bloom filter is a widely-used membership check data structure in networking applications. Correspondingly, it also faces the urgent demand of improving the performance in membership check speed. To this end, this paper proposes a new Bloom filter variant called Ultra-Fast Bloom Filters (UFBF), by leveraging the Single Instruction Multiple Data (SIMD) techniques. We make three improvements for UFBF to accelerate the membership check speed. First, we develop a novel hash computation algorithm which can compute multiple hash functions in parallel with the use of SIMD instructions. Second, we elaborate a Bloom filter's bit-test process from sequential to parallel, enabling more bit-tests per unit time. Third, we improve the cache efficiency of membership check by encoding an element's information to a small block so that it can fit into a cache-line. We further generalize UFBF, called c-UFBF, to make UFBF supporting large number of hash functions. Both theoretical analysis and extensive evaluations show that the UFBF greatly outperforms the state-of-the-art Bloom filter variants on membership check speed."",""1558-2183"","""",""10.1109/TPDS.2018.2869889"",""Huawei Innovation Research Program"; NSFC(grant numbers:61373143,61432009,61872213);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8462781"",""Bloom filter";SIMD;"parallel techniques"",""Information filters";Encoding;Parallel processing;Standards;"Table lookup"","""",""9"","""",""29"",""IEEE"",""12 Sep 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;
"Using Differential Execution Analysis to Identify Thread Interference,""M. S. M. Bouksiaa"; F. Trahay; A. Lescouet; G. Voron; R. Dulong; A. Guermouche; É. Brunet;" G. Thomas"",""Telecom SudParis, Évry, France"; Telecom SudParis, Évry, France; Telecom SudParis, Évry, France; Telecom SudParis, Évry, France; Telecom SudParis, Évry, France; Telecom SudParis, Évry, France; Telecom SudParis, Évry, France;" Telecom SudParis, Évry, France"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2866"",""2878"",""Understanding the performance of a multi-threaded application is difficult. The threads interfere when they access the same shared resource, which slows down their execution. Unfortunately, current profiling tools report the hardware components or the synchronization primitives that saturate, but they cannot tell if the saturation is the cause of a performance bottleneck. In this paper, we propose a holistic metric able to pinpoint the blocks of code that suffer interference the most, regardless of the interference cause. Our metric uses performance variation as a universal indicator of interference problems. With an evaluation of 27 applications we show that our metric can identify interference problems caused by six different kinds of interference in nine applications. We are able to easily remove seven of the bottlenecks, which leads to a performance improvement of up to nine times."",""1558-2183"","""",""10.1109/TPDS.2019.2927481"","""",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8758342"",""Performance analysis";multithreading;"bottleneck detection"",""Real-time systems";Energy storage;Renewable energy sources;Power system stability;Supply and demand;"Generators"","""",""4"","""",""56"",""IEEE"",""9 Jul 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;
"Using High-Bandwidth Networks Efficiently for Fast Graph Computation,""Y. Cheng"; H. Jiang; F. Wang; Y. Hua; D. Feng; W. Guo;" Y. Wu"",""College of Mathematics and Computer Science, FuZhou University, Fuzhou, Fujian, China"; Department of Computer Science & Engineering, University of Texas at Arlington, Arlington, TX, USA; Wuhan National Lab for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Lab for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; Wuhan National Lab for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China; College of Mathematics and Computer Science, FuZhou University, Fuzhou, Fujian, China;" Wuhan National Lab for Optoelectronics, Huazhong University of Science and Technology, Wuhan, Hubei, China"",""IEEE Transactions on Parallel and Distributed Systems"",""11 Apr 2019"",""2019"",""30"",""5"",""1170"",""1183"",""Nowadays, high-bandwidth networks are more easily accessible than ever before. However, existing distributed graph-processing frameworks, such as GPS, fail to efficiently utilize the additional bandwidth capacity in these networks for higher performance, due to their inefficient computation and communication models, leading to very long waiting times experienced by users for the graph-computing results. The root cause lies in the fact that the computation and communication models of these frameworks generate, send and receive messages so slowly that only a small fraction of the available network bandwidth is utilized. In this paper, we propose a high-performance distributed graph-processing framework, called BlitzG, to address this problem. This framework fully exploits the available network bandwidth capacity for fast graph processing. Our approach aims at significant reduction in (i) the computation workload of each vertex for fast message generation by using a new slimmed-down vertex-centric computation model and (ii) the average message overhead for fast message delivery by designing a light-weight message-centric communication model. Evaluation on a 40Gbps Ethernet, driven by real-world graph datasets, shows that BlitzG outperforms GPS by up to 27x with an average of 20.7x."",""1558-2183"","""",""10.1109/TPDS.2018.2875084"",""National Natural Science Foundation of China(grant numbers:61772216,61772216,61772212,61832020,U1705262)"; National Key R&D Program of China(grant numbers:2018YFB10033005); Shenzhen Technology Scheme(grant numbers:JCYJ20170307172248636); Technology Innovation Platform Project of Fujian Province(grant numbers:2014H2005); Fujian Collaborative Innovation Center; Fujian Engineering Research Center of Big Data Analysis and Processing; Hubei Province Technical Innovation Special Project(grant numbers:2017AAA129);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8486673"",""Graph computation";high-bandwidth networks;high performance;computation model;"communication model"",""Computational modeling";Message systems;Kernel;Bandwidth;Computer science;Hardware;"Scalability"","""",""45"","""",""45"",""IEEE"",""9 Oct 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;
"Vertex Coloring with Communication Constraints in Synchronous Broadcast Networks,""H. Lakhlef"; M. Raynal;" F. Taïani"",""Université Technologique de Compiègne (UTC), Compiègne, France"; Polytechnic University of Hong Kong, Hung Hom, Hong Kong;" CNRS, Inria, IRISA, University of Rennes, Rennes, France"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Jun 2019"",""2019"",""30"",""7"",""1672"",""1686"",""This paper considers distributed vertex-coloring in broadcast/receive networks suffering from conflicts and collisions. (A collision occurs when, during the same round, messages are sent to the same process by too many neighbors";" a conflict occurs when a process and one of its neighbors broadcast during the same round.) More specifically, the paper focuses on multi-channel networks, in which a process may either broadcast a message to its neighbors or receive a message from at most γ of them. The paper first provides a new upper bound on the corresponding graph coloring problem (known as frugal coloring) in general graphs, proposes an exact bound for the problem in trees, and presents a deterministic, parallel, color-optimal, collision- and conflict-free distributed coloring algorithm for trees, and proves its correctness."",""1558-2183"","""",""10.1109/TPDS.2018.2889688"",""French ANR project DESCARTES"; Franco-Hong Kong ANR-RGC Joint Research Programme(grant numbers:12-IS02-004-02 CO2Dim);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8588369"",""Distributed algorithms";message-passing;broadcast/receive communication;synchronous systems;multi-coloring;vertex coloring;"TDMA slot allocation"",""Color";Time division multiple access;Wireless networks;Schedules;Upper bound;Clocks;"Indexes"","""",""11"","""",""27"",""IEEE"",""25 Dec 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;
"Visibility Rendering Order: Improving Energy Efficiency on Mobile GPUs through Frame Coherence,""E. de Lucas"; P. Marcuello; J. -M. Parcerisa;" A. González"",""Department of Computer Architecture, Polytechnic University of Catalonia (UPC), Barcelona, Spain"; Department of Computer Architecture, Polytechnic University of Catalonia (UPC), Barcelona, Spain; Department of Computer Architecture, Polytechnic University of Catalonia (UPC), Barcelona, Spain;" Department of Computer Architecture, Polytechnic University of Catalonia (UPC), Barcelona, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""15 Jan 2019"",""2019"",""30"",""2"",""473"",""485"",""During real-time graphics rendering, objects are processed by the GPU in the order they are submitted by the CPU, and occluded surfaces are often processed even though they will end up not being part of the final image, thus wasting precious time and energy. To help discard occluded surfaces, most current GPUs include an Early-Depth test before the fragment processing stage. However, to be effective it requires that opaque objects are processed in a front-to-back order. Depth sorting and other occlusion culling techniques at the object level incur overheads that are only offset for applications having substantial depth and/or fragment shading complexity, which is often not the case in mobile workloads. We propose a novel architectural technique for GPUs, Visibility Rendering Order (VRO), which reorders objects front-to-back entirely in hardware by exploiting the fact that the objects in graphics animated applications tend to keep its relative depth order across consecutive frames (temporal coherence). Since order relationships are already tested by the Depth Test, VRO incurs minimal energy overheads because it just requires adding a small hardware to capture that information and use it later to guide the rendering of the following frame. Moreover, unlike other approaches, this unit works in parallel with the graphics pipeline without any performance overhead. We illustrate the benefits of VRO using various unmodified commercial 3D applications for which VRO achieves 27 percent speed-up and 15.8 percent energy reduction on average over a state-of-the-art mobile GPU."",""1558-2183"","""",""10.1109/TPDS.2018.2866246"",""Spanish State Research Agency(grant numbers:TIN2013-44375-R,TIN2016-75344-R (AEI/FEDER, EU),BES-2014-068225)";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8440753"",""GPU";graphics pipeline;energy-efficiency;rasterization;rendering;fragment processing;pixel shading;occlusion culling;visibility;tile based deferred rendering;tile based rendering;"topological order"",""Graphics processing units";Rendering (computer graphics);Pipelines;Geometry;Hardware;Image color analysis;"Color"","""",""11"","""",""42"",""IEEE"",""19 Aug 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;
"Way Combination for an Adaptive and Scalable Coherence Directory,""R. Titos-Gil"; A. Flores; R. Fernández-Pascual; A. Ros; S. Petit; J. Sahuquillo;" M. E. Acacio"",""Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain"; Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain; Department Informática de Sistemas y Computadores, Universitat Politècnica de València, València, Spain; Department Informática de Sistemas y Computadores, Universitat Politècnica de València, València, Spain;" Department Ingeniería y Tecnología de Computadores, Universidad de Murcia, Murcia, Spain"",""IEEE Transactions on Parallel and Distributed Systems"",""9 Oct 2019"",""2019"",""30"",""11"",""2608"",""2623"",""This manuscript opens the way to a new class of coherence directory structures that are based on the brand-new concept of way combining. A Way-Combining Directory (WC-dir) builds on a typical sparse directory but allows to take advantage of several ways in the same set to codify the sharing information of each memory block. The result is a sparse directory with variable effective associativity per set and variable length entries, thus being able to dynamically adapt the directory structure to the particular requirements of each application. In particular, our proposal uses just enough bits per entry to store a single pointer, which is optimal for the common case of having just one sharer. For those addresses that have more than one sharer, we have observed that in the majority of cases extra bits could be taken from other empty ways in the same set. All in all, our proposal minimizes the storage overheads without losing the flexibility to adapt to several sharing degrees and without the complexities of other previously proposed techniques. Detailed simulations of a 128-core multicore architecture running benchmarks from PARSEC-3.0 and SPLASH-3 demonstrate that WC-dir can closely approach the performance of a non-scalable bit vector sparse directory, beating the state-of-the-art Scalable Coherence Directory (SCD) and Pool directory proposals."",""1558-2183"","""",""10.1109/TPDS.2019.2917185"",""Spanish MCIU and AEI"; European Commission(grant numbers:RTI2018-098156-B-C53);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8716572"",""Cache coherence";sparse directory;way combining;scalability;coverage;bit vector;limited pointers;execution time;"network traffic"",""Proposals";Coherence;Multicore processing;Complexity theory;Memory management;"Scalability"","""",""1"","""",""42"",""IEEE"",""16 May 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;
"Wide-Area Spark Streaming: Automated Routing and Batch Sizing,""W. Li"; D. Niu; Y. Liu; S. Liu;" B. Li"",""School of Computer Science and Technology, Dalian University of Technology, Dalian, Liaoning, China"; Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada; Department of Electrical and Computer Engineering, University of Toronto, ON, Canada; Department of Electrical and Computer Engineering, University of Toronto, ON, Canada;" Department of Electrical and Computer Engineering, University of Toronto, ON, Canada"",""IEEE Transactions on Parallel and Distributed Systems"",""14 May 2019"",""2019"",""30"",""6"",""1434"",""1448"",""Modern stream processing frameworks, such as Spark Streaming, are designed to support a wide variety of stream processing applications, such as real-time data analytics in social networks. As the volume of data to be processed increases rapidly, there is a pressing need for processing them across multiple geo-distributed datacenters. However, these frameworks are not designed to take limited and varying inter-datacenter bandwidth into account, leading to longer query latencies. In this paper, we present the design and implementation of an extended Spark Streaming framework to automatically and optimally schedule tasks, select data flow routes and determine micro-batch sizes across geo-distributed datacenters in wide-area networks. To make these decisions, we propose a sparsity-regularized ADMM algorithm to efficiently solve a nonconvex optimization problem, based on readily measurable operating traces. Toward incremental real-world deployment, we take a non-intrusive approach to support flexible routing of micro-batches by adding a new DStream transformation we have developed to the existing Spark Streaming framework. As a result, our implementation can enforce scheduling decisions by modifying application workflows only. We have deployed our implementation on Amazon EC2 with emulated bandwidth constraints, and our experimental results on various types of queries have demonstrated the effectiveness of our proposed framework, as compared to the existing Spark Streaming scheduler and other data-locality-based heuristics."",""1558-2183"","""",""10.1109/TPDS.2018.2880189"",""NSERC Discovery Research Programs";" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8528861"",""Wide-area networks";spark streaming;"routing and batch sizing"",""Sparks";Routing;Task analysis;Bandwidth;Silicon;Optimization;"Wide area networks"","""",""11"","""",""37"",""IEEE"",""9 Nov 2018"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;;;;;;;
"Workload-Adaptive Configuration Tuning for Hierarchical Cloud Schedulers,""R. Han"; C. H. Liu; Z. Zong; L. Y. Chen; W. Liu; S. Wang;" J. Zhan"",""Beijing Institute of Technology, Beijing Shi, P.R. China"; Beijing Institute of Technology, Beijing Shi, P.R. China; Tsinghua University, Beijing Shi, P.R. China; TU Delft, Delft, the Netherlands; Beijing Institute of Technology, Beijing Shi, P.R. China; Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P.R. China;" Institute of Computing Technology, Chinese Academy of Sciences, Beijing, P.R. China"",""IEEE Transactions on Parallel and Distributed Systems"",""12 Nov 2019"",""2019"",""30"",""12"",""2879"",""2895"",""Cluster schedulers provide flexible resource sharing mechanism for best-effort cloud jobs, which occupy a majority in modern datacenters. Properly tuning a scheduler's configurations is the key to these jobs' performance because it decides how to allocate resources among them. Today's cloud scheduling systems usually rely on cluster operators to set the configuration and thus overlook the potential performance improvement through optimally configuring the scheduler according to the heterogeneous and dynamic cloud workloads. In this paper, we introduce AdaptiveConfig, a run-time configurator for cluster schedulers that automatically adapts to the changing workload and resource status in two steps. First, a comparison approach estimates jobs' performances under different configurations and diverse scheduling scenarios. The key idea here is to transform a scheduler's resource allocation mechanism and their variable influence factors (configurations, scheduling constraints, available resources, and workload status) into business rules and facts in a rule engine, thereby reasoning about these correlated factors in job performance comparison. Second, a workload-adaptive optimizer transforms the cluster-level searching of huge configuration space into an equivalent dynamic programming problem that can be efficiently solved at scale. We implement AdaptiveConfig on the popular YARN Capacity and Fair schedulers and demonstrate its effectiveness using real-world Facebook and Google workloads, i.e., successfully finding best configurations for most of scheduling scenarios and considerably reducing latencies by a factor of two with low optimization time."",""1558-2183"","""",""10.1109/TPDS.2019.2923197"",""National Key Research and Development Plan of China(grant numbers:2018YFB1003701,2018YFB1003700)"; National Natural Science Foundation of China(grant numbers:61872337);" "",""https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8741093"",""Cloud datacenter";cluster scheduler;configuration;job latency;"YARN"",""Resource management";Scheduling;Cloud computing;Transforms;Cloud computing;"Task analysis"","""",""16"","""",""68"",""IEEE"",""19 Jun 2019"","""","""",""IEEE"",""IEEE Journals""";;;;;;;;;;;;